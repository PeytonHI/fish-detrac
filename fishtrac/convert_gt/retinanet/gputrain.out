2018-12-20 10:17:25.737530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-20 10:17:25.738299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:0c:00.0
totalMemory: 7.92GiB freeMemory: 7.83GiB
2018-12-20 10:17:25.738328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0
2018-12-20 10:17:25.739267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-20 10:17:25.739289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 
2018-12-20 10:17:25.739302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N 
2018-12-20 10:17:25.739538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7616 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:0c:00.0, compute capability: 6.1)
Using TensorFlow backend.
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. 
  warnings.warn(str(msg))
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 285278993 bytes but only got 0. Skipping tag 290
  " Skipping tag %s" % (size, len(data), tag))
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 274 bytes but only got 0. Skipping tag 1
  " Skipping tag %s" % (size, len(data), tag))
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 513 bytes but only got 0. Skipping tag 1
  " Skipping tag %s" % (size, len(data), tag))
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 2. Skipping tag 41988
  " Skipping tag %s" % (size, len(data), tag))
WARNING:tensorflow:From /home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Creating model, this may take a second...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0                                            
__________________________________________________________________________________________________
padding_conv1 (ZeroPadding2D)   (None, None, None, 3 0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9408        padding_conv1[0][0]              
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             
__________________________________________________________________________________________________
res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              
__________________________________________________________________________________________________
padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             
__________________________________________________________________________________________________
res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              
__________________________________________________________________________________________________
res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              
                                                                 bn2a_branch1[0][0]               
__________________________________________________________________________________________________
res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             
__________________________________________________________________________________________________
res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              
__________________________________________________________________________________________________
padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             
__________________________________________________________________________________________________
res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             
__________________________________________________________________________________________________
res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              
                                                                 res2a_relu[0][0]                 
__________________________________________________________________________________________________
res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             
__________________________________________________________________________________________________
res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              
__________________________________________________________________________________________________
padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             
__________________________________________________________________________________________________
res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             
__________________________________________________________________________________________________
res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              
                                                                 res2b_relu[0][0]                 
__________________________________________________________________________________________________
res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             
__________________________________________________________________________________________________
res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              
__________________________________________________________________________________________________
padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             
__________________________________________________________________________________________________
res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              
__________________________________________________________________________________________________
res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              
                                                                 bn3a_branch1[0][0]               
__________________________________________________________________________________________________
res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             
__________________________________________________________________________________________________
res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              
__________________________________________________________________________________________________
padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             
__________________________________________________________________________________________________
res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             
__________________________________________________________________________________________________
res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              
                                                                 res3a_relu[0][0]                 
__________________________________________________________________________________________________
res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             
__________________________________________________________________________________________________
res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              
__________________________________________________________________________________________________
padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             
__________________________________________________________________________________________________
res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             
__________________________________________________________________________________________________
res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              
                                                                 res3b_relu[0][0]                 
__________________________________________________________________________________________________
res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             
__________________________________________________________________________________________________
res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              
__________________________________________________________________________________________________
padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             
__________________________________________________________________________________________________
res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             
__________________________________________________________________________________________________
res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              
                                                                 res3c_relu[0][0]                 
__________________________________________________________________________________________________
res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             
__________________________________________________________________________________________________
res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              
__________________________________________________________________________________________________
padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             
__________________________________________________________________________________________________
res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              
__________________________________________________________________________________________________
res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              
                                                                 bn4a_branch1[0][0]               
__________________________________________________________________________________________________
res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             
__________________________________________________________________________________________________
res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              
__________________________________________________________________________________________________
padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             
__________________________________________________________________________________________________
res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             
__________________________________________________________________________________________________
res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              
                                                                 res4a_relu[0][0]                 
__________________________________________________________________________________________________
res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             
__________________________________________________________________________________________________
res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              
__________________________________________________________________________________________________
padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             
__________________________________________________________________________________________________
res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             
__________________________________________________________________________________________________
res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              
                                                                 res4b_relu[0][0]                 
__________________________________________________________________________________________________
res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             
__________________________________________________________________________________________________
res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              
__________________________________________________________________________________________________
padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             
__________________________________________________________________________________________________
res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             
__________________________________________________________________________________________________
res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              
                                                                 res4c_relu[0][0]                 
__________________________________________________________________________________________________
res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             
__________________________________________________________________________________________________
res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              
__________________________________________________________________________________________________
padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             
__________________________________________________________________________________________________
res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             
__________________________________________________________________________________________________
res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              
                                                                 res4d_relu[0][0]                 
__________________________________________________________________________________________________
res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             
__________________________________________________________________________________________________
res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              
__________________________________________________________________________________________________
padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             
__________________________________________________________________________________________________
res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             
__________________________________________________________________________________________________
res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              
                                                                 res4e_relu[0][0]                 
__________________________________________________________________________________________________
res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             
__________________________________________________________________________________________________
res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              
__________________________________________________________________________________________________
padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             
__________________________________________________________________________________________________
res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              
__________________________________________________________________________________________________
res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              
                                                                 bn5a_branch1[0][0]               
__________________________________________________________________________________________________
res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             
__________________________________________________________________________________________________
res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              
__________________________________________________________________________________________________
padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             
__________________________________________________________________________________________________
res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             
__________________________________________________________________________________________________
res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              
                                                                 res5a_relu[0][0]                 
__________________________________________________________________________________________________
res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             
__________________________________________________________________________________________________
res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              
__________________________________________________________________________________________________
padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             
__________________________________________________________________________________________________
res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              
                                                                 res5b_relu[0][0]                 
__________________________________________________________________________________________________
res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 524544      res5c_relu[0][0]                 
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 
                                                                 res4f_relu[0][0]                 
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               
                                                                 C4_reduced[0][0]                 
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  
                                                                 res3d_relu[0][0]                 
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               
                                                                 C3_reduced[0][0]                 
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 1)      2381065     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        
                                                                 regression_submodel[2][0]        
                                                                 regression_submodel[3][0]        
                                                                 regression_submodel[4][0]        
                                                                 regression_submodel[5][0]        
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 1)      0           classification_submodel[1][0]    
                                                                 classification_submodel[2][0]    
                                                                 classification_submodel[3][0]    
                                                                 classification_submodel[4][0]    
                                                                 classification_submodel[5][0]    
==================================================================================================
Total params: 36,382,957
Trainable params: 36,276,717
Non-trainable params: 106,240
__________________________________________________________________________________________________
None
Epoch 1/10

   1/1500 [..............................] - ETA: 6:38:11 - loss: 3.6698 - regression_loss: 2.5377 - classification_loss: 1.1321
   2/1500 [..............................] - ETA: 3:42:00 - loss: 3.9901 - regression_loss: 2.8593 - classification_loss: 1.1308
   3/1500 [..............................] - ETA: 2:31:11 - loss: 4.0215 - regression_loss: 2.8918 - classification_loss: 1.1297
   4/1500 [..............................] - ETA: 2:05:57 - loss: 4.0041 - regression_loss: 2.8746 - classification_loss: 1.1295
   5/1500 [..............................] - ETA: 1:49:58 - loss: 3.9688 - regression_loss: 2.8389 - classification_loss: 1.1299
   6/1500 [..............................] - ETA: 1:33:12 - loss: 3.9424 - regression_loss: 2.8123 - classification_loss: 1.1300
   7/1500 [..............................] - ETA: 1:21:15 - loss: 3.9104 - regression_loss: 2.7802 - classification_loss: 1.1302
   8/1500 [..............................] - ETA: 1:17:03 - loss: 3.8992 - regression_loss: 2.7694 - classification_loss: 1.1298
   9/1500 [..............................] - ETA: 1:13:57 - loss: 3.8973 - regression_loss: 2.7673 - classification_loss: 1.1300
  10/1500 [..............................] - ETA: 1:11:30 - loss: 3.8886 - regression_loss: 2.7586 - classification_loss: 1.1300
  11/1500 [..............................] - ETA: 1:08:11 - loss: 3.8926 - regression_loss: 2.7623 - classification_loss: 1.1303
  12/1500 [..............................] - ETA: 1:06:05 - loss: 3.8835 - regression_loss: 2.7530 - classification_loss: 1.1304
  13/1500 [..............................] - ETA: 1:01:43 - loss: 3.8903 - regression_loss: 2.7601 - classification_loss: 1.1302
  14/1500 [..............................] - ETA: 59:53 - loss: 3.9188 - regression_loss: 2.7885 - classification_loss: 1.1304  
  15/1500 [..............................] - ETA: 58:38 - loss: 3.9319 - regression_loss: 2.8016 - classification_loss: 1.1303
  16/1500 [..............................] - ETA: 55:34 - loss: 3.9168 - regression_loss: 2.7863 - classification_loss: 1.1305
  17/1500 [..............................] - ETA: 55:05 - loss: 3.9299 - regression_loss: 2.7995 - classification_loss: 1.1304
  18/1500 [..............................] - ETA: 52:32 - loss: 3.9203 - regression_loss: 2.7899 - classification_loss: 1.1304
  19/1500 [..............................] - ETA: 51:55 - loss: 3.9031 - regression_loss: 2.7727 - classification_loss: 1.1304
  20/1500 [..............................] - ETA: 49:46 - loss: 3.9038 - regression_loss: 2.7736 - classification_loss: 1.1302
  21/1500 [..............................] - ETA: 47:50 - loss: 3.9062 - regression_loss: 2.7762 - classification_loss: 1.1299
  22/1500 [..............................] - ETA: 47:47 - loss: 3.9255 - regression_loss: 2.7957 - classification_loss: 1.1298
  23/1500 [..............................] - ETA: 46:08 - loss: 3.9314 - regression_loss: 2.8016 - classification_loss: 1.1298
  24/1500 [..............................] - ETA: 45:44 - loss: 3.9211 - regression_loss: 2.7913 - classification_loss: 1.1298
  25/1500 [..............................] - ETA: 45:35 - loss: 3.9170 - regression_loss: 2.7870 - classification_loss: 1.1300
  26/1500 [..............................] - ETA: 44:10 - loss: 3.9054 - regression_loss: 2.7753 - classification_loss: 1.1301
  27/1500 [..............................] - ETA: 42:51 - loss: 3.9152 - regression_loss: 2.7851 - classification_loss: 1.1301
  28/1500 [..............................] - ETA: 41:41 - loss: 3.9114 - regression_loss: 2.7814 - classification_loss: 1.1301
  29/1500 [..............................] - ETA: 40:33 - loss: 3.9184 - regression_loss: 2.7883 - classification_loss: 1.1301
  30/1500 [..............................] - ETA: 39:38 - loss: 3.9263 - regression_loss: 2.7962 - classification_loss: 1.1301
  31/1500 [..............................] - ETA: 39:15 - loss: 3.9310 - regression_loss: 2.8008 - classification_loss: 1.1301
  32/1500 [..............................] - ETA: 38:15 - loss: 3.9307 - regression_loss: 2.8007 - classification_loss: 1.1300
  33/1500 [..............................] - ETA: 37:22 - loss: 3.9309 - regression_loss: 2.8010 - classification_loss: 1.1300
  34/1500 [..............................] - ETA: 36:41 - loss: 3.9241 - regression_loss: 2.7941 - classification_loss: 1.1300
  35/1500 [..............................] - ETA: 35:50 - loss: 3.9319 - regression_loss: 2.8020 - classification_loss: 1.1299
  36/1500 [..............................] - ETA: 35:05 - loss: 3.9299 - regression_loss: 2.8002 - classification_loss: 1.1298
  37/1500 [..............................] - ETA: 35:30 - loss: 3.9330 - regression_loss: 2.8034 - classification_loss: 1.1296
  38/1500 [..............................] - ETA: 35:37 - loss: 3.9285 - regression_loss: 2.7989 - classification_loss: 1.1296
  39/1500 [..............................] - ETA: 34:56 - loss: 3.9290 - regression_loss: 2.7995 - classification_loss: 1.1295
  40/1500 [..............................] - ETA: 35:15 - loss: 3.9309 - regression_loss: 2.8015 - classification_loss: 1.1293
  41/1500 [..............................] - ETA: 35:31 - loss: 3.9270 - regression_loss: 2.7978 - classification_loss: 1.1293
  42/1500 [..............................] - ETA: 34:53 - loss: 3.9240 - regression_loss: 2.7948 - classification_loss: 1.1292
  43/1500 [..............................] - ETA: 34:17 - loss: 3.9256 - regression_loss: 2.7966 - classification_loss: 1.1290
  44/1500 [..............................] - ETA: 33:50 - loss: 3.9380 - regression_loss: 2.8090 - classification_loss: 1.1289
  45/1500 [..............................] - ETA: 33:49 - loss: 3.9387 - regression_loss: 2.8100 - classification_loss: 1.1287
  46/1500 [..............................] - ETA: 33:23 - loss: 3.9407 - regression_loss: 2.8121 - classification_loss: 1.1286
  47/1500 [..............................] - ETA: 32:52 - loss: 3.9406 - regression_loss: 2.8122 - classification_loss: 1.1285
  48/1500 [..............................] - ETA: 33:06 - loss: 3.9406 - regression_loss: 2.8123 - classification_loss: 1.1282
  49/1500 [..............................] - ETA: 32:36 - loss: 3.9409 - regression_loss: 2.8129 - classification_loss: 1.1280
  50/1500 [>.............................] - ETA: 32:08 - loss: 3.9488 - regression_loss: 2.8208 - classification_loss: 1.1280
  51/1500 [>.............................] - ETA: 31:40 - loss: 3.9428 - regression_loss: 2.8149 - classification_loss: 1.1279
  52/1500 [>.............................] - ETA: 31:13 - loss: 3.9414 - regression_loss: 2.8136 - classification_loss: 1.1277
  53/1500 [>.............................] - ETA: 31:23 - loss: 3.9402 - regression_loss: 2.8126 - classification_loss: 1.1276
  54/1500 [>.............................] - ETA: 31:15 - loss: 3.9411 - regression_loss: 2.8136 - classification_loss: 1.1275
  55/1500 [>.............................] - ETA: 30:48 - loss: 3.9419 - regression_loss: 2.8145 - classification_loss: 1.1274
  56/1500 [>.............................] - ETA: 30:40 - loss: 3.9412 - regression_loss: 2.8140 - classification_loss: 1.1272
  57/1500 [>.............................] - ETA: 30:50 - loss: 3.9473 - regression_loss: 2.8201 - classification_loss: 1.1271
  58/1500 [>.............................] - ETA: 31:00 - loss: 3.9468 - regression_loss: 2.8198 - classification_loss: 1.1270
  59/1500 [>.............................] - ETA: 31:01 - loss: 3.9458 - regression_loss: 2.8190 - classification_loss: 1.1268
  60/1500 [>.............................] - ETA: 30:36 - loss: 3.9442 - regression_loss: 2.8175 - classification_loss: 1.1267
  61/1500 [>.............................] - ETA: 30:36 - loss: 3.9466 - regression_loss: 2.8200 - classification_loss: 1.1265
  62/1500 [>.............................] - ETA: 30:14 - loss: 3.9504 - regression_loss: 2.8243 - classification_loss: 1.1261
  63/1500 [>.............................] - ETA: 29:53 - loss: 3.9544 - regression_loss: 2.8284 - classification_loss: 1.1260
  64/1500 [>.............................] - ETA: 29:36 - loss: 3.9564 - regression_loss: 2.8306 - classification_loss: 1.1258
  65/1500 [>.............................] - ETA: 29:38 - loss: 3.9533 - regression_loss: 2.8276 - classification_loss: 1.1258
  66/1500 [>.............................] - ETA: 29:56 - loss: 3.9515 - regression_loss: 2.8259 - classification_loss: 1.1256
  67/1500 [>.............................] - ETA: 29:43 - loss: 3.9509 - regression_loss: 2.8255 - classification_loss: 1.1254
  68/1500 [>.............................] - ETA: 29:36 - loss: 3.9484 - regression_loss: 2.8232 - classification_loss: 1.1252
  69/1500 [>.............................] - ETA: 29:17 - loss: 3.9464 - regression_loss: 2.8213 - classification_loss: 1.1251
  70/1500 [>.............................] - ETA: 29:16 - loss: 3.9447 - regression_loss: 2.8199 - classification_loss: 1.1248
  71/1500 [>.............................] - ETA: 28:59 - loss: 3.9460 - regression_loss: 2.8215 - classification_loss: 1.1245
  72/1500 [>.............................] - ETA: 28:41 - loss: 3.9437 - regression_loss: 2.8197 - classification_loss: 1.1240
  73/1500 [>.............................] - ETA: 28:54 - loss: 3.9417 - regression_loss: 2.8177 - classification_loss: 1.1239
  74/1500 [>.............................] - ETA: 28:37 - loss: 3.9384 - regression_loss: 2.8147 - classification_loss: 1.1237
  75/1500 [>.............................] - ETA: 28:43 - loss: 3.9362 - regression_loss: 2.8127 - classification_loss: 1.1235
  76/1500 [>.............................] - ETA: 28:27 - loss: 3.9375 - regression_loss: 2.8141 - classification_loss: 1.1233
  77/1500 [>.............................] - ETA: 28:28 - loss: 3.9348 - regression_loss: 2.8115 - classification_loss: 1.1232
  78/1500 [>.............................] - ETA: 28:39 - loss: 3.9344 - regression_loss: 2.8113 - classification_loss: 1.1231
  79/1500 [>.............................] - ETA: 28:40 - loss: 3.9304 - regression_loss: 2.8076 - classification_loss: 1.1228
  80/1500 [>.............................] - ETA: 28:25 - loss: 3.9294 - regression_loss: 2.8069 - classification_loss: 1.1225
  81/1500 [>.............................] - ETA: 28:12 - loss: 3.9311 - regression_loss: 2.8088 - classification_loss: 1.1223
  82/1500 [>.............................] - ETA: 28:06 - loss: 3.9292 - regression_loss: 2.8075 - classification_loss: 1.1217
  83/1500 [>.............................] - ETA: 27:57 - loss: 3.9266 - regression_loss: 2.8051 - classification_loss: 1.1215
  84/1500 [>.............................] - ETA: 28:02 - loss: 3.9239 - regression_loss: 2.8026 - classification_loss: 1.1214
  85/1500 [>.............................] - ETA: 27:48 - loss: 3.9267 - regression_loss: 2.8057 - classification_loss: 1.1210
  86/1500 [>.............................] - ETA: 27:35 - loss: 3.9260 - regression_loss: 2.8056 - classification_loss: 1.1203
  87/1500 [>.............................] - ETA: 27:38 - loss: 3.9244 - regression_loss: 2.8044 - classification_loss: 1.1201
  88/1500 [>.............................] - ETA: 27:47 - loss: 3.9234 - regression_loss: 2.8036 - classification_loss: 1.1197
  89/1500 [>.............................] - ETA: 27:34 - loss: 3.9255 - regression_loss: 2.8061 - classification_loss: 1.1194
  90/1500 [>.............................] - ETA: 27:20 - loss: 3.9249 - regression_loss: 2.8062 - classification_loss: 1.1187
  91/1500 [>.............................] - ETA: 27:25 - loss: 3.9257 - regression_loss: 2.8072 - classification_loss: 1.1185
  92/1500 [>.............................] - ETA: 27:11 - loss: 3.9233 - regression_loss: 2.8053 - classification_loss: 1.1180
  93/1500 [>.............................] - ETA: 26:59 - loss: 3.9240 - regression_loss: 2.8065 - classification_loss: 1.1175
  94/1500 [>.............................] - ETA: 26:47 - loss: 3.9246 - regression_loss: 2.8078 - classification_loss: 1.1168
  95/1500 [>.............................] - ETA: 26:40 - loss: 3.9233 - regression_loss: 2.8074 - classification_loss: 1.1159
  96/1500 [>.............................] - ETA: 26:48 - loss: 3.9219 - regression_loss: 2.8066 - classification_loss: 1.1153
  97/1500 [>.............................] - ETA: 26:35 - loss: 3.9202 - regression_loss: 2.8058 - classification_loss: 1.1144
  98/1500 [>.............................] - ETA: 26:43 - loss: 3.9195 - regression_loss: 2.8058 - classification_loss: 1.1136
  99/1500 [>.............................] - ETA: 26:44 - loss: 3.9187 - regression_loss: 2.8061 - classification_loss: 1.1126
 100/1500 [=>............................] - ETA: 26:46 - loss: 3.9191 - regression_loss: 2.8073 - classification_loss: 1.1118
 101/1500 [=>............................] - ETA: 26:35 - loss: 3.9153 - regression_loss: 2.8042 - classification_loss: 1.1112
 102/1500 [=>............................] - ETA: 26:24 - loss: 3.9146 - regression_loss: 2.8038 - classification_loss: 1.1108
 103/1500 [=>............................] - ETA: 26:33 - loss: 3.9122 - regression_loss: 2.8029 - classification_loss: 1.1094
 104/1500 [=>............................] - ETA: 26:41 - loss: 3.9088 - regression_loss: 2.8003 - classification_loss: 1.1085
 105/1500 [=>............................] - ETA: 26:30 - loss: 3.9105 - regression_loss: 2.8028 - classification_loss: 1.1077
 106/1500 [=>............................] - ETA: 26:40 - loss: 3.9122 - regression_loss: 2.8054 - classification_loss: 1.1068
 107/1500 [=>............................] - ETA: 26:34 - loss: 3.9107 - regression_loss: 2.8053 - classification_loss: 1.1054
 108/1500 [=>............................] - ETA: 26:42 - loss: 3.9090 - regression_loss: 2.8052 - classification_loss: 1.1038
 109/1500 [=>............................] - ETA: 26:35 - loss: 3.9071 - regression_loss: 2.8049 - classification_loss: 1.1023
 110/1500 [=>............................] - ETA: 26:53 - loss: 3.9047 - regression_loss: 2.8033 - classification_loss: 1.1013
 111/1500 [=>............................] - ETA: 27:03 - loss: 3.9113 - regression_loss: 2.8103 - classification_loss: 1.1010
 112/1500 [=>............................] - ETA: 26:55 - loss: 3.9098 - regression_loss: 2.8098 - classification_loss: 1.0999
 113/1500 [=>............................] - ETA: 26:44 - loss: 3.9063 - regression_loss: 2.8080 - classification_loss: 1.0982
 114/1500 [=>............................] - ETA: 26:47 - loss: 3.9121 - regression_loss: 2.8145 - classification_loss: 1.0975
 115/1500 [=>............................] - ETA: 26:41 - loss: 3.9083 - regression_loss: 2.8128 - classification_loss: 1.0956
 116/1500 [=>............................] - ETA: 26:40 - loss: 3.9070 - regression_loss: 2.8138 - classification_loss: 1.0933
 117/1500 [=>............................] - ETA: 26:30 - loss: 3.9019 - regression_loss: 2.8117 - classification_loss: 1.0902
 118/1500 [=>............................] - ETA: 26:21 - loss: 3.8984 - regression_loss: 2.8099 - classification_loss: 1.0885
 119/1500 [=>............................] - ETA: 26:24 - loss: 3.8947 - regression_loss: 2.8098 - classification_loss: 1.0848
 120/1500 [=>............................] - ETA: 26:14 - loss: 3.8983 - regression_loss: 2.8095 - classification_loss: 1.0888
 121/1500 [=>............................] - ETA: 26:04 - loss: 3.8938 - regression_loss: 2.8086 - classification_loss: 1.0852
 122/1500 [=>............................] - ETA: 25:55 - loss: 3.8920 - regression_loss: 2.8098 - classification_loss: 1.0822
 123/1500 [=>............................] - ETA: 26:08 - loss: 3.8882 - regression_loss: 2.8089 - classification_loss: 1.0794
 124/1500 [=>............................] - ETA: 26:18 - loss: 3.8852 - regression_loss: 2.8089 - classification_loss: 1.0764
 125/1500 [=>............................] - ETA: 26:08 - loss: 3.8826 - regression_loss: 2.8085 - classification_loss: 1.0742
 126/1500 [=>............................] - ETA: 25:59 - loss: 3.8839 - regression_loss: 2.8077 - classification_loss: 1.0762
 127/1500 [=>............................] - ETA: 25:51 - loss: 3.8807 - regression_loss: 2.8075 - classification_loss: 1.0732
 128/1500 [=>............................] - ETA: 25:42 - loss: 3.8730 - regression_loss: 2.8043 - classification_loss: 1.0688
 129/1500 [=>............................] - ETA: 25:50 - loss: 3.8688 - regression_loss: 2.8029 - classification_loss: 1.0658
 130/1500 [=>............................] - ETA: 25:41 - loss: 3.8655 - regression_loss: 2.8024 - classification_loss: 1.0631
 131/1500 [=>............................] - ETA: 25:39 - loss: 3.8600 - regression_loss: 2.8005 - classification_loss: 1.0595
 132/1500 [=>............................] - ETA: 25:33 - loss: 3.8571 - regression_loss: 2.8003 - classification_loss: 1.0568
 133/1500 [=>............................] - ETA: 25:24 - loss: 3.8548 - regression_loss: 2.8019 - classification_loss: 1.0529
 134/1500 [=>............................] - ETA: 25:24 - loss: 3.8519 - regression_loss: 2.8013 - classification_loss: 1.0506
 135/1500 [=>............................] - ETA: 25:29 - loss: 3.8494 - regression_loss: 2.8024 - classification_loss: 1.0470
 136/1500 [=>............................] - ETA: 25:21 - loss: 3.8443 - regression_loss: 2.8013 - classification_loss: 1.0430
 137/1500 [=>............................] - ETA: 25:15 - loss: 3.8446 - regression_loss: 2.8001 - classification_loss: 1.0446
 138/1500 [=>............................] - ETA: 25:08 - loss: 3.8730 - regression_loss: 2.8090 - classification_loss: 1.0640
 139/1500 [=>............................] - ETA: 25:02 - loss: 3.8708 - regression_loss: 2.8097 - classification_loss: 1.0611
 140/1500 [=>............................] - ETA: 25:00 - loss: 3.8739 - regression_loss: 2.8120 - classification_loss: 1.0619
 141/1500 [=>............................] - ETA: 24:58 - loss: 3.8733 - regression_loss: 2.8140 - classification_loss: 1.0593
 142/1500 [=>............................] - ETA: 24:58 - loss: 3.8656 - regression_loss: 2.8110 - classification_loss: 1.0546
 143/1500 [=>............................] - ETA: 24:51 - loss: 3.8578 - regression_loss: 2.8085 - classification_loss: 1.0493
 144/1500 [=>............................] - ETA: 24:43 - loss: 3.8543 - regression_loss: 2.8089 - classification_loss: 1.0454
 145/1500 [=>............................] - ETA: 24:42 - loss: 3.8508 - regression_loss: 2.8093 - classification_loss: 1.0414
 146/1500 [=>............................] - ETA: 24:49 - loss: 3.8486 - regression_loss: 2.8094 - classification_loss: 1.0392
 147/1500 [=>............................] - ETA: 24:49 - loss: 3.8401 - regression_loss: 2.8061 - classification_loss: 1.0340
 148/1500 [=>............................] - ETA: 24:41 - loss: 3.8364 - regression_loss: 2.8054 - classification_loss: 1.0310
 149/1500 [=>............................] - ETA: 24:36 - loss: 3.8358 - regression_loss: 2.8082 - classification_loss: 1.0276
 150/1500 [==>...........................] - ETA: 24:41 - loss: 3.8351 - regression_loss: 2.8093 - classification_loss: 1.0258
 151/1500 [==>...........................] - ETA: 24:38 - loss: 3.8323 - regression_loss: 2.8071 - classification_loss: 1.0252
 152/1500 [==>...........................] - ETA: 24:39 - loss: 3.8297 - regression_loss: 2.8068 - classification_loss: 1.0229
 153/1500 [==>...........................] - ETA: 24:48 - loss: 3.8318 - regression_loss: 2.8097 - classification_loss: 1.0221
 154/1500 [==>...........................] - ETA: 24:41 - loss: 3.8291 - regression_loss: 2.8091 - classification_loss: 1.0199
 155/1500 [==>...........................] - ETA: 24:41 - loss: 3.8231 - regression_loss: 2.8066 - classification_loss: 1.0166
 156/1500 [==>...........................] - ETA: 24:34 - loss: 3.8187 - regression_loss: 2.8060 - classification_loss: 1.0127
 157/1500 [==>...........................] - ETA: 24:27 - loss: 3.8131 - regression_loss: 2.8041 - classification_loss: 1.0090
 158/1500 [==>...........................] - ETA: 24:27 - loss: 3.8104 - regression_loss: 2.8042 - classification_loss: 1.0062
 159/1500 [==>...........................] - ETA: 24:31 - loss: 3.8112 - regression_loss: 2.8061 - classification_loss: 1.0051
 160/1500 [==>...........................] - ETA: 24:24 - loss: 3.8067 - regression_loss: 2.8057 - classification_loss: 1.0009
 161/1500 [==>...........................] - ETA: 24:31 - loss: 3.8018 - regression_loss: 2.8033 - classification_loss: 0.9984
 162/1500 [==>...........................] - ETA: 24:24 - loss: 3.7969 - regression_loss: 2.8017 - classification_loss: 0.9952
 163/1500 [==>...........................] - ETA: 24:17 - loss: 3.7958 - regression_loss: 2.8009 - classification_loss: 0.9948
 164/1500 [==>...........................] - ETA: 24:20 - loss: 3.7955 - regression_loss: 2.8020 - classification_loss: 0.9935
 165/1500 [==>...........................] - ETA: 24:14 - loss: 3.7943 - regression_loss: 2.8028 - classification_loss: 0.9915
 166/1500 [==>...........................] - ETA: 24:09 - loss: 3.7893 - regression_loss: 2.8011 - classification_loss: 0.9882
 167/1500 [==>...........................] - ETA: 24:03 - loss: 3.7874 - regression_loss: 2.8022 - classification_loss: 0.9852
 168/1500 [==>...........................] - ETA: 23:56 - loss: 3.7868 - regression_loss: 2.8040 - classification_loss: 0.9828
 169/1500 [==>...........................] - ETA: 23:50 - loss: 3.7845 - regression_loss: 2.8040 - classification_loss: 0.9805
 170/1500 [==>...........................] - ETA: 23:45 - loss: 3.7833 - regression_loss: 2.8041 - classification_loss: 0.9792
 171/1500 [==>...........................] - ETA: 23:38 - loss: 3.7818 - regression_loss: 2.8047 - classification_loss: 0.9770
 172/1500 [==>...........................] - ETA: 23:46 - loss: 3.7781 - regression_loss: 2.8039 - classification_loss: 0.9742
 173/1500 [==>...........................] - ETA: 23:48 - loss: 3.7760 - regression_loss: 2.8037 - classification_loss: 0.9723
 174/1500 [==>...........................] - ETA: 23:42 - loss: 3.7714 - regression_loss: 2.8022 - classification_loss: 0.9693
 175/1500 [==>...........................] - ETA: 23:40 - loss: 3.7715 - regression_loss: 2.8035 - classification_loss: 0.9681
 176/1500 [==>...........................] - ETA: 23:51 - loss: 3.7714 - regression_loss: 2.8042 - classification_loss: 0.9672
 177/1500 [==>...........................] - ETA: 23:45 - loss: 3.7695 - regression_loss: 2.8044 - classification_loss: 0.9651
 178/1500 [==>...........................] - ETA: 23:40 - loss: 3.7688 - regression_loss: 2.8049 - classification_loss: 0.9639
 179/1500 [==>...........................] - ETA: 23:42 - loss: 3.7632 - regression_loss: 2.8031 - classification_loss: 0.9601
 180/1500 [==>...........................] - ETA: 23:49 - loss: 3.7569 - regression_loss: 2.8008 - classification_loss: 0.9562
 181/1500 [==>...........................] - ETA: 23:46 - loss: 3.7555 - regression_loss: 2.8022 - classification_loss: 0.9534
 182/1500 [==>...........................] - ETA: 23:49 - loss: 3.7490 - regression_loss: 2.7996 - classification_loss: 0.9494
 183/1500 [==>...........................] - ETA: 23:44 - loss: 3.7445 - regression_loss: 2.7982 - classification_loss: 0.9464
 184/1500 [==>...........................] - ETA: 23:41 - loss: 3.7438 - regression_loss: 2.7983 - classification_loss: 0.9454
 185/1500 [==>...........................] - ETA: 23:41 - loss: 3.7424 - regression_loss: 2.7980 - classification_loss: 0.9444
 186/1500 [==>...........................] - ETA: 23:35 - loss: 3.7384 - regression_loss: 2.7966 - classification_loss: 0.9417
 187/1500 [==>...........................] - ETA: 23:31 - loss: 3.7344 - regression_loss: 2.7959 - classification_loss: 0.9384
 188/1500 [==>...........................] - ETA: 23:29 - loss: 3.7316 - regression_loss: 2.7945 - classification_loss: 0.9371
 189/1500 [==>...........................] - ETA: 23:27 - loss: 3.7297 - regression_loss: 2.7947 - classification_loss: 0.9350
 190/1500 [==>...........................] - ETA: 23:26 - loss: 3.7277 - regression_loss: 2.7946 - classification_loss: 0.9332
 191/1500 [==>...........................] - ETA: 23:24 - loss: 3.7236 - regression_loss: 2.7939 - classification_loss: 0.9296
 192/1500 [==>...........................] - ETA: 23:22 - loss: 3.7201 - regression_loss: 2.7933 - classification_loss: 0.9268
 193/1500 [==>...........................] - ETA: 23:19 - loss: 3.7168 - regression_loss: 2.7927 - classification_loss: 0.9242
 194/1500 [==>...........................] - ETA: 23:14 - loss: 3.7145 - regression_loss: 2.7932 - classification_loss: 0.9213
 195/1500 [==>...........................] - ETA: 23:08 - loss: 3.7113 - regression_loss: 2.7923 - classification_loss: 0.9189
 196/1500 [==>...........................] - ETA: 23:03 - loss: 3.7079 - regression_loss: 2.7914 - classification_loss: 0.9166
 197/1500 [==>...........................] - ETA: 23:00 - loss: 3.7071 - regression_loss: 2.7910 - classification_loss: 0.9161
 198/1500 [==>...........................] - ETA: 22:58 - loss: 3.7045 - regression_loss: 2.7899 - classification_loss: 0.9146
 199/1500 [==>...........................] - ETA: 22:53 - loss: 3.7023 - regression_loss: 2.7894 - classification_loss: 0.9130
 200/1500 [===>..........................] - ETA: 22:48 - loss: 3.7007 - regression_loss: 2.7895 - classification_loss: 0.9112
 201/1500 [===>..........................] - ETA: 22:46 - loss: 3.6962 - regression_loss: 2.7879 - classification_loss: 0.9084
 202/1500 [===>..........................] - ETA: 22:45 - loss: 3.6949 - regression_loss: 2.7878 - classification_loss: 0.9071
 203/1500 [===>..........................] - ETA: 22:41 - loss: 3.6937 - regression_loss: 2.7874 - classification_loss: 0.9063
 204/1500 [===>..........................] - ETA: 22:36 - loss: 3.7042 - regression_loss: 2.7960 - classification_loss: 0.9083
 205/1500 [===>..........................] - ETA: 22:34 - loss: 3.7016 - regression_loss: 2.7955 - classification_loss: 0.9061
 206/1500 [===>..........................] - ETA: 22:29 - loss: 3.7021 - regression_loss: 2.7964 - classification_loss: 0.9057
 207/1500 [===>..........................] - ETA: 22:25 - loss: 3.6989 - regression_loss: 2.7958 - classification_loss: 0.9031
 208/1500 [===>..........................] - ETA: 22:28 - loss: 3.6974 - regression_loss: 2.7957 - classification_loss: 0.9017
 209/1500 [===>..........................] - ETA: 22:27 - loss: 3.6955 - regression_loss: 2.7953 - classification_loss: 0.9002
 210/1500 [===>..........................] - ETA: 22:27 - loss: 3.6924 - regression_loss: 2.7932 - classification_loss: 0.8992
 211/1500 [===>..........................] - ETA: 22:22 - loss: 3.6911 - regression_loss: 2.7932 - classification_loss: 0.8979
 212/1500 [===>..........................] - ETA: 22:17 - loss: 3.6878 - regression_loss: 2.7921 - classification_loss: 0.8957
 213/1500 [===>..........................] - ETA: 22:12 - loss: 3.6835 - regression_loss: 2.7899 - classification_loss: 0.8935
 214/1500 [===>..........................] - ETA: 22:08 - loss: 3.6817 - regression_loss: 2.7899 - classification_loss: 0.8918
 215/1500 [===>..........................] - ETA: 22:09 - loss: 3.6798 - regression_loss: 2.7892 - classification_loss: 0.8906
 216/1500 [===>..........................] - ETA: 22:09 - loss: 3.6769 - regression_loss: 2.7884 - classification_loss: 0.8885
 217/1500 [===>..........................] - ETA: 22:06 - loss: 3.6728 - regression_loss: 2.7870 - classification_loss: 0.8858
 218/1500 [===>..........................] - ETA: 22:11 - loss: 3.6711 - regression_loss: 2.7874 - classification_loss: 0.8837
 219/1500 [===>..........................] - ETA: 22:06 - loss: 3.6678 - regression_loss: 2.7860 - classification_loss: 0.8817
 220/1500 [===>..........................] - ETA: 22:04 - loss: 3.6638 - regression_loss: 2.7847 - classification_loss: 0.8791
 221/1500 [===>..........................] - ETA: 22:02 - loss: 3.6623 - regression_loss: 2.7847 - classification_loss: 0.8777
 222/1500 [===>..........................] - ETA: 21:57 - loss: 3.6593 - regression_loss: 2.7838 - classification_loss: 0.8755
 223/1500 [===>..........................] - ETA: 21:53 - loss: 3.6558 - regression_loss: 2.7816 - classification_loss: 0.8742
 224/1500 [===>..........................] - ETA: 21:52 - loss: 3.6544 - regression_loss: 2.7822 - classification_loss: 0.8722
 225/1500 [===>..........................] - ETA: 21:55 - loss: 3.6538 - regression_loss: 2.7827 - classification_loss: 0.8711
 226/1500 [===>..........................] - ETA: 21:52 - loss: 3.6525 - regression_loss: 2.7828 - classification_loss: 0.8697
 227/1500 [===>..........................] - ETA: 21:53 - loss: 3.6528 - regression_loss: 2.7822 - classification_loss: 0.8706
 228/1500 [===>..........................] - ETA: 21:49 - loss: 3.6503 - regression_loss: 2.7819 - classification_loss: 0.8684
 229/1500 [===>..........................] - ETA: 21:46 - loss: 3.6472 - regression_loss: 2.7811 - classification_loss: 0.8661
 230/1500 [===>..........................] - ETA: 21:41 - loss: 3.6449 - regression_loss: 2.7806 - classification_loss: 0.8644
 231/1500 [===>..........................] - ETA: 21:37 - loss: 3.6418 - regression_loss: 2.7798 - classification_loss: 0.8620
 232/1500 [===>..........................] - ETA: 21:33 - loss: 3.6407 - regression_loss: 2.7794 - classification_loss: 0.8613
 233/1500 [===>..........................] - ETA: 21:35 - loss: 3.6413 - regression_loss: 2.7817 - classification_loss: 0.8596
 234/1500 [===>..........................] - ETA: 21:31 - loss: 3.6387 - regression_loss: 2.7813 - classification_loss: 0.8573
 235/1500 [===>..........................] - ETA: 21:26 - loss: 3.6352 - regression_loss: 2.7799 - classification_loss: 0.8553
 236/1500 [===>..........................] - ETA: 21:22 - loss: 3.6338 - regression_loss: 2.7799 - classification_loss: 0.8539
 237/1500 [===>..........................] - ETA: 21:20 - loss: 3.6314 - regression_loss: 2.7796 - classification_loss: 0.8518
 238/1500 [===>..........................] - ETA: 21:17 - loss: 3.6277 - regression_loss: 2.7778 - classification_loss: 0.8500
 239/1500 [===>..........................] - ETA: 21:14 - loss: 3.6248 - regression_loss: 2.7770 - classification_loss: 0.8478
 240/1500 [===>..........................] - ETA: 21:09 - loss: 3.6208 - regression_loss: 2.7750 - classification_loss: 0.8458
 241/1500 [===>..........................] - ETA: 21:09 - loss: 3.6208 - regression_loss: 2.7752 - classification_loss: 0.8456
 242/1500 [===>..........................] - ETA: 21:07 - loss: 3.6210 - regression_loss: 2.7757 - classification_loss: 0.8453
 243/1500 [===>..........................] - ETA: 21:03 - loss: 3.6176 - regression_loss: 2.7744 - classification_loss: 0.8432
 244/1500 [===>..........................] - ETA: 20:59 - loss: 3.6147 - regression_loss: 2.7729 - classification_loss: 0.8419
 245/1500 [===>..........................] - ETA: 20:55 - loss: 3.6140 - regression_loss: 2.7725 - classification_loss: 0.8415
 246/1500 [===>..........................] - ETA: 21:00 - loss: 3.6120 - regression_loss: 2.7727 - classification_loss: 0.8393
 247/1500 [===>..........................] - ETA: 20:58 - loss: 3.6083 - regression_loss: 2.7713 - classification_loss: 0.8370
 248/1500 [===>..........................] - ETA: 20:56 - loss: 3.6129 - regression_loss: 2.7760 - classification_loss: 0.8369
 249/1500 [===>..........................] - ETA: 20:54 - loss: 3.6123 - regression_loss: 2.7758 - classification_loss: 0.8366
 250/1500 [====>.........................] - ETA: 20:50 - loss: 3.6114 - regression_loss: 2.7757 - classification_loss: 0.8358
 251/1500 [====>.........................] - ETA: 20:48 - loss: 3.6081 - regression_loss: 2.7744 - classification_loss: 0.8337
 252/1500 [====>.........................] - ETA: 20:48 - loss: 3.6062 - regression_loss: 2.7739 - classification_loss: 0.8324
 253/1500 [====>.........................] - ETA: 20:46 - loss: 3.5969 - regression_loss: 2.7629 - classification_loss: 0.8340
 254/1500 [====>.........................] - ETA: 20:46 - loss: 3.5964 - regression_loss: 2.7635 - classification_loss: 0.8329
 255/1500 [====>.........................] - ETA: 20:48 - loss: 3.5961 - regression_loss: 2.7635 - classification_loss: 0.8327
 256/1500 [====>.........................] - ETA: 20:51 - loss: 3.5944 - regression_loss: 2.7629 - classification_loss: 0.8315
 257/1500 [====>.........................] - ETA: 20:53 - loss: 3.5991 - regression_loss: 2.7687 - classification_loss: 0.8304
 258/1500 [====>.........................] - ETA: 20:54 - loss: 3.5988 - regression_loss: 2.7697 - classification_loss: 0.8291
 259/1500 [====>.........................] - ETA: 20:50 - loss: 3.5985 - regression_loss: 2.7695 - classification_loss: 0.8290
 260/1500 [====>.........................] - ETA: 20:50 - loss: 3.5998 - regression_loss: 2.7716 - classification_loss: 0.8282
 261/1500 [====>.........................] - ETA: 20:46 - loss: 3.5971 - regression_loss: 2.7706 - classification_loss: 0.8264
 262/1500 [====>.........................] - ETA: 20:42 - loss: 3.5925 - regression_loss: 2.7684 - classification_loss: 0.8241
 263/1500 [====>.........................] - ETA: 20:42 - loss: 3.5922 - regression_loss: 2.7683 - classification_loss: 0.8239
 264/1500 [====>.........................] - ETA: 20:39 - loss: 3.5889 - regression_loss: 2.7664 - classification_loss: 0.8225
 265/1500 [====>.........................] - ETA: 20:37 - loss: 3.5888 - regression_loss: 2.7675 - classification_loss: 0.8213
 266/1500 [====>.........................] - ETA: 20:34 - loss: 3.5852 - regression_loss: 2.7659 - classification_loss: 0.8192
 267/1500 [====>.........................] - ETA: 20:32 - loss: 3.5867 - regression_loss: 2.7629 - classification_loss: 0.8238
 268/1500 [====>.........................] - ETA: 20:28 - loss: 3.5861 - regression_loss: 2.7632 - classification_loss: 0.8229
 269/1500 [====>.........................] - ETA: 20:28 - loss: 3.5844 - regression_loss: 2.7631 - classification_loss: 0.8213
 270/1500 [====>.........................] - ETA: 20:24 - loss: 3.5810 - regression_loss: 2.7618 - classification_loss: 0.8192
 271/1500 [====>.........................] - ETA: 20:27 - loss: 3.5814 - regression_loss: 2.7630 - classification_loss: 0.8184
 272/1500 [====>.........................] - ETA: 20:23 - loss: 3.5803 - regression_loss: 2.7630 - classification_loss: 0.8173
 273/1500 [====>.........................] - ETA: 20:19 - loss: 3.5780 - regression_loss: 2.7627 - classification_loss: 0.8153
 274/1500 [====>.........................] - ETA: 20:16 - loss: 3.5758 - regression_loss: 2.7621 - classification_loss: 0.8138
 275/1500 [====>.........................] - ETA: 20:12 - loss: 3.5747 - regression_loss: 2.7620 - classification_loss: 0.8126
 276/1500 [====>.........................] - ETA: 20:09 - loss: 3.5724 - regression_loss: 2.7612 - classification_loss: 0.8112
 277/1500 [====>.........................] - ETA: 20:06 - loss: 3.5693 - regression_loss: 2.7599 - classification_loss: 0.8094
 278/1500 [====>.........................] - ETA: 20:04 - loss: 3.5686 - regression_loss: 2.7599 - classification_loss: 0.8087
 279/1500 [====>.........................] - ETA: 20:00 - loss: 3.5675 - regression_loss: 2.7594 - classification_loss: 0.8081
 280/1500 [====>.........................] - ETA: 19:58 - loss: 3.5653 - regression_loss: 2.7586 - classification_loss: 0.8066
 281/1500 [====>.........................] - ETA: 19:56 - loss: 3.5621 - regression_loss: 2.7574 - classification_loss: 0.8047
 282/1500 [====>.........................] - ETA: 19:54 - loss: 3.5588 - regression_loss: 2.7560 - classification_loss: 0.8028
 283/1500 [====>.........................] - ETA: 19:54 - loss: 3.5583 - regression_loss: 2.7561 - classification_loss: 0.8022
 284/1500 [====>.........................] - ETA: 19:52 - loss: 3.5560 - regression_loss: 2.7553 - classification_loss: 0.8007
 285/1500 [====>.........................] - ETA: 19:49 - loss: 3.5546 - regression_loss: 2.7552 - classification_loss: 0.7994
 286/1500 [====>.........................] - ETA: 19:50 - loss: 3.5530 - regression_loss: 2.7548 - classification_loss: 0.7982
 287/1500 [====>.........................] - ETA: 19:46 - loss: 3.5514 - regression_loss: 2.7546 - classification_loss: 0.7968
 288/1500 [====>.........................] - ETA: 19:43 - loss: 3.5478 - regression_loss: 2.7530 - classification_loss: 0.7948
 289/1500 [====>.........................] - ETA: 19:40 - loss: 3.5486 - regression_loss: 2.7544 - classification_loss: 0.7941
 290/1500 [====>.........................] - ETA: 19:39 - loss: 3.5474 - regression_loss: 2.7535 - classification_loss: 0.7939
 291/1500 [====>.........................] - ETA: 19:36 - loss: 3.5459 - regression_loss: 2.7527 - classification_loss: 0.7932
 292/1500 [====>.........................] - ETA: 19:45 - loss: 3.5450 - regression_loss: 2.7532 - classification_loss: 0.7918
 293/1500 [====>.........................] - ETA: 19:41 - loss: 3.5444 - regression_loss: 2.7537 - classification_loss: 0.7907
 294/1500 [====>.........................] - ETA: 19:40 - loss: 3.5429 - regression_loss: 2.7532 - classification_loss: 0.7897
 295/1500 [====>.........................] - ETA: 19:38 - loss: 3.5419 - regression_loss: 2.7530 - classification_loss: 0.7889
 296/1500 [====>.........................] - ETA: 19:36 - loss: 3.5418 - regression_loss: 2.7532 - classification_loss: 0.7886
 297/1500 [====>.........................] - ETA: 19:33 - loss: 3.5400 - regression_loss: 2.7531 - classification_loss: 0.7870
 298/1500 [====>.........................] - ETA: 19:30 - loss: 3.5370 - regression_loss: 2.7518 - classification_loss: 0.7852
 299/1500 [====>.........................] - ETA: 19:27 - loss: 3.5342 - regression_loss: 2.7507 - classification_loss: 0.7835
 300/1500 [=====>........................] - ETA: 19:24 - loss: 3.5326 - regression_loss: 2.7502 - classification_loss: 0.7824
 301/1500 [=====>........................] - ETA: 19:21 - loss: 3.5321 - regression_loss: 2.7506 - classification_loss: 0.7816
 302/1500 [=====>........................] - ETA: 19:17 - loss: 3.5320 - regression_loss: 2.7512 - classification_loss: 0.7808
 303/1500 [=====>........................] - ETA: 19:14 - loss: 3.5317 - regression_loss: 2.7517 - classification_loss: 0.7800
 304/1500 [=====>........................] - ETA: 19:13 - loss: 3.5308 - regression_loss: 2.7516 - classification_loss: 0.7792
 305/1500 [=====>........................] - ETA: 19:09 - loss: 3.5296 - regression_loss: 2.7516 - classification_loss: 0.7780
 306/1500 [=====>........................] - ETA: 19:06 - loss: 3.5260 - regression_loss: 2.7494 - classification_loss: 0.7766
 307/1500 [=====>........................] - ETA: 19:07 - loss: 3.5242 - regression_loss: 2.7485 - classification_loss: 0.7757
 308/1500 [=====>........................] - ETA: 19:04 - loss: 3.5222 - regression_loss: 2.7480 - classification_loss: 0.7742
 309/1500 [=====>........................] - ETA: 19:03 - loss: 3.5221 - regression_loss: 2.7478 - classification_loss: 0.7744
 310/1500 [=====>........................] - ETA: 19:02 - loss: 3.5198 - regression_loss: 2.7467 - classification_loss: 0.7731
 311/1500 [=====>........................] - ETA: 19:02 - loss: 3.5197 - regression_loss: 2.7467 - classification_loss: 0.7730
 312/1500 [=====>........................] - ETA: 19:01 - loss: 3.5186 - regression_loss: 2.7464 - classification_loss: 0.7722
 313/1500 [=====>........................] - ETA: 18:59 - loss: 3.5179 - regression_loss: 2.7462 - classification_loss: 0.7717
 314/1500 [=====>........................] - ETA: 18:56 - loss: 3.5184 - regression_loss: 2.7463 - classification_loss: 0.7721
 315/1500 [=====>........................] - ETA: 19:02 - loss: 3.5184 - regression_loss: 2.7459 - classification_loss: 0.7725
 316/1500 [=====>........................] - ETA: 19:00 - loss: 3.5169 - regression_loss: 2.7456 - classification_loss: 0.7713
 317/1500 [=====>........................] - ETA: 19:03 - loss: 3.5170 - regression_loss: 2.7467 - classification_loss: 0.7703
 318/1500 [=====>........................] - ETA: 19:01 - loss: 3.5159 - regression_loss: 2.7460 - classification_loss: 0.7699
 319/1500 [=====>........................] - ETA: 18:58 - loss: 3.5143 - regression_loss: 2.7453 - classification_loss: 0.7690
 320/1500 [=====>........................] - ETA: 18:57 - loss: 3.5118 - regression_loss: 2.7443 - classification_loss: 0.7675
 321/1500 [=====>........................] - ETA: 18:56 - loss: 3.5081 - regression_loss: 2.7422 - classification_loss: 0.7659
 322/1500 [=====>........................] - ETA: 18:56 - loss: 3.5067 - regression_loss: 2.7421 - classification_loss: 0.7646
 323/1500 [=====>........................] - ETA: 18:55 - loss: 3.5061 - regression_loss: 2.7423 - classification_loss: 0.7639
 324/1500 [=====>........................] - ETA: 18:52 - loss: 3.5043 - regression_loss: 2.7418 - classification_loss: 0.7625
 325/1500 [=====>........................] - ETA: 18:51 - loss: 3.5034 - regression_loss: 2.7417 - classification_loss: 0.7617
 326/1500 [=====>........................] - ETA: 18:49 - loss: 3.5013 - regression_loss: 2.7409 - classification_loss: 0.7604
 327/1500 [=====>........................] - ETA: 18:52 - loss: 3.5007 - regression_loss: 2.7410 - classification_loss: 0.7597
 328/1500 [=====>........................] - ETA: 18:49 - loss: 3.5006 - regression_loss: 2.7415 - classification_loss: 0.7590
 329/1500 [=====>........................] - ETA: 18:46 - loss: 3.4994 - regression_loss: 2.7416 - classification_loss: 0.7578
 330/1500 [=====>........................] - ETA: 18:43 - loss: 3.4999 - regression_loss: 2.7427 - classification_loss: 0.7572
 331/1500 [=====>........................] - ETA: 18:41 - loss: 3.4959 - regression_loss: 2.7404 - classification_loss: 0.7555
 332/1500 [=====>........................] - ETA: 18:39 - loss: 3.4945 - regression_loss: 2.7403 - classification_loss: 0.7542
 333/1500 [=====>........................] - ETA: 18:36 - loss: 3.4931 - regression_loss: 2.7401 - classification_loss: 0.7530
 334/1500 [=====>........................] - ETA: 18:35 - loss: 3.4929 - regression_loss: 2.7400 - classification_loss: 0.7529
 335/1500 [=====>........................] - ETA: 18:33 - loss: 3.4923 - regression_loss: 2.7403 - classification_loss: 0.7520
 336/1500 [=====>........................] - ETA: 18:38 - loss: 3.4907 - regression_loss: 2.7398 - classification_loss: 0.7509
 337/1500 [=====>........................] - ETA: 18:35 - loss: 3.4925 - regression_loss: 2.7421 - classification_loss: 0.7503
 338/1500 [=====>........................] - ETA: 18:32 - loss: 3.4919 - regression_loss: 2.7419 - classification_loss: 0.7500
 339/1500 [=====>........................] - ETA: 18:31 - loss: 3.4909 - regression_loss: 2.7416 - classification_loss: 0.7493
 340/1500 [=====>........................] - ETA: 18:28 - loss: 3.4908 - regression_loss: 2.7424 - classification_loss: 0.7484
 341/1500 [=====>........................] - ETA: 18:29 - loss: 3.4888 - regression_loss: 2.7416 - classification_loss: 0.7472
 342/1500 [=====>........................] - ETA: 18:26 - loss: 3.4896 - regression_loss: 2.7427 - classification_loss: 0.7470
 343/1500 [=====>........................] - ETA: 18:23 - loss: 3.4859 - regression_loss: 2.7405 - classification_loss: 0.7454
 344/1500 [=====>........................] - ETA: 18:22 - loss: 3.4856 - regression_loss: 2.7411 - classification_loss: 0.7445
 345/1500 [=====>........................] - ETA: 18:19 - loss: 3.4842 - regression_loss: 2.7409 - classification_loss: 0.7433
 346/1500 [=====>........................] - ETA: 18:20 - loss: 3.4840 - regression_loss: 2.7406 - classification_loss: 0.7434
 347/1500 [=====>........................] - ETA: 18:19 - loss: 3.4838 - regression_loss: 2.7413 - classification_loss: 0.7425
 348/1500 [=====>........................] - ETA: 18:22 - loss: 3.4812 - regression_loss: 2.7396 - classification_loss: 0.7416
 349/1500 [=====>........................] - ETA: 18:19 - loss: 3.4807 - regression_loss: 2.7397 - classification_loss: 0.7409
 350/1500 [======>.......................] - ETA: 18:17 - loss: 3.4791 - regression_loss: 2.7388 - classification_loss: 0.7403
 351/1500 [======>.......................] - ETA: 18:16 - loss: 3.4771 - regression_loss: 2.7380 - classification_loss: 0.7391
 352/1500 [======>.......................] - ETA: 18:14 - loss: 3.4757 - regression_loss: 2.7371 - classification_loss: 0.7385
 353/1500 [======>.......................] - ETA: 18:11 - loss: 3.4751 - regression_loss: 2.7372 - classification_loss: 0.7379
 354/1500 [======>.......................] - ETA: 18:12 - loss: 3.4747 - regression_loss: 2.7370 - classification_loss: 0.7377
 355/1500 [======>.......................] - ETA: 18:15 - loss: 3.4739 - regression_loss: 2.7373 - classification_loss: 0.7366
 356/1500 [======>.......................] - ETA: 18:15 - loss: 3.4730 - regression_loss: 2.7371 - classification_loss: 0.7360
 357/1500 [======>.......................] - ETA: 18:12 - loss: 3.4718 - regression_loss: 2.7364 - classification_loss: 0.7354
 358/1500 [======>.......................] - ETA: 18:09 - loss: 3.4710 - regression_loss: 2.7361 - classification_loss: 0.7349
 359/1500 [======>.......................] - ETA: 18:09 - loss: 3.4701 - regression_loss: 2.7361 - classification_loss: 0.7340
 360/1500 [======>.......................] - ETA: 18:09 - loss: 3.4697 - regression_loss: 2.7358 - classification_loss: 0.7339
 361/1500 [======>.......................] - ETA: 18:10 - loss: 3.4688 - regression_loss: 2.7354 - classification_loss: 0.7334
 362/1500 [======>.......................] - ETA: 18:08 - loss: 3.4690 - regression_loss: 2.7354 - classification_loss: 0.7336
 363/1500 [======>.......................] - ETA: 18:07 - loss: 3.4683 - regression_loss: 2.7350 - classification_loss: 0.7332
 364/1500 [======>.......................] - ETA: 18:10 - loss: 3.4652 - regression_loss: 2.7331 - classification_loss: 0.7321
 365/1500 [======>.......................] - ETA: 18:07 - loss: 3.4621 - regression_loss: 2.7315 - classification_loss: 0.7306
 366/1500 [======>.......................] - ETA: 18:09 - loss: 3.4610 - regression_loss: 2.7314 - classification_loss: 0.7296
 367/1500 [======>.......................] - ETA: 18:06 - loss: 3.4614 - regression_loss: 2.7321 - classification_loss: 0.7293
 368/1500 [======>.......................] - ETA: 18:04 - loss: 3.4608 - regression_loss: 2.7320 - classification_loss: 0.7288
 369/1500 [======>.......................] - ETA: 18:02 - loss: 3.4601 - regression_loss: 2.7317 - classification_loss: 0.7284
 370/1500 [======>.......................] - ETA: 18:02 - loss: 3.4596 - regression_loss: 2.7318 - classification_loss: 0.7277
 371/1500 [======>.......................] - ETA: 18:00 - loss: 3.4592 - regression_loss: 2.7319 - classification_loss: 0.7273
 372/1500 [======>.......................] - ETA: 17:58 - loss: 3.4584 - regression_loss: 2.7316 - classification_loss: 0.7268
 373/1500 [======>.......................] - ETA: 17:56 - loss: 3.4584 - regression_loss: 2.7323 - classification_loss: 0.7260
 374/1500 [======>.......................] - ETA: 17:56 - loss: 3.4594 - regression_loss: 2.7344 - classification_loss: 0.7250
 375/1500 [======>.......................] - ETA: 17:54 - loss: 3.4593 - regression_loss: 2.7348 - classification_loss: 0.7245
 376/1500 [======>.......................] - ETA: 17:52 - loss: 3.4573 - regression_loss: 2.7340 - classification_loss: 0.7234
 377/1500 [======>.......................] - ETA: 17:49 - loss: 3.4561 - regression_loss: 2.7331 - classification_loss: 0.7230
 378/1500 [======>.......................] - ETA: 17:47 - loss: 3.4546 - regression_loss: 2.7325 - classification_loss: 0.7220
 379/1500 [======>.......................] - ETA: 17:47 - loss: 3.4537 - regression_loss: 2.7323 - classification_loss: 0.7215
 380/1500 [======>.......................] - ETA: 17:44 - loss: 3.4522 - regression_loss: 2.7314 - classification_loss: 0.7209
 381/1500 [======>.......................] - ETA: 17:43 - loss: 3.4524 - regression_loss: 2.7312 - classification_loss: 0.7212
 382/1500 [======>.......................] - ETA: 17:40 - loss: 3.4535 - regression_loss: 2.7322 - classification_loss: 0.7213
 383/1500 [======>.......................] - ETA: 17:41 - loss: 3.4543 - regression_loss: 2.7321 - classification_loss: 0.7221
 384/1500 [======>.......................] - ETA: 17:42 - loss: 3.4528 - regression_loss: 2.7314 - classification_loss: 0.7214
 385/1500 [======>.......................] - ETA: 17:39 - loss: 3.4522 - regression_loss: 2.7312 - classification_loss: 0.7210
 386/1500 [======>.......................] - ETA: 17:41 - loss: 3.4493 - regression_loss: 2.7293 - classification_loss: 0.7200
 387/1500 [======>.......................] - ETA: 17:39 - loss: 3.4486 - regression_loss: 2.7293 - classification_loss: 0.7193
 388/1500 [======>.......................] - ETA: 17:43 - loss: 3.4481 - regression_loss: 2.7296 - classification_loss: 0.7184
 389/1500 [======>.......................] - ETA: 17:40 - loss: 3.4487 - regression_loss: 2.7302 - classification_loss: 0.7186
 390/1500 [======>.......................] - ETA: 17:38 - loss: 3.4474 - regression_loss: 2.7296 - classification_loss: 0.7177
 391/1500 [======>.......................] - ETA: 17:36 - loss: 3.4464 - regression_loss: 2.7292 - classification_loss: 0.7172
 392/1500 [======>.......................] - ETA: 17:35 - loss: 3.4440 - regression_loss: 2.7279 - classification_loss: 0.7161
 393/1500 [======>.......................] - ETA: 17:34 - loss: 3.4448 - regression_loss: 2.7296 - classification_loss: 0.7153
 394/1500 [======>.......................] - ETA: 17:32 - loss: 3.4429 - regression_loss: 2.7284 - classification_loss: 0.7145
 395/1500 [======>.......................] - ETA: 17:30 - loss: 3.4422 - regression_loss: 2.7288 - classification_loss: 0.7134
 396/1500 [======>.......................] - ETA: 17:27 - loss: 3.4413 - regression_loss: 2.7284 - classification_loss: 0.7129
 397/1500 [======>.......................] - ETA: 17:25 - loss: 3.4415 - regression_loss: 2.7286 - classification_loss: 0.7130
 398/1500 [======>.......................] - ETA: 17:25 - loss: 3.4388 - regression_loss: 2.7268 - classification_loss: 0.7120
 399/1500 [======>.......................] - ETA: 17:22 - loss: 3.4368 - regression_loss: 2.7258 - classification_loss: 0.7110
 400/1500 [=======>......................] - ETA: 17:20 - loss: 3.4366 - regression_loss: 2.7264 - classification_loss: 0.7102
 401/1500 [=======>......................] - ETA: 17:20 - loss: 3.4368 - regression_loss: 2.7266 - classification_loss: 0.7102
 402/1500 [=======>......................] - ETA: 17:21 - loss: 3.4356 - regression_loss: 2.7263 - classification_loss: 0.7093
 403/1500 [=======>......................] - ETA: 17:20 - loss: 3.4340 - regression_loss: 2.7253 - classification_loss: 0.7087
 404/1500 [=======>......................] - ETA: 17:19 - loss: 3.4328 - regression_loss: 2.7248 - classification_loss: 0.7079
 405/1500 [=======>......................] - ETA: 17:21 - loss: 3.4297 - regression_loss: 2.7227 - classification_loss: 0.7070
 406/1500 [=======>......................] - ETA: 17:21 - loss: 3.4286 - regression_loss: 2.7225 - classification_loss: 0.7060
 407/1500 [=======>......................] - ETA: 17:20 - loss: 3.4278 - regression_loss: 2.7225 - classification_loss: 0.7053
 408/1500 [=======>......................] - ETA: 17:18 - loss: 3.4262 - regression_loss: 2.7217 - classification_loss: 0.7045
 409/1500 [=======>......................] - ETA: 17:16 - loss: 3.4234 - regression_loss: 2.7201 - classification_loss: 0.7033
 410/1500 [=======>......................] - ETA: 17:13 - loss: 3.4222 - regression_loss: 2.7196 - classification_loss: 0.7026
 411/1500 [=======>......................] - ETA: 17:12 - loss: 3.4217 - regression_loss: 2.7200 - classification_loss: 0.7018
 412/1500 [=======>......................] - ETA: 17:11 - loss: 3.4203 - regression_loss: 2.7191 - classification_loss: 0.7012
 413/1500 [=======>......................] - ETA: 17:09 - loss: 3.4196 - regression_loss: 2.7190 - classification_loss: 0.7006
 414/1500 [=======>......................] - ETA: 17:09 - loss: 3.4172 - regression_loss: 2.7175 - classification_loss: 0.6997
 415/1500 [=======>......................] - ETA: 17:08 - loss: 3.4152 - regression_loss: 2.7166 - classification_loss: 0.6986
 416/1500 [=======>......................] - ETA: 17:07 - loss: 3.4149 - regression_loss: 2.7169 - classification_loss: 0.6980
 417/1500 [=======>......................] - ETA: 17:05 - loss: 3.4141 - regression_loss: 2.7167 - classification_loss: 0.6973
 418/1500 [=======>......................] - ETA: 17:02 - loss: 3.4143 - regression_loss: 2.7169 - classification_loss: 0.6974
 419/1500 [=======>......................] - ETA: 17:00 - loss: 3.4148 - regression_loss: 2.7173 - classification_loss: 0.6975
 420/1500 [=======>......................] - ETA: 16:59 - loss: 3.4139 - regression_loss: 2.7173 - classification_loss: 0.6966
 421/1500 [=======>......................] - ETA: 16:58 - loss: 3.4112 - regression_loss: 2.7157 - classification_loss: 0.6955
 422/1500 [=======>......................] - ETA: 17:00 - loss: 3.4088 - regression_loss: 2.7142 - classification_loss: 0.6946
 423/1500 [=======>......................] - ETA: 17:01 - loss: 3.4078 - regression_loss: 2.7138 - classification_loss: 0.6940
 424/1500 [=======>......................] - ETA: 17:01 - loss: 3.4067 - regression_loss: 2.7134 - classification_loss: 0.6933
 425/1500 [=======>......................] - ETA: 16:59 - loss: 3.4056 - regression_loss: 2.7130 - classification_loss: 0.6926
 426/1500 [=======>......................] - ETA: 16:59 - loss: 3.4033 - regression_loss: 2.7116 - classification_loss: 0.6918
 427/1500 [=======>......................] - ETA: 16:57 - loss: 3.4049 - regression_loss: 2.7137 - classification_loss: 0.6912
 428/1500 [=======>......................] - ETA: 16:56 - loss: 3.4045 - regression_loss: 2.7134 - classification_loss: 0.6912
 429/1500 [=======>......................] - ETA: 16:56 - loss: 3.4060 - regression_loss: 2.7157 - classification_loss: 0.6903
 430/1500 [=======>......................] - ETA: 16:54 - loss: 3.4081 - regression_loss: 2.7181 - classification_loss: 0.6900
 431/1500 [=======>......................] - ETA: 16:55 - loss: 3.4067 - regression_loss: 2.7174 - classification_loss: 0.6892
 432/1500 [=======>......................] - ETA: 16:52 - loss: 3.4053 - regression_loss: 2.7172 - classification_loss: 0.6881
 433/1500 [=======>......................] - ETA: 16:50 - loss: 3.4092 - regression_loss: 2.7211 - classification_loss: 0.6882
 434/1500 [=======>......................] - ETA: 16:48 - loss: 3.4085 - regression_loss: 2.7210 - classification_loss: 0.6875
 435/1500 [=======>......................] - ETA: 16:46 - loss: 3.4067 - regression_loss: 2.7200 - classification_loss: 0.6867
 436/1500 [=======>......................] - ETA: 16:43 - loss: 3.4050 - regression_loss: 2.7192 - classification_loss: 0.6858
 437/1500 [=======>......................] - ETA: 16:42 - loss: 3.4050 - regression_loss: 2.7200 - classification_loss: 0.6851
 438/1500 [=======>......................] - ETA: 16:43 - loss: 3.4050 - regression_loss: 2.7200 - classification_loss: 0.6850
 439/1500 [=======>......................] - ETA: 16:44 - loss: 3.4041 - regression_loss: 2.7197 - classification_loss: 0.6844
 440/1500 [=======>......................] - ETA: 16:44 - loss: 3.4033 - regression_loss: 2.7194 - classification_loss: 0.6840
 441/1500 [=======>......................] - ETA: 16:42 - loss: 3.4008 - regression_loss: 2.7176 - classification_loss: 0.6832
 442/1500 [=======>......................] - ETA: 16:42 - loss: 3.3993 - regression_loss: 2.7171 - classification_loss: 0.6822
 443/1500 [=======>......................] - ETA: 16:42 - loss: 3.3989 - regression_loss: 2.7170 - classification_loss: 0.6819
 444/1500 [=======>......................] - ETA: 16:40 - loss: 3.3969 - regression_loss: 2.7160 - classification_loss: 0.6810
 445/1500 [=======>......................] - ETA: 16:38 - loss: 3.3963 - regression_loss: 2.7156 - classification_loss: 0.6807
 446/1500 [=======>......................] - ETA: 16:39 - loss: 3.3983 - regression_loss: 2.7177 - classification_loss: 0.6805
 447/1500 [=======>......................] - ETA: 16:37 - loss: 3.3979 - regression_loss: 2.7172 - classification_loss: 0.6807
 448/1500 [=======>......................] - ETA: 16:35 - loss: 3.4007 - regression_loss: 2.7203 - classification_loss: 0.6803
 449/1500 [=======>......................] - ETA: 16:32 - loss: 3.4001 - regression_loss: 2.7200 - classification_loss: 0.6802
 450/1500 [========>.....................] - ETA: 16:32 - loss: 3.3983 - regression_loss: 2.7189 - classification_loss: 0.6794
 451/1500 [========>.....................] - ETA: 16:34 - loss: 3.3982 - regression_loss: 2.7180 - classification_loss: 0.6802
 452/1500 [========>.....................] - ETA: 16:32 - loss: 3.3966 - regression_loss: 2.7172 - classification_loss: 0.6794
 453/1500 [========>.....................] - ETA: 16:31 - loss: 3.3943 - regression_loss: 2.7157 - classification_loss: 0.6786
 454/1500 [========>.....................] - ETA: 16:30 - loss: 3.3924 - regression_loss: 2.7148 - classification_loss: 0.6776
 455/1500 [========>.....................] - ETA: 16:29 - loss: 3.3923 - regression_loss: 2.7151 - classification_loss: 0.6772
 456/1500 [========>.....................] - ETA: 16:31 - loss: 3.3919 - regression_loss: 2.7155 - classification_loss: 0.6765
 457/1500 [========>.....................] - ETA: 16:28 - loss: 3.3910 - regression_loss: 2.7150 - classification_loss: 0.6761
 458/1500 [========>.....................] - ETA: 16:26 - loss: 3.3925 - regression_loss: 2.7164 - classification_loss: 0.6760
 459/1500 [========>.....................] - ETA: 16:24 - loss: 3.3922 - regression_loss: 2.7159 - classification_loss: 0.6763
 460/1500 [========>.....................] - ETA: 16:22 - loss: 3.3922 - regression_loss: 2.7159 - classification_loss: 0.6763
 461/1500 [========>.....................] - ETA: 16:21 - loss: 3.3924 - regression_loss: 2.7169 - classification_loss: 0.6755
 462/1500 [========>.....................] - ETA: 16:20 - loss: 3.3927 - regression_loss: 2.7170 - classification_loss: 0.6757
 463/1500 [========>.....................] - ETA: 16:19 - loss: 3.3950 - regression_loss: 2.7170 - classification_loss: 0.6781
 464/1500 [========>.....................] - ETA: 16:16 - loss: 3.3921 - regression_loss: 2.7150 - classification_loss: 0.6771
 465/1500 [========>.....................] - ETA: 16:16 - loss: 3.3914 - regression_loss: 2.7148 - classification_loss: 0.6766
 466/1500 [========>.....................] - ETA: 16:14 - loss: 3.3889 - regression_loss: 2.7132 - classification_loss: 0.6756
 467/1500 [========>.....................] - ETA: 16:12 - loss: 3.3862 - regression_loss: 2.7116 - classification_loss: 0.6746
 468/1500 [========>.....................] - ETA: 16:12 - loss: 3.3834 - regression_loss: 2.7098 - classification_loss: 0.6736
 469/1500 [========>.....................] - ETA: 16:11 - loss: 3.3823 - regression_loss: 2.7092 - classification_loss: 0.6731
 470/1500 [========>.....................] - ETA: 16:12 - loss: 3.3825 - regression_loss: 2.7093 - classification_loss: 0.6732
 471/1500 [========>.....................] - ETA: 16:11 - loss: 3.3829 - regression_loss: 2.7095 - classification_loss: 0.6734
 472/1500 [========>.....................] - ETA: 16:12 - loss: 3.3829 - regression_loss: 2.7097 - classification_loss: 0.6732
 473/1500 [========>.....................] - ETA: 16:09 - loss: 3.3822 - regression_loss: 2.7089 - classification_loss: 0.6733
 474/1500 [========>.....................] - ETA: 16:08 - loss: 3.3820 - regression_loss: 2.7093 - classification_loss: 0.6727
 475/1500 [========>.....................] - ETA: 16:06 - loss: 3.3821 - regression_loss: 2.7093 - classification_loss: 0.6728
 476/1500 [========>.....................] - ETA: 16:04 - loss: 3.3816 - regression_loss: 2.7092 - classification_loss: 0.6723
 477/1500 [========>.....................] - ETA: 16:04 - loss: 3.3803 - regression_loss: 2.7086 - classification_loss: 0.6717
 478/1500 [========>.....................] - ETA: 16:02 - loss: 3.3805 - regression_loss: 2.7086 - classification_loss: 0.6719
 479/1500 [========>.....................] - ETA: 16:00 - loss: 3.3797 - regression_loss: 2.7083 - classification_loss: 0.6714
 480/1500 [========>.....................] - ETA: 15:58 - loss: 3.3791 - regression_loss: 2.7080 - classification_loss: 0.6711
 481/1500 [========>.....................] - ETA: 15:56 - loss: 3.3839 - regression_loss: 2.7102 - classification_loss: 0.6737
 482/1500 [========>.....................] - ETA: 15:55 - loss: 3.3854 - regression_loss: 2.7114 - classification_loss: 0.6739
 483/1500 [========>.....................] - ETA: 15:55 - loss: 3.3853 - regression_loss: 2.7115 - classification_loss: 0.6738
 484/1500 [========>.....................] - ETA: 15:54 - loss: 3.3838 - regression_loss: 2.7105 - classification_loss: 0.6732
 485/1500 [========>.....................] - ETA: 15:52 - loss: 3.3819 - regression_loss: 2.7093 - classification_loss: 0.6725
 486/1500 [========>.....................] - ETA: 15:50 - loss: 3.3810 - regression_loss: 2.7090 - classification_loss: 0.6720
 487/1500 [========>.....................] - ETA: 15:48 - loss: 3.3798 - regression_loss: 2.7083 - classification_loss: 0.6715
 488/1500 [========>.....................] - ETA: 15:47 - loss: 3.3792 - regression_loss: 2.7081 - classification_loss: 0.6711
 489/1500 [========>.....................] - ETA: 15:49 - loss: 3.3789 - regression_loss: 2.7079 - classification_loss: 0.6711
 490/1500 [========>.....................] - ETA: 15:47 - loss: 3.3783 - regression_loss: 2.7078 - classification_loss: 0.6705
 491/1500 [========>.....................] - ETA: 15:46 - loss: 3.3782 - regression_loss: 2.7080 - classification_loss: 0.6701
 492/1500 [========>.....................] - ETA: 15:48 - loss: 3.3755 - regression_loss: 2.7062 - classification_loss: 0.6693
 493/1500 [========>.....................] - ETA: 15:48 - loss: 3.3768 - regression_loss: 2.7066 - classification_loss: 0.6702
 494/1500 [========>.....................] - ETA: 15:48 - loss: 3.3743 - regression_loss: 2.7045 - classification_loss: 0.6698
 495/1500 [========>.....................] - ETA: 15:49 - loss: 3.3731 - regression_loss: 2.7036 - classification_loss: 0.6694
 496/1500 [========>.....................] - ETA: 15:49 - loss: 3.3714 - regression_loss: 2.7024 - classification_loss: 0.6690
 497/1500 [========>.....................] - ETA: 15:49 - loss: 3.3698 - regression_loss: 2.7015 - classification_loss: 0.6683
 498/1500 [========>.....................] - ETA: 15:48 - loss: 3.3688 - regression_loss: 2.7008 - classification_loss: 0.6680
 499/1500 [========>.....................] - ETA: 15:48 - loss: 3.3682 - regression_loss: 2.7004 - classification_loss: 0.6678
 500/1500 [=========>....................] - ETA: 15:48 - loss: 3.3667 - regression_loss: 2.6992 - classification_loss: 0.6675
 501/1500 [=========>....................] - ETA: 15:48 - loss: 3.3685 - regression_loss: 2.7015 - classification_loss: 0.6669
 502/1500 [=========>....................] - ETA: 15:47 - loss: 3.3704 - regression_loss: 2.7032 - classification_loss: 0.6672
 503/1500 [=========>....................] - ETA: 15:45 - loss: 3.3715 - regression_loss: 2.7041 - classification_loss: 0.6674
 504/1500 [=========>....................] - ETA: 15:44 - loss: 3.3718 - regression_loss: 2.7042 - classification_loss: 0.6676
 505/1500 [=========>....................] - ETA: 15:44 - loss: 3.3703 - regression_loss: 2.7032 - classification_loss: 0.6671
 506/1500 [=========>....................] - ETA: 15:45 - loss: 3.3700 - regression_loss: 2.7035 - classification_loss: 0.6665
 507/1500 [=========>....................] - ETA: 15:44 - loss: 3.3695 - regression_loss: 2.7027 - classification_loss: 0.6668
 508/1500 [=========>....................] - ETA: 15:43 - loss: 3.3672 - regression_loss: 2.7012 - classification_loss: 0.6660
 509/1500 [=========>....................] - ETA: 15:43 - loss: 3.3664 - regression_loss: 2.7009 - classification_loss: 0.6655
 510/1500 [=========>....................] - ETA: 15:43 - loss: 3.3655 - regression_loss: 2.7004 - classification_loss: 0.6651
 511/1500 [=========>....................] - ETA: 15:41 - loss: 3.3655 - regression_loss: 2.7006 - classification_loss: 0.6649
 512/1500 [=========>....................] - ETA: 15:40 - loss: 3.3653 - regression_loss: 2.7010 - classification_loss: 0.6643
 513/1500 [=========>....................] - ETA: 15:38 - loss: 3.3644 - regression_loss: 2.7005 - classification_loss: 0.6639
 514/1500 [=========>....................] - ETA: 15:36 - loss: 3.3629 - regression_loss: 2.6994 - classification_loss: 0.6636
 515/1500 [=========>....................] - ETA: 15:34 - loss: 3.3634 - regression_loss: 2.6998 - classification_loss: 0.6636
 516/1500 [=========>....................] - ETA: 15:32 - loss: 3.3623 - regression_loss: 2.6993 - classification_loss: 0.6630
 517/1500 [=========>....................] - ETA: 15:30 - loss: 3.3592 - regression_loss: 2.6971 - classification_loss: 0.6621
 518/1500 [=========>....................] - ETA: 15:29 - loss: 3.3581 - regression_loss: 2.6964 - classification_loss: 0.6617
 519/1500 [=========>....................] - ETA: 15:27 - loss: 3.3577 - regression_loss: 2.6962 - classification_loss: 0.6615
 520/1500 [=========>....................] - ETA: 15:30 - loss: 3.3555 - regression_loss: 2.6948 - classification_loss: 0.6607
 521/1500 [=========>....................] - ETA: 15:30 - loss: 3.3559 - regression_loss: 2.6946 - classification_loss: 0.6613
 522/1500 [=========>....................] - ETA: 15:28 - loss: 3.3549 - regression_loss: 2.6941 - classification_loss: 0.6608
 523/1500 [=========>....................] - ETA: 15:27 - loss: 3.3548 - regression_loss: 2.6938 - classification_loss: 0.6610
 524/1500 [=========>....................] - ETA: 15:27 - loss: 3.3561 - regression_loss: 2.6936 - classification_loss: 0.6626
 525/1500 [=========>....................] - ETA: 15:25 - loss: 3.3558 - regression_loss: 2.6933 - classification_loss: 0.6624
 526/1500 [=========>....................] - ETA: 15:23 - loss: 3.3535 - regression_loss: 2.6919 - classification_loss: 0.6616
 527/1500 [=========>....................] - ETA: 15:22 - loss: 3.3528 - regression_loss: 2.6919 - classification_loss: 0.6609
 528/1500 [=========>....................] - ETA: 15:22 - loss: 3.3515 - regression_loss: 2.6910 - classification_loss: 0.6605
 529/1500 [=========>....................] - ETA: 15:21 - loss: 3.3496 - regression_loss: 2.6899 - classification_loss: 0.6597
 530/1500 [=========>....................] - ETA: 15:20 - loss: 3.3502 - regression_loss: 2.6903 - classification_loss: 0.6599
 531/1500 [=========>....................] - ETA: 15:18 - loss: 3.3506 - regression_loss: 2.6903 - classification_loss: 0.6603
 532/1500 [=========>....................] - ETA: 15:18 - loss: 3.3493 - regression_loss: 2.6897 - classification_loss: 0.6597
 533/1500 [=========>....................] - ETA: 15:16 - loss: 3.3482 - regression_loss: 2.6891 - classification_loss: 0.6591
 534/1500 [=========>....................] - ETA: 15:16 - loss: 3.3484 - regression_loss: 2.6896 - classification_loss: 0.6588
 535/1500 [=========>....................] - ETA: 15:14 - loss: 3.3506 - regression_loss: 2.6920 - classification_loss: 0.6586
 536/1500 [=========>....................] - ETA: 15:14 - loss: 3.3507 - regression_loss: 2.6919 - classification_loss: 0.6588
 537/1500 [=========>....................] - ETA: 15:17 - loss: 3.3506 - regression_loss: 2.6918 - classification_loss: 0.6588
 538/1500 [=========>....................] - ETA: 15:17 - loss: 3.3501 - regression_loss: 2.6917 - classification_loss: 0.6584
 539/1500 [=========>....................] - ETA: 15:15 - loss: 3.3485 - regression_loss: 2.6908 - classification_loss: 0.6577
 540/1500 [=========>....................] - ETA: 15:14 - loss: 3.3481 - regression_loss: 2.6906 - classification_loss: 0.6575
 541/1500 [=========>....................] - ETA: 15:13 - loss: 3.3467 - regression_loss: 2.6898 - classification_loss: 0.6569
 542/1500 [=========>....................] - ETA: 15:11 - loss: 3.3454 - regression_loss: 2.6891 - classification_loss: 0.6563
 543/1500 [=========>....................] - ETA: 15:10 - loss: 3.3450 - regression_loss: 2.6893 - classification_loss: 0.6556
 544/1500 [=========>....................] - ETA: 15:08 - loss: 3.3462 - regression_loss: 2.6904 - classification_loss: 0.6557
 545/1500 [=========>....................] - ETA: 15:06 - loss: 3.3464 - regression_loss: 2.6913 - classification_loss: 0.6551
 546/1500 [=========>....................] - ETA: 15:06 - loss: 3.3457 - regression_loss: 2.6909 - classification_loss: 0.6548
 547/1500 [=========>....................] - ETA: 15:04 - loss: 3.3453 - regression_loss: 2.6911 - classification_loss: 0.6542
 548/1500 [=========>....................] - ETA: 15:02 - loss: 3.3445 - regression_loss: 2.6908 - classification_loss: 0.6537
 549/1500 [=========>....................] - ETA: 15:01 - loss: 3.3455 - regression_loss: 2.6923 - classification_loss: 0.6532
 550/1500 [==========>...................] - ETA: 15:00 - loss: 3.3455 - regression_loss: 2.6923 - classification_loss: 0.6532
 551/1500 [==========>...................] - ETA: 15:00 - loss: 3.3455 - regression_loss: 2.6922 - classification_loss: 0.6534
 552/1500 [==========>...................] - ETA: 14:58 - loss: 3.3439 - regression_loss: 2.6912 - classification_loss: 0.6527
 553/1500 [==========>...................] - ETA: 14:56 - loss: 3.3439 - regression_loss: 2.6913 - classification_loss: 0.6525
 554/1500 [==========>...................] - ETA: 14:54 - loss: 3.3425 - regression_loss: 2.6904 - classification_loss: 0.6521
 555/1500 [==========>...................] - ETA: 14:56 - loss: 3.3411 - regression_loss: 2.6896 - classification_loss: 0.6515
 556/1500 [==========>...................] - ETA: 14:54 - loss: 3.3432 - regression_loss: 2.6888 - classification_loss: 0.6544
 557/1500 [==========>...................] - ETA: 14:52 - loss: 3.3440 - regression_loss: 2.6897 - classification_loss: 0.6543
 558/1500 [==========>...................] - ETA: 14:51 - loss: 3.3445 - regression_loss: 2.6907 - classification_loss: 0.6538
 559/1500 [==========>...................] - ETA: 14:49 - loss: 3.3447 - regression_loss: 2.6908 - classification_loss: 0.6539
 560/1500 [==========>...................] - ETA: 14:48 - loss: 3.3442 - regression_loss: 2.6908 - classification_loss: 0.6533
 561/1500 [==========>...................] - ETA: 14:47 - loss: 3.3452 - regression_loss: 2.6921 - classification_loss: 0.6531
 562/1500 [==========>...................] - ETA: 14:45 - loss: 3.3457 - regression_loss: 2.6923 - classification_loss: 0.6534
 563/1500 [==========>...................] - ETA: 14:43 - loss: 3.3442 - regression_loss: 2.6913 - classification_loss: 0.6529
 564/1500 [==========>...................] - ETA: 14:43 - loss: 3.3431 - regression_loss: 2.6905 - classification_loss: 0.6526
 565/1500 [==========>...................] - ETA: 14:41 - loss: 3.3410 - regression_loss: 2.6890 - classification_loss: 0.6520
 566/1500 [==========>...................] - ETA: 14:41 - loss: 3.3400 - regression_loss: 2.6883 - classification_loss: 0.6517
 567/1500 [==========>...................] - ETA: 14:39 - loss: 3.3390 - regression_loss: 2.6877 - classification_loss: 0.6513
 568/1500 [==========>...................] - ETA: 14:38 - loss: 3.3385 - regression_loss: 2.6874 - classification_loss: 0.6511
 569/1500 [==========>...................] - ETA: 14:36 - loss: 3.3390 - regression_loss: 2.6879 - classification_loss: 0.6511
 570/1500 [==========>...................] - ETA: 14:34 - loss: 3.3381 - regression_loss: 2.6876 - classification_loss: 0.6506
 571/1500 [==========>...................] - ETA: 14:33 - loss: 3.3355 - regression_loss: 2.6855 - classification_loss: 0.6500
 572/1500 [==========>...................] - ETA: 14:31 - loss: 3.3352 - regression_loss: 2.6855 - classification_loss: 0.6496
 573/1500 [==========>...................] - ETA: 14:30 - loss: 3.3350 - regression_loss: 2.6855 - classification_loss: 0.6495
 574/1500 [==========>...................] - ETA: 14:34 - loss: 3.3338 - regression_loss: 2.6849 - classification_loss: 0.6488
 575/1500 [==========>...................] - ETA: 14:32 - loss: 3.3329 - regression_loss: 2.6844 - classification_loss: 0.6485
 576/1500 [==========>...................] - ETA: 14:33 - loss: 3.3317 - regression_loss: 2.6839 - classification_loss: 0.6478
 577/1500 [==========>...................] - ETA: 14:31 - loss: 3.3317 - regression_loss: 2.6839 - classification_loss: 0.6477
 578/1500 [==========>...................] - ETA: 14:29 - loss: 3.3305 - regression_loss: 2.6830 - classification_loss: 0.6475
 579/1500 [==========>...................] - ETA: 14:27 - loss: 3.3295 - regression_loss: 2.6824 - classification_loss: 0.6471
 580/1500 [==========>...................] - ETA: 14:26 - loss: 3.3289 - regression_loss: 2.6820 - classification_loss: 0.6469
 581/1500 [==========>...................] - ETA: 14:25 - loss: 3.3280 - regression_loss: 2.6816 - classification_loss: 0.6464
 582/1500 [==========>...................] - ETA: 14:23 - loss: 3.3275 - regression_loss: 2.6806 - classification_loss: 0.6468
 583/1500 [==========>...................] - ETA: 14:24 - loss: 3.3263 - regression_loss: 2.6799 - classification_loss: 0.6464
 584/1500 [==========>...................] - ETA: 14:22 - loss: 3.3239 - regression_loss: 2.6776 - classification_loss: 0.6462
 585/1500 [==========>...................] - ETA: 14:20 - loss: 3.3217 - regression_loss: 2.6762 - classification_loss: 0.6455
 586/1500 [==========>...................] - ETA: 14:19 - loss: 3.3216 - regression_loss: 2.6763 - classification_loss: 0.6454
 587/1500 [==========>...................] - ETA: 14:17 - loss: 3.3190 - regression_loss: 2.6744 - classification_loss: 0.6447
 588/1500 [==========>...................] - ETA: 14:15 - loss: 3.3186 - regression_loss: 2.6741 - classification_loss: 0.6444
 589/1500 [==========>...................] - ETA: 14:16 - loss: 3.3179 - regression_loss: 2.6734 - classification_loss: 0.6445
 590/1500 [==========>...................] - ETA: 14:15 - loss: 3.3176 - regression_loss: 2.6733 - classification_loss: 0.6443
 591/1500 [==========>...................] - ETA: 14:14 - loss: 3.3159 - regression_loss: 2.6720 - classification_loss: 0.6439
 592/1500 [==========>...................] - ETA: 14:13 - loss: 3.3161 - regression_loss: 2.6724 - classification_loss: 0.6437
 593/1500 [==========>...................] - ETA: 14:13 - loss: 3.3151 - regression_loss: 2.6717 - classification_loss: 0.6434
 594/1500 [==========>...................] - ETA: 14:12 - loss: 3.3142 - regression_loss: 2.6710 - classification_loss: 0.6433
 595/1500 [==========>...................] - ETA: 14:11 - loss: 3.3144 - regression_loss: 2.6714 - classification_loss: 0.6430
 596/1500 [==========>...................] - ETA: 14:09 - loss: 3.3126 - regression_loss: 2.6704 - classification_loss: 0.6423
 597/1500 [==========>...................] - ETA: 14:10 - loss: 3.3114 - regression_loss: 2.6696 - classification_loss: 0.6418
 598/1500 [==========>...................] - ETA: 14:08 - loss: 3.3117 - regression_loss: 2.6697 - classification_loss: 0.6420
 599/1500 [==========>...................] - ETA: 14:06 - loss: 3.3109 - regression_loss: 2.6692 - classification_loss: 0.6418
 600/1500 [===========>..................] - ETA: 14:06 - loss: 3.3105 - regression_loss: 2.6690 - classification_loss: 0.6415
 601/1500 [===========>..................] - ETA: 14:04 - loss: 3.3097 - regression_loss: 2.6686 - classification_loss: 0.6411
 602/1500 [===========>..................] - ETA: 14:03 - loss: 3.3070 - regression_loss: 2.6666 - classification_loss: 0.6404
 603/1500 [===========>..................] - ETA: 14:01 - loss: 3.3057 - regression_loss: 2.6656 - classification_loss: 0.6401
 604/1500 [===========>..................] - ETA: 14:01 - loss: 3.3066 - regression_loss: 2.6669 - classification_loss: 0.6397
 605/1500 [===========>..................] - ETA: 14:00 - loss: 3.3052 - regression_loss: 2.6658 - classification_loss: 0.6394
 606/1500 [===========>..................] - ETA: 13:58 - loss: 3.3044 - regression_loss: 2.6646 - classification_loss: 0.6398
 607/1500 [===========>..................] - ETA: 13:57 - loss: 3.3019 - regression_loss: 2.6629 - classification_loss: 0.6390
 608/1500 [===========>..................] - ETA: 13:55 - loss: 3.2998 - regression_loss: 2.6614 - classification_loss: 0.6384
 609/1500 [===========>..................] - ETA: 13:54 - loss: 3.2968 - regression_loss: 2.6591 - classification_loss: 0.6376
 610/1500 [===========>..................] - ETA: 13:52 - loss: 3.2963 - regression_loss: 2.6591 - classification_loss: 0.6372
 611/1500 [===========>..................] - ETA: 13:51 - loss: 3.2969 - regression_loss: 2.6597 - classification_loss: 0.6373
 612/1500 [===========>..................] - ETA: 13:50 - loss: 3.2960 - regression_loss: 2.6591 - classification_loss: 0.6370
 613/1500 [===========>..................] - ETA: 13:50 - loss: 3.2962 - regression_loss: 2.6593 - classification_loss: 0.6369
 614/1500 [===========>..................] - ETA: 13:49 - loss: 3.2958 - regression_loss: 2.6591 - classification_loss: 0.6367
 615/1500 [===========>..................] - ETA: 13:47 - loss: 3.2937 - regression_loss: 2.6576 - classification_loss: 0.6361
 616/1500 [===========>..................] - ETA: 13:45 - loss: 3.2940 - regression_loss: 2.6577 - classification_loss: 0.6363
 617/1500 [===========>..................] - ETA: 13:44 - loss: 3.2912 - regression_loss: 2.6557 - classification_loss: 0.6355
 618/1500 [===========>..................] - ETA: 13:44 - loss: 3.2904 - regression_loss: 2.6553 - classification_loss: 0.6351
 619/1500 [===========>..................] - ETA: 13:43 - loss: 3.2904 - regression_loss: 2.6554 - classification_loss: 0.6350
 620/1500 [===========>..................] - ETA: 13:42 - loss: 3.2902 - regression_loss: 2.6555 - classification_loss: 0.6348
 621/1500 [===========>..................] - ETA: 13:42 - loss: 3.2921 - regression_loss: 2.6573 - classification_loss: 0.6347
 622/1500 [===========>..................] - ETA: 13:43 - loss: 3.2908 - regression_loss: 2.6567 - classification_loss: 0.6342
 623/1500 [===========>..................] - ETA: 13:42 - loss: 3.2904 - regression_loss: 2.6567 - classification_loss: 0.6337
 624/1500 [===========>..................] - ETA: 13:40 - loss: 3.2904 - regression_loss: 2.6567 - classification_loss: 0.6337
 625/1500 [===========>..................] - ETA: 13:39 - loss: 3.2912 - regression_loss: 2.6573 - classification_loss: 0.6339
 626/1500 [===========>..................] - ETA: 13:37 - loss: 3.2898 - regression_loss: 2.6563 - classification_loss: 0.6335
 627/1500 [===========>..................] - ETA: 13:37 - loss: 3.2909 - regression_loss: 2.6574 - classification_loss: 0.6335
 628/1500 [===========>..................] - ETA: 13:36 - loss: 3.2901 - regression_loss: 2.6567 - classification_loss: 0.6333
 629/1500 [===========>..................] - ETA: 13:36 - loss: 3.2895 - regression_loss: 2.6563 - classification_loss: 0.6332
 630/1500 [===========>..................] - ETA: 13:34 - loss: 3.2894 - regression_loss: 2.6566 - classification_loss: 0.6328
 631/1500 [===========>..................] - ETA: 13:35 - loss: 3.2881 - regression_loss: 2.6555 - classification_loss: 0.6326
 632/1500 [===========>..................] - ETA: 13:33 - loss: 3.2863 - regression_loss: 2.6543 - classification_loss: 0.6321
 633/1500 [===========>..................] - ETA: 13:32 - loss: 3.2857 - regression_loss: 2.6539 - classification_loss: 0.6319
 634/1500 [===========>..................] - ETA: 13:33 - loss: 3.2850 - regression_loss: 2.6533 - classification_loss: 0.6317
 635/1500 [===========>..................] - ETA: 13:31 - loss: 3.2837 - regression_loss: 2.6526 - classification_loss: 0.6311
 636/1500 [===========>..................] - ETA: 13:30 - loss: 3.2816 - regression_loss: 2.6511 - classification_loss: 0.6305
 637/1500 [===========>..................] - ETA: 13:28 - loss: 3.2814 - regression_loss: 2.6509 - classification_loss: 0.6305
 638/1500 [===========>..................] - ETA: 13:27 - loss: 3.2807 - regression_loss: 2.6506 - classification_loss: 0.6301
 639/1500 [===========>..................] - ETA: 13:26 - loss: 3.2801 - regression_loss: 2.6502 - classification_loss: 0.6299
 640/1500 [===========>..................] - ETA: 13:24 - loss: 3.2793 - regression_loss: 2.6499 - classification_loss: 0.6294
 641/1500 [===========>..................] - ETA: 13:23 - loss: 3.2793 - regression_loss: 2.6501 - classification_loss: 0.6292
 642/1500 [===========>..................] - ETA: 13:21 - loss: 3.2803 - regression_loss: 2.6514 - classification_loss: 0.6290
 643/1500 [===========>..................] - ETA: 13:20 - loss: 3.2793 - regression_loss: 2.6508 - classification_loss: 0.6285
 644/1500 [===========>..................] - ETA: 13:19 - loss: 3.2781 - regression_loss: 2.6498 - classification_loss: 0.6283
 645/1500 [===========>..................] - ETA: 13:17 - loss: 3.2753 - regression_loss: 2.6477 - classification_loss: 0.6276
 646/1500 [===========>..................] - ETA: 13:16 - loss: 3.2732 - regression_loss: 2.6462 - classification_loss: 0.6270
 647/1500 [===========>..................] - ETA: 13:15 - loss: 3.2717 - regression_loss: 2.6449 - classification_loss: 0.6267
 648/1500 [===========>..................] - ETA: 13:14 - loss: 3.2722 - regression_loss: 2.6460 - classification_loss: 0.6262
 649/1500 [===========>..................] - ETA: 13:13 - loss: 3.2719 - regression_loss: 2.6459 - classification_loss: 0.6260
 650/1500 [============>.................] - ETA: 13:13 - loss: 3.2696 - regression_loss: 2.6442 - classification_loss: 0.6254
 651/1500 [============>.................] - ETA: 13:11 - loss: 3.2673 - regression_loss: 2.6426 - classification_loss: 0.6247
 652/1500 [============>.................] - ETA: 13:10 - loss: 3.2661 - regression_loss: 2.6418 - classification_loss: 0.6243
 653/1500 [============>.................] - ETA: 13:09 - loss: 3.2660 - regression_loss: 2.6418 - classification_loss: 0.6243
 654/1500 [============>.................] - ETA: 13:09 - loss: 3.2665 - regression_loss: 2.6420 - classification_loss: 0.6245
 655/1500 [============>.................] - ETA: 13:08 - loss: 3.2660 - regression_loss: 2.6418 - classification_loss: 0.6241
 656/1500 [============>.................] - ETA: 13:07 - loss: 3.2643 - regression_loss: 2.6409 - classification_loss: 0.6235
 657/1500 [============>.................] - ETA: 13:06 - loss: 3.2644 - regression_loss: 2.6409 - classification_loss: 0.6235
 658/1500 [============>.................] - ETA: 13:04 - loss: 3.2641 - regression_loss: 2.6408 - classification_loss: 0.6233
 659/1500 [============>.................] - ETA: 13:04 - loss: 3.2637 - regression_loss: 2.6406 - classification_loss: 0.6231
 660/1500 [============>.................] - ETA: 13:03 - loss: 3.2629 - regression_loss: 2.6402 - classification_loss: 0.6227
 661/1500 [============>.................] - ETA: 13:03 - loss: 3.2624 - regression_loss: 2.6398 - classification_loss: 0.6226
 662/1500 [============>.................] - ETA: 13:01 - loss: 3.2621 - regression_loss: 2.6397 - classification_loss: 0.6223
 663/1500 [============>.................] - ETA: 13:01 - loss: 3.2622 - regression_loss: 2.6397 - classification_loss: 0.6225
 664/1500 [============>.................] - ETA: 13:00 - loss: 3.2618 - regression_loss: 2.6394 - classification_loss: 0.6224
 665/1500 [============>.................] - ETA: 12:58 - loss: 3.2603 - regression_loss: 2.6384 - classification_loss: 0.6219
 666/1500 [============>.................] - ETA: 12:57 - loss: 3.2581 - regression_loss: 2.6367 - classification_loss: 0.6214
 667/1500 [============>.................] - ETA: 12:56 - loss: 3.2560 - regression_loss: 2.6352 - classification_loss: 0.6208
 668/1500 [============>.................] - ETA: 12:56 - loss: 3.2549 - regression_loss: 2.6346 - classification_loss: 0.6203
 669/1500 [============>.................] - ETA: 12:54 - loss: 3.2543 - regression_loss: 2.6344 - classification_loss: 0.6199
 670/1500 [============>.................] - ETA: 12:52 - loss: 3.2525 - regression_loss: 2.6331 - classification_loss: 0.6194
 671/1500 [============>.................] - ETA: 12:51 - loss: 3.2514 - regression_loss: 2.6319 - classification_loss: 0.6195
 672/1500 [============>.................] - ETA: 12:50 - loss: 3.2519 - regression_loss: 2.6326 - classification_loss: 0.6192
 673/1500 [============>.................] - ETA: 12:49 - loss: 3.2501 - regression_loss: 2.6313 - classification_loss: 0.6188
 674/1500 [============>.................] - ETA: 12:48 - loss: 3.2494 - regression_loss: 2.6309 - classification_loss: 0.6185
 675/1500 [============>.................] - ETA: 12:46 - loss: 3.2483 - regression_loss: 2.6303 - classification_loss: 0.6180
 676/1500 [============>.................] - ETA: 12:44 - loss: 3.2463 - regression_loss: 2.6286 - classification_loss: 0.6177
 677/1500 [============>.................] - ETA: 12:44 - loss: 3.2452 - regression_loss: 2.6279 - classification_loss: 0.6173
 678/1500 [============>.................] - ETA: 12:44 - loss: 3.2455 - regression_loss: 2.6279 - classification_loss: 0.6175
 679/1500 [============>.................] - ETA: 12:43 - loss: 3.2449 - regression_loss: 2.6278 - classification_loss: 0.6171
 680/1500 [============>.................] - ETA: 12:42 - loss: 3.2440 - regression_loss: 2.6271 - classification_loss: 0.6169
 681/1500 [============>.................] - ETA: 12:41 - loss: 3.2424 - regression_loss: 2.6260 - classification_loss: 0.6164
 682/1500 [============>.................] - ETA: 12:40 - loss: 3.2416 - regression_loss: 2.6256 - classification_loss: 0.6161
 683/1500 [============>.................] - ETA: 12:39 - loss: 3.2431 - regression_loss: 2.6269 - classification_loss: 0.6162
 684/1500 [============>.................] - ETA: 12:38 - loss: 3.2426 - regression_loss: 2.6266 - classification_loss: 0.6160
 685/1500 [============>.................] - ETA: 12:36 - loss: 3.2409 - regression_loss: 2.6254 - classification_loss: 0.6155
 686/1500 [============>.................] - ETA: 12:35 - loss: 3.2416 - regression_loss: 2.6263 - classification_loss: 0.6153
 687/1500 [============>.................] - ETA: 12:34 - loss: 3.2406 - regression_loss: 2.6257 - classification_loss: 0.6149
 688/1500 [============>.................] - ETA: 12:32 - loss: 3.2410 - regression_loss: 2.6260 - classification_loss: 0.6149
 689/1500 [============>.................] - ETA: 12:31 - loss: 3.2419 - regression_loss: 2.6269 - classification_loss: 0.6150
 690/1500 [============>.................] - ETA: 12:30 - loss: 3.2414 - regression_loss: 2.6267 - classification_loss: 0.6147
 691/1500 [============>.................] - ETA: 12:28 - loss: 3.2420 - regression_loss: 2.6273 - classification_loss: 0.6148
 692/1500 [============>.................] - ETA: 12:27 - loss: 3.2407 - regression_loss: 2.6260 - classification_loss: 0.6147
 693/1500 [============>.................] - ETA: 12:25 - loss: 3.2386 - regression_loss: 2.6240 - classification_loss: 0.6146
 694/1500 [============>.................] - ETA: 12:24 - loss: 3.2370 - regression_loss: 2.6228 - classification_loss: 0.6142
 695/1500 [============>.................] - ETA: 12:22 - loss: 3.2360 - regression_loss: 2.6222 - classification_loss: 0.6139
 696/1500 [============>.................] - ETA: 12:21 - loss: 3.2357 - regression_loss: 2.6224 - classification_loss: 0.6133
 697/1500 [============>.................] - ETA: 12:19 - loss: 3.2362 - regression_loss: 2.6230 - classification_loss: 0.6132
 698/1500 [============>.................] - ETA: 12:18 - loss: 3.2355 - regression_loss: 2.6226 - classification_loss: 0.6129
 699/1500 [============>.................] - ETA: 12:18 - loss: 3.2351 - regression_loss: 2.6225 - classification_loss: 0.6126
 700/1500 [=============>................] - ETA: 12:16 - loss: 3.2346 - regression_loss: 2.6224 - classification_loss: 0.6122
 701/1500 [=============>................] - ETA: 12:16 - loss: 3.2340 - regression_loss: 2.6221 - classification_loss: 0.6119
 702/1500 [=============>................] - ETA: 12:14 - loss: 3.2337 - regression_loss: 2.6213 - classification_loss: 0.6124
 703/1500 [=============>................] - ETA: 12:13 - loss: 3.2342 - regression_loss: 2.6220 - classification_loss: 0.6122
 704/1500 [=============>................] - ETA: 12:12 - loss: 3.2343 - regression_loss: 2.6223 - classification_loss: 0.6120
 705/1500 [=============>................] - ETA: 12:10 - loss: 3.2339 - regression_loss: 2.6221 - classification_loss: 0.6118
 706/1500 [=============>................] - ETA: 12:09 - loss: 3.2340 - regression_loss: 2.6221 - classification_loss: 0.6119
 707/1500 [=============>................] - ETA: 12:09 - loss: 3.2332 - regression_loss: 2.6215 - classification_loss: 0.6117
 708/1500 [=============>................] - ETA: 12:07 - loss: 3.2323 - regression_loss: 2.6211 - classification_loss: 0.6112
 709/1500 [=============>................] - ETA: 12:07 - loss: 3.2330 - regression_loss: 2.6220 - classification_loss: 0.6110
 710/1500 [=============>................] - ETA: 12:06 - loss: 3.2326 - regression_loss: 2.6217 - classification_loss: 0.6109
 711/1500 [=============>................] - ETA: 12:04 - loss: 3.2316 - regression_loss: 2.6210 - classification_loss: 0.6107
 712/1500 [=============>................] - ETA: 12:04 - loss: 3.2316 - regression_loss: 2.6213 - classification_loss: 0.6103
 713/1500 [=============>................] - ETA: 12:03 - loss: 3.2306 - regression_loss: 2.6201 - classification_loss: 0.6106
 714/1500 [=============>................] - ETA: 12:02 - loss: 3.2297 - regression_loss: 2.6196 - classification_loss: 0.6101
 715/1500 [=============>................] - ETA: 12:00 - loss: 3.2283 - regression_loss: 2.6187 - classification_loss: 0.6097
 716/1500 [=============>................] - ETA: 11:59 - loss: 3.2279 - regression_loss: 2.6184 - classification_loss: 0.6095
 717/1500 [=============>................] - ETA: 11:57 - loss: 3.2272 - regression_loss: 2.6181 - classification_loss: 0.6091
 718/1500 [=============>................] - ETA: 11:57 - loss: 3.2264 - regression_loss: 2.6177 - classification_loss: 0.6087
 719/1500 [=============>................] - ETA: 11:56 - loss: 3.2259 - regression_loss: 2.6175 - classification_loss: 0.6084
 720/1500 [=============>................] - ETA: 11:56 - loss: 3.2247 - regression_loss: 2.6168 - classification_loss: 0.6078
 721/1500 [=============>................] - ETA: 11:54 - loss: 3.2232 - regression_loss: 2.6155 - classification_loss: 0.6076
 722/1500 [=============>................] - ETA: 11:53 - loss: 3.2229 - regression_loss: 2.6156 - classification_loss: 0.6073
 723/1500 [=============>................] - ETA: 11:53 - loss: 3.2232 - regression_loss: 2.6162 - classification_loss: 0.6070
 724/1500 [=============>................] - ETA: 11:52 - loss: 3.2219 - regression_loss: 2.6147 - classification_loss: 0.6071
 725/1500 [=============>................] - ETA: 11:50 - loss: 3.2207 - regression_loss: 2.6138 - classification_loss: 0.6068
 726/1500 [=============>................] - ETA: 11:50 - loss: 3.2194 - regression_loss: 2.6129 - classification_loss: 0.6065
 727/1500 [=============>................] - ETA: 11:49 - loss: 3.2177 - regression_loss: 2.6117 - classification_loss: 0.6061
 728/1500 [=============>................] - ETA: 11:48 - loss: 3.2173 - regression_loss: 2.6115 - classification_loss: 0.6058
 729/1500 [=============>................] - ETA: 11:48 - loss: 3.2162 - regression_loss: 2.6109 - classification_loss: 0.6053
 730/1500 [=============>................] - ETA: 11:46 - loss: 3.2164 - regression_loss: 2.6110 - classification_loss: 0.6053
 731/1500 [=============>................] - ETA: 11:45 - loss: 3.2162 - regression_loss: 2.6108 - classification_loss: 0.6054
 732/1500 [=============>................] - ETA: 11:44 - loss: 3.2150 - regression_loss: 2.6101 - classification_loss: 0.6049
 733/1500 [=============>................] - ETA: 11:44 - loss: 3.2148 - regression_loss: 2.6099 - classification_loss: 0.6048
 734/1500 [=============>................] - ETA: 11:42 - loss: 3.2147 - regression_loss: 2.6101 - classification_loss: 0.6046
 735/1500 [=============>................] - ETA: 11:41 - loss: 3.2132 - regression_loss: 2.6091 - classification_loss: 0.6041
 736/1500 [=============>................] - ETA: 11:41 - loss: 3.2131 - regression_loss: 2.6089 - classification_loss: 0.6042
 737/1500 [=============>................] - ETA: 11:40 - loss: 3.2118 - regression_loss: 2.6081 - classification_loss: 0.6037
 738/1500 [=============>................] - ETA: 11:39 - loss: 3.2100 - regression_loss: 2.6067 - classification_loss: 0.6033
 739/1500 [=============>................] - ETA: 11:38 - loss: 3.2079 - regression_loss: 2.6051 - classification_loss: 0.6028
 740/1500 [=============>................] - ETA: 11:36 - loss: 3.2070 - regression_loss: 2.6044 - classification_loss: 0.6026
 741/1500 [=============>................] - ETA: 11:36 - loss: 3.2046 - regression_loss: 2.6025 - classification_loss: 0.6021
 742/1500 [=============>................] - ETA: 11:35 - loss: 3.2035 - regression_loss: 2.6017 - classification_loss: 0.6018
 743/1500 [=============>................] - ETA: 11:34 - loss: 3.2033 - regression_loss: 2.6014 - classification_loss: 0.6018
 744/1500 [=============>................] - ETA: 11:33 - loss: 3.2044 - regression_loss: 2.6027 - classification_loss: 0.6017
 745/1500 [=============>................] - ETA: 11:33 - loss: 3.2034 - regression_loss: 2.6020 - classification_loss: 0.6015
 746/1500 [=============>................] - ETA: 11:31 - loss: 3.2027 - regression_loss: 2.6015 - classification_loss: 0.6012
 747/1500 [=============>................] - ETA: 11:30 - loss: 3.2038 - regression_loss: 2.6026 - classification_loss: 0.6012
 748/1500 [=============>................] - ETA: 11:29 - loss: 3.2026 - regression_loss: 2.6018 - classification_loss: 0.6008
 749/1500 [=============>................] - ETA: 11:28 - loss: 3.2026 - regression_loss: 2.6020 - classification_loss: 0.6006
 750/1500 [==============>...............] - ETA: 11:26 - loss: 3.2026 - regression_loss: 2.6021 - classification_loss: 0.6005
 751/1500 [==============>...............] - ETA: 11:25 - loss: 3.2009 - regression_loss: 2.6009 - classification_loss: 0.6000
 752/1500 [==============>...............] - ETA: 11:26 - loss: 3.2000 - regression_loss: 2.6004 - classification_loss: 0.5996
 753/1500 [==============>...............] - ETA: 11:26 - loss: 3.1997 - regression_loss: 2.6003 - classification_loss: 0.5994
 754/1500 [==============>...............] - ETA: 11:25 - loss: 3.1994 - regression_loss: 2.6002 - classification_loss: 0.5991
 755/1500 [==============>...............] - ETA: 11:24 - loss: 3.1997 - regression_loss: 2.6005 - classification_loss: 0.5992
 756/1500 [==============>...............] - ETA: 11:23 - loss: 3.2002 - regression_loss: 2.6006 - classification_loss: 0.5996
 757/1500 [==============>...............] - ETA: 11:21 - loss: 3.2000 - regression_loss: 2.6006 - classification_loss: 0.5994
 758/1500 [==============>...............] - ETA: 11:21 - loss: 3.1999 - regression_loss: 2.6005 - classification_loss: 0.5993
 759/1500 [==============>...............] - ETA: 11:19 - loss: 3.1987 - regression_loss: 2.5997 - classification_loss: 0.5990
 760/1500 [==============>...............] - ETA: 11:18 - loss: 3.1972 - regression_loss: 2.5987 - classification_loss: 0.5986
 761/1500 [==============>...............] - ETA: 11:17 - loss: 3.1978 - regression_loss: 2.5990 - classification_loss: 0.5988
 762/1500 [==============>...............] - ETA: 11:16 - loss: 3.1967 - regression_loss: 2.5983 - classification_loss: 0.5984
 763/1500 [==============>...............] - ETA: 11:17 - loss: 3.1962 - regression_loss: 2.5980 - classification_loss: 0.5982
 764/1500 [==============>...............] - ETA: 11:15 - loss: 3.1953 - regression_loss: 2.5974 - classification_loss: 0.5979
 765/1500 [==============>...............] - ETA: 11:14 - loss: 3.1937 - regression_loss: 2.5962 - classification_loss: 0.5976
 766/1500 [==============>...............] - ETA: 11:14 - loss: 3.1918 - regression_loss: 2.5947 - classification_loss: 0.5972
 767/1500 [==============>...............] - ETA: 11:12 - loss: 3.1904 - regression_loss: 2.5937 - classification_loss: 0.5968
 768/1500 [==============>...............] - ETA: 11:12 - loss: 3.1910 - regression_loss: 2.5940 - classification_loss: 0.5970
 769/1500 [==============>...............] - ETA: 11:11 - loss: 3.1892 - regression_loss: 2.5926 - classification_loss: 0.5966
 770/1500 [==============>...............] - ETA: 11:09 - loss: 3.1885 - regression_loss: 2.5922 - classification_loss: 0.5963
 771/1500 [==============>...............] - ETA: 11:09 - loss: 3.1870 - regression_loss: 2.5904 - classification_loss: 0.5965
 772/1500 [==============>...............] - ETA: 11:08 - loss: 3.1867 - regression_loss: 2.5902 - classification_loss: 0.5965
 773/1500 [==============>...............] - ETA: 11:07 - loss: 3.1863 - regression_loss: 2.5900 - classification_loss: 0.5962
 774/1500 [==============>...............] - ETA: 11:05 - loss: 3.1866 - regression_loss: 2.5906 - classification_loss: 0.5961
 775/1500 [==============>...............] - ETA: 11:04 - loss: 3.1861 - regression_loss: 2.5902 - classification_loss: 0.5959
 776/1500 [==============>...............] - ETA: 11:03 - loss: 3.1844 - regression_loss: 2.5889 - classification_loss: 0.5955
 777/1500 [==============>...............] - ETA: 11:02 - loss: 3.1823 - regression_loss: 2.5872 - classification_loss: 0.5951
 778/1500 [==============>...............] - ETA: 11:00 - loss: 3.1818 - regression_loss: 2.5868 - classification_loss: 0.5950
 779/1500 [==============>...............] - ETA: 10:59 - loss: 3.1815 - regression_loss: 2.5868 - classification_loss: 0.5947
 780/1500 [==============>...............] - ETA: 10:59 - loss: 3.1798 - regression_loss: 2.5856 - classification_loss: 0.5942
 781/1500 [==============>...............] - ETA: 10:58 - loss: 3.1800 - regression_loss: 2.5856 - classification_loss: 0.5944
 782/1500 [==============>...............] - ETA: 10:56 - loss: 3.1786 - regression_loss: 2.5847 - classification_loss: 0.5940
 783/1500 [==============>...............] - ETA: 10:55 - loss: 3.1786 - regression_loss: 2.5842 - classification_loss: 0.5944
 784/1500 [==============>...............] - ETA: 10:55 - loss: 3.1790 - regression_loss: 2.5848 - classification_loss: 0.5942
 785/1500 [==============>...............] - ETA: 10:54 - loss: 3.1789 - regression_loss: 2.5850 - classification_loss: 0.5938
 786/1500 [==============>...............] - ETA: 10:54 - loss: 3.1801 - regression_loss: 2.5859 - classification_loss: 0.5942
 787/1500 [==============>...............] - ETA: 10:53 - loss: 3.1806 - regression_loss: 2.5862 - classification_loss: 0.5944
 788/1500 [==============>...............] - ETA: 10:53 - loss: 3.1792 - regression_loss: 2.5852 - classification_loss: 0.5940
 789/1500 [==============>...............] - ETA: 10:52 - loss: 3.1785 - regression_loss: 2.5848 - classification_loss: 0.5937
 790/1500 [==============>...............] - ETA: 10:51 - loss: 3.1781 - regression_loss: 2.5845 - classification_loss: 0.5936
 791/1500 [==============>...............] - ETA: 10:50 - loss: 3.1774 - regression_loss: 2.5841 - classification_loss: 0.5933
 792/1500 [==============>...............] - ETA: 10:49 - loss: 3.1769 - regression_loss: 2.5839 - classification_loss: 0.5931
 793/1500 [==============>...............] - ETA: 10:49 - loss: 3.1771 - regression_loss: 2.5841 - classification_loss: 0.5931
 794/1500 [==============>...............] - ETA: 10:48 - loss: 3.1754 - regression_loss: 2.5827 - classification_loss: 0.5927
 795/1500 [==============>...............] - ETA: 10:47 - loss: 3.1740 - regression_loss: 2.5815 - classification_loss: 0.5924
 796/1500 [==============>...............] - ETA: 10:45 - loss: 3.1732 - regression_loss: 2.5812 - classification_loss: 0.5921
 797/1500 [==============>...............] - ETA: 10:44 - loss: 3.1723 - regression_loss: 2.5805 - classification_loss: 0.5919
 798/1500 [==============>...............] - ETA: 10:43 - loss: 3.1724 - regression_loss: 2.5806 - classification_loss: 0.5918
 799/1500 [==============>...............] - ETA: 10:42 - loss: 3.1721 - regression_loss: 2.5805 - classification_loss: 0.5915
 800/1500 [===============>..............] - ETA: 10:41 - loss: 3.1714 - regression_loss: 2.5802 - classification_loss: 0.5912
 801/1500 [===============>..............] - ETA: 10:40 - loss: 3.1703 - regression_loss: 2.5794 - classification_loss: 0.5909
 802/1500 [===============>..............] - ETA: 10:39 - loss: 3.1691 - regression_loss: 2.5786 - classification_loss: 0.5906
 803/1500 [===============>..............] - ETA: 10:38 - loss: 3.1684 - regression_loss: 2.5780 - classification_loss: 0.5904
 804/1500 [===============>..............] - ETA: 10:36 - loss: 3.1672 - regression_loss: 2.5771 - classification_loss: 0.5901
 805/1500 [===============>..............] - ETA: 10:35 - loss: 3.1661 - regression_loss: 2.5762 - classification_loss: 0.5899
 806/1500 [===============>..............] - ETA: 10:34 - loss: 3.1667 - regression_loss: 2.5767 - classification_loss: 0.5900
 807/1500 [===============>..............] - ETA: 10:33 - loss: 3.1657 - regression_loss: 2.5760 - classification_loss: 0.5897
 808/1500 [===============>..............] - ETA: 10:32 - loss: 3.1642 - regression_loss: 2.5749 - classification_loss: 0.5893
 809/1500 [===============>..............] - ETA: 10:31 - loss: 3.1644 - regression_loss: 2.5750 - classification_loss: 0.5893
 810/1500 [===============>..............] - ETA: 10:30 - loss: 3.1660 - regression_loss: 2.5747 - classification_loss: 0.5913
 811/1500 [===============>..............] - ETA: 10:29 - loss: 3.1657 - regression_loss: 2.5745 - classification_loss: 0.5912
 812/1500 [===============>..............] - ETA: 10:28 - loss: 3.1650 - regression_loss: 2.5742 - classification_loss: 0.5908
 813/1500 [===============>..............] - ETA: 10:28 - loss: 3.1650 - regression_loss: 2.5743 - classification_loss: 0.5907
 814/1500 [===============>..............] - ETA: 10:26 - loss: 3.1648 - regression_loss: 2.5744 - classification_loss: 0.5904
 815/1500 [===============>..............] - ETA: 10:25 - loss: 3.1667 - regression_loss: 2.5761 - classification_loss: 0.5906
 816/1500 [===============>..............] - ETA: 10:24 - loss: 3.1671 - regression_loss: 2.5763 - classification_loss: 0.5908
 817/1500 [===============>..............] - ETA: 10:22 - loss: 3.1680 - regression_loss: 2.5774 - classification_loss: 0.5906
 818/1500 [===============>..............] - ETA: 10:21 - loss: 3.1667 - regression_loss: 2.5766 - classification_loss: 0.5901
 819/1500 [===============>..............] - ETA: 10:21 - loss: 3.1662 - regression_loss: 2.5763 - classification_loss: 0.5899
 820/1500 [===============>..............] - ETA: 10:20 - loss: 3.1665 - regression_loss: 2.5762 - classification_loss: 0.5902
 821/1500 [===============>..............] - ETA: 10:21 - loss: 3.1649 - regression_loss: 2.5749 - classification_loss: 0.5899
 822/1500 [===============>..............] - ETA: 10:20 - loss: 3.1644 - regression_loss: 2.5745 - classification_loss: 0.5899
 823/1500 [===============>..............] - ETA: 10:18 - loss: 3.1633 - regression_loss: 2.5737 - classification_loss: 0.5896
 824/1500 [===============>..............] - ETA: 10:17 - loss: 3.1631 - regression_loss: 2.5736 - classification_loss: 0.5895
 825/1500 [===============>..............] - ETA: 10:16 - loss: 3.1620 - regression_loss: 2.5728 - classification_loss: 0.5891
 826/1500 [===============>..............] - ETA: 10:15 - loss: 3.1601 - regression_loss: 2.5714 - classification_loss: 0.5887
 827/1500 [===============>..............] - ETA: 10:14 - loss: 3.1584 - regression_loss: 2.5702 - classification_loss: 0.5883
 828/1500 [===============>..............] - ETA: 10:13 - loss: 3.1588 - regression_loss: 2.5707 - classification_loss: 0.5881
 829/1500 [===============>..............] - ETA: 10:12 - loss: 3.1578 - regression_loss: 2.5701 - classification_loss: 0.5878
 830/1500 [===============>..............] - ETA: 10:11 - loss: 3.1579 - regression_loss: 2.5702 - classification_loss: 0.5876
 831/1500 [===============>..............] - ETA: 10:11 - loss: 3.1566 - regression_loss: 2.5693 - classification_loss: 0.5874
 832/1500 [===============>..............] - ETA: 10:09 - loss: 3.1560 - regression_loss: 2.5688 - classification_loss: 0.5871
 833/1500 [===============>..............] - ETA: 10:08 - loss: 3.1563 - regression_loss: 2.5692 - classification_loss: 0.5871
 834/1500 [===============>..............] - ETA: 10:07 - loss: 3.1554 - regression_loss: 2.5687 - classification_loss: 0.5867
 835/1500 [===============>..............] - ETA: 10:06 - loss: 3.1546 - regression_loss: 2.5682 - classification_loss: 0.5864
 836/1500 [===============>..............] - ETA: 10:05 - loss: 3.1526 - regression_loss: 2.5666 - classification_loss: 0.5860
 837/1500 [===============>..............] - ETA: 10:04 - loss: 3.1523 - regression_loss: 2.5664 - classification_loss: 0.5858
 838/1500 [===============>..............] - ETA: 10:02 - loss: 3.1523 - regression_loss: 2.5665 - classification_loss: 0.5858
 839/1500 [===============>..............] - ETA: 10:01 - loss: 3.1512 - regression_loss: 2.5659 - classification_loss: 0.5854
 840/1500 [===============>..............] - ETA: 10:00 - loss: 3.1509 - regression_loss: 2.5657 - classification_loss: 0.5852
 841/1500 [===============>..............] - ETA: 9:59 - loss: 3.1500 - regression_loss: 2.5650 - classification_loss: 0.5850 
 842/1500 [===============>..............] - ETA: 9:58 - loss: 3.1487 - regression_loss: 2.5640 - classification_loss: 0.5847
 843/1500 [===============>..............] - ETA: 9:57 - loss: 3.1481 - regression_loss: 2.5638 - classification_loss: 0.5843
 844/1500 [===============>..............] - ETA: 9:55 - loss: 3.1482 - regression_loss: 2.5640 - classification_loss: 0.5842
 845/1500 [===============>..............] - ETA: 9:54 - loss: 3.1469 - regression_loss: 2.5630 - classification_loss: 0.5840
 846/1500 [===============>..............] - ETA: 9:53 - loss: 3.1455 - regression_loss: 2.5619 - classification_loss: 0.5835
 847/1500 [===============>..............] - ETA: 9:53 - loss: 3.1433 - regression_loss: 2.5602 - classification_loss: 0.5831
 848/1500 [===============>..............] - ETA: 9:52 - loss: 3.1429 - regression_loss: 2.5600 - classification_loss: 0.5829
 849/1500 [===============>..............] - ETA: 9:51 - loss: 3.1424 - regression_loss: 2.5598 - classification_loss: 0.5826
 850/1500 [================>.............] - ETA: 9:51 - loss: 3.1421 - regression_loss: 2.5596 - classification_loss: 0.5825
 851/1500 [================>.............] - ETA: 9:49 - loss: 3.1420 - regression_loss: 2.5598 - classification_loss: 0.5822
 852/1500 [================>.............] - ETA: 9:48 - loss: 3.1410 - regression_loss: 2.5590 - classification_loss: 0.5820
 853/1500 [================>.............] - ETA: 9:47 - loss: 3.1402 - regression_loss: 2.5584 - classification_loss: 0.5818
 854/1500 [================>.............] - ETA: 9:45 - loss: 3.1398 - regression_loss: 2.5581 - classification_loss: 0.5817
 855/1500 [================>.............] - ETA: 9:45 - loss: 3.1379 - regression_loss: 2.5565 - classification_loss: 0.5814
 856/1500 [================>.............] - ETA: 9:43 - loss: 3.1378 - regression_loss: 2.5565 - classification_loss: 0.5813
 857/1500 [================>.............] - ETA: 9:42 - loss: 3.1377 - regression_loss: 2.5565 - classification_loss: 0.5812
 858/1500 [================>.............] - ETA: 9:41 - loss: 3.1365 - regression_loss: 2.5556 - classification_loss: 0.5809
 859/1500 [================>.............] - ETA: 9:40 - loss: 3.1359 - regression_loss: 2.5552 - classification_loss: 0.5807
 860/1500 [================>.............] - ETA: 9:39 - loss: 3.1351 - regression_loss: 2.5545 - classification_loss: 0.5806
 861/1500 [================>.............] - ETA: 9:38 - loss: 3.1344 - regression_loss: 2.5540 - classification_loss: 0.5803
 862/1500 [================>.............] - ETA: 9:37 - loss: 3.1348 - regression_loss: 2.5536 - classification_loss: 0.5812
 863/1500 [================>.............] - ETA: 9:36 - loss: 3.1345 - regression_loss: 2.5535 - classification_loss: 0.5809
 864/1500 [================>.............] - ETA: 9:35 - loss: 3.1320 - regression_loss: 2.5514 - classification_loss: 0.5805
 865/1500 [================>.............] - ETA: 9:34 - loss: 3.1311 - regression_loss: 2.5509 - classification_loss: 0.5801
 866/1500 [================>.............] - ETA: 9:33 - loss: 3.1297 - regression_loss: 2.5499 - classification_loss: 0.5798
 867/1500 [================>.............] - ETA: 9:32 - loss: 3.1295 - regression_loss: 2.5497 - classification_loss: 0.5798
 868/1500 [================>.............] - ETA: 9:31 - loss: 3.1285 - regression_loss: 2.5490 - classification_loss: 0.5794
 869/1500 [================>.............] - ETA: 9:30 - loss: 3.1272 - regression_loss: 2.5480 - classification_loss: 0.5792
 870/1500 [================>.............] - ETA: 9:29 - loss: 3.1266 - regression_loss: 2.5476 - classification_loss: 0.5790
 871/1500 [================>.............] - ETA: 9:27 - loss: 3.1260 - regression_loss: 2.5473 - classification_loss: 0.5788
 872/1500 [================>.............] - ETA: 9:26 - loss: 3.1243 - regression_loss: 2.5459 - classification_loss: 0.5784
 873/1500 [================>.............] - ETA: 9:25 - loss: 3.1226 - regression_loss: 2.5447 - classification_loss: 0.5780
 874/1500 [================>.............] - ETA: 9:24 - loss: 3.1231 - regression_loss: 2.5453 - classification_loss: 0.5778
 875/1500 [================>.............] - ETA: 9:24 - loss: 3.1231 - regression_loss: 2.5454 - classification_loss: 0.5777
 876/1500 [================>.............] - ETA: 9:23 - loss: 3.1221 - regression_loss: 2.5446 - classification_loss: 0.5775
 877/1500 [================>.............] - ETA: 9:21 - loss: 3.1217 - regression_loss: 2.5445 - classification_loss: 0.5771
 878/1500 [================>.............] - ETA: 9:21 - loss: 3.1214 - regression_loss: 2.5444 - classification_loss: 0.5769
 879/1500 [================>.............] - ETA: 9:21 - loss: 3.1213 - regression_loss: 2.5444 - classification_loss: 0.5769
 880/1500 [================>.............] - ETA: 9:20 - loss: 3.1210 - regression_loss: 2.5441 - classification_loss: 0.5769
 881/1500 [================>.............] - ETA: 9:19 - loss: 3.1200 - regression_loss: 2.5434 - classification_loss: 0.5767
 882/1500 [================>.............] - ETA: 9:17 - loss: 3.1195 - regression_loss: 2.5430 - classification_loss: 0.5765
 883/1500 [================>.............] - ETA: 9:17 - loss: 3.1190 - regression_loss: 2.5429 - classification_loss: 0.5761
 884/1500 [================>.............] - ETA: 9:16 - loss: 3.1194 - regression_loss: 2.5431 - classification_loss: 0.5763
 885/1500 [================>.............] - ETA: 9:15 - loss: 3.1187 - regression_loss: 2.5427 - classification_loss: 0.5761
 886/1500 [================>.............] - ETA: 9:14 - loss: 3.1193 - regression_loss: 2.5423 - classification_loss: 0.5770
 887/1500 [================>.............] - ETA: 9:13 - loss: 3.1187 - regression_loss: 2.5420 - classification_loss: 0.5767
 888/1500 [================>.............] - ETA: 9:12 - loss: 3.1187 - regression_loss: 2.5421 - classification_loss: 0.5766
 889/1500 [================>.............] - ETA: 9:12 - loss: 3.1192 - regression_loss: 2.5425 - classification_loss: 0.5767
 890/1500 [================>.............] - ETA: 9:10 - loss: 3.1199 - regression_loss: 2.5431 - classification_loss: 0.5768
 891/1500 [================>.............] - ETA: 9:10 - loss: 3.1199 - regression_loss: 2.5432 - classification_loss: 0.5767
 892/1500 [================>.............] - ETA: 9:09 - loss: 3.1197 - regression_loss: 2.5428 - classification_loss: 0.5769
 893/1500 [================>.............] - ETA: 9:08 - loss: 3.1187 - regression_loss: 2.5421 - classification_loss: 0.5766
 894/1500 [================>.............] - ETA: 9:09 - loss: 3.1201 - regression_loss: 2.5433 - classification_loss: 0.5768
 895/1500 [================>.............] - ETA: 9:08 - loss: 3.1194 - regression_loss: 2.5426 - classification_loss: 0.5768
 896/1500 [================>.............] - ETA: 9:07 - loss: 3.1208 - regression_loss: 2.5439 - classification_loss: 0.5769
 897/1500 [================>.............] - ETA: 9:07 - loss: 3.1208 - regression_loss: 2.5440 - classification_loss: 0.5769
 898/1500 [================>.............] - ETA: 9:06 - loss: 3.1206 - regression_loss: 2.5438 - classification_loss: 0.5767
 899/1500 [================>.............] - ETA: 9:04 - loss: 3.1207 - regression_loss: 2.5440 - classification_loss: 0.5768
 900/1500 [=================>............] - ETA: 9:03 - loss: 3.1210 - regression_loss: 2.5440 - classification_loss: 0.5769
 901/1500 [=================>............] - ETA: 9:02 - loss: 3.1205 - regression_loss: 2.5438 - classification_loss: 0.5767
 902/1500 [=================>............] - ETA: 9:02 - loss: 3.1207 - regression_loss: 2.5438 - classification_loss: 0.5769
 903/1500 [=================>............] - ETA: 9:00 - loss: 3.1202 - regression_loss: 2.5432 - classification_loss: 0.5769
 904/1500 [=================>............] - ETA: 8:59 - loss: 3.1205 - regression_loss: 2.5437 - classification_loss: 0.5768
 905/1500 [=================>............] - ETA: 8:59 - loss: 3.1205 - regression_loss: 2.5438 - classification_loss: 0.5767
 906/1500 [=================>............] - ETA: 8:57 - loss: 3.1208 - regression_loss: 2.5442 - classification_loss: 0.5766
 907/1500 [=================>............] - ETA: 8:56 - loss: 3.1203 - regression_loss: 2.5439 - classification_loss: 0.5764
 908/1500 [=================>............] - ETA: 8:55 - loss: 3.1191 - regression_loss: 2.5429 - classification_loss: 0.5762
 909/1500 [=================>............] - ETA: 8:54 - loss: 3.1187 - regression_loss: 2.5428 - classification_loss: 0.5759
 910/1500 [=================>............] - ETA: 8:53 - loss: 3.1184 - regression_loss: 2.5426 - classification_loss: 0.5758
 911/1500 [=================>............] - ETA: 8:52 - loss: 3.1180 - regression_loss: 2.5424 - classification_loss: 0.5756
 912/1500 [=================>............] - ETA: 8:51 - loss: 3.1182 - regression_loss: 2.5426 - classification_loss: 0.5756
 913/1500 [=================>............] - ETA: 8:50 - loss: 3.1180 - regression_loss: 2.5424 - classification_loss: 0.5755
 914/1500 [=================>............] - ETA: 8:49 - loss: 3.1171 - regression_loss: 2.5418 - classification_loss: 0.5754
 915/1500 [=================>............] - ETA: 8:49 - loss: 3.1159 - regression_loss: 2.5408 - classification_loss: 0.5752
 916/1500 [=================>............] - ETA: 8:49 - loss: 3.1160 - regression_loss: 2.5408 - classification_loss: 0.5751
 917/1500 [=================>............] - ETA: 8:48 - loss: 3.1149 - regression_loss: 2.5400 - classification_loss: 0.5749
 918/1500 [=================>............] - ETA: 8:47 - loss: 3.1145 - regression_loss: 2.5397 - classification_loss: 0.5748
 919/1500 [=================>............] - ETA: 8:46 - loss: 3.1136 - regression_loss: 2.5390 - classification_loss: 0.5745
 920/1500 [=================>............] - ETA: 8:44 - loss: 3.1134 - regression_loss: 2.5391 - classification_loss: 0.5743
 921/1500 [=================>............] - ETA: 8:44 - loss: 3.1135 - regression_loss: 2.5392 - classification_loss: 0.5743
 922/1500 [=================>............] - ETA: 8:42 - loss: 3.1118 - regression_loss: 2.5379 - classification_loss: 0.5739
 923/1500 [=================>............] - ETA: 8:42 - loss: 3.1117 - regression_loss: 2.5380 - classification_loss: 0.5738
 924/1500 [=================>............] - ETA: 8:41 - loss: 3.1111 - regression_loss: 2.5376 - classification_loss: 0.5735
 925/1500 [=================>............] - ETA: 8:40 - loss: 3.1113 - regression_loss: 2.5379 - classification_loss: 0.5733
 926/1500 [=================>............] - ETA: 8:39 - loss: 3.1097 - regression_loss: 2.5367 - classification_loss: 0.5730
 927/1500 [=================>............] - ETA: 8:37 - loss: 3.1100 - regression_loss: 2.5370 - classification_loss: 0.5729
 928/1500 [=================>............] - ETA: 8:36 - loss: 3.1101 - regression_loss: 2.5372 - classification_loss: 0.5729
 929/1500 [=================>............] - ETA: 8:35 - loss: 3.1084 - regression_loss: 2.5359 - classification_loss: 0.5726
 930/1500 [=================>............] - ETA: 8:34 - loss: 3.1080 - regression_loss: 2.5357 - classification_loss: 0.5723
 931/1500 [=================>............] - ETA: 8:33 - loss: 3.1085 - regression_loss: 2.5363 - classification_loss: 0.5722
 932/1500 [=================>............] - ETA: 8:32 - loss: 3.1078 - regression_loss: 2.5358 - classification_loss: 0.5721
 933/1500 [=================>............] - ETA: 8:31 - loss: 3.1072 - regression_loss: 2.5354 - classification_loss: 0.5718
 934/1500 [=================>............] - ETA: 8:30 - loss: 3.1067 - regression_loss: 2.5351 - classification_loss: 0.5716
 935/1500 [=================>............] - ETA: 8:30 - loss: 3.1072 - regression_loss: 2.5356 - classification_loss: 0.5716
 936/1500 [=================>............] - ETA: 8:29 - loss: 3.1068 - regression_loss: 2.5353 - classification_loss: 0.5715
 937/1500 [=================>............] - ETA: 8:28 - loss: 3.1067 - regression_loss: 2.5353 - classification_loss: 0.5714
 938/1500 [=================>............] - ETA: 8:27 - loss: 3.1070 - regression_loss: 2.5349 - classification_loss: 0.5720
 939/1500 [=================>............] - ETA: 8:26 - loss: 3.1068 - regression_loss: 2.5349 - classification_loss: 0.5718
 940/1500 [=================>............] - ETA: 8:25 - loss: 3.1067 - regression_loss: 2.5349 - classification_loss: 0.5719
 941/1500 [=================>............] - ETA: 8:24 - loss: 3.1062 - regression_loss: 2.5344 - classification_loss: 0.5718
 942/1500 [=================>............] - ETA: 8:23 - loss: 3.1054 - regression_loss: 2.5339 - classification_loss: 0.5716
 943/1500 [=================>............] - ETA: 8:22 - loss: 3.1049 - regression_loss: 2.5336 - classification_loss: 0.5713
 944/1500 [=================>............] - ETA: 8:21 - loss: 3.1044 - regression_loss: 2.5333 - classification_loss: 0.5711
 945/1500 [=================>............] - ETA: 8:19 - loss: 3.1043 - regression_loss: 2.5332 - classification_loss: 0.5711
 946/1500 [=================>............] - ETA: 8:19 - loss: 3.1039 - regression_loss: 2.5332 - classification_loss: 0.5707
 947/1500 [=================>............] - ETA: 8:17 - loss: 3.1029 - regression_loss: 2.5325 - classification_loss: 0.5704
 948/1500 [=================>............] - ETA: 8:16 - loss: 3.1019 - regression_loss: 2.5318 - classification_loss: 0.5701
 949/1500 [=================>............] - ETA: 8:15 - loss: 3.1009 - regression_loss: 2.5311 - classification_loss: 0.5698
 950/1500 [==================>...........] - ETA: 8:14 - loss: 3.1004 - regression_loss: 2.5309 - classification_loss: 0.5695
 951/1500 [==================>...........] - ETA: 8:13 - loss: 3.1006 - regression_loss: 2.5310 - classification_loss: 0.5696
 952/1500 [==================>...........] - ETA: 8:12 - loss: 3.1004 - regression_loss: 2.5310 - classification_loss: 0.5694
 953/1500 [==================>...........] - ETA: 8:11 - loss: 3.1000 - regression_loss: 2.5309 - classification_loss: 0.5691
 954/1500 [==================>...........] - ETA: 8:09 - loss: 3.0989 - regression_loss: 2.5301 - classification_loss: 0.5688
 955/1500 [==================>...........] - ETA: 8:08 - loss: 3.0995 - regression_loss: 2.5305 - classification_loss: 0.5690
 956/1500 [==================>...........] - ETA: 8:07 - loss: 3.0993 - regression_loss: 2.5304 - classification_loss: 0.5689
 957/1500 [==================>...........] - ETA: 8:06 - loss: 3.0983 - regression_loss: 2.5296 - classification_loss: 0.5687
 958/1500 [==================>...........] - ETA: 8:05 - loss: 3.0989 - regression_loss: 2.5302 - classification_loss: 0.5688
 959/1500 [==================>...........] - ETA: 8:04 - loss: 3.0985 - regression_loss: 2.5298 - classification_loss: 0.5687
 960/1500 [==================>...........] - ETA: 8:03 - loss: 3.0981 - regression_loss: 2.5296 - classification_loss: 0.5685
 961/1500 [==================>...........] - ETA: 8:02 - loss: 3.0969 - regression_loss: 2.5286 - classification_loss: 0.5683
 962/1500 [==================>...........] - ETA: 8:01 - loss: 3.0958 - regression_loss: 2.5277 - classification_loss: 0.5680
 963/1500 [==================>...........] - ETA: 8:00 - loss: 3.0956 - regression_loss: 2.5274 - classification_loss: 0.5682
 964/1500 [==================>...........] - ETA: 7:59 - loss: 3.0945 - regression_loss: 2.5266 - classification_loss: 0.5679
 965/1500 [==================>...........] - ETA: 7:58 - loss: 3.0944 - regression_loss: 2.5267 - classification_loss: 0.5678
 966/1500 [==================>...........] - ETA: 7:57 - loss: 3.0943 - regression_loss: 2.5267 - classification_loss: 0.5676
 967/1500 [==================>...........] - ETA: 7:56 - loss: 3.0940 - regression_loss: 2.5267 - classification_loss: 0.5674
 968/1500 [==================>...........] - ETA: 7:55 - loss: 3.0936 - regression_loss: 2.5263 - classification_loss: 0.5673
 969/1500 [==================>...........] - ETA: 7:54 - loss: 3.0927 - regression_loss: 2.5257 - classification_loss: 0.5670
 970/1500 [==================>...........] - ETA: 7:54 - loss: 3.0918 - regression_loss: 2.5252 - classification_loss: 0.5666
 971/1500 [==================>...........] - ETA: 7:53 - loss: 3.0915 - regression_loss: 2.5251 - classification_loss: 0.5664
 972/1500 [==================>...........] - ETA: 7:52 - loss: 3.0902 - regression_loss: 2.5241 - classification_loss: 0.5661
 973/1500 [==================>...........] - ETA: 7:51 - loss: 3.0888 - regression_loss: 2.5231 - classification_loss: 0.5657
 974/1500 [==================>...........] - ETA: 7:50 - loss: 3.0877 - regression_loss: 2.5222 - classification_loss: 0.5655
 975/1500 [==================>...........] - ETA: 7:49 - loss: 3.0876 - regression_loss: 2.5222 - classification_loss: 0.5655
 976/1500 [==================>...........] - ETA: 7:48 - loss: 3.0876 - regression_loss: 2.5223 - classification_loss: 0.5653
 977/1500 [==================>...........] - ETA: 7:47 - loss: 3.0871 - regression_loss: 2.5218 - classification_loss: 0.5653
 978/1500 [==================>...........] - ETA: 7:46 - loss: 3.0859 - regression_loss: 2.5209 - classification_loss: 0.5650
 979/1500 [==================>...........] - ETA: 7:45 - loss: 3.0857 - regression_loss: 2.5207 - classification_loss: 0.5650
 980/1500 [==================>...........] - ETA: 7:44 - loss: 3.0863 - regression_loss: 2.5212 - classification_loss: 0.5650
 981/1500 [==================>...........] - ETA: 7:43 - loss: 3.0869 - regression_loss: 2.5219 - classification_loss: 0.5650
 982/1500 [==================>...........] - ETA: 7:42 - loss: 3.0879 - regression_loss: 2.5227 - classification_loss: 0.5652
 983/1500 [==================>...........] - ETA: 7:41 - loss: 3.0861 - regression_loss: 2.5213 - classification_loss: 0.5648
 984/1500 [==================>...........] - ETA: 7:40 - loss: 3.0845 - regression_loss: 2.5201 - classification_loss: 0.5644
 985/1500 [==================>...........] - ETA: 7:39 - loss: 3.0837 - regression_loss: 2.5194 - classification_loss: 0.5643
 986/1500 [==================>...........] - ETA: 7:38 - loss: 3.0839 - regression_loss: 2.5197 - classification_loss: 0.5643
 987/1500 [==================>...........] - ETA: 7:38 - loss: 3.0826 - regression_loss: 2.5186 - classification_loss: 0.5639
 988/1500 [==================>...........] - ETA: 7:37 - loss: 3.0832 - regression_loss: 2.5192 - classification_loss: 0.5640
 989/1500 [==================>...........] - ETA: 7:36 - loss: 3.0822 - regression_loss: 2.5185 - classification_loss: 0.5637
 990/1500 [==================>...........] - ETA: 7:35 - loss: 3.0817 - regression_loss: 2.5183 - classification_loss: 0.5634
 991/1500 [==================>...........] - ETA: 7:34 - loss: 3.0809 - regression_loss: 2.5176 - classification_loss: 0.5633
 992/1500 [==================>...........] - ETA: 7:33 - loss: 3.0800 - regression_loss: 2.5168 - classification_loss: 0.5632
 993/1500 [==================>...........] - ETA: 7:32 - loss: 3.0791 - regression_loss: 2.5162 - classification_loss: 0.5629
 994/1500 [==================>...........] - ETA: 7:32 - loss: 3.0792 - regression_loss: 2.5163 - classification_loss: 0.5629
 995/1500 [==================>...........] - ETA: 7:31 - loss: 3.0793 - regression_loss: 2.5166 - classification_loss: 0.5627
 996/1500 [==================>...........] - ETA: 7:31 - loss: 3.0791 - regression_loss: 2.5167 - classification_loss: 0.5625
 997/1500 [==================>...........] - ETA: 7:30 - loss: 3.0789 - regression_loss: 2.5166 - classification_loss: 0.5623
 998/1500 [==================>...........] - ETA: 7:29 - loss: 3.0787 - regression_loss: 2.5167 - classification_loss: 0.5621
 999/1500 [==================>...........] - ETA: 7:28 - loss: 3.0786 - regression_loss: 2.5165 - classification_loss: 0.5621
1000/1500 [===================>..........] - ETA: 7:27 - loss: 3.0780 - regression_loss: 2.5162 - classification_loss: 0.5617
1001/1500 [===================>..........] - ETA: 7:26 - loss: 3.0775 - regression_loss: 2.5158 - classification_loss: 0.5617
1002/1500 [===================>..........] - ETA: 7:25 - loss: 3.0772 - regression_loss: 2.5156 - classification_loss: 0.5616
1003/1500 [===================>..........] - ETA: 7:24 - loss: 3.0766 - regression_loss: 2.5150 - classification_loss: 0.5616
1004/1500 [===================>..........] - ETA: 7:22 - loss: 3.0755 - regression_loss: 2.5142 - classification_loss: 0.5614
1005/1500 [===================>..........] - ETA: 7:22 - loss: 3.0742 - regression_loss: 2.5132 - classification_loss: 0.5610
1006/1500 [===================>..........] - ETA: 7:21 - loss: 3.0739 - regression_loss: 2.5132 - classification_loss: 0.5608
1007/1500 [===================>..........] - ETA: 7:20 - loss: 3.0734 - regression_loss: 2.5128 - classification_loss: 0.5605
1008/1500 [===================>..........] - ETA: 7:19 - loss: 3.0721 - regression_loss: 2.5118 - classification_loss: 0.5603
1009/1500 [===================>..........] - ETA: 7:18 - loss: 3.0720 - regression_loss: 2.5117 - classification_loss: 0.5603
1010/1500 [===================>..........] - ETA: 7:18 - loss: 3.0723 - regression_loss: 2.5122 - classification_loss: 0.5601
1011/1500 [===================>..........] - ETA: 7:17 - loss: 3.0722 - regression_loss: 2.5122 - classification_loss: 0.5600
1012/1500 [===================>..........] - ETA: 7:16 - loss: 3.0722 - regression_loss: 2.5121 - classification_loss: 0.5601
1013/1500 [===================>..........] - ETA: 7:15 - loss: 3.0710 - regression_loss: 2.5107 - classification_loss: 0.5602
1014/1500 [===================>..........] - ETA: 7:14 - loss: 3.0718 - regression_loss: 2.5117 - classification_loss: 0.5600
1015/1500 [===================>..........] - ETA: 7:13 - loss: 3.0710 - regression_loss: 2.5112 - classification_loss: 0.5598
1016/1500 [===================>..........] - ETA: 7:12 - loss: 3.0705 - regression_loss: 2.5106 - classification_loss: 0.5599
1017/1500 [===================>..........] - ETA: 7:11 - loss: 3.0697 - regression_loss: 2.5101 - classification_loss: 0.5596
1018/1500 [===================>..........] - ETA: 7:11 - loss: 3.0689 - regression_loss: 2.5095 - classification_loss: 0.5593
1019/1500 [===================>..........] - ETA: 7:10 - loss: 3.0677 - regression_loss: 2.5086 - classification_loss: 0.5591
1020/1500 [===================>..........] - ETA: 7:09 - loss: 3.0666 - regression_loss: 2.5077 - classification_loss: 0.5589
1021/1500 [===================>..........] - ETA: 7:08 - loss: 3.0668 - regression_loss: 2.5079 - classification_loss: 0.5589
1022/1500 [===================>..........] - ETA: 7:07 - loss: 3.0657 - regression_loss: 2.5070 - classification_loss: 0.5586
1023/1500 [===================>..........] - ETA: 7:06 - loss: 3.0640 - regression_loss: 2.5058 - classification_loss: 0.5583
1024/1500 [===================>..........] - ETA: 7:05 - loss: 3.0640 - regression_loss: 2.5057 - classification_loss: 0.5583
1025/1500 [===================>..........] - ETA: 7:04 - loss: 3.0624 - regression_loss: 2.5045 - classification_loss: 0.5579
1026/1500 [===================>..........] - ETA: 7:03 - loss: 3.0611 - regression_loss: 2.5035 - classification_loss: 0.5576
1027/1500 [===================>..........] - ETA: 7:02 - loss: 3.0614 - regression_loss: 2.5040 - classification_loss: 0.5574
1028/1500 [===================>..........] - ETA: 7:01 - loss: 3.0614 - regression_loss: 2.5040 - classification_loss: 0.5575
1029/1500 [===================>..........] - ETA: 7:00 - loss: 3.0611 - regression_loss: 2.5038 - classification_loss: 0.5573
1030/1500 [===================>..........] - ETA: 6:59 - loss: 3.0609 - regression_loss: 2.5037 - classification_loss: 0.5572
1031/1500 [===================>..........] - ETA: 6:58 - loss: 3.0609 - regression_loss: 2.5038 - classification_loss: 0.5571
1032/1500 [===================>..........] - ETA: 6:58 - loss: 3.0613 - regression_loss: 2.5040 - classification_loss: 0.5573
1033/1500 [===================>..........] - ETA: 6:57 - loss: 3.0607 - regression_loss: 2.5036 - classification_loss: 0.5571
1034/1500 [===================>..........] - ETA: 6:57 - loss: 3.0607 - regression_loss: 2.5037 - classification_loss: 0.5570
1035/1500 [===================>..........] - ETA: 6:56 - loss: 3.0590 - regression_loss: 2.5024 - classification_loss: 0.5567
1036/1500 [===================>..........] - ETA: 6:55 - loss: 3.0583 - regression_loss: 2.5019 - classification_loss: 0.5564
1037/1500 [===================>..........] - ETA: 6:54 - loss: 3.0585 - regression_loss: 2.5013 - classification_loss: 0.5572
1038/1500 [===================>..........] - ETA: 6:54 - loss: 3.0569 - regression_loss: 2.5000 - classification_loss: 0.5569
1039/1500 [===================>..........] - ETA: 6:53 - loss: 3.0570 - regression_loss: 2.5002 - classification_loss: 0.5568
1040/1500 [===================>..........] - ETA: 6:52 - loss: 3.0553 - regression_loss: 2.4990 - classification_loss: 0.5564
1041/1500 [===================>..........] - ETA: 6:51 - loss: 3.0551 - regression_loss: 2.4988 - classification_loss: 0.5563
1042/1500 [===================>..........] - ETA: 6:50 - loss: 3.0537 - regression_loss: 2.4977 - classification_loss: 0.5560
1043/1500 [===================>..........] - ETA: 6:49 - loss: 3.0546 - regression_loss: 2.4983 - classification_loss: 0.5563
1044/1500 [===================>..........] - ETA: 6:48 - loss: 3.0543 - regression_loss: 2.4980 - classification_loss: 0.5563
1045/1500 [===================>..........] - ETA: 6:47 - loss: 3.0548 - regression_loss: 2.4985 - classification_loss: 0.5564
1046/1500 [===================>..........] - ETA: 6:46 - loss: 3.0544 - regression_loss: 2.4982 - classification_loss: 0.5562
1047/1500 [===================>..........] - ETA: 6:46 - loss: 3.0546 - regression_loss: 2.4985 - classification_loss: 0.5562
1048/1500 [===================>..........] - ETA: 6:45 - loss: 3.0545 - regression_loss: 2.4984 - classification_loss: 0.5561
1049/1500 [===================>..........] - ETA: 6:44 - loss: 3.0542 - regression_loss: 2.4984 - classification_loss: 0.5558
1050/1500 [====================>.........] - ETA: 6:43 - loss: 3.0546 - regression_loss: 2.4986 - classification_loss: 0.5560
1051/1500 [====================>.........] - ETA: 6:42 - loss: 3.0535 - regression_loss: 2.4978 - classification_loss: 0.5557
1052/1500 [====================>.........] - ETA: 6:41 - loss: 3.0529 - regression_loss: 2.4976 - classification_loss: 0.5554
1053/1500 [====================>.........] - ETA: 6:40 - loss: 3.0528 - regression_loss: 2.4975 - classification_loss: 0.5553
1054/1500 [====================>.........] - ETA: 6:39 - loss: 3.0531 - regression_loss: 2.4979 - classification_loss: 0.5552
1055/1500 [====================>.........] - ETA: 6:38 - loss: 3.0523 - regression_loss: 2.4973 - classification_loss: 0.5550
1056/1500 [====================>.........] - ETA: 6:37 - loss: 3.0521 - regression_loss: 2.4973 - classification_loss: 0.5548
1057/1500 [====================>.........] - ETA: 6:36 - loss: 3.0511 - regression_loss: 2.4966 - classification_loss: 0.5545
1058/1500 [====================>.........] - ETA: 6:35 - loss: 3.0505 - regression_loss: 2.4961 - classification_loss: 0.5544
1059/1500 [====================>.........] - ETA: 6:34 - loss: 3.0501 - regression_loss: 2.4957 - classification_loss: 0.5544
1060/1500 [====================>.........] - ETA: 6:34 - loss: 3.0504 - regression_loss: 2.4960 - classification_loss: 0.5543
1061/1500 [====================>.........] - ETA: 6:32 - loss: 3.0489 - regression_loss: 2.4949 - classification_loss: 0.5541
1062/1500 [====================>.........] - ETA: 6:31 - loss: 3.0482 - regression_loss: 2.4944 - classification_loss: 0.5538
1063/1500 [====================>.........] - ETA: 6:30 - loss: 3.0472 - regression_loss: 2.4936 - classification_loss: 0.5536
1064/1500 [====================>.........] - ETA: 6:29 - loss: 3.0473 - regression_loss: 2.4937 - classification_loss: 0.5536
1065/1500 [====================>.........] - ETA: 6:28 - loss: 3.0472 - regression_loss: 2.4937 - classification_loss: 0.5535
1066/1500 [====================>.........] - ETA: 6:27 - loss: 3.0468 - regression_loss: 2.4934 - classification_loss: 0.5533
1067/1500 [====================>.........] - ETA: 6:26 - loss: 3.0466 - regression_loss: 2.4934 - classification_loss: 0.5532
1068/1500 [====================>.........] - ETA: 6:25 - loss: 3.0459 - regression_loss: 2.4928 - classification_loss: 0.5531
1069/1500 [====================>.........] - ETA: 6:25 - loss: 3.0457 - regression_loss: 2.4926 - classification_loss: 0.5531
1070/1500 [====================>.........] - ETA: 6:24 - loss: 3.0452 - regression_loss: 2.4923 - classification_loss: 0.5529
1071/1500 [====================>.........] - ETA: 6:23 - loss: 3.0442 - regression_loss: 2.4916 - classification_loss: 0.5526
1072/1500 [====================>.........] - ETA: 6:22 - loss: 3.0438 - regression_loss: 2.4913 - classification_loss: 0.5524
1073/1500 [====================>.........] - ETA: 6:21 - loss: 3.0438 - regression_loss: 2.4913 - classification_loss: 0.5525
1074/1500 [====================>.........] - ETA: 6:20 - loss: 3.0450 - regression_loss: 2.4915 - classification_loss: 0.5534
1075/1500 [====================>.........] - ETA: 6:19 - loss: 3.0438 - regression_loss: 2.4907 - classification_loss: 0.5532
1076/1500 [====================>.........] - ETA: 6:18 - loss: 3.0439 - regression_loss: 2.4907 - classification_loss: 0.5532
1077/1500 [====================>.........] - ETA: 6:18 - loss: 3.0426 - regression_loss: 2.4897 - classification_loss: 0.5529
1078/1500 [====================>.........] - ETA: 6:17 - loss: 3.0419 - regression_loss: 2.4893 - classification_loss: 0.5526
1079/1500 [====================>.........] - ETA: 6:16 - loss: 3.0419 - regression_loss: 2.4893 - classification_loss: 0.5526
1080/1500 [====================>.........] - ETA: 6:15 - loss: 3.0413 - regression_loss: 2.4888 - classification_loss: 0.5525
1081/1500 [====================>.........] - ETA: 6:14 - loss: 3.0396 - regression_loss: 2.4874 - classification_loss: 0.5522
1082/1500 [====================>.........] - ETA: 6:13 - loss: 3.0400 - regression_loss: 2.4878 - classification_loss: 0.5523
1083/1500 [====================>.........] - ETA: 6:11 - loss: 3.0396 - regression_loss: 2.4874 - classification_loss: 0.5522
1084/1500 [====================>.........] - ETA: 6:10 - loss: 3.0385 - regression_loss: 2.4865 - classification_loss: 0.5520
1085/1500 [====================>.........] - ETA: 6:10 - loss: 3.0375 - regression_loss: 2.4858 - classification_loss: 0.5517
1086/1500 [====================>.........] - ETA: 6:09 - loss: 3.0362 - regression_loss: 2.4849 - classification_loss: 0.5514
1087/1500 [====================>.........] - ETA: 6:08 - loss: 3.0359 - regression_loss: 2.4845 - classification_loss: 0.5513
1088/1500 [====================>.........] - ETA: 6:07 - loss: 3.0355 - regression_loss: 2.4844 - classification_loss: 0.5511
1089/1500 [====================>.........] - ETA: 6:06 - loss: 3.0347 - regression_loss: 2.4838 - classification_loss: 0.5509
1090/1500 [====================>.........] - ETA: 6:05 - loss: 3.0336 - regression_loss: 2.4830 - classification_loss: 0.5506
1091/1500 [====================>.........] - ETA: 6:04 - loss: 3.0326 - regression_loss: 2.4821 - classification_loss: 0.5505
1092/1500 [====================>.........] - ETA: 6:03 - loss: 3.0313 - regression_loss: 2.4811 - classification_loss: 0.5501
1093/1500 [====================>.........] - ETA: 6:02 - loss: 3.0305 - regression_loss: 2.4806 - classification_loss: 0.5499
1094/1500 [====================>.........] - ETA: 6:01 - loss: 3.0296 - regression_loss: 2.4797 - classification_loss: 0.5498
1095/1500 [====================>.........] - ETA: 6:00 - loss: 3.0295 - regression_loss: 2.4798 - classification_loss: 0.5497
1096/1500 [====================>.........] - ETA: 5:59 - loss: 3.0294 - regression_loss: 2.4797 - classification_loss: 0.5497
1097/1500 [====================>.........] - ETA: 5:58 - loss: 3.0291 - regression_loss: 2.4794 - classification_loss: 0.5498
1098/1500 [====================>.........] - ETA: 5:58 - loss: 3.0277 - regression_loss: 2.4783 - classification_loss: 0.5494
1099/1500 [====================>.........] - ETA: 5:56 - loss: 3.0282 - regression_loss: 2.4788 - classification_loss: 0.5494
1100/1500 [=====================>........] - ETA: 5:55 - loss: 3.0270 - regression_loss: 2.4779 - classification_loss: 0.5491
1101/1500 [=====================>........] - ETA: 5:54 - loss: 3.0260 - regression_loss: 2.4772 - classification_loss: 0.5488
1102/1500 [=====================>........] - ETA: 5:53 - loss: 3.0257 - regression_loss: 2.4770 - classification_loss: 0.5487
1103/1500 [=====================>........] - ETA: 5:53 - loss: 3.0250 - regression_loss: 2.4766 - classification_loss: 0.5484
1104/1500 [=====================>........] - ETA: 5:52 - loss: 3.0243 - regression_loss: 2.4761 - classification_loss: 0.5483
1105/1500 [=====================>........] - ETA: 5:50 - loss: 3.0241 - regression_loss: 2.4761 - classification_loss: 0.5480
1106/1500 [=====================>........] - ETA: 5:50 - loss: 3.0241 - regression_loss: 2.4760 - classification_loss: 0.5481
1107/1500 [=====================>........] - ETA: 5:49 - loss: 3.0237 - regression_loss: 2.4758 - classification_loss: 0.5479
1108/1500 [=====================>........] - ETA: 5:48 - loss: 3.0225 - regression_loss: 2.4749 - classification_loss: 0.5476
1109/1500 [=====================>........] - ETA: 5:47 - loss: 3.0209 - regression_loss: 2.4736 - classification_loss: 0.5473
1110/1500 [=====================>........] - ETA: 5:46 - loss: 3.0201 - regression_loss: 2.4730 - classification_loss: 0.5471
1111/1500 [=====================>........] - ETA: 5:45 - loss: 3.0196 - regression_loss: 2.4729 - classification_loss: 0.5468
1112/1500 [=====================>........] - ETA: 5:44 - loss: 3.0191 - regression_loss: 2.4723 - classification_loss: 0.5468
1113/1500 [=====================>........] - ETA: 5:43 - loss: 3.0187 - regression_loss: 2.4720 - classification_loss: 0.5467
1114/1500 [=====================>........] - ETA: 5:43 - loss: 3.0186 - regression_loss: 2.4720 - classification_loss: 0.5467
1115/1500 [=====================>........] - ETA: 5:42 - loss: 3.0180 - regression_loss: 2.4716 - classification_loss: 0.5465
1116/1500 [=====================>........] - ETA: 5:41 - loss: 3.0182 - regression_loss: 2.4718 - classification_loss: 0.5463
1117/1500 [=====================>........] - ETA: 5:40 - loss: 3.0178 - regression_loss: 2.4716 - classification_loss: 0.5462
1118/1500 [=====================>........] - ETA: 5:39 - loss: 3.0180 - regression_loss: 2.4719 - classification_loss: 0.5461
1119/1500 [=====================>........] - ETA: 5:38 - loss: 3.0175 - regression_loss: 2.4716 - classification_loss: 0.5459
1120/1500 [=====================>........] - ETA: 5:37 - loss: 3.0174 - regression_loss: 2.4712 - classification_loss: 0.5461
1121/1500 [=====================>........] - ETA: 5:36 - loss: 3.0177 - regression_loss: 2.4717 - classification_loss: 0.5460
1122/1500 [=====================>........] - ETA: 5:35 - loss: 3.0168 - regression_loss: 2.4711 - classification_loss: 0.5457
1123/1500 [=====================>........] - ETA: 5:34 - loss: 3.0158 - regression_loss: 2.4703 - classification_loss: 0.5455
1124/1500 [=====================>........] - ETA: 5:33 - loss: 3.0144 - regression_loss: 2.4693 - classification_loss: 0.5452
1125/1500 [=====================>........] - ETA: 5:32 - loss: 3.0143 - regression_loss: 2.4693 - classification_loss: 0.5450
1126/1500 [=====================>........] - ETA: 5:31 - loss: 3.0137 - regression_loss: 2.4689 - classification_loss: 0.5449
1127/1500 [=====================>........] - ETA: 5:30 - loss: 3.0133 - regression_loss: 2.4686 - classification_loss: 0.5447
1128/1500 [=====================>........] - ETA: 5:29 - loss: 3.0120 - regression_loss: 2.4675 - classification_loss: 0.5444
1129/1500 [=====================>........] - ETA: 5:28 - loss: 3.0120 - regression_loss: 2.4677 - classification_loss: 0.5443
1130/1500 [=====================>........] - ETA: 5:27 - loss: 3.0116 - regression_loss: 2.4674 - classification_loss: 0.5442
1131/1500 [=====================>........] - ETA: 5:26 - loss: 3.0121 - regression_loss: 2.4676 - classification_loss: 0.5444
1132/1500 [=====================>........] - ETA: 5:26 - loss: 3.0126 - regression_loss: 2.4681 - classification_loss: 0.5445
1133/1500 [=====================>........] - ETA: 5:25 - loss: 3.0120 - regression_loss: 2.4677 - classification_loss: 0.5443
1134/1500 [=====================>........] - ETA: 5:24 - loss: 3.0118 - regression_loss: 2.4676 - classification_loss: 0.5442
1135/1500 [=====================>........] - ETA: 5:23 - loss: 3.0118 - regression_loss: 2.4677 - classification_loss: 0.5441
1136/1500 [=====================>........] - ETA: 5:22 - loss: 3.0115 - regression_loss: 2.4675 - classification_loss: 0.5440
1137/1500 [=====================>........] - ETA: 5:21 - loss: 3.0107 - regression_loss: 2.4668 - classification_loss: 0.5438
1138/1500 [=====================>........] - ETA: 5:20 - loss: 3.0102 - regression_loss: 2.4664 - classification_loss: 0.5438
1139/1500 [=====================>........] - ETA: 5:19 - loss: 3.0100 - regression_loss: 2.4663 - classification_loss: 0.5437
1140/1500 [=====================>........] - ETA: 5:18 - loss: 3.0096 - regression_loss: 2.4661 - classification_loss: 0.5434
1141/1500 [=====================>........] - ETA: 5:17 - loss: 3.0090 - regression_loss: 2.4658 - classification_loss: 0.5431
1142/1500 [=====================>........] - ETA: 5:17 - loss: 3.0091 - regression_loss: 2.4659 - classification_loss: 0.5433
1143/1500 [=====================>........] - ETA: 5:16 - loss: 3.0082 - regression_loss: 2.4651 - classification_loss: 0.5430
1144/1500 [=====================>........] - ETA: 5:15 - loss: 3.0082 - regression_loss: 2.4652 - classification_loss: 0.5430
1145/1500 [=====================>........] - ETA: 5:14 - loss: 3.0072 - regression_loss: 2.4644 - classification_loss: 0.5428
1146/1500 [=====================>........] - ETA: 5:13 - loss: 3.0075 - regression_loss: 2.4646 - classification_loss: 0.5429
1147/1500 [=====================>........] - ETA: 5:13 - loss: 3.0071 - regression_loss: 2.4640 - classification_loss: 0.5430
1148/1500 [=====================>........] - ETA: 5:12 - loss: 3.0063 - regression_loss: 2.4635 - classification_loss: 0.5428
1149/1500 [=====================>........] - ETA: 5:11 - loss: 3.0059 - regression_loss: 2.4632 - classification_loss: 0.5427
1150/1500 [======================>.......] - ETA: 5:10 - loss: 3.0048 - regression_loss: 2.4623 - classification_loss: 0.5425
1151/1500 [======================>.......] - ETA: 5:09 - loss: 3.0037 - regression_loss: 2.4614 - classification_loss: 0.5423
1152/1500 [======================>.......] - ETA: 5:08 - loss: 3.0035 - regression_loss: 2.4614 - classification_loss: 0.5421
1153/1500 [======================>.......] - ETA: 5:07 - loss: 3.0022 - regression_loss: 2.4604 - classification_loss: 0.5418
1154/1500 [======================>.......] - ETA: 5:06 - loss: 3.0021 - regression_loss: 2.4604 - classification_loss: 0.5417
1155/1500 [======================>.......] - ETA: 5:05 - loss: 3.0020 - regression_loss: 2.4604 - classification_loss: 0.5416
1156/1500 [======================>.......] - ETA: 5:04 - loss: 3.0018 - regression_loss: 2.4604 - classification_loss: 0.5414
1157/1500 [======================>.......] - ETA: 5:03 - loss: 3.0014 - regression_loss: 2.4601 - classification_loss: 0.5413
1158/1500 [======================>.......] - ETA: 5:02 - loss: 3.0009 - regression_loss: 2.4598 - classification_loss: 0.5412
1159/1500 [======================>.......] - ETA: 5:01 - loss: 3.0010 - regression_loss: 2.4598 - classification_loss: 0.5412
1160/1500 [======================>.......] - ETA: 5:00 - loss: 3.0000 - regression_loss: 2.4591 - classification_loss: 0.5409
1161/1500 [======================>.......] - ETA: 5:00 - loss: 2.9994 - regression_loss: 2.4586 - classification_loss: 0.5407
1162/1500 [======================>.......] - ETA: 4:59 - loss: 2.9984 - regression_loss: 2.4580 - classification_loss: 0.5405
1163/1500 [======================>.......] - ETA: 4:58 - loss: 2.9981 - regression_loss: 2.4579 - classification_loss: 0.5402
1164/1500 [======================>.......] - ETA: 4:57 - loss: 2.9984 - regression_loss: 2.4580 - classification_loss: 0.5403
1165/1500 [======================>.......] - ETA: 4:56 - loss: 2.9981 - regression_loss: 2.4579 - classification_loss: 0.5401
1166/1500 [======================>.......] - ETA: 4:55 - loss: 2.9986 - regression_loss: 2.4582 - classification_loss: 0.5404
1167/1500 [======================>.......] - ETA: 4:54 - loss: 2.9990 - regression_loss: 2.4586 - classification_loss: 0.5404
1168/1500 [======================>.......] - ETA: 4:53 - loss: 2.9981 - regression_loss: 2.4580 - classification_loss: 0.5402
1169/1500 [======================>.......] - ETA: 4:52 - loss: 2.9972 - regression_loss: 2.4573 - classification_loss: 0.5399
1170/1500 [======================>.......] - ETA: 4:51 - loss: 2.9967 - regression_loss: 2.4569 - classification_loss: 0.5398
1171/1500 [======================>.......] - ETA: 4:50 - loss: 2.9962 - regression_loss: 2.4566 - classification_loss: 0.5396
1172/1500 [======================>.......] - ETA: 4:50 - loss: 2.9954 - regression_loss: 2.4560 - classification_loss: 0.5393
1173/1500 [======================>.......] - ETA: 4:49 - loss: 2.9949 - regression_loss: 2.4557 - classification_loss: 0.5391
1174/1500 [======================>.......] - ETA: 4:49 - loss: 2.9953 - regression_loss: 2.4561 - classification_loss: 0.5392
1175/1500 [======================>.......] - ETA: 4:48 - loss: 2.9948 - regression_loss: 2.4558 - classification_loss: 0.5390
1176/1500 [======================>.......] - ETA: 4:47 - loss: 2.9946 - regression_loss: 2.4557 - classification_loss: 0.5389
1177/1500 [======================>.......] - ETA: 4:46 - loss: 2.9931 - regression_loss: 2.4544 - classification_loss: 0.5387
1178/1500 [======================>.......] - ETA: 4:45 - loss: 2.9930 - regression_loss: 2.4545 - classification_loss: 0.5385
1179/1500 [======================>.......] - ETA: 4:44 - loss: 2.9926 - regression_loss: 2.4543 - classification_loss: 0.5383
1180/1500 [======================>.......] - ETA: 4:43 - loss: 2.9923 - regression_loss: 2.4539 - classification_loss: 0.5384
1181/1500 [======================>.......] - ETA: 4:42 - loss: 2.9919 - regression_loss: 2.4528 - classification_loss: 0.5390
1182/1500 [======================>.......] - ETA: 4:41 - loss: 2.9922 - regression_loss: 2.4528 - classification_loss: 0.5394
1183/1500 [======================>.......] - ETA: 4:40 - loss: 2.9921 - regression_loss: 2.4528 - classification_loss: 0.5393
1184/1500 [======================>.......] - ETA: 4:39 - loss: 2.9914 - regression_loss: 2.4523 - classification_loss: 0.5391
1185/1500 [======================>.......] - ETA: 4:39 - loss: 2.9913 - regression_loss: 2.4524 - classification_loss: 0.5388
1186/1500 [======================>.......] - ETA: 4:38 - loss: 2.9906 - regression_loss: 2.4520 - classification_loss: 0.5385
1187/1500 [======================>.......] - ETA: 4:37 - loss: 2.9897 - regression_loss: 2.4514 - classification_loss: 0.5383
1188/1500 [======================>.......] - ETA: 4:36 - loss: 2.9897 - regression_loss: 2.4514 - classification_loss: 0.5384
1189/1500 [======================>.......] - ETA: 4:35 - loss: 2.9901 - regression_loss: 2.4517 - classification_loss: 0.5385
1190/1500 [======================>.......] - ETA: 4:34 - loss: 2.9907 - regression_loss: 2.4522 - classification_loss: 0.5385
1191/1500 [======================>.......] - ETA: 4:33 - loss: 2.9904 - regression_loss: 2.4520 - classification_loss: 0.5384
1192/1500 [======================>.......] - ETA: 4:32 - loss: 2.9892 - regression_loss: 2.4511 - classification_loss: 0.5381
1193/1500 [======================>.......] - ETA: 4:31 - loss: 2.9891 - regression_loss: 2.4510 - classification_loss: 0.5381
1194/1500 [======================>.......] - ETA: 4:30 - loss: 2.9889 - regression_loss: 2.4509 - classification_loss: 0.5380
1195/1500 [======================>.......] - ETA: 4:29 - loss: 2.9886 - regression_loss: 2.4507 - classification_loss: 0.5379
1196/1500 [======================>.......] - ETA: 4:28 - loss: 2.9872 - regression_loss: 2.4496 - classification_loss: 0.5376
1197/1500 [======================>.......] - ETA: 4:27 - loss: 2.9863 - regression_loss: 2.4489 - classification_loss: 0.5373
1198/1500 [======================>.......] - ETA: 4:27 - loss: 2.9855 - regression_loss: 2.4483 - classification_loss: 0.5372
1199/1500 [======================>.......] - ETA: 4:26 - loss: 2.9848 - regression_loss: 2.4478 - classification_loss: 0.5370
1200/1500 [=======================>......] - ETA: 4:25 - loss: 2.9844 - regression_loss: 2.4476 - classification_loss: 0.5369
1201/1500 [=======================>......] - ETA: 4:24 - loss: 2.9844 - regression_loss: 2.4477 - classification_loss: 0.5367
1202/1500 [=======================>......] - ETA: 4:23 - loss: 2.9842 - regression_loss: 2.4476 - classification_loss: 0.5366
1203/1500 [=======================>......] - ETA: 4:22 - loss: 2.9857 - regression_loss: 2.4491 - classification_loss: 0.5367
1204/1500 [=======================>......] - ETA: 4:21 - loss: 2.9859 - regression_loss: 2.4492 - classification_loss: 0.5367
1205/1500 [=======================>......] - ETA: 4:20 - loss: 2.9855 - regression_loss: 2.4487 - classification_loss: 0.5368
1206/1500 [=======================>......] - ETA: 4:19 - loss: 2.9853 - regression_loss: 2.4485 - classification_loss: 0.5367
1207/1500 [=======================>......] - ETA: 4:18 - loss: 2.9841 - regression_loss: 2.4476 - classification_loss: 0.5365
1208/1500 [=======================>......] - ETA: 4:18 - loss: 2.9845 - regression_loss: 2.4479 - classification_loss: 0.5366
1209/1500 [=======================>......] - ETA: 4:17 - loss: 2.9834 - regression_loss: 2.4471 - classification_loss: 0.5363
1210/1500 [=======================>......] - ETA: 4:16 - loss: 2.9828 - regression_loss: 2.4467 - classification_loss: 0.5361
1211/1500 [=======================>......] - ETA: 4:15 - loss: 2.9818 - regression_loss: 2.4459 - classification_loss: 0.5359
1212/1500 [=======================>......] - ETA: 4:14 - loss: 2.9815 - regression_loss: 2.4455 - classification_loss: 0.5359
1213/1500 [=======================>......] - ETA: 4:13 - loss: 2.9806 - regression_loss: 2.4450 - classification_loss: 0.5357
1214/1500 [=======================>......] - ETA: 4:12 - loss: 2.9806 - regression_loss: 2.4449 - classification_loss: 0.5357
1215/1500 [=======================>......] - ETA: 4:11 - loss: 2.9809 - regression_loss: 2.4451 - classification_loss: 0.5357
1216/1500 [=======================>......] - ETA: 4:10 - loss: 2.9806 - regression_loss: 2.4449 - classification_loss: 0.5357
1217/1500 [=======================>......] - ETA: 4:09 - loss: 2.9794 - regression_loss: 2.4440 - classification_loss: 0.5354
1218/1500 [=======================>......] - ETA: 4:08 - loss: 2.9791 - regression_loss: 2.4438 - classification_loss: 0.5352
1219/1500 [=======================>......] - ETA: 4:07 - loss: 2.9794 - regression_loss: 2.4441 - classification_loss: 0.5352
1220/1500 [=======================>......] - ETA: 4:07 - loss: 2.9789 - regression_loss: 2.4438 - classification_loss: 0.5351
1221/1500 [=======================>......] - ETA: 4:06 - loss: 2.9791 - regression_loss: 2.4438 - classification_loss: 0.5353
1222/1500 [=======================>......] - ETA: 4:05 - loss: 2.9783 - regression_loss: 2.4433 - classification_loss: 0.5351
1223/1500 [=======================>......] - ETA: 4:04 - loss: 2.9779 - regression_loss: 2.4429 - classification_loss: 0.5350
1224/1500 [=======================>......] - ETA: 4:03 - loss: 2.9769 - regression_loss: 2.4422 - classification_loss: 0.5347
1225/1500 [=======================>......] - ETA: 4:02 - loss: 2.9768 - regression_loss: 2.4421 - classification_loss: 0.5347
1226/1500 [=======================>......] - ETA: 4:01 - loss: 2.9762 - regression_loss: 2.4417 - classification_loss: 0.5345
1227/1500 [=======================>......] - ETA: 4:00 - loss: 2.9762 - regression_loss: 2.4416 - classification_loss: 0.5345
1228/1500 [=======================>......] - ETA: 3:59 - loss: 2.9753 - regression_loss: 2.4410 - classification_loss: 0.5343
1229/1500 [=======================>......] - ETA: 3:58 - loss: 2.9747 - regression_loss: 2.4406 - classification_loss: 0.5341
1230/1500 [=======================>......] - ETA: 3:57 - loss: 2.9738 - regression_loss: 2.4399 - classification_loss: 0.5339
1231/1500 [=======================>......] - ETA: 3:57 - loss: 2.9736 - regression_loss: 2.4397 - classification_loss: 0.5339
1232/1500 [=======================>......] - ETA: 3:56 - loss: 2.9735 - regression_loss: 2.4397 - classification_loss: 0.5339
1233/1500 [=======================>......] - ETA: 3:55 - loss: 2.9727 - regression_loss: 2.4388 - classification_loss: 0.5339
1234/1500 [=======================>......] - ETA: 3:54 - loss: 2.9727 - regression_loss: 2.4387 - classification_loss: 0.5340
1235/1500 [=======================>......] - ETA: 3:53 - loss: 2.9729 - regression_loss: 2.4389 - classification_loss: 0.5340
1236/1500 [=======================>......] - ETA: 3:52 - loss: 2.9726 - regression_loss: 2.4388 - classification_loss: 0.5339
1237/1500 [=======================>......] - ETA: 3:51 - loss: 2.9717 - regression_loss: 2.4380 - classification_loss: 0.5337
1238/1500 [=======================>......] - ETA: 3:51 - loss: 2.9717 - regression_loss: 2.4380 - classification_loss: 0.5336
1239/1500 [=======================>......] - ETA: 3:50 - loss: 2.9707 - regression_loss: 2.4373 - classification_loss: 0.5334
1240/1500 [=======================>......] - ETA: 3:49 - loss: 2.9695 - regression_loss: 2.4363 - classification_loss: 0.5332
1241/1500 [=======================>......] - ETA: 3:48 - loss: 2.9687 - regression_loss: 2.4358 - classification_loss: 0.5329
1242/1500 [=======================>......] - ETA: 3:47 - loss: 2.9685 - regression_loss: 2.4356 - classification_loss: 0.5329
1243/1500 [=======================>......] - ETA: 3:46 - loss: 2.9671 - regression_loss: 2.4345 - classification_loss: 0.5326
1244/1500 [=======================>......] - ETA: 3:45 - loss: 2.9663 - regression_loss: 2.4339 - classification_loss: 0.5324
1245/1500 [=======================>......] - ETA: 3:44 - loss: 2.9665 - regression_loss: 2.4341 - classification_loss: 0.5324
1246/1500 [=======================>......] - ETA: 3:43 - loss: 2.9660 - regression_loss: 2.4337 - classification_loss: 0.5323
1247/1500 [=======================>......] - ETA: 3:42 - loss: 2.9660 - regression_loss: 2.4338 - classification_loss: 0.5322
1248/1500 [=======================>......] - ETA: 3:41 - loss: 2.9656 - regression_loss: 2.4335 - classification_loss: 0.5321
1249/1500 [=======================>......] - ETA: 3:40 - loss: 2.9648 - regression_loss: 2.4330 - classification_loss: 0.5318
1250/1500 [========================>.....] - ETA: 3:39 - loss: 2.9646 - regression_loss: 2.4328 - classification_loss: 0.5318
1251/1500 [========================>.....] - ETA: 3:39 - loss: 2.9646 - regression_loss: 2.4327 - classification_loss: 0.5318
1252/1500 [========================>.....] - ETA: 3:38 - loss: 2.9651 - regression_loss: 2.4331 - classification_loss: 0.5320
1253/1500 [========================>.....] - ETA: 3:37 - loss: 2.9648 - regression_loss: 2.4327 - classification_loss: 0.5321
1254/1500 [========================>.....] - ETA: 3:36 - loss: 2.9652 - regression_loss: 2.4329 - classification_loss: 0.5322
1255/1500 [========================>.....] - ETA: 3:35 - loss: 2.9645 - regression_loss: 2.4324 - classification_loss: 0.5320
1256/1500 [========================>.....] - ETA: 3:34 - loss: 2.9645 - regression_loss: 2.4324 - classification_loss: 0.5321
1257/1500 [========================>.....] - ETA: 3:34 - loss: 2.9651 - regression_loss: 2.4329 - classification_loss: 0.5321
1258/1500 [========================>.....] - ETA: 3:33 - loss: 2.9638 - regression_loss: 2.4320 - classification_loss: 0.5318
1259/1500 [========================>.....] - ETA: 3:32 - loss: 2.9634 - regression_loss: 2.4319 - classification_loss: 0.5316
1260/1500 [========================>.....] - ETA: 3:31 - loss: 2.9625 - regression_loss: 2.4312 - classification_loss: 0.5313
1261/1500 [========================>.....] - ETA: 3:30 - loss: 2.9623 - regression_loss: 2.4311 - classification_loss: 0.5312
1262/1500 [========================>.....] - ETA: 3:29 - loss: 2.9621 - regression_loss: 2.4310 - classification_loss: 0.5311
1263/1500 [========================>.....] - ETA: 3:28 - loss: 2.9611 - regression_loss: 2.4300 - classification_loss: 0.5311
1264/1500 [========================>.....] - ETA: 3:28 - loss: 2.9611 - regression_loss: 2.4301 - classification_loss: 0.5311
1265/1500 [========================>.....] - ETA: 3:27 - loss: 2.9597 - regression_loss: 2.4289 - classification_loss: 0.5308
1266/1500 [========================>.....] - ETA: 3:26 - loss: 2.9584 - regression_loss: 2.4279 - classification_loss: 0.5305
1267/1500 [========================>.....] - ETA: 3:25 - loss: 2.9583 - regression_loss: 2.4276 - classification_loss: 0.5308
1268/1500 [========================>.....] - ETA: 3:24 - loss: 2.9580 - regression_loss: 2.4272 - classification_loss: 0.5307
1269/1500 [========================>.....] - ETA: 3:23 - loss: 2.9579 - regression_loss: 2.4271 - classification_loss: 0.5308
1270/1500 [========================>.....] - ETA: 3:22 - loss: 2.9575 - regression_loss: 2.4268 - classification_loss: 0.5307
1271/1500 [========================>.....] - ETA: 3:21 - loss: 2.9563 - regression_loss: 2.4259 - classification_loss: 0.5304
1272/1500 [========================>.....] - ETA: 3:20 - loss: 2.9565 - regression_loss: 2.4260 - classification_loss: 0.5305
1273/1500 [========================>.....] - ETA: 3:20 - loss: 2.9555 - regression_loss: 2.4252 - classification_loss: 0.5303
1274/1500 [========================>.....] - ETA: 3:19 - loss: 2.9554 - regression_loss: 2.4252 - classification_loss: 0.5302
1275/1500 [========================>.....] - ETA: 3:18 - loss: 2.9556 - regression_loss: 2.4254 - classification_loss: 0.5302
1276/1500 [========================>.....] - ETA: 3:17 - loss: 2.9554 - regression_loss: 2.4253 - classification_loss: 0.5301
1277/1500 [========================>.....] - ETA: 3:16 - loss: 2.9548 - regression_loss: 2.4250 - classification_loss: 0.5299
1278/1500 [========================>.....] - ETA: 3:15 - loss: 2.9556 - regression_loss: 2.4258 - classification_loss: 0.5298
1279/1500 [========================>.....] - ETA: 3:14 - loss: 2.9552 - regression_loss: 2.4256 - classification_loss: 0.5296
1280/1500 [========================>.....] - ETA: 3:13 - loss: 2.9544 - regression_loss: 2.4250 - classification_loss: 0.5294
1281/1500 [========================>.....] - ETA: 3:12 - loss: 2.9538 - regression_loss: 2.4245 - classification_loss: 0.5293
1282/1500 [========================>.....] - ETA: 3:11 - loss: 2.9539 - regression_loss: 2.4244 - classification_loss: 0.5294
1283/1500 [========================>.....] - ETA: 3:11 - loss: 2.9536 - regression_loss: 2.4242 - classification_loss: 0.5294
1284/1500 [========================>.....] - ETA: 3:10 - loss: 2.9533 - regression_loss: 2.4239 - classification_loss: 0.5293
1285/1500 [========================>.....] - ETA: 3:09 - loss: 2.9522 - regression_loss: 2.4232 - classification_loss: 0.5290
1286/1500 [========================>.....] - ETA: 3:08 - loss: 2.9509 - regression_loss: 2.4221 - classification_loss: 0.5288
1287/1500 [========================>.....] - ETA: 3:07 - loss: 2.9505 - regression_loss: 2.4218 - classification_loss: 0.5287
1288/1500 [========================>.....] - ETA: 3:06 - loss: 2.9500 - regression_loss: 2.4214 - classification_loss: 0.5285
1289/1500 [========================>.....] - ETA: 3:05 - loss: 2.9491 - regression_loss: 2.4207 - classification_loss: 0.5283
1290/1500 [========================>.....] - ETA: 3:04 - loss: 2.9482 - regression_loss: 2.4201 - classification_loss: 0.5281
1291/1500 [========================>.....] - ETA: 3:03 - loss: 2.9475 - regression_loss: 2.4196 - classification_loss: 0.5279
1292/1500 [========================>.....] - ETA: 3:02 - loss: 2.9469 - regression_loss: 2.4192 - classification_loss: 0.5277
1293/1500 [========================>.....] - ETA: 3:01 - loss: 2.9468 - regression_loss: 2.4190 - classification_loss: 0.5277
1294/1500 [========================>.....] - ETA: 3:01 - loss: 2.9459 - regression_loss: 2.4184 - classification_loss: 0.5275
1295/1500 [========================>.....] - ETA: 3:00 - loss: 2.9458 - regression_loss: 2.4183 - classification_loss: 0.5274
1296/1500 [========================>.....] - ETA: 2:59 - loss: 2.9453 - regression_loss: 2.4179 - classification_loss: 0.5274
1297/1500 [========================>.....] - ETA: 2:58 - loss: 2.9442 - regression_loss: 2.4171 - classification_loss: 0.5272
1298/1500 [========================>.....] - ETA: 2:57 - loss: 2.9435 - regression_loss: 2.4165 - classification_loss: 0.5270
1299/1500 [========================>.....] - ETA: 2:56 - loss: 2.9432 - regression_loss: 2.4163 - classification_loss: 0.5269
1300/1500 [=========================>....] - ETA: 2:55 - loss: 2.9429 - regression_loss: 2.4161 - classification_loss: 0.5268
1301/1500 [=========================>....] - ETA: 2:54 - loss: 2.9418 - regression_loss: 2.4153 - classification_loss: 0.5265
1302/1500 [=========================>....] - ETA: 2:53 - loss: 2.9406 - regression_loss: 2.4143 - classification_loss: 0.5263
1303/1500 [=========================>....] - ETA: 2:52 - loss: 2.9414 - regression_loss: 2.4150 - classification_loss: 0.5264
1304/1500 [=========================>....] - ETA: 2:52 - loss: 2.9406 - regression_loss: 2.4145 - classification_loss: 0.5261
1305/1500 [=========================>....] - ETA: 2:51 - loss: 2.9401 - regression_loss: 2.4141 - classification_loss: 0.5261
1306/1500 [=========================>....] - ETA: 2:50 - loss: 2.9395 - regression_loss: 2.4137 - classification_loss: 0.5259
1307/1500 [=========================>....] - ETA: 2:49 - loss: 2.9393 - regression_loss: 2.4135 - classification_loss: 0.5258
1308/1500 [=========================>....] - ETA: 2:48 - loss: 2.9394 - regression_loss: 2.4137 - classification_loss: 0.5258
1309/1500 [=========================>....] - ETA: 2:47 - loss: 2.9386 - regression_loss: 2.4130 - classification_loss: 0.5256
1310/1500 [=========================>....] - ETA: 2:46 - loss: 2.9389 - regression_loss: 2.4134 - classification_loss: 0.5256
1311/1500 [=========================>....] - ETA: 2:45 - loss: 2.9390 - regression_loss: 2.4135 - classification_loss: 0.5255
1312/1500 [=========================>....] - ETA: 2:44 - loss: 2.9389 - regression_loss: 2.4135 - classification_loss: 0.5254
1313/1500 [=========================>....] - ETA: 2:44 - loss: 2.9383 - regression_loss: 2.4130 - classification_loss: 0.5253
1314/1500 [=========================>....] - ETA: 2:43 - loss: 2.9384 - regression_loss: 2.4132 - classification_loss: 0.5253
1315/1500 [=========================>....] - ETA: 2:42 - loss: 2.9375 - regression_loss: 2.4125 - classification_loss: 0.5250
1316/1500 [=========================>....] - ETA: 2:41 - loss: 2.9376 - regression_loss: 2.4126 - classification_loss: 0.5250
1317/1500 [=========================>....] - ETA: 2:40 - loss: 2.9373 - regression_loss: 2.4124 - classification_loss: 0.5249
1318/1500 [=========================>....] - ETA: 2:39 - loss: 2.9374 - regression_loss: 2.4126 - classification_loss: 0.5249
1319/1500 [=========================>....] - ETA: 2:38 - loss: 2.9361 - regression_loss: 2.4115 - classification_loss: 0.5246
1320/1500 [=========================>....] - ETA: 2:38 - loss: 2.9357 - regression_loss: 2.4112 - classification_loss: 0.5245
1321/1500 [=========================>....] - ETA: 2:37 - loss: 2.9354 - regression_loss: 2.4110 - classification_loss: 0.5244
1322/1500 [=========================>....] - ETA: 2:36 - loss: 2.9346 - regression_loss: 2.4104 - classification_loss: 0.5242
1323/1500 [=========================>....] - ETA: 2:35 - loss: 2.9340 - regression_loss: 2.4099 - classification_loss: 0.5240
1324/1500 [=========================>....] - ETA: 2:34 - loss: 2.9343 - regression_loss: 2.4101 - classification_loss: 0.5242
1325/1500 [=========================>....] - ETA: 2:33 - loss: 2.9334 - regression_loss: 2.4094 - classification_loss: 0.5240
1326/1500 [=========================>....] - ETA: 2:32 - loss: 2.9335 - regression_loss: 2.4096 - classification_loss: 0.5239
1327/1500 [=========================>....] - ETA: 2:32 - loss: 2.9329 - regression_loss: 2.4092 - classification_loss: 0.5237
1328/1500 [=========================>....] - ETA: 2:31 - loss: 2.9322 - regression_loss: 2.4087 - classification_loss: 0.5235
1329/1500 [=========================>....] - ETA: 2:30 - loss: 2.9319 - regression_loss: 2.4085 - classification_loss: 0.5234
1330/1500 [=========================>....] - ETA: 2:29 - loss: 2.9315 - regression_loss: 2.4083 - classification_loss: 0.5233
1331/1500 [=========================>....] - ETA: 2:28 - loss: 2.9306 - regression_loss: 2.4075 - classification_loss: 0.5231
1332/1500 [=========================>....] - ETA: 2:27 - loss: 2.9302 - regression_loss: 2.4072 - classification_loss: 0.5230
1333/1500 [=========================>....] - ETA: 2:26 - loss: 2.9290 - regression_loss: 2.4062 - classification_loss: 0.5228
1334/1500 [=========================>....] - ETA: 2:25 - loss: 2.9288 - regression_loss: 2.4062 - classification_loss: 0.5226
1335/1500 [=========================>....] - ETA: 2:24 - loss: 2.9281 - regression_loss: 2.4057 - classification_loss: 0.5224
1336/1500 [=========================>....] - ETA: 2:23 - loss: 2.9271 - regression_loss: 2.4049 - classification_loss: 0.5222
1337/1500 [=========================>....] - ETA: 2:22 - loss: 2.9265 - regression_loss: 2.4044 - classification_loss: 0.5220
1338/1500 [=========================>....] - ETA: 2:21 - loss: 2.9259 - regression_loss: 2.4040 - classification_loss: 0.5219
1339/1500 [=========================>....] - ETA: 2:21 - loss: 2.9260 - regression_loss: 2.4041 - classification_loss: 0.5219
1340/1500 [=========================>....] - ETA: 2:20 - loss: 2.9256 - regression_loss: 2.4039 - classification_loss: 0.5217
1341/1500 [=========================>....] - ETA: 2:19 - loss: 2.9253 - regression_loss: 2.4037 - classification_loss: 0.5216
1342/1500 [=========================>....] - ETA: 2:18 - loss: 2.9241 - regression_loss: 2.4027 - classification_loss: 0.5214
1343/1500 [=========================>....] - ETA: 2:17 - loss: 2.9237 - regression_loss: 2.4025 - classification_loss: 0.5212
1344/1500 [=========================>....] - ETA: 2:16 - loss: 2.9238 - regression_loss: 2.4028 - classification_loss: 0.5211
1345/1500 [=========================>....] - ETA: 2:15 - loss: 2.9238 - regression_loss: 2.4027 - classification_loss: 0.5211
1346/1500 [=========================>....] - ETA: 2:15 - loss: 2.9235 - regression_loss: 2.4026 - classification_loss: 0.5209
1347/1500 [=========================>....] - ETA: 2:14 - loss: 2.9226 - regression_loss: 2.4019 - classification_loss: 0.5207
1348/1500 [=========================>....] - ETA: 2:13 - loss: 2.9225 - regression_loss: 2.4020 - classification_loss: 0.5205
1349/1500 [=========================>....] - ETA: 2:12 - loss: 2.9225 - regression_loss: 2.4020 - classification_loss: 0.5205
1350/1500 [==========================>...] - ETA: 2:11 - loss: 2.9213 - regression_loss: 2.4010 - classification_loss: 0.5203
1351/1500 [==========================>...] - ETA: 2:10 - loss: 2.9218 - regression_loss: 2.4016 - classification_loss: 0.5202
1352/1500 [==========================>...] - ETA: 2:09 - loss: 2.9210 - regression_loss: 2.4011 - classification_loss: 0.5199
1353/1500 [==========================>...] - ETA: 2:09 - loss: 2.9210 - regression_loss: 2.4011 - classification_loss: 0.5200
1354/1500 [==========================>...] - ETA: 2:08 - loss: 2.9206 - regression_loss: 2.4008 - classification_loss: 0.5198
1355/1500 [==========================>...] - ETA: 2:07 - loss: 2.9197 - regression_loss: 2.4000 - classification_loss: 0.5197
1356/1500 [==========================>...] - ETA: 2:06 - loss: 2.9193 - regression_loss: 2.3998 - classification_loss: 0.5195
1357/1500 [==========================>...] - ETA: 2:05 - loss: 2.9190 - regression_loss: 2.3996 - classification_loss: 0.5194
1358/1500 [==========================>...] - ETA: 2:04 - loss: 2.9179 - regression_loss: 2.3988 - classification_loss: 0.5192
1359/1500 [==========================>...] - ETA: 2:03 - loss: 2.9171 - regression_loss: 2.3981 - classification_loss: 0.5190
1360/1500 [==========================>...] - ETA: 2:02 - loss: 2.9162 - regression_loss: 2.3974 - classification_loss: 0.5188
1361/1500 [==========================>...] - ETA: 2:01 - loss: 2.9158 - regression_loss: 2.3971 - classification_loss: 0.5187
1362/1500 [==========================>...] - ETA: 2:01 - loss: 2.9165 - regression_loss: 2.3978 - classification_loss: 0.5187
1363/1500 [==========================>...] - ETA: 2:00 - loss: 2.9160 - regression_loss: 2.3974 - classification_loss: 0.5185
1364/1500 [==========================>...] - ETA: 1:59 - loss: 2.9157 - regression_loss: 2.3973 - classification_loss: 0.5184
1365/1500 [==========================>...] - ETA: 1:58 - loss: 2.9151 - regression_loss: 2.3968 - classification_loss: 0.5182
1366/1500 [==========================>...] - ETA: 1:57 - loss: 2.9143 - regression_loss: 2.3963 - classification_loss: 0.5180
1367/1500 [==========================>...] - ETA: 1:56 - loss: 2.9138 - regression_loss: 2.3959 - classification_loss: 0.5179
1368/1500 [==========================>...] - ETA: 1:55 - loss: 2.9133 - regression_loss: 2.3956 - classification_loss: 0.5177
1369/1500 [==========================>...] - ETA: 1:54 - loss: 2.9136 - regression_loss: 2.3958 - classification_loss: 0.5179
1370/1500 [==========================>...] - ETA: 1:53 - loss: 2.9130 - regression_loss: 2.3953 - classification_loss: 0.5177
1371/1500 [==========================>...] - ETA: 1:52 - loss: 2.9130 - regression_loss: 2.3954 - classification_loss: 0.5176
1372/1500 [==========================>...] - ETA: 1:51 - loss: 2.9121 - regression_loss: 2.3946 - classification_loss: 0.5175
1373/1500 [==========================>...] - ETA: 1:51 - loss: 2.9123 - regression_loss: 2.3947 - classification_loss: 0.5176
1374/1500 [==========================>...] - ETA: 1:50 - loss: 2.9120 - regression_loss: 2.3945 - classification_loss: 0.5175
1375/1500 [==========================>...] - ETA: 1:49 - loss: 2.9119 - regression_loss: 2.3945 - classification_loss: 0.5173
1376/1500 [==========================>...] - ETA: 1:48 - loss: 2.9117 - regression_loss: 2.3943 - classification_loss: 0.5173
1377/1500 [==========================>...] - ETA: 1:47 - loss: 2.9105 - regression_loss: 2.3934 - classification_loss: 0.5171
1378/1500 [==========================>...] - ETA: 1:46 - loss: 2.9100 - regression_loss: 2.3930 - classification_loss: 0.5170
1379/1500 [==========================>...] - ETA: 1:45 - loss: 2.9085 - regression_loss: 2.3916 - classification_loss: 0.5169
1380/1500 [==========================>...] - ETA: 1:45 - loss: 2.9074 - regression_loss: 2.3907 - classification_loss: 0.5167
1381/1500 [==========================>...] - ETA: 1:44 - loss: 2.9073 - regression_loss: 2.3906 - classification_loss: 0.5167
1382/1500 [==========================>...] - ETA: 1:43 - loss: 2.9071 - regression_loss: 2.3903 - classification_loss: 0.5167
1383/1500 [==========================>...] - ETA: 1:42 - loss: 2.9066 - regression_loss: 2.3900 - classification_loss: 0.5166
1384/1500 [==========================>...] - ETA: 1:41 - loss: 2.9058 - regression_loss: 2.3894 - classification_loss: 0.5164
1385/1500 [==========================>...] - ETA: 1:40 - loss: 2.9060 - regression_loss: 2.3896 - classification_loss: 0.5164
1386/1500 [==========================>...] - ETA: 1:39 - loss: 2.9058 - regression_loss: 2.3895 - classification_loss: 0.5164
1387/1500 [==========================>...] - ETA: 1:38 - loss: 2.9049 - regression_loss: 2.3888 - classification_loss: 0.5162
1388/1500 [==========================>...] - ETA: 1:37 - loss: 2.9044 - regression_loss: 2.3884 - classification_loss: 0.5159
1389/1500 [==========================>...] - ETA: 1:37 - loss: 2.9039 - regression_loss: 2.3880 - classification_loss: 0.5159
1390/1500 [==========================>...] - ETA: 1:36 - loss: 2.9042 - regression_loss: 2.3881 - classification_loss: 0.5160
1391/1500 [==========================>...] - ETA: 1:35 - loss: 2.9037 - regression_loss: 2.3879 - classification_loss: 0.5158
1392/1500 [==========================>...] - ETA: 1:34 - loss: 2.9033 - regression_loss: 2.3875 - classification_loss: 0.5158
1393/1500 [==========================>...] - ETA: 1:33 - loss: 2.9028 - regression_loss: 2.3871 - classification_loss: 0.5156
1394/1500 [==========================>...] - ETA: 1:32 - loss: 2.9020 - regression_loss: 2.3866 - classification_loss: 0.5154
1395/1500 [==========================>...] - ETA: 1:31 - loss: 2.9022 - regression_loss: 2.3867 - classification_loss: 0.5155
1396/1500 [==========================>...] - ETA: 1:30 - loss: 2.9019 - regression_loss: 2.3864 - classification_loss: 0.5155
1397/1500 [==========================>...] - ETA: 1:30 - loss: 2.9011 - regression_loss: 2.3858 - classification_loss: 0.5153
1398/1500 [==========================>...] - ETA: 1:29 - loss: 2.9011 - regression_loss: 2.3859 - classification_loss: 0.5152
1399/1500 [==========================>...] - ETA: 1:28 - loss: 2.9004 - regression_loss: 2.3853 - classification_loss: 0.5151
1400/1500 [===========================>..] - ETA: 1:27 - loss: 2.9001 - regression_loss: 2.3851 - classification_loss: 0.5150
1401/1500 [===========================>..] - ETA: 1:26 - loss: 2.8994 - regression_loss: 2.3846 - classification_loss: 0.5148
1402/1500 [===========================>..] - ETA: 1:25 - loss: 2.8987 - regression_loss: 2.3841 - classification_loss: 0.5146
1403/1500 [===========================>..] - ETA: 1:24 - loss: 2.8982 - regression_loss: 2.3834 - classification_loss: 0.5147
1404/1500 [===========================>..] - ETA: 1:24 - loss: 2.8983 - regression_loss: 2.3836 - classification_loss: 0.5147
1405/1500 [===========================>..] - ETA: 1:23 - loss: 2.8981 - regression_loss: 2.3833 - classification_loss: 0.5147
1406/1500 [===========================>..] - ETA: 1:22 - loss: 2.8981 - regression_loss: 2.3834 - classification_loss: 0.5147
1407/1500 [===========================>..] - ETA: 1:21 - loss: 2.8992 - regression_loss: 2.3843 - classification_loss: 0.5149
1408/1500 [===========================>..] - ETA: 1:20 - loss: 2.8995 - regression_loss: 2.3846 - classification_loss: 0.5149
1409/1500 [===========================>..] - ETA: 1:19 - loss: 2.8991 - regression_loss: 2.3842 - classification_loss: 0.5149
1410/1500 [===========================>..] - ETA: 1:18 - loss: 2.8993 - regression_loss: 2.3843 - classification_loss: 0.5149
1411/1500 [===========================>..] - ETA: 1:17 - loss: 2.8990 - regression_loss: 2.3841 - classification_loss: 0.5149
1412/1500 [===========================>..] - ETA: 1:17 - loss: 2.8992 - regression_loss: 2.3842 - classification_loss: 0.5149
1413/1500 [===========================>..] - ETA: 1:16 - loss: 2.8985 - regression_loss: 2.3838 - classification_loss: 0.5147
1414/1500 [===========================>..] - ETA: 1:15 - loss: 2.8979 - regression_loss: 2.3834 - classification_loss: 0.5145
1415/1500 [===========================>..] - ETA: 1:14 - loss: 2.8978 - regression_loss: 2.3834 - classification_loss: 0.5145
1416/1500 [===========================>..] - ETA: 1:13 - loss: 2.8972 - regression_loss: 2.3830 - classification_loss: 0.5143
1417/1500 [===========================>..] - ETA: 1:12 - loss: 2.8975 - regression_loss: 2.3830 - classification_loss: 0.5144
1418/1500 [===========================>..] - ETA: 1:11 - loss: 2.8970 - regression_loss: 2.3827 - classification_loss: 0.5143
1419/1500 [===========================>..] - ETA: 1:10 - loss: 2.8966 - regression_loss: 2.3824 - classification_loss: 0.5142
1420/1500 [===========================>..] - ETA: 1:09 - loss: 2.8959 - regression_loss: 2.3819 - classification_loss: 0.5141
1421/1500 [===========================>..] - ETA: 1:09 - loss: 2.8959 - regression_loss: 2.3819 - classification_loss: 0.5140
1422/1500 [===========================>..] - ETA: 1:08 - loss: 2.8954 - regression_loss: 2.3815 - classification_loss: 0.5139
1423/1500 [===========================>..] - ETA: 1:07 - loss: 2.8951 - regression_loss: 2.3813 - classification_loss: 0.5138
1424/1500 [===========================>..] - ETA: 1:06 - loss: 2.8946 - regression_loss: 2.3808 - classification_loss: 0.5138
1425/1500 [===========================>..] - ETA: 1:05 - loss: 2.8938 - regression_loss: 2.3802 - classification_loss: 0.5136
1426/1500 [===========================>..] - ETA: 1:04 - loss: 2.8937 - regression_loss: 2.3802 - classification_loss: 0.5136
1427/1500 [===========================>..] - ETA: 1:03 - loss: 2.8930 - regression_loss: 2.3797 - classification_loss: 0.5134
1428/1500 [===========================>..] - ETA: 1:02 - loss: 2.8931 - regression_loss: 2.3798 - classification_loss: 0.5133
1429/1500 [===========================>..] - ETA: 1:01 - loss: 2.8926 - regression_loss: 2.3792 - classification_loss: 0.5134
1430/1500 [===========================>..] - ETA: 1:01 - loss: 2.8921 - regression_loss: 2.3786 - classification_loss: 0.5134
1431/1500 [===========================>..] - ETA: 1:00 - loss: 2.8919 - regression_loss: 2.3783 - classification_loss: 0.5136
1432/1500 [===========================>..] - ETA: 59s - loss: 2.8918 - regression_loss: 2.3783 - classification_loss: 0.5135 
1433/1500 [===========================>..] - ETA: 58s - loss: 2.8921 - regression_loss: 2.3784 - classification_loss: 0.5137
1434/1500 [===========================>..] - ETA: 57s - loss: 2.8919 - regression_loss: 2.3782 - classification_loss: 0.5137
1435/1500 [===========================>..] - ETA: 56s - loss: 2.8918 - regression_loss: 2.3783 - classification_loss: 0.5136
1436/1500 [===========================>..] - ETA: 55s - loss: 2.8912 - regression_loss: 2.3778 - classification_loss: 0.5134
1437/1500 [===========================>..] - ETA: 54s - loss: 2.8900 - regression_loss: 2.3769 - classification_loss: 0.5132
1438/1500 [===========================>..] - ETA: 54s - loss: 2.8901 - regression_loss: 2.3769 - classification_loss: 0.5131
1439/1500 [===========================>..] - ETA: 53s - loss: 2.8897 - regression_loss: 2.3768 - classification_loss: 0.5129
1440/1500 [===========================>..] - ETA: 52s - loss: 2.8894 - regression_loss: 2.3766 - classification_loss: 0.5128
1441/1500 [===========================>..] - ETA: 51s - loss: 2.8888 - regression_loss: 2.3762 - classification_loss: 0.5127
1442/1500 [===========================>..] - ETA: 50s - loss: 2.8878 - regression_loss: 2.3753 - classification_loss: 0.5124
1443/1500 [===========================>..] - ETA: 49s - loss: 2.8871 - regression_loss: 2.3748 - classification_loss: 0.5123
1444/1500 [===========================>..] - ETA: 48s - loss: 2.8870 - regression_loss: 2.3746 - classification_loss: 0.5124
1445/1500 [===========================>..] - ETA: 47s - loss: 2.8861 - regression_loss: 2.3739 - classification_loss: 0.5122
1446/1500 [===========================>..] - ETA: 47s - loss: 2.8856 - regression_loss: 2.3735 - classification_loss: 0.5120
1447/1500 [===========================>..] - ETA: 46s - loss: 2.8855 - regression_loss: 2.3736 - classification_loss: 0.5119
1448/1500 [===========================>..] - ETA: 45s - loss: 2.8842 - regression_loss: 2.3725 - classification_loss: 0.5117
1449/1500 [===========================>..] - ETA: 44s - loss: 2.8832 - regression_loss: 2.3718 - classification_loss: 0.5114
1450/1500 [============================>.] - ETA: 43s - loss: 2.8836 - regression_loss: 2.3721 - classification_loss: 0.5116
1451/1500 [============================>.] - ETA: 42s - loss: 2.8833 - regression_loss: 2.3719 - classification_loss: 0.5114
1452/1500 [============================>.] - ETA: 41s - loss: 2.8827 - regression_loss: 2.3715 - classification_loss: 0.5113
1453/1500 [============================>.] - ETA: 40s - loss: 2.8822 - regression_loss: 2.3710 - classification_loss: 0.5112
1454/1500 [============================>.] - ETA: 40s - loss: 2.8822 - regression_loss: 2.3709 - classification_loss: 0.5113
1455/1500 [============================>.] - ETA: 39s - loss: 2.8821 - regression_loss: 2.3708 - classification_loss: 0.5112
1456/1500 [============================>.] - ETA: 38s - loss: 2.8818 - regression_loss: 2.3707 - classification_loss: 0.5111
1457/1500 [============================>.] - ETA: 37s - loss: 2.8815 - regression_loss: 2.3705 - classification_loss: 0.5110
1458/1500 [============================>.] - ETA: 36s - loss: 2.8815 - regression_loss: 2.3705 - classification_loss: 0.5110
1459/1500 [============================>.] - ETA: 35s - loss: 2.8805 - regression_loss: 2.3697 - classification_loss: 0.5108
1460/1500 [============================>.] - ETA: 34s - loss: 2.8804 - regression_loss: 2.3696 - classification_loss: 0.5108
1461/1500 [============================>.] - ETA: 33s - loss: 2.8802 - regression_loss: 2.3695 - classification_loss: 0.5107
1462/1500 [============================>.] - ETA: 33s - loss: 2.8802 - regression_loss: 2.3695 - classification_loss: 0.5107
1463/1500 [============================>.] - ETA: 32s - loss: 2.8798 - regression_loss: 2.3693 - classification_loss: 0.5105
1464/1500 [============================>.] - ETA: 31s - loss: 2.8795 - regression_loss: 2.3692 - classification_loss: 0.5103
1465/1500 [============================>.] - ETA: 30s - loss: 2.8787 - regression_loss: 2.3685 - classification_loss: 0.5102
1466/1500 [============================>.] - ETA: 29s - loss: 2.8781 - regression_loss: 2.3681 - classification_loss: 0.5100
1467/1500 [============================>.] - ETA: 28s - loss: 2.8782 - regression_loss: 2.3683 - classification_loss: 0.5099
1468/1500 [============================>.] - ETA: 27s - loss: 2.8780 - regression_loss: 2.3682 - classification_loss: 0.5098
1469/1500 [============================>.] - ETA: 26s - loss: 2.8780 - regression_loss: 2.3682 - classification_loss: 0.5098
1470/1500 [============================>.] - ETA: 26s - loss: 2.8780 - regression_loss: 2.3682 - classification_loss: 0.5099
1471/1500 [============================>.] - ETA: 25s - loss: 2.8778 - regression_loss: 2.3680 - classification_loss: 0.5098
1472/1500 [============================>.] - ETA: 24s - loss: 2.8776 - regression_loss: 2.3679 - classification_loss: 0.5096
1473/1500 [============================>.] - ETA: 23s - loss: 2.8769 - regression_loss: 2.3672 - classification_loss: 0.5096
1474/1500 [============================>.] - ETA: 22s - loss: 2.8760 - regression_loss: 2.3665 - classification_loss: 0.5095
1475/1500 [============================>.] - ETA: 21s - loss: 2.8760 - regression_loss: 2.3664 - classification_loss: 0.5095
1476/1500 [============================>.] - ETA: 20s - loss: 2.8759 - regression_loss: 2.3664 - classification_loss: 0.5095
1477/1500 [============================>.] - ETA: 19s - loss: 2.8752 - regression_loss: 2.3659 - classification_loss: 0.5093
1478/1500 [============================>.] - ETA: 19s - loss: 2.8749 - regression_loss: 2.3657 - classification_loss: 0.5093
1479/1500 [============================>.] - ETA: 18s - loss: 2.8745 - regression_loss: 2.3652 - classification_loss: 0.5092
1480/1500 [============================>.] - ETA: 17s - loss: 2.8747 - regression_loss: 2.3655 - classification_loss: 0.5092
1481/1500 [============================>.] - ETA: 16s - loss: 2.8744 - regression_loss: 2.3653 - classification_loss: 0.5090
1482/1500 [============================>.] - ETA: 15s - loss: 2.8743 - regression_loss: 2.3654 - classification_loss: 0.5089
1483/1500 [============================>.] - ETA: 14s - loss: 2.8742 - regression_loss: 2.3653 - classification_loss: 0.5088
1484/1500 [============================>.] - ETA: 13s - loss: 2.8742 - regression_loss: 2.3654 - classification_loss: 0.5088
1485/1500 [============================>.] - ETA: 13s - loss: 2.8733 - regression_loss: 2.3647 - classification_loss: 0.5086
1486/1500 [============================>.] - ETA: 12s - loss: 2.8725 - regression_loss: 2.3640 - classification_loss: 0.5085
1487/1500 [============================>.] - ETA: 11s - loss: 2.8718 - regression_loss: 2.3635 - classification_loss: 0.5084
1488/1500 [============================>.] - ETA: 10s - loss: 2.8715 - regression_loss: 2.3632 - classification_loss: 0.5083
1489/1500 [============================>.] - ETA: 9s - loss: 2.8708 - regression_loss: 2.3626 - classification_loss: 0.5082 
1490/1500 [============================>.] - ETA: 8s - loss: 2.8701 - regression_loss: 2.3621 - classification_loss: 0.5080
1491/1500 [============================>.] - ETA: 7s - loss: 2.8705 - regression_loss: 2.3626 - classification_loss: 0.5079
1492/1500 [============================>.] - ETA: 6s - loss: 2.8704 - regression_loss: 2.3626 - classification_loss: 0.5078
1493/1500 [============================>.] - ETA: 6s - loss: 2.8697 - regression_loss: 2.3620 - classification_loss: 0.5076
1494/1500 [============================>.] - ETA: 5s - loss: 2.8685 - regression_loss: 2.3611 - classification_loss: 0.5074
1495/1500 [============================>.] - ETA: 4s - loss: 2.8678 - regression_loss: 2.3606 - classification_loss: 0.5073
1496/1500 [============================>.] - ETA: 3s - loss: 2.8674 - regression_loss: 2.3603 - classification_loss: 0.5071
1497/1500 [============================>.] - ETA: 2s - loss: 2.8670 - regression_loss: 2.3600 - classification_loss: 0.5070
1498/1500 [============================>.] - ETA: 1s - loss: 2.8667 - regression_loss: 2.3596 - classification_loss: 0.5071
1499/1500 [============================>.] - ETA: 0s - loss: 2.8658 - regression_loss: 2.3589 - classification_loss: 0.5069
1500/1500 [==============================] - 1300s 867ms/step - loss: 2.8657 - regression_loss: 2.3587 - classification_loss: 0.5070

Epoch 00001: saving model to ./snapshots/resnet50_csv_01.h5
Epoch 2/10

   1/1500 [..............................] - ETA: 32:38 - loss: 4.7542 - regression_loss: 4.1186 - classification_loss: 0.6356
   2/1500 [..............................] - ETA: 21:16 - loss: 3.1860 - regression_loss: 2.7612 - classification_loss: 0.4248
   3/1500 [..............................] - ETA: 16:47 - loss: 2.7507 - regression_loss: 2.4020 - classification_loss: 0.3487
   4/1500 [..............................] - ETA: 15:25 - loss: 2.8051 - regression_loss: 2.4286 - classification_loss: 0.3764
   5/1500 [..............................] - ETA: 14:16 - loss: 2.8044 - regression_loss: 2.3991 - classification_loss: 0.4054
   6/1500 [..............................] - ETA: 13:44 - loss: 2.6529 - regression_loss: 2.2704 - classification_loss: 0.3825
   7/1500 [..............................] - ETA: 13:14 - loss: 2.4551 - regression_loss: 2.1089 - classification_loss: 0.3462
   8/1500 [..............................] - ETA: 12:51 - loss: 2.3110 - regression_loss: 1.9955 - classification_loss: 0.3155
   9/1500 [..............................] - ETA: 12:30 - loss: 2.3541 - regression_loss: 2.0309 - classification_loss: 0.3232
  10/1500 [..............................] - ETA: 12:12 - loss: 2.3521 - regression_loss: 2.0180 - classification_loss: 0.3341
  11/1500 [..............................] - ETA: 11:58 - loss: 2.3734 - regression_loss: 2.0177 - classification_loss: 0.3556
  12/1500 [..............................] - ETA: 11:47 - loss: 2.3598 - regression_loss: 2.0182 - classification_loss: 0.3416
  13/1500 [..............................] - ETA: 11:31 - loss: 2.3687 - regression_loss: 2.0225 - classification_loss: 0.3462
  14/1500 [..............................] - ETA: 11:23 - loss: 2.2986 - regression_loss: 1.9619 - classification_loss: 0.3368
  15/1500 [..............................] - ETA: 11:16 - loss: 2.2875 - regression_loss: 1.9468 - classification_loss: 0.3407
  16/1500 [..............................] - ETA: 11:17 - loss: 2.2719 - regression_loss: 1.9153 - classification_loss: 0.3566
  17/1500 [..............................] - ETA: 11:05 - loss: 2.2791 - regression_loss: 1.9222 - classification_loss: 0.3569
  18/1500 [..............................] - ETA: 11:00 - loss: 2.2657 - regression_loss: 1.9071 - classification_loss: 0.3586
  19/1500 [..............................] - ETA: 11:00 - loss: 2.3096 - regression_loss: 1.9526 - classification_loss: 0.3570
  20/1500 [..............................] - ETA: 10:56 - loss: 2.2642 - regression_loss: 1.9185 - classification_loss: 0.3457
  21/1500 [..............................] - ETA: 11:30 - loss: 2.2467 - regression_loss: 1.9023 - classification_loss: 0.3444
  22/1500 [..............................] - ETA: 12:45 - loss: 2.2149 - regression_loss: 1.8737 - classification_loss: 0.3412
  23/1500 [..............................] - ETA: 14:47 - loss: 2.2050 - regression_loss: 1.8678 - classification_loss: 0.3372
  24/1500 [..............................] - ETA: 15:40 - loss: 2.2472 - regression_loss: 1.9115 - classification_loss: 0.3357
  25/1500 [..............................] - ETA: 15:23 - loss: 2.2341 - regression_loss: 1.9003 - classification_loss: 0.3337
  26/1500 [..............................] - ETA: 15:23 - loss: 2.2335 - regression_loss: 1.8979 - classification_loss: 0.3355
  27/1500 [..............................] - ETA: 15:45 - loss: 2.2312 - regression_loss: 1.9008 - classification_loss: 0.3305
  28/1500 [..............................] - ETA: 15:46 - loss: 2.2109 - regression_loss: 1.8828 - classification_loss: 0.3282
  29/1500 [..............................] - ETA: 16:16 - loss: 2.2350 - regression_loss: 1.9018 - classification_loss: 0.3332
  30/1500 [..............................] - ETA: 16:03 - loss: 2.2199 - regression_loss: 1.8906 - classification_loss: 0.3292
  31/1500 [..............................] - ETA: 15:50 - loss: 2.2195 - regression_loss: 1.8890 - classification_loss: 0.3305
  32/1500 [..............................] - ETA: 15:42 - loss: 2.2400 - regression_loss: 1.9078 - classification_loss: 0.3322
  33/1500 [..............................] - ETA: 15:30 - loss: 2.2130 - regression_loss: 1.8840 - classification_loss: 0.3290
  34/1500 [..............................] - ETA: 15:20 - loss: 2.2282 - regression_loss: 1.8937 - classification_loss: 0.3345
  35/1500 [..............................] - ETA: 15:11 - loss: 2.2423 - regression_loss: 1.9054 - classification_loss: 0.3368
  36/1500 [..............................] - ETA: 15:10 - loss: 2.2650 - regression_loss: 1.9177 - classification_loss: 0.3473
  37/1500 [..............................] - ETA: 15:42 - loss: 2.2668 - regression_loss: 1.9216 - classification_loss: 0.3451
  38/1500 [..............................] - ETA: 15:54 - loss: 2.2592 - regression_loss: 1.9163 - classification_loss: 0.3428
  39/1500 [..............................] - ETA: 16:27 - loss: 2.2334 - regression_loss: 1.8927 - classification_loss: 0.3407
  40/1500 [..............................] - ETA: 16:40 - loss: 2.2427 - regression_loss: 1.8981 - classification_loss: 0.3447
  41/1500 [..............................] - ETA: 16:30 - loss: 2.2302 - regression_loss: 1.8893 - classification_loss: 0.3409
  42/1500 [..............................] - ETA: 16:54 - loss: 2.2178 - regression_loss: 1.8804 - classification_loss: 0.3374
  43/1500 [..............................] - ETA: 17:16 - loss: 2.2326 - regression_loss: 1.8880 - classification_loss: 0.3445
  44/1500 [..............................] - ETA: 17:49 - loss: 2.2260 - regression_loss: 1.8818 - classification_loss: 0.3443
  45/1500 [..............................] - ETA: 17:47 - loss: 2.2045 - regression_loss: 1.8641 - classification_loss: 0.3404
  46/1500 [..............................] - ETA: 17:44 - loss: 2.2079 - regression_loss: 1.8652 - classification_loss: 0.3427
  47/1500 [..............................] - ETA: 18:26 - loss: 2.2073 - regression_loss: 1.8638 - classification_loss: 0.3435
  48/1500 [..............................] - ETA: 18:41 - loss: 2.2017 - regression_loss: 1.8592 - classification_loss: 0.3425
  49/1500 [..............................] - ETA: 18:41 - loss: 2.2057 - regression_loss: 1.8621 - classification_loss: 0.3435
  50/1500 [>.............................] - ETA: 18:30 - loss: 2.2088 - regression_loss: 1.8667 - classification_loss: 0.3421
  51/1500 [>.............................] - ETA: 18:28 - loss: 2.2133 - regression_loss: 1.8693 - classification_loss: 0.3440
  52/1500 [>.............................] - ETA: 18:18 - loss: 2.2300 - regression_loss: 1.8845 - classification_loss: 0.3455
  53/1500 [>.............................] - ETA: 18:18 - loss: 2.2089 - regression_loss: 1.8665 - classification_loss: 0.3423
  54/1500 [>.............................] - ETA: 18:16 - loss: 2.2098 - regression_loss: 1.8674 - classification_loss: 0.3424
  55/1500 [>.............................] - ETA: 18:41 - loss: 2.2222 - regression_loss: 1.8792 - classification_loss: 0.3430
  56/1500 [>.............................] - ETA: 19:04 - loss: 2.2282 - regression_loss: 1.8855 - classification_loss: 0.3428
  57/1500 [>.............................] - ETA: 19:42 - loss: 2.2192 - regression_loss: 1.8776 - classification_loss: 0.3416
  58/1500 [>.............................] - ETA: 19:30 - loss: 2.2171 - regression_loss: 1.8777 - classification_loss: 0.3394
  59/1500 [>.............................] - ETA: 19:19 - loss: 2.2266 - regression_loss: 1.8852 - classification_loss: 0.3414
  60/1500 [>.............................] - ETA: 19:09 - loss: 2.2327 - regression_loss: 1.8912 - classification_loss: 0.3415
  61/1500 [>.............................] - ETA: 18:58 - loss: 2.2437 - regression_loss: 1.8967 - classification_loss: 0.3471
  62/1500 [>.............................] - ETA: 19:15 - loss: 2.2387 - regression_loss: 1.8925 - classification_loss: 0.3462
  63/1500 [>.............................] - ETA: 19:11 - loss: 2.2260 - regression_loss: 1.8833 - classification_loss: 0.3427
  64/1500 [>.............................] - ETA: 19:12 - loss: 2.2379 - regression_loss: 1.8911 - classification_loss: 0.3468
  65/1500 [>.............................] - ETA: 19:03 - loss: 2.2341 - regression_loss: 1.8874 - classification_loss: 0.3467
  66/1500 [>.............................] - ETA: 18:56 - loss: 2.2312 - regression_loss: 1.8866 - classification_loss: 0.3446
  67/1500 [>.............................] - ETA: 18:56 - loss: 2.2394 - regression_loss: 1.8939 - classification_loss: 0.3455
  68/1500 [>.............................] - ETA: 18:48 - loss: 2.2352 - regression_loss: 1.8914 - classification_loss: 0.3438
  69/1500 [>.............................] - ETA: 18:55 - loss: 2.2346 - regression_loss: 1.8893 - classification_loss: 0.3453
  70/1500 [>.............................] - ETA: 18:47 - loss: 2.2357 - regression_loss: 1.8877 - classification_loss: 0.3480
  71/1500 [>.............................] - ETA: 18:44 - loss: 2.2206 - regression_loss: 1.8755 - classification_loss: 0.3451
  72/1500 [>.............................] - ETA: 18:37 - loss: 2.2038 - regression_loss: 1.8612 - classification_loss: 0.3425
  73/1500 [>.............................] - ETA: 18:29 - loss: 2.1913 - regression_loss: 1.8521 - classification_loss: 0.3393
  74/1500 [>.............................] - ETA: 18:24 - loss: 2.1934 - regression_loss: 1.8544 - classification_loss: 0.3390
  75/1500 [>.............................] - ETA: 18:19 - loss: 2.1999 - regression_loss: 1.8581 - classification_loss: 0.3417
  76/1500 [>.............................] - ETA: 18:25 - loss: 2.2139 - regression_loss: 1.8667 - classification_loss: 0.3472
  77/1500 [>.............................] - ETA: 18:43 - loss: 2.2190 - regression_loss: 1.8716 - classification_loss: 0.3473
  78/1500 [>.............................] - ETA: 18:36 - loss: 2.2044 - regression_loss: 1.8597 - classification_loss: 0.3447
  79/1500 [>.............................] - ETA: 18:28 - loss: 2.1970 - regression_loss: 1.8534 - classification_loss: 0.3437
  80/1500 [>.............................] - ETA: 18:25 - loss: 2.2115 - regression_loss: 1.8664 - classification_loss: 0.3451
  81/1500 [>.............................] - ETA: 18:17 - loss: 2.2190 - regression_loss: 1.8724 - classification_loss: 0.3466
  82/1500 [>.............................] - ETA: 18:09 - loss: 2.2114 - regression_loss: 1.8664 - classification_loss: 0.3450
  83/1500 [>.............................] - ETA: 18:12 - loss: 2.2141 - regression_loss: 1.8694 - classification_loss: 0.3447
  84/1500 [>.............................] - ETA: 18:10 - loss: 2.2076 - regression_loss: 1.8644 - classification_loss: 0.3432
  85/1500 [>.............................] - ETA: 18:07 - loss: 2.2081 - regression_loss: 1.8651 - classification_loss: 0.3431
  86/1500 [>.............................] - ETA: 18:18 - loss: 2.2195 - regression_loss: 1.8735 - classification_loss: 0.3460
  87/1500 [>.............................] - ETA: 18:19 - loss: 2.2100 - regression_loss: 1.8656 - classification_loss: 0.3445
  88/1500 [>.............................] - ETA: 18:20 - loss: 2.2067 - regression_loss: 1.8641 - classification_loss: 0.3426
  89/1500 [>.............................] - ETA: 18:16 - loss: 2.2162 - regression_loss: 1.8736 - classification_loss: 0.3427
  90/1500 [>.............................] - ETA: 18:09 - loss: 2.2269 - regression_loss: 1.8719 - classification_loss: 0.3550
  91/1500 [>.............................] - ETA: 18:21 - loss: 2.2252 - regression_loss: 1.8683 - classification_loss: 0.3568
  92/1500 [>.............................] - ETA: 18:14 - loss: 2.2260 - regression_loss: 1.8692 - classification_loss: 0.3568
  93/1500 [>.............................] - ETA: 18:08 - loss: 2.2270 - regression_loss: 1.8705 - classification_loss: 0.3565
  94/1500 [>.............................] - ETA: 18:01 - loss: 2.2366 - regression_loss: 1.8782 - classification_loss: 0.3583
  95/1500 [>.............................] - ETA: 18:08 - loss: 2.2357 - regression_loss: 1.8782 - classification_loss: 0.3575
  96/1500 [>.............................] - ETA: 18:03 - loss: 2.2477 - regression_loss: 1.8882 - classification_loss: 0.3595
  97/1500 [>.............................] - ETA: 17:57 - loss: 2.2594 - regression_loss: 1.8973 - classification_loss: 0.3620
  98/1500 [>.............................] - ETA: 17:53 - loss: 2.2622 - regression_loss: 1.9008 - classification_loss: 0.3614
  99/1500 [>.............................] - ETA: 17:47 - loss: 2.2800 - regression_loss: 1.9132 - classification_loss: 0.3667
 100/1500 [=>............................] - ETA: 17:42 - loss: 2.2770 - regression_loss: 1.9116 - classification_loss: 0.3654
 101/1500 [=>............................] - ETA: 17:36 - loss: 2.2790 - regression_loss: 1.9141 - classification_loss: 0.3649
 102/1500 [=>............................] - ETA: 17:52 - loss: 2.2780 - regression_loss: 1.9137 - classification_loss: 0.3643
 103/1500 [=>............................] - ETA: 17:58 - loss: 2.2753 - regression_loss: 1.9132 - classification_loss: 0.3620
 104/1500 [=>............................] - ETA: 17:56 - loss: 2.2675 - regression_loss: 1.9064 - classification_loss: 0.3611
 105/1500 [=>............................] - ETA: 17:49 - loss: 2.2660 - regression_loss: 1.9045 - classification_loss: 0.3614
 106/1500 [=>............................] - ETA: 17:52 - loss: 2.2639 - regression_loss: 1.9041 - classification_loss: 0.3597
 107/1500 [=>............................] - ETA: 17:47 - loss: 2.2598 - regression_loss: 1.9005 - classification_loss: 0.3592
 108/1500 [=>............................] - ETA: 17:41 - loss: 2.2705 - regression_loss: 1.9094 - classification_loss: 0.3611
 109/1500 [=>............................] - ETA: 17:48 - loss: 2.2774 - regression_loss: 1.9127 - classification_loss: 0.3647
 110/1500 [=>............................] - ETA: 17:46 - loss: 2.2843 - regression_loss: 1.9180 - classification_loss: 0.3663
 111/1500 [=>............................] - ETA: 17:40 - loss: 2.2806 - regression_loss: 1.9146 - classification_loss: 0.3661
 112/1500 [=>............................] - ETA: 17:35 - loss: 2.2703 - regression_loss: 1.9061 - classification_loss: 0.3641
 113/1500 [=>............................] - ETA: 17:30 - loss: 2.2643 - regression_loss: 1.9017 - classification_loss: 0.3625
 114/1500 [=>............................] - ETA: 17:33 - loss: 2.2737 - regression_loss: 1.9092 - classification_loss: 0.3646
 115/1500 [=>............................] - ETA: 17:32 - loss: 2.2713 - regression_loss: 1.9061 - classification_loss: 0.3652
 116/1500 [=>............................] - ETA: 18:04 - loss: 2.2713 - regression_loss: 1.9065 - classification_loss: 0.3648
 117/1500 [=>............................] - ETA: 18:01 - loss: 2.2722 - regression_loss: 1.9080 - classification_loss: 0.3642
 118/1500 [=>............................] - ETA: 18:08 - loss: 2.2701 - regression_loss: 1.9070 - classification_loss: 0.3631
 119/1500 [=>............................] - ETA: 18:03 - loss: 2.2660 - regression_loss: 1.9039 - classification_loss: 0.3621
 120/1500 [=>............................] - ETA: 18:11 - loss: 2.2687 - regression_loss: 1.9057 - classification_loss: 0.3631
 121/1500 [=>............................] - ETA: 18:06 - loss: 2.2748 - regression_loss: 1.9108 - classification_loss: 0.3640
 122/1500 [=>............................] - ETA: 18:13 - loss: 2.2811 - regression_loss: 1.9146 - classification_loss: 0.3665
 123/1500 [=>............................] - ETA: 18:23 - loss: 2.2750 - regression_loss: 1.9095 - classification_loss: 0.3654
 124/1500 [=>............................] - ETA: 18:18 - loss: 2.2726 - regression_loss: 1.9076 - classification_loss: 0.3650
 125/1500 [=>............................] - ETA: 18:13 - loss: 2.2719 - regression_loss: 1.9072 - classification_loss: 0.3647
 126/1500 [=>............................] - ETA: 18:08 - loss: 2.2709 - regression_loss: 1.9069 - classification_loss: 0.3640
 127/1500 [=>............................] - ETA: 18:14 - loss: 2.2740 - regression_loss: 1.9107 - classification_loss: 0.3633
 128/1500 [=>............................] - ETA: 18:15 - loss: 2.2811 - regression_loss: 1.9173 - classification_loss: 0.3638
 129/1500 [=>............................] - ETA: 18:10 - loss: 2.2758 - regression_loss: 1.9131 - classification_loss: 0.3627
 130/1500 [=>............................] - ETA: 18:04 - loss: 2.2719 - regression_loss: 1.9095 - classification_loss: 0.3624
 131/1500 [=>............................] - ETA: 18:07 - loss: 2.2747 - regression_loss: 1.9115 - classification_loss: 0.3632
 132/1500 [=>............................] - ETA: 18:02 - loss: 2.2707 - regression_loss: 1.9084 - classification_loss: 0.3624
 133/1500 [=>............................] - ETA: 18:02 - loss: 2.2721 - regression_loss: 1.9102 - classification_loss: 0.3619
 134/1500 [=>............................] - ETA: 18:17 - loss: 2.2775 - regression_loss: 1.9148 - classification_loss: 0.3627
 135/1500 [=>............................] - ETA: 18:12 - loss: 2.2756 - regression_loss: 1.9134 - classification_loss: 0.3622
 136/1500 [=>............................] - ETA: 18:17 - loss: 2.2864 - regression_loss: 1.9236 - classification_loss: 0.3628
 137/1500 [=>............................] - ETA: 18:12 - loss: 2.2780 - regression_loss: 1.9169 - classification_loss: 0.3611
 138/1500 [=>............................] - ETA: 18:07 - loss: 2.2816 - regression_loss: 1.9200 - classification_loss: 0.3616
 139/1500 [=>............................] - ETA: 18:04 - loss: 2.2830 - regression_loss: 1.9219 - classification_loss: 0.3612
 140/1500 [=>............................] - ETA: 18:00 - loss: 2.2811 - regression_loss: 1.9207 - classification_loss: 0.3604
 141/1500 [=>............................] - ETA: 17:55 - loss: 2.2763 - regression_loss: 1.9177 - classification_loss: 0.3587
 142/1500 [=>............................] - ETA: 17:56 - loss: 2.2805 - regression_loss: 1.9214 - classification_loss: 0.3591
 143/1500 [=>............................] - ETA: 17:52 - loss: 2.2816 - regression_loss: 1.9206 - classification_loss: 0.3610
 144/1500 [=>............................] - ETA: 17:54 - loss: 2.2771 - regression_loss: 1.9171 - classification_loss: 0.3601
 145/1500 [=>............................] - ETA: 17:50 - loss: 2.2779 - regression_loss: 1.9168 - classification_loss: 0.3611
 146/1500 [=>............................] - ETA: 17:48 - loss: 2.2728 - regression_loss: 1.9126 - classification_loss: 0.3602
 147/1500 [=>............................] - ETA: 18:03 - loss: 2.2743 - regression_loss: 1.9138 - classification_loss: 0.3604
 148/1500 [=>............................] - ETA: 18:07 - loss: 2.2773 - regression_loss: 1.9164 - classification_loss: 0.3608
 149/1500 [=>............................] - ETA: 18:14 - loss: 2.2811 - regression_loss: 1.9186 - classification_loss: 0.3625
 150/1500 [==>...........................] - ETA: 18:10 - loss: 2.2759 - regression_loss: 1.9135 - classification_loss: 0.3624
 151/1500 [==>...........................] - ETA: 18:19 - loss: 2.2817 - regression_loss: 1.9177 - classification_loss: 0.3639
 152/1500 [==>...........................] - ETA: 18:15 - loss: 2.2739 - regression_loss: 1.9117 - classification_loss: 0.3623
 153/1500 [==>...........................] - ETA: 18:16 - loss: 2.2817 - regression_loss: 1.9177 - classification_loss: 0.3641
 154/1500 [==>...........................] - ETA: 18:12 - loss: 2.2762 - regression_loss: 1.9138 - classification_loss: 0.3624
 155/1500 [==>...........................] - ETA: 18:14 - loss: 2.2760 - regression_loss: 1.9133 - classification_loss: 0.3627
 156/1500 [==>...........................] - ETA: 18:11 - loss: 2.2786 - regression_loss: 1.9156 - classification_loss: 0.3630
 157/1500 [==>...........................] - ETA: 18:13 - loss: 2.2724 - regression_loss: 1.9108 - classification_loss: 0.3616
 158/1500 [==>...........................] - ETA: 18:08 - loss: 2.2741 - regression_loss: 1.9118 - classification_loss: 0.3623
 159/1500 [==>...........................] - ETA: 18:07 - loss: 2.2670 - regression_loss: 1.9061 - classification_loss: 0.3609
 160/1500 [==>...........................] - ETA: 18:11 - loss: 2.2704 - regression_loss: 1.9090 - classification_loss: 0.3614
 161/1500 [==>...........................] - ETA: 18:14 - loss: 2.2692 - regression_loss: 1.9081 - classification_loss: 0.3611
 162/1500 [==>...........................] - ETA: 18:19 - loss: 2.2698 - regression_loss: 1.9074 - classification_loss: 0.3625
 163/1500 [==>...........................] - ETA: 18:21 - loss: 2.2637 - regression_loss: 1.9020 - classification_loss: 0.3618
 164/1500 [==>...........................] - ETA: 18:17 - loss: 2.2689 - regression_loss: 1.9052 - classification_loss: 0.3637
 165/1500 [==>...........................] - ETA: 18:13 - loss: 2.2738 - regression_loss: 1.9099 - classification_loss: 0.3639
 166/1500 [==>...........................] - ETA: 18:09 - loss: 2.2801 - regression_loss: 1.9149 - classification_loss: 0.3653
 167/1500 [==>...........................] - ETA: 18:05 - loss: 2.2781 - regression_loss: 1.9133 - classification_loss: 0.3648
 168/1500 [==>...........................] - ETA: 18:01 - loss: 2.2710 - regression_loss: 1.9077 - classification_loss: 0.3634
 169/1500 [==>...........................] - ETA: 17:58 - loss: 2.2664 - regression_loss: 1.9042 - classification_loss: 0.3622
 170/1500 [==>...........................] - ETA: 17:54 - loss: 2.2676 - regression_loss: 1.9039 - classification_loss: 0.3637
 171/1500 [==>...........................] - ETA: 17:50 - loss: 2.2737 - regression_loss: 1.9093 - classification_loss: 0.3644
 172/1500 [==>...........................] - ETA: 17:51 - loss: 2.2738 - regression_loss: 1.9104 - classification_loss: 0.3634
 173/1500 [==>...........................] - ETA: 17:48 - loss: 2.2684 - regression_loss: 1.9063 - classification_loss: 0.3621
 174/1500 [==>...........................] - ETA: 17:45 - loss: 2.2714 - regression_loss: 1.9082 - classification_loss: 0.3632
 175/1500 [==>...........................] - ETA: 17:41 - loss: 2.2695 - regression_loss: 1.9076 - classification_loss: 0.3620
 176/1500 [==>...........................] - ETA: 17:37 - loss: 2.2776 - regression_loss: 1.9145 - classification_loss: 0.3631
 177/1500 [==>...........................] - ETA: 17:37 - loss: 2.2804 - regression_loss: 1.9172 - classification_loss: 0.3632
 178/1500 [==>...........................] - ETA: 17:39 - loss: 2.2802 - regression_loss: 1.9174 - classification_loss: 0.3628
 179/1500 [==>...........................] - ETA: 17:36 - loss: 2.2835 - regression_loss: 1.9197 - classification_loss: 0.3637
 180/1500 [==>...........................] - ETA: 17:38 - loss: 2.2826 - regression_loss: 1.9185 - classification_loss: 0.3641
 181/1500 [==>...........................] - ETA: 17:35 - loss: 2.2781 - regression_loss: 1.9147 - classification_loss: 0.3634
 182/1500 [==>...........................] - ETA: 17:35 - loss: 2.2755 - regression_loss: 1.9129 - classification_loss: 0.3626
 183/1500 [==>...........................] - ETA: 17:33 - loss: 2.2731 - regression_loss: 1.9116 - classification_loss: 0.3615
 184/1500 [==>...........................] - ETA: 17:29 - loss: 2.2745 - regression_loss: 1.9134 - classification_loss: 0.3611
 185/1500 [==>...........................] - ETA: 17:27 - loss: 2.2802 - regression_loss: 1.9173 - classification_loss: 0.3628
 186/1500 [==>...........................] - ETA: 17:24 - loss: 2.2903 - regression_loss: 1.9270 - classification_loss: 0.3633
 187/1500 [==>...........................] - ETA: 17:26 - loss: 2.2847 - regression_loss: 1.9227 - classification_loss: 0.3620
 188/1500 [==>...........................] - ETA: 17:23 - loss: 2.2817 - regression_loss: 1.9196 - classification_loss: 0.3621
 189/1500 [==>...........................] - ETA: 17:21 - loss: 2.2860 - regression_loss: 1.9239 - classification_loss: 0.3622
 190/1500 [==>...........................] - ETA: 17:18 - loss: 2.2840 - regression_loss: 1.9225 - classification_loss: 0.3615
 191/1500 [==>...........................] - ETA: 17:15 - loss: 2.2873 - regression_loss: 1.9260 - classification_loss: 0.3613
 192/1500 [==>...........................] - ETA: 17:14 - loss: 2.2847 - regression_loss: 1.9243 - classification_loss: 0.3604
 193/1500 [==>...........................] - ETA: 17:10 - loss: 2.2818 - regression_loss: 1.9223 - classification_loss: 0.3595
 194/1500 [==>...........................] - ETA: 17:13 - loss: 2.2783 - regression_loss: 1.9185 - classification_loss: 0.3599
 195/1500 [==>...........................] - ETA: 17:14 - loss: 2.2723 - regression_loss: 1.9135 - classification_loss: 0.3588
 196/1500 [==>...........................] - ETA: 17:15 - loss: 2.2763 - regression_loss: 1.9161 - classification_loss: 0.3602
 197/1500 [==>...........................] - ETA: 17:12 - loss: 2.2814 - regression_loss: 1.9205 - classification_loss: 0.3609
 198/1500 [==>...........................] - ETA: 17:10 - loss: 2.2779 - regression_loss: 1.9180 - classification_loss: 0.3599
 199/1500 [==>...........................] - ETA: 17:09 - loss: 2.2805 - regression_loss: 1.9206 - classification_loss: 0.3599
 200/1500 [===>..........................] - ETA: 17:06 - loss: 2.2775 - regression_loss: 1.9172 - classification_loss: 0.3603
 201/1500 [===>..........................] - ETA: 17:08 - loss: 2.2765 - regression_loss: 1.9165 - classification_loss: 0.3600
 202/1500 [===>..........................] - ETA: 17:07 - loss: 2.2794 - regression_loss: 1.9188 - classification_loss: 0.3606
 203/1500 [===>..........................] - ETA: 17:05 - loss: 2.2790 - regression_loss: 1.9183 - classification_loss: 0.3607
 204/1500 [===>..........................] - ETA: 17:05 - loss: 2.2788 - regression_loss: 1.9184 - classification_loss: 0.3604
 205/1500 [===>..........................] - ETA: 17:05 - loss: 2.2822 - regression_loss: 1.9208 - classification_loss: 0.3615
 206/1500 [===>..........................] - ETA: 17:01 - loss: 2.2847 - regression_loss: 1.9227 - classification_loss: 0.3621
 207/1500 [===>..........................] - ETA: 17:00 - loss: 2.2860 - regression_loss: 1.9232 - classification_loss: 0.3628
 208/1500 [===>..........................] - ETA: 16:57 - loss: 2.2874 - regression_loss: 1.9246 - classification_loss: 0.3628
 209/1500 [===>..........................] - ETA: 16:54 - loss: 2.2836 - regression_loss: 1.9214 - classification_loss: 0.3622
 210/1500 [===>..........................] - ETA: 16:52 - loss: 2.2800 - regression_loss: 1.9186 - classification_loss: 0.3614
 211/1500 [===>..........................] - ETA: 16:49 - loss: 2.2809 - regression_loss: 1.9197 - classification_loss: 0.3613
 212/1500 [===>..........................] - ETA: 16:50 - loss: 2.2841 - regression_loss: 1.9223 - classification_loss: 0.3618
 213/1500 [===>..........................] - ETA: 16:50 - loss: 2.2857 - regression_loss: 1.9235 - classification_loss: 0.3622
 214/1500 [===>..........................] - ETA: 16:52 - loss: 2.2875 - regression_loss: 1.9242 - classification_loss: 0.3633
 215/1500 [===>..........................] - ETA: 16:56 - loss: 2.2922 - regression_loss: 1.9270 - classification_loss: 0.3652
 216/1500 [===>..........................] - ETA: 16:53 - loss: 2.2939 - regression_loss: 1.9277 - classification_loss: 0.3662
 217/1500 [===>..........................] - ETA: 16:50 - loss: 2.2935 - regression_loss: 1.9278 - classification_loss: 0.3657
 218/1500 [===>..........................] - ETA: 16:49 - loss: 2.2889 - regression_loss: 1.9240 - classification_loss: 0.3649
 219/1500 [===>..........................] - ETA: 16:49 - loss: 2.2857 - regression_loss: 1.9210 - classification_loss: 0.3646
 220/1500 [===>..........................] - ETA: 16:46 - loss: 2.2809 - regression_loss: 1.9168 - classification_loss: 0.3642
 221/1500 [===>..........................] - ETA: 16:45 - loss: 2.2784 - regression_loss: 1.9148 - classification_loss: 0.3637
 222/1500 [===>..........................] - ETA: 16:44 - loss: 2.2863 - regression_loss: 1.9221 - classification_loss: 0.3642
 223/1500 [===>..........................] - ETA: 16:44 - loss: 2.2854 - regression_loss: 1.9220 - classification_loss: 0.3634
 224/1500 [===>..........................] - ETA: 16:41 - loss: 2.2855 - regression_loss: 1.9219 - classification_loss: 0.3637
 225/1500 [===>..........................] - ETA: 16:38 - loss: 2.2853 - regression_loss: 1.9213 - classification_loss: 0.3640
 226/1500 [===>..........................] - ETA: 16:38 - loss: 2.2822 - regression_loss: 1.9186 - classification_loss: 0.3635
 227/1500 [===>..........................] - ETA: 16:39 - loss: 2.2806 - regression_loss: 1.9167 - classification_loss: 0.3639
 228/1500 [===>..........................] - ETA: 16:36 - loss: 2.2802 - regression_loss: 1.9156 - classification_loss: 0.3646
 229/1500 [===>..........................] - ETA: 16:34 - loss: 2.2849 - regression_loss: 1.9189 - classification_loss: 0.3660
 230/1500 [===>..........................] - ETA: 16:31 - loss: 2.2900 - regression_loss: 1.9227 - classification_loss: 0.3673
 231/1500 [===>..........................] - ETA: 16:28 - loss: 2.2888 - regression_loss: 1.9220 - classification_loss: 0.3669
 232/1500 [===>..........................] - ETA: 16:26 - loss: 2.2918 - regression_loss: 1.9248 - classification_loss: 0.3671
 233/1500 [===>..........................] - ETA: 16:23 - loss: 2.2930 - regression_loss: 1.9258 - classification_loss: 0.3672
 234/1500 [===>..........................] - ETA: 16:22 - loss: 2.2941 - regression_loss: 1.9266 - classification_loss: 0.3675
 235/1500 [===>..........................] - ETA: 16:19 - loss: 2.2909 - regression_loss: 1.9240 - classification_loss: 0.3670
 236/1500 [===>..........................] - ETA: 16:22 - loss: 2.2926 - regression_loss: 1.9255 - classification_loss: 0.3671
 237/1500 [===>..........................] - ETA: 16:19 - loss: 2.2909 - regression_loss: 1.9244 - classification_loss: 0.3665
 238/1500 [===>..........................] - ETA: 16:17 - loss: 2.2915 - regression_loss: 1.9251 - classification_loss: 0.3664
 239/1500 [===>..........................] - ETA: 16:15 - loss: 2.2965 - regression_loss: 1.9291 - classification_loss: 0.3674
 240/1500 [===>..........................] - ETA: 16:12 - loss: 2.2955 - regression_loss: 1.9283 - classification_loss: 0.3672
 241/1500 [===>..........................] - ETA: 16:11 - loss: 2.2931 - regression_loss: 1.9266 - classification_loss: 0.3665
 242/1500 [===>..........................] - ETA: 16:09 - loss: 2.2941 - regression_loss: 1.9277 - classification_loss: 0.3664
 243/1500 [===>..........................] - ETA: 16:08 - loss: 2.2980 - regression_loss: 1.9312 - classification_loss: 0.3668
 244/1500 [===>..........................] - ETA: 16:06 - loss: 2.3011 - regression_loss: 1.9340 - classification_loss: 0.3671
 245/1500 [===>..........................] - ETA: 16:12 - loss: 2.3007 - regression_loss: 1.9332 - classification_loss: 0.3675
 246/1500 [===>..........................] - ETA: 16:12 - loss: 2.3032 - regression_loss: 1.9356 - classification_loss: 0.3676
 247/1500 [===>..........................] - ETA: 16:09 - loss: 2.3070 - regression_loss: 1.9387 - classification_loss: 0.3684
 248/1500 [===>..........................] - ETA: 16:07 - loss: 2.3081 - regression_loss: 1.9396 - classification_loss: 0.3685
 249/1500 [===>..........................] - ETA: 16:04 - loss: 2.3085 - regression_loss: 1.9405 - classification_loss: 0.3680
 250/1500 [====>.........................] - ETA: 16:03 - loss: 2.3089 - regression_loss: 1.9412 - classification_loss: 0.3677
 251/1500 [====>.........................] - ETA: 16:03 - loss: 2.3114 - regression_loss: 1.9432 - classification_loss: 0.3682
 252/1500 [====>.........................] - ETA: 16:02 - loss: 2.3085 - regression_loss: 1.9405 - classification_loss: 0.3680
 253/1500 [====>.........................] - ETA: 16:03 - loss: 2.3049 - regression_loss: 1.9375 - classification_loss: 0.3674
 254/1500 [====>.........................] - ETA: 16:01 - loss: 2.3084 - regression_loss: 1.9410 - classification_loss: 0.3674
 255/1500 [====>.........................] - ETA: 16:04 - loss: 2.3094 - regression_loss: 1.9420 - classification_loss: 0.3674
 256/1500 [====>.........................] - ETA: 16:05 - loss: 2.3120 - regression_loss: 1.9442 - classification_loss: 0.3678
 257/1500 [====>.........................] - ETA: 16:05 - loss: 2.3108 - regression_loss: 1.9431 - classification_loss: 0.3677
 258/1500 [====>.........................] - ETA: 16:03 - loss: 2.3101 - regression_loss: 1.9427 - classification_loss: 0.3674
 259/1500 [====>.........................] - ETA: 16:01 - loss: 2.3069 - regression_loss: 1.9402 - classification_loss: 0.3667
 260/1500 [====>.........................] - ETA: 15:59 - loss: 2.3069 - regression_loss: 1.9400 - classification_loss: 0.3669
 261/1500 [====>.........................] - ETA: 15:59 - loss: 2.3047 - regression_loss: 1.9384 - classification_loss: 0.3662
 262/1500 [====>.........................] - ETA: 15:57 - loss: 2.3022 - regression_loss: 1.9364 - classification_loss: 0.3658
 263/1500 [====>.........................] - ETA: 15:54 - loss: 2.3015 - regression_loss: 1.9360 - classification_loss: 0.3655
 264/1500 [====>.........................] - ETA: 15:52 - loss: 2.3057 - regression_loss: 1.9393 - classification_loss: 0.3665
 265/1500 [====>.........................] - ETA: 15:49 - loss: 2.3054 - regression_loss: 1.9395 - classification_loss: 0.3659
 266/1500 [====>.........................] - ETA: 15:47 - loss: 2.3053 - regression_loss: 1.9396 - classification_loss: 0.3657
 267/1500 [====>.........................] - ETA: 15:44 - loss: 2.3066 - regression_loss: 1.9404 - classification_loss: 0.3662
 268/1500 [====>.........................] - ETA: 15:41 - loss: 2.3020 - regression_loss: 1.9368 - classification_loss: 0.3652
 269/1500 [====>.........................] - ETA: 15:41 - loss: 2.3030 - regression_loss: 1.9376 - classification_loss: 0.3654
 270/1500 [====>.........................] - ETA: 15:40 - loss: 2.3052 - regression_loss: 1.9392 - classification_loss: 0.3660
 271/1500 [====>.........................] - ETA: 15:40 - loss: 2.3042 - regression_loss: 1.9381 - classification_loss: 0.3661
 272/1500 [====>.........................] - ETA: 15:37 - loss: 2.3027 - regression_loss: 1.9369 - classification_loss: 0.3658
 273/1500 [====>.........................] - ETA: 15:35 - loss: 2.3027 - regression_loss: 1.9369 - classification_loss: 0.3658
 274/1500 [====>.........................] - ETA: 15:33 - loss: 2.3030 - regression_loss: 1.9375 - classification_loss: 0.3655
 275/1500 [====>.........................] - ETA: 15:31 - loss: 2.3040 - regression_loss: 1.9383 - classification_loss: 0.3657
 276/1500 [====>.........................] - ETA: 15:35 - loss: 2.3018 - regression_loss: 1.9367 - classification_loss: 0.3651
 277/1500 [====>.........................] - ETA: 15:36 - loss: 2.2978 - regression_loss: 1.9337 - classification_loss: 0.3642
 278/1500 [====>.........................] - ETA: 15:38 - loss: 2.2941 - regression_loss: 1.9298 - classification_loss: 0.3643
 279/1500 [====>.........................] - ETA: 15:36 - loss: 2.2927 - regression_loss: 1.9287 - classification_loss: 0.3640
 280/1500 [====>.........................] - ETA: 15:33 - loss: 2.2900 - regression_loss: 1.9267 - classification_loss: 0.3633
 281/1500 [====>.........................] - ETA: 15:33 - loss: 2.2889 - regression_loss: 1.9260 - classification_loss: 0.3630
 282/1500 [====>.........................] - ETA: 15:31 - loss: 2.2837 - regression_loss: 1.9215 - classification_loss: 0.3622
 283/1500 [====>.........................] - ETA: 15:32 - loss: 2.2834 - regression_loss: 1.9215 - classification_loss: 0.3619
 284/1500 [====>.........................] - ETA: 15:29 - loss: 2.2862 - regression_loss: 1.9233 - classification_loss: 0.3629
 285/1500 [====>.........................] - ETA: 15:27 - loss: 2.2822 - regression_loss: 1.9198 - classification_loss: 0.3625
 286/1500 [====>.........................] - ETA: 15:30 - loss: 2.2865 - regression_loss: 1.9236 - classification_loss: 0.3629
 287/1500 [====>.........................] - ETA: 15:28 - loss: 2.2882 - regression_loss: 1.9254 - classification_loss: 0.3628
 288/1500 [====>.........................] - ETA: 15:27 - loss: 2.2881 - regression_loss: 1.9252 - classification_loss: 0.3629
 289/1500 [====>.........................] - ETA: 15:25 - loss: 2.2881 - regression_loss: 1.9255 - classification_loss: 0.3626
 290/1500 [====>.........................] - ETA: 15:22 - loss: 2.2893 - regression_loss: 1.9267 - classification_loss: 0.3626
 291/1500 [====>.........................] - ETA: 15:20 - loss: 2.2919 - regression_loss: 1.9287 - classification_loss: 0.3632
 292/1500 [====>.........................] - ETA: 15:18 - loss: 2.2941 - regression_loss: 1.9303 - classification_loss: 0.3638
 293/1500 [====>.........................] - ETA: 15:16 - loss: 2.2935 - regression_loss: 1.9299 - classification_loss: 0.3636
 294/1500 [====>.........................] - ETA: 15:14 - loss: 2.2967 - regression_loss: 1.9329 - classification_loss: 0.3638
 295/1500 [====>.........................] - ETA: 15:15 - loss: 2.2997 - regression_loss: 1.9350 - classification_loss: 0.3648
 296/1500 [====>.........................] - ETA: 15:13 - loss: 2.3037 - regression_loss: 1.9386 - classification_loss: 0.3651
 297/1500 [====>.........................] - ETA: 15:10 - loss: 2.3008 - regression_loss: 1.9364 - classification_loss: 0.3645
 298/1500 [====>.........................] - ETA: 15:08 - loss: 2.2974 - regression_loss: 1.9335 - classification_loss: 0.3639
 299/1500 [====>.........................] - ETA: 15:07 - loss: 2.2993 - regression_loss: 1.9352 - classification_loss: 0.3642
 300/1500 [=====>........................] - ETA: 15:07 - loss: 2.3016 - regression_loss: 1.9368 - classification_loss: 0.3648
 301/1500 [=====>........................] - ETA: 15:05 - loss: 2.3018 - regression_loss: 1.9371 - classification_loss: 0.3647
 302/1500 [=====>........................] - ETA: 15:04 - loss: 2.3001 - regression_loss: 1.9358 - classification_loss: 0.3642
 303/1500 [=====>........................] - ETA: 15:03 - loss: 2.2969 - regression_loss: 1.9334 - classification_loss: 0.3635
 304/1500 [=====>........................] - ETA: 15:01 - loss: 2.2937 - regression_loss: 1.9308 - classification_loss: 0.3628
 305/1500 [=====>........................] - ETA: 15:01 - loss: 2.2918 - regression_loss: 1.9295 - classification_loss: 0.3623
 306/1500 [=====>........................] - ETA: 15:04 - loss: 2.2904 - regression_loss: 1.9287 - classification_loss: 0.3617
 307/1500 [=====>........................] - ETA: 15:03 - loss: 2.2904 - regression_loss: 1.9283 - classification_loss: 0.3620
 308/1500 [=====>........................] - ETA: 15:05 - loss: 2.2908 - regression_loss: 1.9288 - classification_loss: 0.3621
 309/1500 [=====>........................] - ETA: 15:03 - loss: 2.2873 - regression_loss: 1.9255 - classification_loss: 0.3619
 310/1500 [=====>........................] - ETA: 15:02 - loss: 2.2839 - regression_loss: 1.9227 - classification_loss: 0.3612
 311/1500 [=====>........................] - ETA: 15:00 - loss: 2.2817 - regression_loss: 1.9208 - classification_loss: 0.3608
 312/1500 [=====>........................] - ETA: 15:03 - loss: 2.2834 - regression_loss: 1.9219 - classification_loss: 0.3615
 313/1500 [=====>........................] - ETA: 15:03 - loss: 2.2801 - regression_loss: 1.9193 - classification_loss: 0.3609
 314/1500 [=====>........................] - ETA: 15:03 - loss: 2.2771 - regression_loss: 1.9167 - classification_loss: 0.3604
 315/1500 [=====>........................] - ETA: 15:00 - loss: 2.2786 - regression_loss: 1.9177 - classification_loss: 0.3609
 316/1500 [=====>........................] - ETA: 15:01 - loss: 2.2800 - regression_loss: 1.9190 - classification_loss: 0.3610
 317/1500 [=====>........................] - ETA: 15:00 - loss: 2.2795 - regression_loss: 1.9190 - classification_loss: 0.3605
 318/1500 [=====>........................] - ETA: 15:00 - loss: 2.2753 - regression_loss: 1.9154 - classification_loss: 0.3599
 319/1500 [=====>........................] - ETA: 15:02 - loss: 2.2762 - regression_loss: 1.9164 - classification_loss: 0.3599
 320/1500 [=====>........................] - ETA: 15:00 - loss: 2.2729 - regression_loss: 1.9135 - classification_loss: 0.3594
 321/1500 [=====>........................] - ETA: 14:58 - loss: 2.2717 - regression_loss: 1.9130 - classification_loss: 0.3588
 322/1500 [=====>........................] - ETA: 14:56 - loss: 2.2738 - regression_loss: 1.9148 - classification_loss: 0.3591
 323/1500 [=====>........................] - ETA: 14:55 - loss: 2.2717 - regression_loss: 1.9132 - classification_loss: 0.3585
 324/1500 [=====>........................] - ETA: 14:53 - loss: 2.2697 - regression_loss: 1.9116 - classification_loss: 0.3581
 325/1500 [=====>........................] - ETA: 14:51 - loss: 2.2682 - regression_loss: 1.9104 - classification_loss: 0.3578
 326/1500 [=====>........................] - ETA: 14:50 - loss: 2.2686 - regression_loss: 1.9107 - classification_loss: 0.3579
 327/1500 [=====>........................] - ETA: 14:52 - loss: 2.2696 - regression_loss: 1.9114 - classification_loss: 0.3582
 328/1500 [=====>........................] - ETA: 14:52 - loss: 2.2671 - regression_loss: 1.9095 - classification_loss: 0.3576
 329/1500 [=====>........................] - ETA: 14:55 - loss: 2.2646 - regression_loss: 1.9074 - classification_loss: 0.3572
 330/1500 [=====>........................] - ETA: 14:56 - loss: 2.2649 - regression_loss: 1.9080 - classification_loss: 0.3569
 331/1500 [=====>........................] - ETA: 15:02 - loss: 2.2684 - regression_loss: 1.9107 - classification_loss: 0.3577
 332/1500 [=====>........................] - ETA: 15:04 - loss: 2.2680 - regression_loss: 1.9099 - classification_loss: 0.3582
 333/1500 [=====>........................] - ETA: 15:11 - loss: 2.2688 - regression_loss: 1.9105 - classification_loss: 0.3583
 334/1500 [=====>........................] - ETA: 15:09 - loss: 2.2657 - regression_loss: 1.9080 - classification_loss: 0.3577
 335/1500 [=====>........................] - ETA: 15:07 - loss: 2.2622 - regression_loss: 1.9052 - classification_loss: 0.3571
 336/1500 [=====>........................] - ETA: 15:14 - loss: 2.2679 - regression_loss: 1.9097 - classification_loss: 0.3582
 337/1500 [=====>........................] - ETA: 15:14 - loss: 2.2660 - regression_loss: 1.9081 - classification_loss: 0.3578
 338/1500 [=====>........................] - ETA: 15:15 - loss: 2.2669 - regression_loss: 1.9090 - classification_loss: 0.3579
 339/1500 [=====>........................] - ETA: 15:14 - loss: 2.2672 - regression_loss: 1.9091 - classification_loss: 0.3581
 340/1500 [=====>........................] - ETA: 15:14 - loss: 2.2647 - regression_loss: 1.9072 - classification_loss: 0.3575
 341/1500 [=====>........................] - ETA: 15:14 - loss: 2.2628 - regression_loss: 1.9059 - classification_loss: 0.3569
 342/1500 [=====>........................] - ETA: 15:14 - loss: 2.2637 - regression_loss: 1.9064 - classification_loss: 0.3573
 343/1500 [=====>........................] - ETA: 15:15 - loss: 2.2625 - regression_loss: 1.9055 - classification_loss: 0.3570
 344/1500 [=====>........................] - ETA: 15:13 - loss: 2.2599 - regression_loss: 1.9035 - classification_loss: 0.3564
 345/1500 [=====>........................] - ETA: 15:11 - loss: 2.2609 - regression_loss: 1.9040 - classification_loss: 0.3569
 346/1500 [=====>........................] - ETA: 15:11 - loss: 2.2616 - regression_loss: 1.9043 - classification_loss: 0.3572
 347/1500 [=====>........................] - ETA: 15:11 - loss: 2.2603 - regression_loss: 1.9034 - classification_loss: 0.3570
 348/1500 [=====>........................] - ETA: 15:09 - loss: 2.2622 - regression_loss: 1.9054 - classification_loss: 0.3568
 349/1500 [=====>........................] - ETA: 15:08 - loss: 2.2629 - regression_loss: 1.9057 - classification_loss: 0.3572
 350/1500 [======>.......................] - ETA: 15:06 - loss: 2.2631 - regression_loss: 1.9058 - classification_loss: 0.3573
 351/1500 [======>.......................] - ETA: 15:04 - loss: 2.2636 - regression_loss: 1.9060 - classification_loss: 0.3576
 352/1500 [======>.......................] - ETA: 15:02 - loss: 2.2642 - regression_loss: 1.9068 - classification_loss: 0.3574
 353/1500 [======>.......................] - ETA: 15:00 - loss: 2.2622 - regression_loss: 1.9052 - classification_loss: 0.3570
 354/1500 [======>.......................] - ETA: 14:59 - loss: 2.2600 - regression_loss: 1.9034 - classification_loss: 0.3567
 355/1500 [======>.......................] - ETA: 15:00 - loss: 2.2612 - regression_loss: 1.9044 - classification_loss: 0.3568
 356/1500 [======>.......................] - ETA: 14:58 - loss: 2.2637 - regression_loss: 1.9068 - classification_loss: 0.3569
 357/1500 [======>.......................] - ETA: 14:56 - loss: 2.2671 - regression_loss: 1.9090 - classification_loss: 0.3581
 358/1500 [======>.......................] - ETA: 14:54 - loss: 2.2681 - regression_loss: 1.9102 - classification_loss: 0.3579
 359/1500 [======>.......................] - ETA: 14:54 - loss: 2.2694 - regression_loss: 1.9114 - classification_loss: 0.3580
 360/1500 [======>.......................] - ETA: 14:55 - loss: 2.2679 - regression_loss: 1.9100 - classification_loss: 0.3579
 361/1500 [======>.......................] - ETA: 14:53 - loss: 2.2654 - regression_loss: 1.9075 - classification_loss: 0.3579
 362/1500 [======>.......................] - ETA: 14:51 - loss: 2.2633 - regression_loss: 1.9055 - classification_loss: 0.3577
 363/1500 [======>.......................] - ETA: 14:50 - loss: 2.2657 - regression_loss: 1.9071 - classification_loss: 0.3586
 364/1500 [======>.......................] - ETA: 14:48 - loss: 2.2647 - regression_loss: 1.9065 - classification_loss: 0.3582
 365/1500 [======>.......................] - ETA: 14:46 - loss: 2.2672 - regression_loss: 1.9082 - classification_loss: 0.3590
 366/1500 [======>.......................] - ETA: 14:44 - loss: 2.2683 - regression_loss: 1.9094 - classification_loss: 0.3589
 367/1500 [======>.......................] - ETA: 14:46 - loss: 2.2700 - regression_loss: 1.9105 - classification_loss: 0.3595
 368/1500 [======>.......................] - ETA: 14:44 - loss: 2.2701 - regression_loss: 1.9107 - classification_loss: 0.3594
 369/1500 [======>.......................] - ETA: 14:44 - loss: 2.2721 - regression_loss: 1.9119 - classification_loss: 0.3601
 370/1500 [======>.......................] - ETA: 14:46 - loss: 2.2707 - regression_loss: 1.9109 - classification_loss: 0.3598
 371/1500 [======>.......................] - ETA: 14:46 - loss: 2.2702 - regression_loss: 1.9110 - classification_loss: 0.3593
 372/1500 [======>.......................] - ETA: 14:47 - loss: 2.2707 - regression_loss: 1.9110 - classification_loss: 0.3597
 373/1500 [======>.......................] - ETA: 14:45 - loss: 2.2687 - regression_loss: 1.9093 - classification_loss: 0.3595
 374/1500 [======>.......................] - ETA: 14:43 - loss: 2.2698 - regression_loss: 1.9103 - classification_loss: 0.3595
 375/1500 [======>.......................] - ETA: 14:42 - loss: 2.2710 - regression_loss: 1.9112 - classification_loss: 0.3598
 376/1500 [======>.......................] - ETA: 14:41 - loss: 2.2720 - regression_loss: 1.9121 - classification_loss: 0.3599
 377/1500 [======>.......................] - ETA: 14:39 - loss: 2.2708 - regression_loss: 1.9112 - classification_loss: 0.3596
 378/1500 [======>.......................] - ETA: 14:37 - loss: 2.2715 - regression_loss: 1.9117 - classification_loss: 0.3598
 379/1500 [======>.......................] - ETA: 14:38 - loss: 2.2709 - regression_loss: 1.9112 - classification_loss: 0.3597
 380/1500 [======>.......................] - ETA: 14:38 - loss: 2.2707 - regression_loss: 1.9112 - classification_loss: 0.3595
 381/1500 [======>.......................] - ETA: 14:37 - loss: 2.2685 - regression_loss: 1.9094 - classification_loss: 0.3590
 382/1500 [======>.......................] - ETA: 14:35 - loss: 2.2711 - regression_loss: 1.9116 - classification_loss: 0.3595
 383/1500 [======>.......................] - ETA: 14:37 - loss: 2.2723 - regression_loss: 1.9128 - classification_loss: 0.3595
 384/1500 [======>.......................] - ETA: 14:37 - loss: 2.2706 - regression_loss: 1.9115 - classification_loss: 0.3590
 385/1500 [======>.......................] - ETA: 14:35 - loss: 2.2715 - regression_loss: 1.9126 - classification_loss: 0.3589
 386/1500 [======>.......................] - ETA: 14:34 - loss: 2.2743 - regression_loss: 1.9153 - classification_loss: 0.3589
 387/1500 [======>.......................] - ETA: 14:32 - loss: 2.2727 - regression_loss: 1.9142 - classification_loss: 0.3586
 388/1500 [======>.......................] - ETA: 14:32 - loss: 2.2734 - regression_loss: 1.9151 - classification_loss: 0.3583
 389/1500 [======>.......................] - ETA: 14:30 - loss: 2.2754 - regression_loss: 1.9167 - classification_loss: 0.3586
 390/1500 [======>.......................] - ETA: 14:29 - loss: 2.2775 - regression_loss: 1.9183 - classification_loss: 0.3592
 391/1500 [======>.......................] - ETA: 14:27 - loss: 2.2757 - regression_loss: 1.9170 - classification_loss: 0.3588
 392/1500 [======>.......................] - ETA: 14:25 - loss: 2.2759 - regression_loss: 1.9172 - classification_loss: 0.3587
 393/1500 [======>.......................] - ETA: 14:24 - loss: 2.2769 - regression_loss: 1.9178 - classification_loss: 0.3590
 394/1500 [======>.......................] - ETA: 14:23 - loss: 2.2762 - regression_loss: 1.9171 - classification_loss: 0.3591
 395/1500 [======>.......................] - ETA: 14:21 - loss: 2.2752 - regression_loss: 1.9164 - classification_loss: 0.3589
 396/1500 [======>.......................] - ETA: 14:20 - loss: 2.2731 - regression_loss: 1.9146 - classification_loss: 0.3585
 397/1500 [======>.......................] - ETA: 14:18 - loss: 2.2726 - regression_loss: 1.9141 - classification_loss: 0.3585
 398/1500 [======>.......................] - ETA: 14:17 - loss: 2.2735 - regression_loss: 1.9148 - classification_loss: 0.3588
 399/1500 [======>.......................] - ETA: 14:16 - loss: 2.2751 - regression_loss: 1.9165 - classification_loss: 0.3586
 400/1500 [=======>......................] - ETA: 14:14 - loss: 2.2746 - regression_loss: 1.9160 - classification_loss: 0.3586
 401/1500 [=======>......................] - ETA: 14:12 - loss: 2.2752 - regression_loss: 1.9168 - classification_loss: 0.3585
 402/1500 [=======>......................] - ETA: 14:11 - loss: 2.2762 - regression_loss: 1.9177 - classification_loss: 0.3586
 403/1500 [=======>......................] - ETA: 14:11 - loss: 2.2767 - regression_loss: 1.9181 - classification_loss: 0.3586
 404/1500 [=======>......................] - ETA: 14:10 - loss: 2.2772 - regression_loss: 1.9188 - classification_loss: 0.3584
 405/1500 [=======>......................] - ETA: 14:11 - loss: 2.2765 - regression_loss: 1.9185 - classification_loss: 0.3580
 406/1500 [=======>......................] - ETA: 14:10 - loss: 2.2785 - regression_loss: 1.9205 - classification_loss: 0.3581
 407/1500 [=======>......................] - ETA: 14:09 - loss: 2.2799 - regression_loss: 1.9215 - classification_loss: 0.3584
 408/1500 [=======>......................] - ETA: 14:09 - loss: 2.2795 - regression_loss: 1.9215 - classification_loss: 0.3580
 409/1500 [=======>......................] - ETA: 14:08 - loss: 2.2819 - regression_loss: 1.9235 - classification_loss: 0.3584
 410/1500 [=======>......................] - ETA: 14:08 - loss: 2.2831 - regression_loss: 1.9248 - classification_loss: 0.3583
 411/1500 [=======>......................] - ETA: 14:06 - loss: 2.2839 - regression_loss: 1.9254 - classification_loss: 0.3585
 412/1500 [=======>......................] - ETA: 14:08 - loss: 2.2814 - regression_loss: 1.9234 - classification_loss: 0.3580
 413/1500 [=======>......................] - ETA: 14:09 - loss: 2.2825 - regression_loss: 1.9247 - classification_loss: 0.3579
 414/1500 [=======>......................] - ETA: 14:07 - loss: 2.2826 - regression_loss: 1.9247 - classification_loss: 0.3579
 415/1500 [=======>......................] - ETA: 14:06 - loss: 2.2798 - regression_loss: 1.9224 - classification_loss: 0.3574
 416/1500 [=======>......................] - ETA: 14:04 - loss: 2.2816 - regression_loss: 1.9240 - classification_loss: 0.3576
 417/1500 [=======>......................] - ETA: 14:04 - loss: 2.2804 - regression_loss: 1.9231 - classification_loss: 0.3573
 418/1500 [=======>......................] - ETA: 14:02 - loss: 2.2790 - regression_loss: 1.9221 - classification_loss: 0.3569
 419/1500 [=======>......................] - ETA: 14:00 - loss: 2.2778 - regression_loss: 1.9211 - classification_loss: 0.3566
 420/1500 [=======>......................] - ETA: 13:59 - loss: 2.2787 - regression_loss: 1.9215 - classification_loss: 0.3572
 421/1500 [=======>......................] - ETA: 13:57 - loss: 2.2798 - regression_loss: 1.9225 - classification_loss: 0.3573
 422/1500 [=======>......................] - ETA: 13:55 - loss: 2.2819 - regression_loss: 1.9237 - classification_loss: 0.3582
 423/1500 [=======>......................] - ETA: 13:57 - loss: 2.2814 - regression_loss: 1.9234 - classification_loss: 0.3581
 424/1500 [=======>......................] - ETA: 13:55 - loss: 2.2811 - regression_loss: 1.9232 - classification_loss: 0.3579
 425/1500 [=======>......................] - ETA: 13:53 - loss: 2.2802 - regression_loss: 1.9227 - classification_loss: 0.3575
 426/1500 [=======>......................] - ETA: 13:53 - loss: 2.2799 - regression_loss: 1.9227 - classification_loss: 0.3572
 427/1500 [=======>......................] - ETA: 13:55 - loss: 2.2803 - regression_loss: 1.9229 - classification_loss: 0.3573
 428/1500 [=======>......................] - ETA: 13:56 - loss: 2.2810 - regression_loss: 1.9235 - classification_loss: 0.3575
 429/1500 [=======>......................] - ETA: 13:54 - loss: 2.2790 - regression_loss: 1.9215 - classification_loss: 0.3575
 430/1500 [=======>......................] - ETA: 13:52 - loss: 2.2779 - regression_loss: 1.9206 - classification_loss: 0.3573
 431/1500 [=======>......................] - ETA: 13:51 - loss: 2.2774 - regression_loss: 1.9205 - classification_loss: 0.3569
 432/1500 [=======>......................] - ETA: 13:49 - loss: 2.2789 - regression_loss: 1.9219 - classification_loss: 0.3570
 433/1500 [=======>......................] - ETA: 13:48 - loss: 2.2792 - regression_loss: 1.9224 - classification_loss: 0.3568
 434/1500 [=======>......................] - ETA: 13:48 - loss: 2.2798 - regression_loss: 1.9232 - classification_loss: 0.3567
 435/1500 [=======>......................] - ETA: 13:47 - loss: 2.2797 - regression_loss: 1.9231 - classification_loss: 0.3567
 436/1500 [=======>......................] - ETA: 13:46 - loss: 2.2785 - regression_loss: 1.9223 - classification_loss: 0.3562
 437/1500 [=======>......................] - ETA: 13:46 - loss: 2.2787 - regression_loss: 1.9224 - classification_loss: 0.3562
 438/1500 [=======>......................] - ETA: 13:45 - loss: 2.2787 - regression_loss: 1.9227 - classification_loss: 0.3561
 439/1500 [=======>......................] - ETA: 13:43 - loss: 2.2777 - regression_loss: 1.9218 - classification_loss: 0.3559
 440/1500 [=======>......................] - ETA: 13:42 - loss: 2.2768 - regression_loss: 1.9212 - classification_loss: 0.3556
 441/1500 [=======>......................] - ETA: 13:40 - loss: 2.2784 - regression_loss: 1.9226 - classification_loss: 0.3558
 442/1500 [=======>......................] - ETA: 13:38 - loss: 2.2833 - regression_loss: 1.9265 - classification_loss: 0.3569
 443/1500 [=======>......................] - ETA: 13:38 - loss: 2.2811 - regression_loss: 1.9247 - classification_loss: 0.3564
 444/1500 [=======>......................] - ETA: 13:38 - loss: 2.2792 - regression_loss: 1.9229 - classification_loss: 0.3563
 445/1500 [=======>......................] - ETA: 13:37 - loss: 2.2767 - regression_loss: 1.9209 - classification_loss: 0.3558
 446/1500 [=======>......................] - ETA: 13:39 - loss: 2.2767 - regression_loss: 1.9211 - classification_loss: 0.3556
 447/1500 [=======>......................] - ETA: 13:37 - loss: 2.2785 - regression_loss: 1.9226 - classification_loss: 0.3559
 448/1500 [=======>......................] - ETA: 13:40 - loss: 2.2787 - regression_loss: 1.9228 - classification_loss: 0.3559
 449/1500 [=======>......................] - ETA: 13:38 - loss: 2.2780 - regression_loss: 1.9223 - classification_loss: 0.3557
 450/1500 [========>.....................] - ETA: 13:37 - loss: 2.2770 - regression_loss: 1.9213 - classification_loss: 0.3557
 451/1500 [========>.....................] - ETA: 13:35 - loss: 2.2758 - regression_loss: 1.9203 - classification_loss: 0.3555
 452/1500 [========>.....................] - ETA: 13:33 - loss: 2.2760 - regression_loss: 1.9208 - classification_loss: 0.3552
 453/1500 [========>.....................] - ETA: 13:32 - loss: 2.2749 - regression_loss: 1.9201 - classification_loss: 0.3548
 454/1500 [========>.....................] - ETA: 13:32 - loss: 2.2739 - regression_loss: 1.9195 - classification_loss: 0.3544
 455/1500 [========>.....................] - ETA: 13:33 - loss: 2.2718 - regression_loss: 1.9178 - classification_loss: 0.3539
 456/1500 [========>.....................] - ETA: 13:31 - loss: 2.2699 - regression_loss: 1.9163 - classification_loss: 0.3537
 457/1500 [========>.....................] - ETA: 13:30 - loss: 2.2706 - regression_loss: 1.9167 - classification_loss: 0.3539
 458/1500 [========>.....................] - ETA: 13:28 - loss: 2.2711 - regression_loss: 1.9172 - classification_loss: 0.3539
 459/1500 [========>.....................] - ETA: 13:27 - loss: 2.2703 - regression_loss: 1.9165 - classification_loss: 0.3539
 460/1500 [========>.....................] - ETA: 13:25 - loss: 2.2690 - regression_loss: 1.9156 - classification_loss: 0.3534
 461/1500 [========>.....................] - ETA: 13:26 - loss: 2.2718 - regression_loss: 1.9175 - classification_loss: 0.3543
 462/1500 [========>.....................] - ETA: 13:26 - loss: 2.2691 - regression_loss: 1.9153 - classification_loss: 0.3538
 463/1500 [========>.....................] - ETA: 13:26 - loss: 2.2707 - regression_loss: 1.9166 - classification_loss: 0.3542
 464/1500 [========>.....................] - ETA: 13:25 - loss: 2.2697 - regression_loss: 1.9158 - classification_loss: 0.3539
 465/1500 [========>.....................] - ETA: 13:23 - loss: 2.2700 - regression_loss: 1.9163 - classification_loss: 0.3538
 466/1500 [========>.....................] - ETA: 13:26 - loss: 2.2690 - regression_loss: 1.9155 - classification_loss: 0.3536
 467/1500 [========>.....................] - ETA: 13:25 - loss: 2.2680 - regression_loss: 1.9147 - classification_loss: 0.3533
 468/1500 [========>.....................] - ETA: 13:24 - loss: 2.2681 - regression_loss: 1.9143 - classification_loss: 0.3538
 469/1500 [========>.....................] - ETA: 13:26 - loss: 2.2702 - regression_loss: 1.9159 - classification_loss: 0.3543
 470/1500 [========>.....................] - ETA: 13:25 - loss: 2.2690 - regression_loss: 1.9149 - classification_loss: 0.3541
 471/1500 [========>.....................] - ETA: 13:23 - loss: 2.2665 - regression_loss: 1.9129 - classification_loss: 0.3537
 472/1500 [========>.....................] - ETA: 13:23 - loss: 2.2663 - regression_loss: 1.9130 - classification_loss: 0.3533
 473/1500 [========>.....................] - ETA: 13:21 - loss: 2.2688 - regression_loss: 1.9139 - classification_loss: 0.3548
 474/1500 [========>.....................] - ETA: 13:20 - loss: 2.2706 - regression_loss: 1.9156 - classification_loss: 0.3550
 475/1500 [========>.....................] - ETA: 13:19 - loss: 2.2704 - regression_loss: 1.9154 - classification_loss: 0.3550
 476/1500 [========>.....................] - ETA: 13:17 - loss: 2.2684 - regression_loss: 1.9139 - classification_loss: 0.3545
 477/1500 [========>.....................] - ETA: 13:16 - loss: 2.2680 - regression_loss: 1.9138 - classification_loss: 0.3543
 478/1500 [========>.....................] - ETA: 13:15 - loss: 2.2666 - regression_loss: 1.9126 - classification_loss: 0.3540
 479/1500 [========>.....................] - ETA: 13:13 - loss: 2.2646 - regression_loss: 1.9110 - classification_loss: 0.3536
 480/1500 [========>.....................] - ETA: 13:12 - loss: 2.2636 - regression_loss: 1.9103 - classification_loss: 0.3534
 481/1500 [========>.....................] - ETA: 13:11 - loss: 2.2638 - regression_loss: 1.9103 - classification_loss: 0.3535
 482/1500 [========>.....................] - ETA: 13:11 - loss: 2.2628 - regression_loss: 1.9095 - classification_loss: 0.3533
 483/1500 [========>.....................] - ETA: 13:09 - loss: 2.2613 - regression_loss: 1.9080 - classification_loss: 0.3532
 484/1500 [========>.....................] - ETA: 13:09 - loss: 2.2626 - regression_loss: 1.9091 - classification_loss: 0.3535
 485/1500 [========>.....................] - ETA: 13:07 - loss: 2.2622 - regression_loss: 1.9089 - classification_loss: 0.3533
 486/1500 [========>.....................] - ETA: 13:07 - loss: 2.2614 - regression_loss: 1.9080 - classification_loss: 0.3534
 487/1500 [========>.....................] - ETA: 13:07 - loss: 2.2594 - regression_loss: 1.9063 - classification_loss: 0.3531
 488/1500 [========>.....................] - ETA: 13:05 - loss: 2.2597 - regression_loss: 1.9063 - classification_loss: 0.3533
 489/1500 [========>.....................] - ETA: 13:06 - loss: 2.2616 - regression_loss: 1.9077 - classification_loss: 0.3538
 490/1500 [========>.....................] - ETA: 13:04 - loss: 2.2619 - regression_loss: 1.9080 - classification_loss: 0.3539
 491/1500 [========>.....................] - ETA: 13:05 - loss: 2.2600 - regression_loss: 1.9063 - classification_loss: 0.3537
 492/1500 [========>.....................] - ETA: 13:06 - loss: 2.2587 - regression_loss: 1.9051 - classification_loss: 0.3536
 493/1500 [========>.....................] - ETA: 13:06 - loss: 2.2579 - regression_loss: 1.9043 - classification_loss: 0.3535
 494/1500 [========>.....................] - ETA: 13:05 - loss: 2.2593 - regression_loss: 1.9058 - classification_loss: 0.3535
 495/1500 [========>.....................] - ETA: 13:03 - loss: 2.2605 - regression_loss: 1.9069 - classification_loss: 0.3536
 496/1500 [========>.....................] - ETA: 13:07 - loss: 2.2620 - regression_loss: 1.9081 - classification_loss: 0.3539
 497/1500 [========>.....................] - ETA: 13:07 - loss: 2.2636 - regression_loss: 1.9091 - classification_loss: 0.3545
 498/1500 [========>.....................] - ETA: 13:05 - loss: 2.2635 - regression_loss: 1.9094 - classification_loss: 0.3541
 499/1500 [========>.....................] - ETA: 13:06 - loss: 2.2644 - regression_loss: 1.9100 - classification_loss: 0.3544
 500/1500 [=========>....................] - ETA: 13:06 - loss: 2.2653 - regression_loss: 1.9107 - classification_loss: 0.3546
 501/1500 [=========>....................] - ETA: 13:05 - loss: 2.2633 - regression_loss: 1.9090 - classification_loss: 0.3543
 502/1500 [=========>....................] - ETA: 13:05 - loss: 2.2618 - regression_loss: 1.9079 - classification_loss: 0.3539
 503/1500 [=========>....................] - ETA: 13:04 - loss: 2.2634 - regression_loss: 1.9092 - classification_loss: 0.3542
 504/1500 [=========>....................] - ETA: 13:04 - loss: 2.2656 - regression_loss: 1.9107 - classification_loss: 0.3548
 505/1500 [=========>....................] - ETA: 13:09 - loss: 2.2663 - regression_loss: 1.9115 - classification_loss: 0.3548
 506/1500 [=========>....................] - ETA: 13:07 - loss: 2.2675 - regression_loss: 1.9124 - classification_loss: 0.3550
 507/1500 [=========>....................] - ETA: 13:06 - loss: 2.2677 - regression_loss: 1.9127 - classification_loss: 0.3549
 508/1500 [=========>....................] - ETA: 13:05 - loss: 2.2676 - regression_loss: 1.9129 - classification_loss: 0.3548
 509/1500 [=========>....................] - ETA: 13:04 - loss: 2.2679 - regression_loss: 1.9130 - classification_loss: 0.3549
 510/1500 [=========>....................] - ETA: 13:03 - loss: 2.2682 - regression_loss: 1.9133 - classification_loss: 0.3549
 511/1500 [=========>....................] - ETA: 13:02 - loss: 2.2671 - regression_loss: 1.9125 - classification_loss: 0.3546
 512/1500 [=========>....................] - ETA: 13:01 - loss: 2.2673 - regression_loss: 1.9127 - classification_loss: 0.3545
 513/1500 [=========>....................] - ETA: 13:01 - loss: 2.2664 - regression_loss: 1.9122 - classification_loss: 0.3542
 514/1500 [=========>....................] - ETA: 13:00 - loss: 2.2651 - regression_loss: 1.9112 - classification_loss: 0.3539
 515/1500 [=========>....................] - ETA: 12:59 - loss: 2.2635 - regression_loss: 1.9100 - classification_loss: 0.3535
 516/1500 [=========>....................] - ETA: 12:57 - loss: 2.2629 - regression_loss: 1.9094 - classification_loss: 0.3534
 517/1500 [=========>....................] - ETA: 12:57 - loss: 2.2622 - regression_loss: 1.9088 - classification_loss: 0.3534
 518/1500 [=========>....................] - ETA: 12:55 - loss: 2.2597 - regression_loss: 1.9067 - classification_loss: 0.3530
 519/1500 [=========>....................] - ETA: 12:54 - loss: 2.2597 - regression_loss: 1.9068 - classification_loss: 0.3529
 520/1500 [=========>....................] - ETA: 12:52 - loss: 2.2579 - regression_loss: 1.9052 - classification_loss: 0.3527
 521/1500 [=========>....................] - ETA: 12:51 - loss: 2.2611 - regression_loss: 1.9081 - classification_loss: 0.3530
 522/1500 [=========>....................] - ETA: 12:51 - loss: 2.2605 - regression_loss: 1.9076 - classification_loss: 0.3529
 523/1500 [=========>....................] - ETA: 12:49 - loss: 2.2588 - regression_loss: 1.9061 - classification_loss: 0.3527
 524/1500 [=========>....................] - ETA: 12:50 - loss: 2.2598 - regression_loss: 1.9067 - classification_loss: 0.3531
 525/1500 [=========>....................] - ETA: 12:50 - loss: 2.2580 - regression_loss: 1.9053 - classification_loss: 0.3527
 526/1500 [=========>....................] - ETA: 12:49 - loss: 2.2568 - regression_loss: 1.9045 - classification_loss: 0.3524
 527/1500 [=========>....................] - ETA: 12:48 - loss: 2.2557 - regression_loss: 1.9037 - classification_loss: 0.3520
 528/1500 [=========>....................] - ETA: 12:47 - loss: 2.2560 - regression_loss: 1.9039 - classification_loss: 0.3521
 529/1500 [=========>....................] - ETA: 12:45 - loss: 2.2547 - regression_loss: 1.9028 - classification_loss: 0.3518
 530/1500 [=========>....................] - ETA: 12:44 - loss: 2.2547 - regression_loss: 1.9030 - classification_loss: 0.3517
 531/1500 [=========>....................] - ETA: 12:44 - loss: 2.2566 - regression_loss: 1.9045 - classification_loss: 0.3521
 532/1500 [=========>....................] - ETA: 12:42 - loss: 2.2547 - regression_loss: 1.9029 - classification_loss: 0.3517
 533/1500 [=========>....................] - ETA: 12:41 - loss: 2.2532 - regression_loss: 1.9016 - classification_loss: 0.3515
 534/1500 [=========>....................] - ETA: 12:40 - loss: 2.2545 - regression_loss: 1.9027 - classification_loss: 0.3518
 535/1500 [=========>....................] - ETA: 12:38 - loss: 2.2537 - regression_loss: 1.9022 - classification_loss: 0.3516
 536/1500 [=========>....................] - ETA: 12:37 - loss: 2.2557 - regression_loss: 1.9043 - classification_loss: 0.3514
 537/1500 [=========>....................] - ETA: 12:35 - loss: 2.2567 - regression_loss: 1.9050 - classification_loss: 0.3517
 538/1500 [=========>....................] - ETA: 12:34 - loss: 2.2552 - regression_loss: 1.9037 - classification_loss: 0.3515
 539/1500 [=========>....................] - ETA: 12:33 - loss: 2.2547 - regression_loss: 1.9033 - classification_loss: 0.3514
 540/1500 [=========>....................] - ETA: 12:32 - loss: 2.2571 - regression_loss: 1.9036 - classification_loss: 0.3534
 541/1500 [=========>....................] - ETA: 12:31 - loss: 2.2561 - regression_loss: 1.9030 - classification_loss: 0.3531
 542/1500 [=========>....................] - ETA: 12:30 - loss: 2.2564 - regression_loss: 1.9031 - classification_loss: 0.3533
 543/1500 [=========>....................] - ETA: 12:28 - loss: 2.2562 - regression_loss: 1.9027 - classification_loss: 0.3535
 544/1500 [=========>....................] - ETA: 12:28 - loss: 2.2553 - regression_loss: 1.9020 - classification_loss: 0.3533
 545/1500 [=========>....................] - ETA: 12:28 - loss: 2.2557 - regression_loss: 1.9026 - classification_loss: 0.3531
 546/1500 [=========>....................] - ETA: 12:28 - loss: 2.2540 - regression_loss: 1.9011 - classification_loss: 0.3529
 547/1500 [=========>....................] - ETA: 12:27 - loss: 2.2547 - regression_loss: 1.9017 - classification_loss: 0.3529
 548/1500 [=========>....................] - ETA: 12:26 - loss: 2.2546 - regression_loss: 1.9018 - classification_loss: 0.3528
 549/1500 [=========>....................] - ETA: 12:25 - loss: 2.2549 - regression_loss: 1.9023 - classification_loss: 0.3526
 550/1500 [==========>...................] - ETA: 12:23 - loss: 2.2555 - regression_loss: 1.9028 - classification_loss: 0.3527
 551/1500 [==========>...................] - ETA: 12:22 - loss: 2.2575 - regression_loss: 1.9040 - classification_loss: 0.3535
 552/1500 [==========>...................] - ETA: 12:21 - loss: 2.2566 - regression_loss: 1.9034 - classification_loss: 0.3532
 553/1500 [==========>...................] - ETA: 12:19 - loss: 2.2546 - regression_loss: 1.9015 - classification_loss: 0.3531
 554/1500 [==========>...................] - ETA: 12:18 - loss: 2.2555 - regression_loss: 1.9026 - classification_loss: 0.3528
 555/1500 [==========>...................] - ETA: 12:17 - loss: 2.2538 - regression_loss: 1.9013 - classification_loss: 0.3524
 556/1500 [==========>...................] - ETA: 12:17 - loss: 2.2522 - regression_loss: 1.9001 - classification_loss: 0.3522
 557/1500 [==========>...................] - ETA: 12:16 - loss: 2.2498 - regression_loss: 1.8979 - classification_loss: 0.3518
 558/1500 [==========>...................] - ETA: 12:15 - loss: 2.2505 - regression_loss: 1.8988 - classification_loss: 0.3517
 559/1500 [==========>...................] - ETA: 12:14 - loss: 2.2509 - regression_loss: 1.8992 - classification_loss: 0.3517
 560/1500 [==========>...................] - ETA: 12:13 - loss: 2.2506 - regression_loss: 1.8991 - classification_loss: 0.3515
 561/1500 [==========>...................] - ETA: 12:13 - loss: 2.2500 - regression_loss: 1.8985 - classification_loss: 0.3515
 562/1500 [==========>...................] - ETA: 12:12 - loss: 2.2510 - regression_loss: 1.8994 - classification_loss: 0.3516
 563/1500 [==========>...................] - ETA: 12:11 - loss: 2.2516 - regression_loss: 1.8998 - classification_loss: 0.3518
 564/1500 [==========>...................] - ETA: 12:10 - loss: 2.2534 - regression_loss: 1.9015 - classification_loss: 0.3519
 565/1500 [==========>...................] - ETA: 12:09 - loss: 2.2532 - regression_loss: 1.9016 - classification_loss: 0.3517
 566/1500 [==========>...................] - ETA: 12:08 - loss: 2.2543 - regression_loss: 1.9026 - classification_loss: 0.3517
 567/1500 [==========>...................] - ETA: 12:07 - loss: 2.2532 - regression_loss: 1.9017 - classification_loss: 0.3515
 568/1500 [==========>...................] - ETA: 12:05 - loss: 2.2532 - regression_loss: 1.9020 - classification_loss: 0.3512
 569/1500 [==========>...................] - ETA: 12:04 - loss: 2.2530 - regression_loss: 1.9019 - classification_loss: 0.3510
 570/1500 [==========>...................] - ETA: 12:03 - loss: 2.2539 - regression_loss: 1.9027 - classification_loss: 0.3512
 571/1500 [==========>...................] - ETA: 12:02 - loss: 2.2548 - regression_loss: 1.9037 - classification_loss: 0.3511
 572/1500 [==========>...................] - ETA: 12:02 - loss: 2.2566 - regression_loss: 1.9050 - classification_loss: 0.3516
 573/1500 [==========>...................] - ETA: 12:00 - loss: 2.2565 - regression_loss: 1.9050 - classification_loss: 0.3515
 574/1500 [==========>...................] - ETA: 12:00 - loss: 2.2555 - regression_loss: 1.9042 - classification_loss: 0.3513
 575/1500 [==========>...................] - ETA: 12:00 - loss: 2.2553 - regression_loss: 1.9041 - classification_loss: 0.3512
 576/1500 [==========>...................] - ETA: 11:59 - loss: 2.2548 - regression_loss: 1.9039 - classification_loss: 0.3509
 577/1500 [==========>...................] - ETA: 11:58 - loss: 2.2554 - regression_loss: 1.9043 - classification_loss: 0.3512
 578/1500 [==========>...................] - ETA: 11:56 - loss: 2.2539 - regression_loss: 1.9029 - classification_loss: 0.3509
 579/1500 [==========>...................] - ETA: 11:55 - loss: 2.2545 - regression_loss: 1.9035 - classification_loss: 0.3511
 580/1500 [==========>...................] - ETA: 11:54 - loss: 2.2539 - regression_loss: 1.9031 - classification_loss: 0.3508
 581/1500 [==========>...................] - ETA: 11:55 - loss: 2.2550 - regression_loss: 1.9038 - classification_loss: 0.3511
 582/1500 [==========>...................] - ETA: 11:54 - loss: 2.2544 - regression_loss: 1.9033 - classification_loss: 0.3511
 583/1500 [==========>...................] - ETA: 11:53 - loss: 2.2535 - regression_loss: 1.9027 - classification_loss: 0.3508
 584/1500 [==========>...................] - ETA: 11:52 - loss: 2.2546 - regression_loss: 1.9034 - classification_loss: 0.3513
 585/1500 [==========>...................] - ETA: 11:52 - loss: 2.2559 - regression_loss: 1.9040 - classification_loss: 0.3519
 586/1500 [==========>...................] - ETA: 11:50 - loss: 2.2542 - regression_loss: 1.9025 - classification_loss: 0.3517
 587/1500 [==========>...................] - ETA: 11:49 - loss: 2.2553 - regression_loss: 1.9033 - classification_loss: 0.3520
 588/1500 [==========>...................] - ETA: 11:48 - loss: 2.2549 - regression_loss: 1.9031 - classification_loss: 0.3518
 589/1500 [==========>...................] - ETA: 11:46 - loss: 2.2557 - regression_loss: 1.9035 - classification_loss: 0.3522
 590/1500 [==========>...................] - ETA: 11:45 - loss: 2.2557 - regression_loss: 1.9037 - classification_loss: 0.3520
 591/1500 [==========>...................] - ETA: 11:45 - loss: 2.2539 - regression_loss: 1.9022 - classification_loss: 0.3516
 592/1500 [==========>...................] - ETA: 11:43 - loss: 2.2522 - regression_loss: 1.9009 - classification_loss: 0.3513
 593/1500 [==========>...................] - ETA: 11:42 - loss: 2.2522 - regression_loss: 1.9011 - classification_loss: 0.3511
 594/1500 [==========>...................] - ETA: 11:42 - loss: 2.2516 - regression_loss: 1.9007 - classification_loss: 0.3509
 595/1500 [==========>...................] - ETA: 11:41 - loss: 2.2502 - regression_loss: 1.8995 - classification_loss: 0.3507
 596/1500 [==========>...................] - ETA: 11:40 - loss: 2.2507 - regression_loss: 1.8999 - classification_loss: 0.3508
 597/1500 [==========>...................] - ETA: 11:39 - loss: 2.2496 - regression_loss: 1.8989 - classification_loss: 0.3507
 598/1500 [==========>...................] - ETA: 11:37 - loss: 2.2485 - regression_loss: 1.8981 - classification_loss: 0.3504
 599/1500 [==========>...................] - ETA: 11:37 - loss: 2.2475 - regression_loss: 1.8971 - classification_loss: 0.3504
 600/1500 [===========>..................] - ETA: 11:40 - loss: 2.2460 - regression_loss: 1.8959 - classification_loss: 0.3501
 601/1500 [===========>..................] - ETA: 11:41 - loss: 2.2471 - regression_loss: 1.8964 - classification_loss: 0.3507
 602/1500 [===========>..................] - ETA: 11:39 - loss: 2.2457 - regression_loss: 1.8953 - classification_loss: 0.3503
 603/1500 [===========>..................] - ETA: 11:39 - loss: 2.2471 - regression_loss: 1.8966 - classification_loss: 0.3505
 604/1500 [===========>..................] - ETA: 11:38 - loss: 2.2456 - regression_loss: 1.8953 - classification_loss: 0.3502
 605/1500 [===========>..................] - ETA: 11:38 - loss: 2.2448 - regression_loss: 1.8948 - classification_loss: 0.3500
 606/1500 [===========>..................] - ETA: 11:38 - loss: 2.2460 - regression_loss: 1.8956 - classification_loss: 0.3504
 607/1500 [===========>..................] - ETA: 11:37 - loss: 2.2455 - regression_loss: 1.8950 - classification_loss: 0.3505
 608/1500 [===========>..................] - ETA: 11:36 - loss: 2.2454 - regression_loss: 1.8948 - classification_loss: 0.3507
 609/1500 [===========>..................] - ETA: 11:35 - loss: 2.2483 - regression_loss: 1.8971 - classification_loss: 0.3511
 610/1500 [===========>..................] - ETA: 11:34 - loss: 2.2466 - regression_loss: 1.8958 - classification_loss: 0.3508
 611/1500 [===========>..................] - ETA: 11:32 - loss: 2.2460 - regression_loss: 1.8951 - classification_loss: 0.3509
 612/1500 [===========>..................] - ETA: 11:32 - loss: 2.2462 - regression_loss: 1.8955 - classification_loss: 0.3507
 613/1500 [===========>..................] - ETA: 11:31 - loss: 2.2453 - regression_loss: 1.8949 - classification_loss: 0.3504
 614/1500 [===========>..................] - ETA: 11:29 - loss: 2.2438 - regression_loss: 1.8936 - classification_loss: 0.3502
 615/1500 [===========>..................] - ETA: 11:28 - loss: 2.2468 - regression_loss: 1.8961 - classification_loss: 0.3507
 616/1500 [===========>..................] - ETA: 11:27 - loss: 2.2467 - regression_loss: 1.8958 - classification_loss: 0.3509
 617/1500 [===========>..................] - ETA: 11:25 - loss: 2.2464 - regression_loss: 1.8957 - classification_loss: 0.3507
 618/1500 [===========>..................] - ETA: 11:25 - loss: 2.2451 - regression_loss: 1.8946 - classification_loss: 0.3505
 619/1500 [===========>..................] - ETA: 11:24 - loss: 2.2451 - regression_loss: 1.8946 - classification_loss: 0.3505
 620/1500 [===========>..................] - ETA: 11:23 - loss: 2.2425 - regression_loss: 1.8924 - classification_loss: 0.3501
 621/1500 [===========>..................] - ETA: 11:22 - loss: 2.2434 - regression_loss: 1.8931 - classification_loss: 0.3503
 622/1500 [===========>..................] - ETA: 11:21 - loss: 2.2434 - regression_loss: 1.8933 - classification_loss: 0.3502
 623/1500 [===========>..................] - ETA: 11:21 - loss: 2.2426 - regression_loss: 1.8926 - classification_loss: 0.3500
 624/1500 [===========>..................] - ETA: 11:20 - loss: 2.2430 - regression_loss: 1.8923 - classification_loss: 0.3507
 625/1500 [===========>..................] - ETA: 11:20 - loss: 2.2405 - regression_loss: 1.8902 - classification_loss: 0.3503
 626/1500 [===========>..................] - ETA: 11:19 - loss: 2.2408 - regression_loss: 1.8905 - classification_loss: 0.3503
 627/1500 [===========>..................] - ETA: 11:18 - loss: 2.2412 - regression_loss: 1.8910 - classification_loss: 0.3502
 628/1500 [===========>..................] - ETA: 11:16 - loss: 2.2429 - regression_loss: 1.8922 - classification_loss: 0.3507
 629/1500 [===========>..................] - ETA: 11:17 - loss: 2.2422 - regression_loss: 1.8917 - classification_loss: 0.3505
 630/1500 [===========>..................] - ETA: 11:16 - loss: 2.2404 - regression_loss: 1.8903 - classification_loss: 0.3501
 631/1500 [===========>..................] - ETA: 11:15 - loss: 2.2414 - regression_loss: 1.8909 - classification_loss: 0.3505
 632/1500 [===========>..................] - ETA: 11:14 - loss: 2.2418 - regression_loss: 1.8913 - classification_loss: 0.3505
 633/1500 [===========>..................] - ETA: 11:13 - loss: 2.2414 - regression_loss: 1.8909 - classification_loss: 0.3505
 634/1500 [===========>..................] - ETA: 11:11 - loss: 2.2398 - regression_loss: 1.8895 - classification_loss: 0.3503
 635/1500 [===========>..................] - ETA: 11:11 - loss: 2.2382 - regression_loss: 1.8882 - classification_loss: 0.3499
 636/1500 [===========>..................] - ETA: 11:10 - loss: 2.2385 - regression_loss: 1.8886 - classification_loss: 0.3499
 637/1500 [===========>..................] - ETA: 11:10 - loss: 2.2376 - regression_loss: 1.8876 - classification_loss: 0.3500
 638/1500 [===========>..................] - ETA: 11:08 - loss: 2.2395 - regression_loss: 1.8889 - classification_loss: 0.3506
 639/1500 [===========>..................] - ETA: 11:07 - loss: 2.2387 - regression_loss: 1.8883 - classification_loss: 0.3505
 640/1500 [===========>..................] - ETA: 11:06 - loss: 2.2373 - regression_loss: 1.8870 - classification_loss: 0.3504
 641/1500 [===========>..................] - ETA: 11:05 - loss: 2.2378 - regression_loss: 1.8874 - classification_loss: 0.3504
 642/1500 [===========>..................] - ETA: 11:04 - loss: 2.2370 - regression_loss: 1.8868 - classification_loss: 0.3502
 643/1500 [===========>..................] - ETA: 11:03 - loss: 2.2350 - regression_loss: 1.8852 - classification_loss: 0.3498
 644/1500 [===========>..................] - ETA: 11:02 - loss: 2.2334 - regression_loss: 1.8838 - classification_loss: 0.3496
 645/1500 [===========>..................] - ETA: 11:01 - loss: 2.2331 - regression_loss: 1.8835 - classification_loss: 0.3495
 646/1500 [===========>..................] - ETA: 11:00 - loss: 2.2329 - regression_loss: 1.8832 - classification_loss: 0.3497
 647/1500 [===========>..................] - ETA: 10:58 - loss: 2.2316 - regression_loss: 1.8821 - classification_loss: 0.3495
 648/1500 [===========>..................] - ETA: 10:57 - loss: 2.2312 - regression_loss: 1.8819 - classification_loss: 0.3493
 649/1500 [===========>..................] - ETA: 10:56 - loss: 2.2320 - regression_loss: 1.8827 - classification_loss: 0.3493
 650/1500 [============>.................] - ETA: 10:55 - loss: 2.2319 - regression_loss: 1.8828 - classification_loss: 0.3491
 651/1500 [============>.................] - ETA: 10:54 - loss: 2.2326 - regression_loss: 1.8834 - classification_loss: 0.3492
 652/1500 [============>.................] - ETA: 10:53 - loss: 2.2318 - regression_loss: 1.8828 - classification_loss: 0.3490
 653/1500 [============>.................] - ETA: 10:51 - loss: 2.2309 - regression_loss: 1.8821 - classification_loss: 0.3488
 654/1500 [============>.................] - ETA: 10:50 - loss: 2.2327 - regression_loss: 1.8835 - classification_loss: 0.3492
 655/1500 [============>.................] - ETA: 10:49 - loss: 2.2323 - regression_loss: 1.8833 - classification_loss: 0.3490
 656/1500 [============>.................] - ETA: 10:48 - loss: 2.2324 - regression_loss: 1.8835 - classification_loss: 0.3489
 657/1500 [============>.................] - ETA: 10:47 - loss: 2.2324 - regression_loss: 1.8837 - classification_loss: 0.3487
 658/1500 [============>.................] - ETA: 10:46 - loss: 2.2314 - regression_loss: 1.8827 - classification_loss: 0.3488
 659/1500 [============>.................] - ETA: 10:44 - loss: 2.2321 - regression_loss: 1.8833 - classification_loss: 0.3489
 660/1500 [============>.................] - ETA: 10:43 - loss: 2.2315 - regression_loss: 1.8829 - classification_loss: 0.3487
 661/1500 [============>.................] - ETA: 10:42 - loss: 2.2313 - regression_loss: 1.8826 - classification_loss: 0.3487
 662/1500 [============>.................] - ETA: 10:41 - loss: 2.2298 - regression_loss: 1.8815 - classification_loss: 0.3483
 663/1500 [============>.................] - ETA: 10:41 - loss: 2.2310 - regression_loss: 1.8824 - classification_loss: 0.3486
 664/1500 [============>.................] - ETA: 10:40 - loss: 2.2302 - regression_loss: 1.8817 - classification_loss: 0.3485
 665/1500 [============>.................] - ETA: 10:40 - loss: 2.2314 - regression_loss: 1.8826 - classification_loss: 0.3487
 666/1500 [============>.................] - ETA: 10:39 - loss: 2.2313 - regression_loss: 1.8824 - classification_loss: 0.3489
 667/1500 [============>.................] - ETA: 10:39 - loss: 2.2340 - regression_loss: 1.8844 - classification_loss: 0.3495
 668/1500 [============>.................] - ETA: 10:39 - loss: 2.2340 - regression_loss: 1.8844 - classification_loss: 0.3496
 669/1500 [============>.................] - ETA: 10:38 - loss: 2.2344 - regression_loss: 1.8846 - classification_loss: 0.3498
 670/1500 [============>.................] - ETA: 10:37 - loss: 2.2352 - regression_loss: 1.8856 - classification_loss: 0.3496
 671/1500 [============>.................] - ETA: 10:37 - loss: 2.2351 - regression_loss: 1.8852 - classification_loss: 0.3498
 672/1500 [============>.................] - ETA: 10:36 - loss: 2.2359 - regression_loss: 1.8863 - classification_loss: 0.3496
 673/1500 [============>.................] - ETA: 10:36 - loss: 2.2366 - regression_loss: 1.8868 - classification_loss: 0.3498
 674/1500 [============>.................] - ETA: 10:34 - loss: 2.2372 - regression_loss: 1.8873 - classification_loss: 0.3499
 675/1500 [============>.................] - ETA: 10:34 - loss: 2.2371 - regression_loss: 1.8871 - classification_loss: 0.3500
 676/1500 [============>.................] - ETA: 10:33 - loss: 2.2376 - regression_loss: 1.8874 - classification_loss: 0.3502
 677/1500 [============>.................] - ETA: 10:33 - loss: 2.2383 - regression_loss: 1.8873 - classification_loss: 0.3510
 678/1500 [============>.................] - ETA: 10:33 - loss: 2.2383 - regression_loss: 1.8873 - classification_loss: 0.3510
 679/1500 [============>.................] - ETA: 10:32 - loss: 2.2371 - regression_loss: 1.8863 - classification_loss: 0.3508
 680/1500 [============>.................] - ETA: 10:31 - loss: 2.2365 - regression_loss: 1.8857 - classification_loss: 0.3508
 681/1500 [============>.................] - ETA: 10:29 - loss: 2.2362 - regression_loss: 1.8856 - classification_loss: 0.3506
 682/1500 [============>.................] - ETA: 10:28 - loss: 2.2344 - regression_loss: 1.8840 - classification_loss: 0.3504
 683/1500 [============>.................] - ETA: 10:27 - loss: 2.2353 - regression_loss: 1.8849 - classification_loss: 0.3504
 684/1500 [============>.................] - ETA: 10:26 - loss: 2.2337 - regression_loss: 1.8835 - classification_loss: 0.3502
 685/1500 [============>.................] - ETA: 10:25 - loss: 2.2352 - regression_loss: 1.8849 - classification_loss: 0.3503
 686/1500 [============>.................] - ETA: 10:25 - loss: 2.2368 - regression_loss: 1.8858 - classification_loss: 0.3510
 687/1500 [============>.................] - ETA: 10:24 - loss: 2.2380 - regression_loss: 1.8863 - classification_loss: 0.3517
 688/1500 [============>.................] - ETA: 10:23 - loss: 2.2387 - regression_loss: 1.8868 - classification_loss: 0.3519
 689/1500 [============>.................] - ETA: 10:21 - loss: 2.2383 - regression_loss: 1.8865 - classification_loss: 0.3518
 690/1500 [============>.................] - ETA: 10:21 - loss: 2.2369 - regression_loss: 1.8854 - classification_loss: 0.3515
 691/1500 [============>.................] - ETA: 10:19 - loss: 2.2373 - regression_loss: 1.8858 - classification_loss: 0.3515
 692/1500 [============>.................] - ETA: 10:19 - loss: 2.2370 - regression_loss: 1.8854 - classification_loss: 0.3516
 693/1500 [============>.................] - ETA: 10:18 - loss: 2.2375 - regression_loss: 1.8859 - classification_loss: 0.3516
 694/1500 [============>.................] - ETA: 10:17 - loss: 2.2379 - regression_loss: 1.8862 - classification_loss: 0.3517
 695/1500 [============>.................] - ETA: 10:16 - loss: 2.2395 - regression_loss: 1.8878 - classification_loss: 0.3518
 696/1500 [============>.................] - ETA: 10:15 - loss: 2.2401 - regression_loss: 1.8884 - classification_loss: 0.3517
 697/1500 [============>.................] - ETA: 10:14 - loss: 2.2409 - regression_loss: 1.8891 - classification_loss: 0.3518
 698/1500 [============>.................] - ETA: 10:13 - loss: 2.2398 - regression_loss: 1.8880 - classification_loss: 0.3518
 699/1500 [============>.................] - ETA: 10:12 - loss: 2.2405 - regression_loss: 1.8886 - classification_loss: 0.3518
 700/1500 [=============>................] - ETA: 10:11 - loss: 2.2419 - regression_loss: 1.8900 - classification_loss: 0.3519
 701/1500 [=============>................] - ETA: 10:10 - loss: 2.2411 - regression_loss: 1.8894 - classification_loss: 0.3517
 702/1500 [=============>................] - ETA: 10:08 - loss: 2.2424 - regression_loss: 1.8904 - classification_loss: 0.3521
 703/1500 [=============>................] - ETA: 10:08 - loss: 2.2429 - regression_loss: 1.8908 - classification_loss: 0.3520
 704/1500 [=============>................] - ETA: 10:08 - loss: 2.2433 - regression_loss: 1.8909 - classification_loss: 0.3523
 705/1500 [=============>................] - ETA: 10:07 - loss: 2.2436 - regression_loss: 1.8912 - classification_loss: 0.3524
 706/1500 [=============>................] - ETA: 10:07 - loss: 2.2437 - regression_loss: 1.8913 - classification_loss: 0.3524
 707/1500 [=============>................] - ETA: 10:06 - loss: 2.2431 - regression_loss: 1.8907 - classification_loss: 0.3525
 708/1500 [=============>................] - ETA: 10:06 - loss: 2.2444 - regression_loss: 1.8916 - classification_loss: 0.3528
 709/1500 [=============>................] - ETA: 10:06 - loss: 2.2448 - regression_loss: 1.8919 - classification_loss: 0.3529
 710/1500 [=============>................] - ETA: 10:06 - loss: 2.2435 - regression_loss: 1.8909 - classification_loss: 0.3526
 711/1500 [=============>................] - ETA: 10:05 - loss: 2.2420 - regression_loss: 1.8895 - classification_loss: 0.3524
 712/1500 [=============>................] - ETA: 10:04 - loss: 2.2412 - regression_loss: 1.8887 - classification_loss: 0.3524
 713/1500 [=============>................] - ETA: 10:03 - loss: 2.2420 - regression_loss: 1.8892 - classification_loss: 0.3529
 714/1500 [=============>................] - ETA: 10:01 - loss: 2.2418 - regression_loss: 1.8891 - classification_loss: 0.3527
 715/1500 [=============>................] - ETA: 10:02 - loss: 2.2408 - regression_loss: 1.8884 - classification_loss: 0.3524
 716/1500 [=============>................] - ETA: 10:01 - loss: 2.2413 - regression_loss: 1.8889 - classification_loss: 0.3524
 717/1500 [=============>................] - ETA: 10:00 - loss: 2.2398 - regression_loss: 1.8877 - classification_loss: 0.3521
 718/1500 [=============>................] - ETA: 9:59 - loss: 2.2406 - regression_loss: 1.8883 - classification_loss: 0.3524 
 719/1500 [=============>................] - ETA: 9:58 - loss: 2.2404 - regression_loss: 1.8882 - classification_loss: 0.3522
 720/1500 [=============>................] - ETA: 9:56 - loss: 2.2394 - regression_loss: 1.8874 - classification_loss: 0.3520
 721/1500 [=============>................] - ETA: 9:55 - loss: 2.2378 - regression_loss: 1.8862 - classification_loss: 0.3517
 722/1500 [=============>................] - ETA: 9:55 - loss: 2.2391 - regression_loss: 1.8873 - classification_loss: 0.3518
 723/1500 [=============>................] - ETA: 9:55 - loss: 2.2396 - regression_loss: 1.8877 - classification_loss: 0.3520
 724/1500 [=============>................] - ETA: 9:54 - loss: 2.2403 - regression_loss: 1.8882 - classification_loss: 0.3521
 725/1500 [=============>................] - ETA: 9:53 - loss: 2.2401 - regression_loss: 1.8882 - classification_loss: 0.3520
 726/1500 [=============>................] - ETA: 9:52 - loss: 2.2402 - regression_loss: 1.8885 - classification_loss: 0.3517
 727/1500 [=============>................] - ETA: 9:51 - loss: 2.2404 - regression_loss: 1.8885 - classification_loss: 0.3518
 728/1500 [=============>................] - ETA: 9:50 - loss: 2.2394 - regression_loss: 1.8877 - classification_loss: 0.3517
 729/1500 [=============>................] - ETA: 9:49 - loss: 2.2391 - regression_loss: 1.8875 - classification_loss: 0.3516
 730/1500 [=============>................] - ETA: 9:48 - loss: 2.2396 - regression_loss: 1.8877 - classification_loss: 0.3519
 731/1500 [=============>................] - ETA: 9:49 - loss: 2.2399 - regression_loss: 1.8881 - classification_loss: 0.3518
 732/1500 [=============>................] - ETA: 9:49 - loss: 2.2411 - regression_loss: 1.8889 - classification_loss: 0.3521
 733/1500 [=============>................] - ETA: 9:47 - loss: 2.2402 - regression_loss: 1.8883 - classification_loss: 0.3519
 734/1500 [=============>................] - ETA: 9:46 - loss: 2.2401 - regression_loss: 1.8883 - classification_loss: 0.3518
 735/1500 [=============>................] - ETA: 9:46 - loss: 2.2388 - regression_loss: 1.8872 - classification_loss: 0.3515
 736/1500 [=============>................] - ETA: 9:44 - loss: 2.2384 - regression_loss: 1.8869 - classification_loss: 0.3515
 737/1500 [=============>................] - ETA: 9:44 - loss: 2.2394 - regression_loss: 1.8877 - classification_loss: 0.3518
 738/1500 [=============>................] - ETA: 9:43 - loss: 2.2391 - regression_loss: 1.8875 - classification_loss: 0.3516
 739/1500 [=============>................] - ETA: 9:42 - loss: 2.2392 - regression_loss: 1.8875 - classification_loss: 0.3517
 740/1500 [=============>................] - ETA: 9:41 - loss: 2.2383 - regression_loss: 1.8865 - classification_loss: 0.3517
 741/1500 [=============>................] - ETA: 9:40 - loss: 2.2381 - regression_loss: 1.8864 - classification_loss: 0.3517
 742/1500 [=============>................] - ETA: 9:41 - loss: 2.2375 - regression_loss: 1.8859 - classification_loss: 0.3516
 743/1500 [=============>................] - ETA: 9:39 - loss: 2.2383 - regression_loss: 1.8867 - classification_loss: 0.3515
 744/1500 [=============>................] - ETA: 9:39 - loss: 2.2398 - regression_loss: 1.8877 - classification_loss: 0.3521
 745/1500 [=============>................] - ETA: 9:38 - loss: 2.2397 - regression_loss: 1.8874 - classification_loss: 0.3523
 746/1500 [=============>................] - ETA: 9:37 - loss: 2.2384 - regression_loss: 1.8864 - classification_loss: 0.3520
 747/1500 [=============>................] - ETA: 9:36 - loss: 2.2384 - regression_loss: 1.8866 - classification_loss: 0.3518
 748/1500 [=============>................] - ETA: 9:34 - loss: 2.2384 - regression_loss: 1.8867 - classification_loss: 0.3517
 749/1500 [=============>................] - ETA: 9:34 - loss: 2.2373 - regression_loss: 1.8857 - classification_loss: 0.3516
 750/1500 [==============>...............] - ETA: 9:33 - loss: 2.2384 - regression_loss: 1.8866 - classification_loss: 0.3518
 751/1500 [==============>...............] - ETA: 9:33 - loss: 2.2367 - regression_loss: 1.8851 - classification_loss: 0.3516
 752/1500 [==============>...............] - ETA: 9:32 - loss: 2.2363 - regression_loss: 1.8848 - classification_loss: 0.3515
 753/1500 [==============>...............] - ETA: 9:30 - loss: 2.2350 - regression_loss: 1.8838 - classification_loss: 0.3512
 754/1500 [==============>...............] - ETA: 9:29 - loss: 2.2353 - regression_loss: 1.8840 - classification_loss: 0.3513
 755/1500 [==============>...............] - ETA: 9:28 - loss: 2.2341 - regression_loss: 1.8829 - classification_loss: 0.3512
 756/1500 [==============>...............] - ETA: 9:27 - loss: 2.2335 - regression_loss: 1.8824 - classification_loss: 0.3511
 757/1500 [==============>...............] - ETA: 9:26 - loss: 2.2334 - regression_loss: 1.8825 - classification_loss: 0.3509
 758/1500 [==============>...............] - ETA: 9:25 - loss: 2.2333 - regression_loss: 1.8825 - classification_loss: 0.3508
 759/1500 [==============>...............] - ETA: 9:24 - loss: 2.2325 - regression_loss: 1.8819 - classification_loss: 0.3506
 760/1500 [==============>...............] - ETA: 9:26 - loss: 2.2321 - regression_loss: 1.8814 - classification_loss: 0.3508
 761/1500 [==============>...............] - ETA: 9:25 - loss: 2.2321 - regression_loss: 1.8815 - classification_loss: 0.3506
 762/1500 [==============>...............] - ETA: 9:24 - loss: 2.2320 - regression_loss: 1.8816 - classification_loss: 0.3504
 763/1500 [==============>...............] - ETA: 9:24 - loss: 2.2317 - regression_loss: 1.8814 - classification_loss: 0.3503
 764/1500 [==============>...............] - ETA: 9:23 - loss: 2.2320 - regression_loss: 1.8818 - classification_loss: 0.3501
 765/1500 [==============>...............] - ETA: 9:23 - loss: 2.2311 - regression_loss: 1.8810 - classification_loss: 0.3501
 766/1500 [==============>...............] - ETA: 9:22 - loss: 2.2311 - regression_loss: 1.8809 - classification_loss: 0.3501
 767/1500 [==============>...............] - ETA: 9:21 - loss: 2.2311 - regression_loss: 1.8810 - classification_loss: 0.3502
 768/1500 [==============>...............] - ETA: 9:20 - loss: 2.2318 - regression_loss: 1.8816 - classification_loss: 0.3502
 769/1500 [==============>...............] - ETA: 9:20 - loss: 2.2319 - regression_loss: 1.8819 - classification_loss: 0.3500
 770/1500 [==============>...............] - ETA: 9:19 - loss: 2.2319 - regression_loss: 1.8818 - classification_loss: 0.3501
 771/1500 [==============>...............] - ETA: 9:18 - loss: 2.2306 - regression_loss: 1.8807 - classification_loss: 0.3499
 772/1500 [==============>...............] - ETA: 9:17 - loss: 2.2298 - regression_loss: 1.8802 - classification_loss: 0.3496
 773/1500 [==============>...............] - ETA: 9:17 - loss: 2.2283 - regression_loss: 1.8790 - classification_loss: 0.3493
 774/1500 [==============>...............] - ETA: 9:16 - loss: 2.2272 - regression_loss: 1.8781 - classification_loss: 0.3490
 775/1500 [==============>...............] - ETA: 9:15 - loss: 2.2281 - regression_loss: 1.8791 - classification_loss: 0.3490
 776/1500 [==============>...............] - ETA: 9:14 - loss: 2.2272 - regression_loss: 1.8784 - classification_loss: 0.3488
 777/1500 [==============>...............] - ETA: 9:13 - loss: 2.2266 - regression_loss: 1.8776 - classification_loss: 0.3489
 778/1500 [==============>...............] - ETA: 9:12 - loss: 2.2249 - regression_loss: 1.8762 - classification_loss: 0.3487
 779/1500 [==============>...............] - ETA: 9:11 - loss: 2.2266 - regression_loss: 1.8779 - classification_loss: 0.3487
 780/1500 [==============>...............] - ETA: 9:10 - loss: 2.2260 - regression_loss: 1.8775 - classification_loss: 0.3485
 781/1500 [==============>...............] - ETA: 9:10 - loss: 2.2247 - regression_loss: 1.8764 - classification_loss: 0.3483
 782/1500 [==============>...............] - ETA: 9:09 - loss: 2.2237 - regression_loss: 1.8756 - classification_loss: 0.3481
 783/1500 [==============>...............] - ETA: 9:08 - loss: 2.2231 - regression_loss: 1.8752 - classification_loss: 0.3479
 784/1500 [==============>...............] - ETA: 9:08 - loss: 2.2213 - regression_loss: 1.8736 - classification_loss: 0.3477
 785/1500 [==============>...............] - ETA: 9:07 - loss: 2.2222 - regression_loss: 1.8742 - classification_loss: 0.3480
 786/1500 [==============>...............] - ETA: 9:06 - loss: 2.2211 - regression_loss: 1.8734 - classification_loss: 0.3477
 787/1500 [==============>...............] - ETA: 9:06 - loss: 2.2204 - regression_loss: 1.8728 - classification_loss: 0.3477
 788/1500 [==============>...............] - ETA: 9:05 - loss: 2.2195 - regression_loss: 1.8721 - classification_loss: 0.3474
 789/1500 [==============>...............] - ETA: 9:04 - loss: 2.2192 - regression_loss: 1.8720 - classification_loss: 0.3472
 790/1500 [==============>...............] - ETA: 9:03 - loss: 2.2195 - regression_loss: 1.8723 - classification_loss: 0.3472
 791/1500 [==============>...............] - ETA: 9:03 - loss: 2.2195 - regression_loss: 1.8723 - classification_loss: 0.3472
 792/1500 [==============>...............] - ETA: 9:03 - loss: 2.2188 - regression_loss: 1.8716 - classification_loss: 0.3471
 793/1500 [==============>...............] - ETA: 9:02 - loss: 2.2188 - regression_loss: 1.8716 - classification_loss: 0.3472
 794/1500 [==============>...............] - ETA: 9:01 - loss: 2.2181 - regression_loss: 1.8709 - classification_loss: 0.3472
 795/1500 [==============>...............] - ETA: 8:59 - loss: 2.2180 - regression_loss: 1.8705 - classification_loss: 0.3476
 796/1500 [==============>...............] - ETA: 8:59 - loss: 2.2178 - regression_loss: 1.8701 - classification_loss: 0.3477
 797/1500 [==============>...............] - ETA: 8:58 - loss: 2.2178 - regression_loss: 1.8701 - classification_loss: 0.3477
 798/1500 [==============>...............] - ETA: 8:57 - loss: 2.2170 - regression_loss: 1.8694 - classification_loss: 0.3476
 799/1500 [==============>...............] - ETA: 8:56 - loss: 2.2165 - regression_loss: 1.8691 - classification_loss: 0.3475
 800/1500 [===============>..............] - ETA: 8:56 - loss: 2.2164 - regression_loss: 1.8689 - classification_loss: 0.3475
 801/1500 [===============>..............] - ETA: 8:55 - loss: 2.2173 - regression_loss: 1.8696 - classification_loss: 0.3477
 802/1500 [===============>..............] - ETA: 8:54 - loss: 2.2162 - regression_loss: 1.8688 - classification_loss: 0.3475
 803/1500 [===============>..............] - ETA: 8:53 - loss: 2.2161 - regression_loss: 1.8687 - classification_loss: 0.3474
 804/1500 [===============>..............] - ETA: 8:52 - loss: 2.2163 - regression_loss: 1.8689 - classification_loss: 0.3474
 805/1500 [===============>..............] - ETA: 8:51 - loss: 2.2160 - regression_loss: 1.8684 - classification_loss: 0.3476
 806/1500 [===============>..............] - ETA: 8:50 - loss: 2.2147 - regression_loss: 1.8672 - classification_loss: 0.3474
 807/1500 [===============>..............] - ETA: 8:49 - loss: 2.2142 - regression_loss: 1.8669 - classification_loss: 0.3473
 808/1500 [===============>..............] - ETA: 8:50 - loss: 2.2135 - regression_loss: 1.8664 - classification_loss: 0.3471
 809/1500 [===============>..............] - ETA: 8:48 - loss: 2.2130 - regression_loss: 1.8660 - classification_loss: 0.3469
 810/1500 [===============>..............] - ETA: 8:48 - loss: 2.2143 - regression_loss: 1.8669 - classification_loss: 0.3473
 811/1500 [===============>..............] - ETA: 8:47 - loss: 2.2130 - regression_loss: 1.8659 - classification_loss: 0.3471
 812/1500 [===============>..............] - ETA: 8:47 - loss: 2.2141 - regression_loss: 1.8667 - classification_loss: 0.3474
 813/1500 [===============>..............] - ETA: 8:46 - loss: 2.2148 - regression_loss: 1.8671 - classification_loss: 0.3477
 814/1500 [===============>..............] - ETA: 8:45 - loss: 2.2137 - regression_loss: 1.8662 - classification_loss: 0.3475
 815/1500 [===============>..............] - ETA: 8:44 - loss: 2.2141 - regression_loss: 1.8666 - classification_loss: 0.3475
 816/1500 [===============>..............] - ETA: 8:43 - loss: 2.2137 - regression_loss: 1.8664 - classification_loss: 0.3474
 817/1500 [===============>..............] - ETA: 8:42 - loss: 2.2132 - regression_loss: 1.8659 - classification_loss: 0.3472
 818/1500 [===============>..............] - ETA: 8:41 - loss: 2.2128 - regression_loss: 1.8657 - classification_loss: 0.3471
 819/1500 [===============>..............] - ETA: 8:40 - loss: 2.2121 - regression_loss: 1.8651 - classification_loss: 0.3471
 820/1500 [===============>..............] - ETA: 8:41 - loss: 2.2124 - regression_loss: 1.8648 - classification_loss: 0.3476
 821/1500 [===============>..............] - ETA: 8:40 - loss: 2.2122 - regression_loss: 1.8647 - classification_loss: 0.3475
 822/1500 [===============>..............] - ETA: 8:40 - loss: 2.2114 - regression_loss: 1.8641 - classification_loss: 0.3473
 823/1500 [===============>..............] - ETA: 8:39 - loss: 2.2100 - regression_loss: 1.8630 - classification_loss: 0.3471
 824/1500 [===============>..............] - ETA: 8:38 - loss: 2.2095 - regression_loss: 1.8626 - classification_loss: 0.3469
 825/1500 [===============>..............] - ETA: 8:37 - loss: 2.2093 - regression_loss: 1.8625 - classification_loss: 0.3468
 826/1500 [===============>..............] - ETA: 8:36 - loss: 2.2087 - regression_loss: 1.8621 - classification_loss: 0.3466
 827/1500 [===============>..............] - ETA: 8:35 - loss: 2.2087 - regression_loss: 1.8622 - classification_loss: 0.3465
 828/1500 [===============>..............] - ETA: 8:34 - loss: 2.2082 - regression_loss: 1.8619 - classification_loss: 0.3464
 829/1500 [===============>..............] - ETA: 8:33 - loss: 2.2081 - regression_loss: 1.8618 - classification_loss: 0.3463
 830/1500 [===============>..............] - ETA: 8:32 - loss: 2.2065 - regression_loss: 1.8605 - classification_loss: 0.3460
 831/1500 [===============>..............] - ETA: 8:34 - loss: 2.2061 - regression_loss: 1.8601 - classification_loss: 0.3460
 832/1500 [===============>..............] - ETA: 8:33 - loss: 2.2060 - regression_loss: 1.8600 - classification_loss: 0.3460
 833/1500 [===============>..............] - ETA: 8:32 - loss: 2.2067 - regression_loss: 1.8605 - classification_loss: 0.3462
 834/1500 [===============>..............] - ETA: 8:31 - loss: 2.2076 - regression_loss: 1.8610 - classification_loss: 0.3466
 835/1500 [===============>..............] - ETA: 8:31 - loss: 2.2083 - regression_loss: 1.8615 - classification_loss: 0.3469
 836/1500 [===============>..............] - ETA: 8:30 - loss: 2.2085 - regression_loss: 1.8614 - classification_loss: 0.3471
 837/1500 [===============>..............] - ETA: 8:30 - loss: 2.2070 - regression_loss: 1.8601 - classification_loss: 0.3469
 838/1500 [===============>..............] - ETA: 8:30 - loss: 2.2068 - regression_loss: 1.8599 - classification_loss: 0.3469
 839/1500 [===============>..............] - ETA: 8:29 - loss: 2.2071 - regression_loss: 1.8603 - classification_loss: 0.3468
 840/1500 [===============>..............] - ETA: 8:27 - loss: 2.2064 - regression_loss: 1.8597 - classification_loss: 0.3466
 841/1500 [===============>..............] - ETA: 8:28 - loss: 2.2078 - regression_loss: 1.8608 - classification_loss: 0.3469
 842/1500 [===============>..............] - ETA: 8:27 - loss: 2.2065 - regression_loss: 1.8599 - classification_loss: 0.3467
 843/1500 [===============>..............] - ETA: 8:26 - loss: 2.2051 - regression_loss: 1.8583 - classification_loss: 0.3467
 844/1500 [===============>..............] - ETA: 8:25 - loss: 2.2040 - regression_loss: 1.8575 - classification_loss: 0.3465
 845/1500 [===============>..............] - ETA: 8:24 - loss: 2.2034 - regression_loss: 1.8569 - classification_loss: 0.3466
 846/1500 [===============>..............] - ETA: 8:23 - loss: 2.2037 - regression_loss: 1.8571 - classification_loss: 0.3465
 847/1500 [===============>..............] - ETA: 8:22 - loss: 2.2036 - regression_loss: 1.8571 - classification_loss: 0.3465
 848/1500 [===============>..............] - ETA: 8:21 - loss: 2.2027 - regression_loss: 1.8563 - classification_loss: 0.3464
 849/1500 [===============>..............] - ETA: 8:20 - loss: 2.2016 - regression_loss: 1.8554 - classification_loss: 0.3462
 850/1500 [================>.............] - ETA: 8:19 - loss: 2.2015 - regression_loss: 1.8553 - classification_loss: 0.3461
 851/1500 [================>.............] - ETA: 8:18 - loss: 2.2015 - regression_loss: 1.8552 - classification_loss: 0.3462
 852/1500 [================>.............] - ETA: 8:17 - loss: 2.2019 - regression_loss: 1.8558 - classification_loss: 0.3462
 853/1500 [================>.............] - ETA: 8:17 - loss: 2.2019 - regression_loss: 1.8556 - classification_loss: 0.3462
 854/1500 [================>.............] - ETA: 8:16 - loss: 2.2032 - regression_loss: 1.8568 - classification_loss: 0.3463
 855/1500 [================>.............] - ETA: 8:15 - loss: 2.2030 - regression_loss: 1.8566 - classification_loss: 0.3464
 856/1500 [================>.............] - ETA: 8:14 - loss: 2.2027 - regression_loss: 1.8564 - classification_loss: 0.3463
 857/1500 [================>.............] - ETA: 8:13 - loss: 2.2024 - regression_loss: 1.8563 - classification_loss: 0.3462
 858/1500 [================>.............] - ETA: 8:12 - loss: 2.2030 - regression_loss: 1.8568 - classification_loss: 0.3463
 859/1500 [================>.............] - ETA: 8:13 - loss: 2.2027 - regression_loss: 1.8566 - classification_loss: 0.3462
 860/1500 [================>.............] - ETA: 8:12 - loss: 2.2021 - regression_loss: 1.8562 - classification_loss: 0.3459
 861/1500 [================>.............] - ETA: 8:11 - loss: 2.2025 - regression_loss: 1.8567 - classification_loss: 0.3459
 862/1500 [================>.............] - ETA: 8:10 - loss: 2.2012 - regression_loss: 1.8556 - classification_loss: 0.3456
 863/1500 [================>.............] - ETA: 8:10 - loss: 2.2015 - regression_loss: 1.8558 - classification_loss: 0.3457
 864/1500 [================>.............] - ETA: 8:09 - loss: 2.2009 - regression_loss: 1.8553 - classification_loss: 0.3456
 865/1500 [================>.............] - ETA: 8:08 - loss: 2.2003 - regression_loss: 1.8548 - classification_loss: 0.3455
 866/1500 [================>.............] - ETA: 8:07 - loss: 2.1999 - regression_loss: 1.8544 - classification_loss: 0.3455
 867/1500 [================>.............] - ETA: 8:06 - loss: 2.2009 - regression_loss: 1.8551 - classification_loss: 0.3458
 868/1500 [================>.............] - ETA: 8:06 - loss: 2.2014 - regression_loss: 1.8556 - classification_loss: 0.3459
 869/1500 [================>.............] - ETA: 8:05 - loss: 2.2014 - regression_loss: 1.8556 - classification_loss: 0.3458
 870/1500 [================>.............] - ETA: 8:04 - loss: 2.2023 - regression_loss: 1.8563 - classification_loss: 0.3460
 871/1500 [================>.............] - ETA: 8:03 - loss: 2.2024 - regression_loss: 1.8565 - classification_loss: 0.3459
 872/1500 [================>.............] - ETA: 8:02 - loss: 2.2017 - regression_loss: 1.8558 - classification_loss: 0.3459
 873/1500 [================>.............] - ETA: 8:01 - loss: 2.2014 - regression_loss: 1.8556 - classification_loss: 0.3459
 874/1500 [================>.............] - ETA: 8:00 - loss: 2.2010 - regression_loss: 1.8552 - classification_loss: 0.3458
 875/1500 [================>.............] - ETA: 7:59 - loss: 2.2003 - regression_loss: 1.8546 - classification_loss: 0.3457
 876/1500 [================>.............] - ETA: 7:58 - loss: 2.1993 - regression_loss: 1.8538 - classification_loss: 0.3455
 877/1500 [================>.............] - ETA: 7:58 - loss: 2.1992 - regression_loss: 1.8537 - classification_loss: 0.3455
 878/1500 [================>.............] - ETA: 7:57 - loss: 2.1983 - regression_loss: 1.8530 - classification_loss: 0.3454
 879/1500 [================>.............] - ETA: 7:58 - loss: 2.1988 - regression_loss: 1.8533 - classification_loss: 0.3455
 880/1500 [================>.............] - ETA: 7:57 - loss: 2.1997 - regression_loss: 1.8541 - classification_loss: 0.3456
 881/1500 [================>.............] - ETA: 7:56 - loss: 2.1990 - regression_loss: 1.8534 - classification_loss: 0.3456
 882/1500 [================>.............] - ETA: 7:56 - loss: 2.1987 - regression_loss: 1.8531 - classification_loss: 0.3456
 883/1500 [================>.............] - ETA: 7:55 - loss: 2.1981 - regression_loss: 1.8526 - classification_loss: 0.3454
 884/1500 [================>.............] - ETA: 7:54 - loss: 2.1971 - regression_loss: 1.8518 - classification_loss: 0.3453
 885/1500 [================>.............] - ETA: 7:52 - loss: 2.1970 - regression_loss: 1.8517 - classification_loss: 0.3453
 886/1500 [================>.............] - ETA: 7:52 - loss: 2.1965 - regression_loss: 1.8513 - classification_loss: 0.3452
 887/1500 [================>.............] - ETA: 7:51 - loss: 2.1960 - regression_loss: 1.8507 - classification_loss: 0.3454
 888/1500 [================>.............] - ETA: 7:50 - loss: 2.1954 - regression_loss: 1.8502 - classification_loss: 0.3452
 889/1500 [================>.............] - ETA: 7:49 - loss: 2.1951 - regression_loss: 1.8502 - classification_loss: 0.3450
 890/1500 [================>.............] - ETA: 7:49 - loss: 2.1942 - regression_loss: 1.8494 - classification_loss: 0.3448
 891/1500 [================>.............] - ETA: 7:49 - loss: 2.1942 - regression_loss: 1.8491 - classification_loss: 0.3451
 892/1500 [================>.............] - ETA: 7:48 - loss: 2.1950 - regression_loss: 1.8497 - classification_loss: 0.3453
 893/1500 [================>.............] - ETA: 7:47 - loss: 2.1944 - regression_loss: 1.8492 - classification_loss: 0.3452
 894/1500 [================>.............] - ETA: 7:46 - loss: 2.1937 - regression_loss: 1.8486 - classification_loss: 0.3450
 895/1500 [================>.............] - ETA: 7:45 - loss: 2.1929 - regression_loss: 1.8480 - classification_loss: 0.3449
 896/1500 [================>.............] - ETA: 7:44 - loss: 2.1932 - regression_loss: 1.8483 - classification_loss: 0.3449
 897/1500 [================>.............] - ETA: 7:43 - loss: 2.1923 - regression_loss: 1.8474 - classification_loss: 0.3449
 898/1500 [================>.............] - ETA: 7:42 - loss: 2.1923 - regression_loss: 1.8475 - classification_loss: 0.3448
 899/1500 [================>.............] - ETA: 7:41 - loss: 2.1914 - regression_loss: 1.8468 - classification_loss: 0.3446
 900/1500 [=================>............] - ETA: 7:40 - loss: 2.1918 - regression_loss: 1.8471 - classification_loss: 0.3447
 901/1500 [=================>............] - ETA: 7:39 - loss: 2.1920 - regression_loss: 1.8470 - classification_loss: 0.3449
 902/1500 [=================>............] - ETA: 7:39 - loss: 2.1923 - regression_loss: 1.8473 - classification_loss: 0.3450
 903/1500 [=================>............] - ETA: 7:38 - loss: 2.1914 - regression_loss: 1.8466 - classification_loss: 0.3448
 904/1500 [=================>............] - ETA: 7:37 - loss: 2.1909 - regression_loss: 1.8463 - classification_loss: 0.3446
 905/1500 [=================>............] - ETA: 7:36 - loss: 2.1905 - regression_loss: 1.8460 - classification_loss: 0.3444
 906/1500 [=================>............] - ETA: 7:35 - loss: 2.1899 - regression_loss: 1.8457 - classification_loss: 0.3442
 907/1500 [=================>............] - ETA: 7:35 - loss: 2.1894 - regression_loss: 1.8454 - classification_loss: 0.3441
 908/1500 [=================>............] - ETA: 7:35 - loss: 2.1903 - regression_loss: 1.8459 - classification_loss: 0.3444
 909/1500 [=================>............] - ETA: 7:34 - loss: 2.1912 - regression_loss: 1.8465 - classification_loss: 0.3447
 910/1500 [=================>............] - ETA: 7:33 - loss: 2.1913 - regression_loss: 1.8466 - classification_loss: 0.3448
 911/1500 [=================>............] - ETA: 7:32 - loss: 2.1919 - regression_loss: 1.8471 - classification_loss: 0.3448
 912/1500 [=================>............] - ETA: 7:31 - loss: 2.1922 - regression_loss: 1.8474 - classification_loss: 0.3448
 913/1500 [=================>............] - ETA: 7:31 - loss: 2.1930 - regression_loss: 1.8480 - classification_loss: 0.3449
 914/1500 [=================>............] - ETA: 7:30 - loss: 2.1934 - regression_loss: 1.8484 - classification_loss: 0.3450
 915/1500 [=================>............] - ETA: 7:28 - loss: 2.1930 - regression_loss: 1.8481 - classification_loss: 0.3450
 916/1500 [=================>............] - ETA: 7:28 - loss: 2.1933 - regression_loss: 1.8483 - classification_loss: 0.3449
 917/1500 [=================>............] - ETA: 7:27 - loss: 2.1938 - regression_loss: 1.8488 - classification_loss: 0.3450
 918/1500 [=================>............] - ETA: 7:28 - loss: 2.1939 - regression_loss: 1.8488 - classification_loss: 0.3451
 919/1500 [=================>............] - ETA: 7:28 - loss: 2.1933 - regression_loss: 1.8483 - classification_loss: 0.3450
 920/1500 [=================>............] - ETA: 7:28 - loss: 2.1937 - regression_loss: 1.8486 - classification_loss: 0.3450
 921/1500 [=================>............] - ETA: 7:27 - loss: 2.1941 - regression_loss: 1.8489 - classification_loss: 0.3452
 922/1500 [=================>............] - ETA: 7:26 - loss: 2.1936 - regression_loss: 1.8485 - classification_loss: 0.3451
 923/1500 [=================>............] - ETA: 7:25 - loss: 2.1940 - regression_loss: 1.8490 - classification_loss: 0.3450
 924/1500 [=================>............] - ETA: 7:24 - loss: 2.1952 - regression_loss: 1.8502 - classification_loss: 0.3450
 925/1500 [=================>............] - ETA: 7:24 - loss: 2.1950 - regression_loss: 1.8501 - classification_loss: 0.3449
 926/1500 [=================>............] - ETA: 7:23 - loss: 2.1944 - regression_loss: 1.8497 - classification_loss: 0.3447
 927/1500 [=================>............] - ETA: 7:23 - loss: 2.1939 - regression_loss: 1.8493 - classification_loss: 0.3446
 928/1500 [=================>............] - ETA: 7:22 - loss: 2.1939 - regression_loss: 1.8494 - classification_loss: 0.3444
 929/1500 [=================>............] - ETA: 7:21 - loss: 2.1930 - regression_loss: 1.8488 - classification_loss: 0.3442
 930/1500 [=================>............] - ETA: 7:20 - loss: 2.1927 - regression_loss: 1.8486 - classification_loss: 0.3441
 931/1500 [=================>............] - ETA: 7:19 - loss: 2.1926 - regression_loss: 1.8485 - classification_loss: 0.3441
 932/1500 [=================>............] - ETA: 7:18 - loss: 2.1928 - regression_loss: 1.8486 - classification_loss: 0.3442
 933/1500 [=================>............] - ETA: 7:18 - loss: 2.1919 - regression_loss: 1.8480 - classification_loss: 0.3439
 934/1500 [=================>............] - ETA: 7:17 - loss: 2.1917 - regression_loss: 1.8479 - classification_loss: 0.3437
 935/1500 [=================>............] - ETA: 7:16 - loss: 2.1911 - regression_loss: 1.8474 - classification_loss: 0.3437
 936/1500 [=================>............] - ETA: 7:15 - loss: 2.1905 - regression_loss: 1.8470 - classification_loss: 0.3435
 937/1500 [=================>............] - ETA: 7:14 - loss: 2.1915 - regression_loss: 1.8480 - classification_loss: 0.3435
 938/1500 [=================>............] - ETA: 7:14 - loss: 2.1916 - regression_loss: 1.8480 - classification_loss: 0.3436
 939/1500 [=================>............] - ETA: 7:13 - loss: 2.1910 - regression_loss: 1.8476 - classification_loss: 0.3434
 940/1500 [=================>............] - ETA: 7:12 - loss: 2.1904 - regression_loss: 1.8471 - classification_loss: 0.3432
 941/1500 [=================>............] - ETA: 7:11 - loss: 2.1898 - regression_loss: 1.8467 - classification_loss: 0.3431
 942/1500 [=================>............] - ETA: 7:10 - loss: 2.1897 - regression_loss: 1.8465 - classification_loss: 0.3431
 943/1500 [=================>............] - ETA: 7:10 - loss: 2.1899 - regression_loss: 1.8467 - classification_loss: 0.3432
 944/1500 [=================>............] - ETA: 7:09 - loss: 2.1888 - regression_loss: 1.8458 - classification_loss: 0.3430
 945/1500 [=================>............] - ETA: 7:08 - loss: 2.1887 - regression_loss: 1.8458 - classification_loss: 0.3429
 946/1500 [=================>............] - ETA: 7:07 - loss: 2.1880 - regression_loss: 1.8451 - classification_loss: 0.3429
 947/1500 [=================>............] - ETA: 7:06 - loss: 2.1879 - regression_loss: 1.8450 - classification_loss: 0.3430
 948/1500 [=================>............] - ETA: 7:05 - loss: 2.1876 - regression_loss: 1.8446 - classification_loss: 0.3430
 949/1500 [=================>............] - ETA: 7:05 - loss: 2.1879 - regression_loss: 1.8447 - classification_loss: 0.3432
 950/1500 [==================>...........] - ETA: 7:04 - loss: 2.1899 - regression_loss: 1.8464 - classification_loss: 0.3435
 951/1500 [==================>...........] - ETA: 7:03 - loss: 2.1906 - regression_loss: 1.8472 - classification_loss: 0.3435
 952/1500 [==================>...........] - ETA: 7:02 - loss: 2.1913 - regression_loss: 1.8475 - classification_loss: 0.3438
 953/1500 [==================>...........] - ETA: 7:01 - loss: 2.1903 - regression_loss: 1.8467 - classification_loss: 0.3436
 954/1500 [==================>...........] - ETA: 7:00 - loss: 2.1907 - regression_loss: 1.8470 - classification_loss: 0.3437
 955/1500 [==================>...........] - ETA: 6:59 - loss: 2.1901 - regression_loss: 1.8466 - classification_loss: 0.3435
 956/1500 [==================>...........] - ETA: 6:58 - loss: 2.1915 - regression_loss: 1.8475 - classification_loss: 0.3439
 957/1500 [==================>...........] - ETA: 6:58 - loss: 2.1915 - regression_loss: 1.8476 - classification_loss: 0.3439
 958/1500 [==================>...........] - ETA: 6:58 - loss: 2.1907 - regression_loss: 1.8470 - classification_loss: 0.3437
 959/1500 [==================>...........] - ETA: 6:57 - loss: 2.1896 - regression_loss: 1.8462 - classification_loss: 0.3435
 960/1500 [==================>...........] - ETA: 6:56 - loss: 2.1896 - regression_loss: 1.8461 - classification_loss: 0.3435
 961/1500 [==================>...........] - ETA: 6:55 - loss: 2.1889 - regression_loss: 1.8456 - classification_loss: 0.3433
 962/1500 [==================>...........] - ETA: 6:55 - loss: 2.1894 - regression_loss: 1.8459 - classification_loss: 0.3434
 963/1500 [==================>...........] - ETA: 6:54 - loss: 2.1891 - regression_loss: 1.8457 - classification_loss: 0.3433
 964/1500 [==================>...........] - ETA: 6:53 - loss: 2.1896 - regression_loss: 1.8462 - classification_loss: 0.3434
 965/1500 [==================>...........] - ETA: 6:52 - loss: 2.1896 - regression_loss: 1.8462 - classification_loss: 0.3434
 966/1500 [==================>...........] - ETA: 6:52 - loss: 2.1881 - regression_loss: 1.8449 - classification_loss: 0.3432
 967/1500 [==================>...........] - ETA: 6:51 - loss: 2.1870 - regression_loss: 1.8439 - classification_loss: 0.3430
 968/1500 [==================>...........] - ETA: 6:50 - loss: 2.1874 - regression_loss: 1.8444 - classification_loss: 0.3430
 969/1500 [==================>...........] - ETA: 6:49 - loss: 2.1867 - regression_loss: 1.8438 - classification_loss: 0.3428
 970/1500 [==================>...........] - ETA: 6:48 - loss: 2.1859 - regression_loss: 1.8432 - classification_loss: 0.3426
 971/1500 [==================>...........] - ETA: 6:48 - loss: 2.1857 - regression_loss: 1.8431 - classification_loss: 0.3426
 972/1500 [==================>...........] - ETA: 6:47 - loss: 2.1854 - regression_loss: 1.8428 - classification_loss: 0.3427
 973/1500 [==================>...........] - ETA: 6:46 - loss: 2.1843 - regression_loss: 1.8418 - classification_loss: 0.3425
 974/1500 [==================>...........] - ETA: 6:46 - loss: 2.1849 - regression_loss: 1.8423 - classification_loss: 0.3426
 975/1500 [==================>...........] - ETA: 6:45 - loss: 2.1856 - regression_loss: 1.8428 - classification_loss: 0.3428
 976/1500 [==================>...........] - ETA: 6:44 - loss: 2.1847 - regression_loss: 1.8421 - classification_loss: 0.3426
 977/1500 [==================>...........] - ETA: 6:43 - loss: 2.1854 - regression_loss: 1.8428 - classification_loss: 0.3426
 978/1500 [==================>...........] - ETA: 6:42 - loss: 2.1848 - regression_loss: 1.8424 - classification_loss: 0.3424
 979/1500 [==================>...........] - ETA: 6:42 - loss: 2.1837 - regression_loss: 1.8415 - classification_loss: 0.3422
 980/1500 [==================>...........] - ETA: 6:41 - loss: 2.1833 - regression_loss: 1.8410 - classification_loss: 0.3422
 981/1500 [==================>...........] - ETA: 6:40 - loss: 2.1827 - regression_loss: 1.8406 - classification_loss: 0.3421
 982/1500 [==================>...........] - ETA: 6:40 - loss: 2.1833 - regression_loss: 1.8411 - classification_loss: 0.3422
 983/1500 [==================>...........] - ETA: 6:39 - loss: 2.1832 - regression_loss: 1.8412 - classification_loss: 0.3421
 984/1500 [==================>...........] - ETA: 6:38 - loss: 2.1839 - regression_loss: 1.8409 - classification_loss: 0.3430
 985/1500 [==================>...........] - ETA: 6:37 - loss: 2.1837 - regression_loss: 1.8406 - classification_loss: 0.3431
 986/1500 [==================>...........] - ETA: 6:36 - loss: 2.1830 - regression_loss: 1.8400 - classification_loss: 0.3430
 987/1500 [==================>...........] - ETA: 6:36 - loss: 2.1824 - regression_loss: 1.8395 - classification_loss: 0.3429
 988/1500 [==================>...........] - ETA: 6:35 - loss: 2.1824 - regression_loss: 1.8396 - classification_loss: 0.3428
 989/1500 [==================>...........] - ETA: 6:34 - loss: 2.1818 - regression_loss: 1.8392 - classification_loss: 0.3426
 990/1500 [==================>...........] - ETA: 6:33 - loss: 2.1824 - regression_loss: 1.8396 - classification_loss: 0.3427
 991/1500 [==================>...........] - ETA: 6:32 - loss: 2.1834 - regression_loss: 1.8403 - classification_loss: 0.3431
 992/1500 [==================>...........] - ETA: 6:31 - loss: 2.1827 - regression_loss: 1.8397 - classification_loss: 0.3430
 993/1500 [==================>...........] - ETA: 6:31 - loss: 2.1832 - regression_loss: 1.8402 - classification_loss: 0.3430
 994/1500 [==================>...........] - ETA: 6:30 - loss: 2.1824 - regression_loss: 1.8396 - classification_loss: 0.3429
 995/1500 [==================>...........] - ETA: 6:29 - loss: 2.1824 - regression_loss: 1.8397 - classification_loss: 0.3428
 996/1500 [==================>...........] - ETA: 6:28 - loss: 2.1827 - regression_loss: 1.8398 - classification_loss: 0.3429
 997/1500 [==================>...........] - ETA: 6:27 - loss: 2.1816 - regression_loss: 1.8389 - classification_loss: 0.3427
 998/1500 [==================>...........] - ETA: 6:26 - loss: 2.1815 - regression_loss: 1.8385 - classification_loss: 0.3430
 999/1500 [==================>...........] - ETA: 6:25 - loss: 2.1812 - regression_loss: 1.8382 - classification_loss: 0.3429
1000/1500 [===================>..........] - ETA: 6:25 - loss: 2.1810 - regression_loss: 1.8382 - classification_loss: 0.3428
1001/1500 [===================>..........] - ETA: 6:24 - loss: 2.1816 - regression_loss: 1.8387 - classification_loss: 0.3429
1002/1500 [===================>..........] - ETA: 6:23 - loss: 2.1819 - regression_loss: 1.8391 - classification_loss: 0.3428
1003/1500 [===================>..........] - ETA: 6:23 - loss: 2.1821 - regression_loss: 1.8392 - classification_loss: 0.3429
1004/1500 [===================>..........] - ETA: 6:22 - loss: 2.1818 - regression_loss: 1.8390 - classification_loss: 0.3428
1005/1500 [===================>..........] - ETA: 6:21 - loss: 2.1822 - regression_loss: 1.8394 - classification_loss: 0.3428
1006/1500 [===================>..........] - ETA: 6:21 - loss: 2.1818 - regression_loss: 1.8391 - classification_loss: 0.3427
1007/1500 [===================>..........] - ETA: 6:20 - loss: 2.1815 - regression_loss: 1.8389 - classification_loss: 0.3426
1008/1500 [===================>..........] - ETA: 6:19 - loss: 2.1811 - regression_loss: 1.8386 - classification_loss: 0.3425
1009/1500 [===================>..........] - ETA: 6:19 - loss: 2.1806 - regression_loss: 1.8383 - classification_loss: 0.3423
1010/1500 [===================>..........] - ETA: 6:18 - loss: 2.1808 - regression_loss: 1.8386 - classification_loss: 0.3422
1011/1500 [===================>..........] - ETA: 6:18 - loss: 2.1799 - regression_loss: 1.8378 - classification_loss: 0.3421
1012/1500 [===================>..........] - ETA: 6:17 - loss: 2.1795 - regression_loss: 1.8377 - classification_loss: 0.3419
1013/1500 [===================>..........] - ETA: 6:16 - loss: 2.1799 - regression_loss: 1.8380 - classification_loss: 0.3419
1014/1500 [===================>..........] - ETA: 6:15 - loss: 2.1803 - regression_loss: 1.8385 - classification_loss: 0.3418
1015/1500 [===================>..........] - ETA: 6:15 - loss: 2.1795 - regression_loss: 1.8378 - classification_loss: 0.3417
1016/1500 [===================>..........] - ETA: 6:14 - loss: 2.1801 - regression_loss: 1.8384 - classification_loss: 0.3417
1017/1500 [===================>..........] - ETA: 6:14 - loss: 2.1809 - regression_loss: 1.8390 - classification_loss: 0.3419
1018/1500 [===================>..........] - ETA: 6:13 - loss: 2.1807 - regression_loss: 1.8388 - classification_loss: 0.3419
1019/1500 [===================>..........] - ETA: 6:12 - loss: 2.1812 - regression_loss: 1.8391 - classification_loss: 0.3420
1020/1500 [===================>..........] - ETA: 6:12 - loss: 2.1814 - regression_loss: 1.8393 - classification_loss: 0.3421
1021/1500 [===================>..........] - ETA: 6:11 - loss: 2.1812 - regression_loss: 1.8390 - classification_loss: 0.3422
1022/1500 [===================>..........] - ETA: 6:10 - loss: 2.1809 - regression_loss: 1.8386 - classification_loss: 0.3423
1023/1500 [===================>..........] - ETA: 6:09 - loss: 2.1812 - regression_loss: 1.8389 - classification_loss: 0.3423
1024/1500 [===================>..........] - ETA: 6:08 - loss: 2.1806 - regression_loss: 1.8384 - classification_loss: 0.3422
1025/1500 [===================>..........] - ETA: 6:07 - loss: 2.1798 - regression_loss: 1.8378 - classification_loss: 0.3420
1026/1500 [===================>..........] - ETA: 6:06 - loss: 2.1791 - regression_loss: 1.8372 - classification_loss: 0.3418
1027/1500 [===================>..........] - ETA: 6:06 - loss: 2.1778 - regression_loss: 1.8362 - classification_loss: 0.3416
1028/1500 [===================>..........] - ETA: 6:05 - loss: 2.1770 - regression_loss: 1.8355 - classification_loss: 0.3415
1029/1500 [===================>..........] - ETA: 6:04 - loss: 2.1770 - regression_loss: 1.8356 - classification_loss: 0.3414
1030/1500 [===================>..........] - ETA: 6:03 - loss: 2.1771 - regression_loss: 1.8358 - classification_loss: 0.3413
1031/1500 [===================>..........] - ETA: 6:02 - loss: 2.1780 - regression_loss: 1.8364 - classification_loss: 0.3416
1032/1500 [===================>..........] - ETA: 6:02 - loss: 2.1781 - regression_loss: 1.8364 - classification_loss: 0.3417
1033/1500 [===================>..........] - ETA: 6:02 - loss: 2.1782 - regression_loss: 1.8364 - classification_loss: 0.3418
1034/1500 [===================>..........] - ETA: 6:01 - loss: 2.1782 - regression_loss: 1.8365 - classification_loss: 0.3418
1035/1500 [===================>..........] - ETA: 6:00 - loss: 2.1774 - regression_loss: 1.8357 - classification_loss: 0.3417
1036/1500 [===================>..........] - ETA: 5:59 - loss: 2.1766 - regression_loss: 1.8349 - classification_loss: 0.3417
1037/1500 [===================>..........] - ETA: 5:59 - loss: 2.1763 - regression_loss: 1.8347 - classification_loss: 0.3416
1038/1500 [===================>..........] - ETA: 5:58 - loss: 2.1764 - regression_loss: 1.8348 - classification_loss: 0.3416
1039/1500 [===================>..........] - ETA: 5:57 - loss: 2.1767 - regression_loss: 1.8345 - classification_loss: 0.3422
1040/1500 [===================>..........] - ETA: 5:56 - loss: 2.1768 - regression_loss: 1.8345 - classification_loss: 0.3423
1041/1500 [===================>..........] - ETA: 5:56 - loss: 2.1766 - regression_loss: 1.8342 - classification_loss: 0.3423
1042/1500 [===================>..........] - ETA: 5:55 - loss: 2.1769 - regression_loss: 1.8347 - classification_loss: 0.3423
1043/1500 [===================>..........] - ETA: 5:54 - loss: 2.1778 - regression_loss: 1.8354 - classification_loss: 0.3425
1044/1500 [===================>..........] - ETA: 5:53 - loss: 2.1775 - regression_loss: 1.8350 - classification_loss: 0.3425
1045/1500 [===================>..........] - ETA: 5:52 - loss: 2.1773 - regression_loss: 1.8348 - classification_loss: 0.3425
1046/1500 [===================>..........] - ETA: 5:51 - loss: 2.1776 - regression_loss: 1.8351 - classification_loss: 0.3425
1047/1500 [===================>..........] - ETA: 5:50 - loss: 2.1780 - regression_loss: 1.8354 - classification_loss: 0.3426
1048/1500 [===================>..........] - ETA: 5:50 - loss: 2.1778 - regression_loss: 1.8353 - classification_loss: 0.3426
1049/1500 [===================>..........] - ETA: 5:49 - loss: 2.1788 - regression_loss: 1.8359 - classification_loss: 0.3429
1050/1500 [====================>.........] - ETA: 5:48 - loss: 2.1795 - regression_loss: 1.8364 - classification_loss: 0.3430
1051/1500 [====================>.........] - ETA: 5:47 - loss: 2.1793 - regression_loss: 1.8362 - classification_loss: 0.3431
1052/1500 [====================>.........] - ETA: 5:46 - loss: 2.1785 - regression_loss: 1.8356 - classification_loss: 0.3428
1053/1500 [====================>.........] - ETA: 5:46 - loss: 2.1798 - regression_loss: 1.8367 - classification_loss: 0.3431
1054/1500 [====================>.........] - ETA: 5:45 - loss: 2.1800 - regression_loss: 1.8370 - classification_loss: 0.3430
1055/1500 [====================>.........] - ETA: 5:44 - loss: 2.1792 - regression_loss: 1.8363 - classification_loss: 0.3428
1056/1500 [====================>.........] - ETA: 5:43 - loss: 2.1787 - regression_loss: 1.8360 - classification_loss: 0.3427
1057/1500 [====================>.........] - ETA: 5:42 - loss: 2.1784 - regression_loss: 1.8357 - classification_loss: 0.3427
1058/1500 [====================>.........] - ETA: 5:42 - loss: 2.1777 - regression_loss: 1.8352 - classification_loss: 0.3425
1059/1500 [====================>.........] - ETA: 5:41 - loss: 2.1775 - regression_loss: 1.8350 - classification_loss: 0.3425
1060/1500 [====================>.........] - ETA: 5:40 - loss: 2.1769 - regression_loss: 1.8345 - classification_loss: 0.3424
1061/1500 [====================>.........] - ETA: 5:40 - loss: 2.1769 - regression_loss: 1.8342 - classification_loss: 0.3427
1062/1500 [====================>.........] - ETA: 5:39 - loss: 2.1775 - regression_loss: 1.8348 - classification_loss: 0.3427
1063/1500 [====================>.........] - ETA: 5:38 - loss: 2.1777 - regression_loss: 1.8349 - classification_loss: 0.3428
1064/1500 [====================>.........] - ETA: 5:37 - loss: 2.1771 - regression_loss: 1.8344 - classification_loss: 0.3426
1065/1500 [====================>.........] - ETA: 5:37 - loss: 2.1764 - regression_loss: 1.8339 - classification_loss: 0.3425
1066/1500 [====================>.........] - ETA: 5:36 - loss: 2.1760 - regression_loss: 1.8335 - classification_loss: 0.3425
1067/1500 [====================>.........] - ETA: 5:35 - loss: 2.1760 - regression_loss: 1.8334 - classification_loss: 0.3426
1068/1500 [====================>.........] - ETA: 5:35 - loss: 2.1753 - regression_loss: 1.8328 - classification_loss: 0.3425
1069/1500 [====================>.........] - ETA: 5:35 - loss: 2.1752 - regression_loss: 1.8327 - classification_loss: 0.3425
1070/1500 [====================>.........] - ETA: 5:34 - loss: 2.1757 - regression_loss: 1.8332 - classification_loss: 0.3426
1071/1500 [====================>.........] - ETA: 5:33 - loss: 2.1762 - regression_loss: 1.8335 - classification_loss: 0.3427
1072/1500 [====================>.........] - ETA: 5:32 - loss: 2.1775 - regression_loss: 1.8347 - classification_loss: 0.3429
1073/1500 [====================>.........] - ETA: 5:31 - loss: 2.1779 - regression_loss: 1.8351 - classification_loss: 0.3427
1074/1500 [====================>.........] - ETA: 5:30 - loss: 2.1770 - regression_loss: 1.8344 - classification_loss: 0.3426
1075/1500 [====================>.........] - ETA: 5:30 - loss: 2.1765 - regression_loss: 1.8340 - classification_loss: 0.3425
1076/1500 [====================>.........] - ETA: 5:29 - loss: 2.1767 - regression_loss: 1.8341 - classification_loss: 0.3426
1077/1500 [====================>.........] - ETA: 5:28 - loss: 2.1769 - regression_loss: 1.8343 - classification_loss: 0.3426
1078/1500 [====================>.........] - ETA: 5:28 - loss: 2.1777 - regression_loss: 1.8351 - classification_loss: 0.3426
1079/1500 [====================>.........] - ETA: 5:27 - loss: 2.1772 - regression_loss: 1.8348 - classification_loss: 0.3424
1080/1500 [====================>.........] - ETA: 5:26 - loss: 2.1771 - regression_loss: 1.8346 - classification_loss: 0.3425
1081/1500 [====================>.........] - ETA: 5:25 - loss: 2.1784 - regression_loss: 1.8359 - classification_loss: 0.3425
1082/1500 [====================>.........] - ETA: 5:24 - loss: 2.1788 - regression_loss: 1.8360 - classification_loss: 0.3428
1083/1500 [====================>.........] - ETA: 5:23 - loss: 2.1776 - regression_loss: 1.8350 - classification_loss: 0.3426
1084/1500 [====================>.........] - ETA: 5:23 - loss: 2.1771 - regression_loss: 1.8346 - classification_loss: 0.3425
1085/1500 [====================>.........] - ETA: 5:22 - loss: 2.1766 - regression_loss: 1.8343 - classification_loss: 0.3423
1086/1500 [====================>.........] - ETA: 5:21 - loss: 2.1768 - regression_loss: 1.8344 - classification_loss: 0.3425
1087/1500 [====================>.........] - ETA: 5:20 - loss: 2.1760 - regression_loss: 1.8337 - classification_loss: 0.3423
1088/1500 [====================>.........] - ETA: 5:19 - loss: 2.1760 - regression_loss: 1.8338 - classification_loss: 0.3422
1089/1500 [====================>.........] - ETA: 5:19 - loss: 2.1758 - regression_loss: 1.8338 - classification_loss: 0.3420
1090/1500 [====================>.........] - ETA: 5:18 - loss: 2.1755 - regression_loss: 1.8336 - classification_loss: 0.3419
1091/1500 [====================>.........] - ETA: 5:17 - loss: 2.1760 - regression_loss: 1.8340 - classification_loss: 0.3420
1092/1500 [====================>.........] - ETA: 5:16 - loss: 2.1763 - regression_loss: 1.8343 - classification_loss: 0.3420
1093/1500 [====================>.........] - ETA: 5:15 - loss: 2.1753 - regression_loss: 1.8334 - classification_loss: 0.3419
1094/1500 [====================>.........] - ETA: 5:15 - loss: 2.1753 - regression_loss: 1.8335 - classification_loss: 0.3418
1095/1500 [====================>.........] - ETA: 5:14 - loss: 2.1762 - regression_loss: 1.8341 - classification_loss: 0.3421
1096/1500 [====================>.........] - ETA: 5:13 - loss: 2.1763 - regression_loss: 1.8341 - classification_loss: 0.3422
1097/1500 [====================>.........] - ETA: 5:12 - loss: 2.1751 - regression_loss: 1.8331 - classification_loss: 0.3420
1098/1500 [====================>.........] - ETA: 5:11 - loss: 2.1742 - regression_loss: 1.8323 - classification_loss: 0.3420
1099/1500 [====================>.........] - ETA: 5:11 - loss: 2.1743 - regression_loss: 1.8322 - classification_loss: 0.3421
1100/1500 [=====================>........] - ETA: 5:10 - loss: 2.1747 - regression_loss: 1.8325 - classification_loss: 0.3422
1101/1500 [=====================>........] - ETA: 5:10 - loss: 2.1746 - regression_loss: 1.8322 - classification_loss: 0.3424
1102/1500 [=====================>........] - ETA: 5:09 - loss: 2.1746 - regression_loss: 1.8322 - classification_loss: 0.3424
1103/1500 [=====================>........] - ETA: 5:08 - loss: 2.1737 - regression_loss: 1.8315 - classification_loss: 0.3422
1104/1500 [=====================>........] - ETA: 5:07 - loss: 2.1742 - regression_loss: 1.8319 - classification_loss: 0.3423
1105/1500 [=====================>........] - ETA: 5:06 - loss: 2.1741 - regression_loss: 1.8318 - classification_loss: 0.3423
1106/1500 [=====================>........] - ETA: 5:05 - loss: 2.1744 - regression_loss: 1.8320 - classification_loss: 0.3424
1107/1500 [=====================>........] - ETA: 5:05 - loss: 2.1753 - regression_loss: 1.8326 - classification_loss: 0.3427
1108/1500 [=====================>........] - ETA: 5:04 - loss: 2.1746 - regression_loss: 1.8321 - classification_loss: 0.3425
1109/1500 [=====================>........] - ETA: 5:03 - loss: 2.1741 - regression_loss: 1.8317 - classification_loss: 0.3425
1110/1500 [=====================>........] - ETA: 5:03 - loss: 2.1741 - regression_loss: 1.8317 - classification_loss: 0.3424
1111/1500 [=====================>........] - ETA: 5:02 - loss: 2.1739 - regression_loss: 1.8315 - classification_loss: 0.3423
1112/1500 [=====================>........] - ETA: 5:01 - loss: 2.1743 - regression_loss: 1.8318 - classification_loss: 0.3425
1113/1500 [=====================>........] - ETA: 5:00 - loss: 2.1733 - regression_loss: 1.8310 - classification_loss: 0.3423
1114/1500 [=====================>........] - ETA: 4:59 - loss: 2.1730 - regression_loss: 1.8308 - classification_loss: 0.3422
1115/1500 [=====================>........] - ETA: 4:59 - loss: 2.1730 - regression_loss: 1.8307 - classification_loss: 0.3423
1116/1500 [=====================>........] - ETA: 4:58 - loss: 2.1719 - regression_loss: 1.8299 - classification_loss: 0.3420
1117/1500 [=====================>........] - ETA: 4:57 - loss: 2.1734 - regression_loss: 1.8311 - classification_loss: 0.3424
1118/1500 [=====================>........] - ETA: 4:56 - loss: 2.1738 - regression_loss: 1.8313 - classification_loss: 0.3425
1119/1500 [=====================>........] - ETA: 4:55 - loss: 2.1733 - regression_loss: 1.8310 - classification_loss: 0.3423
1120/1500 [=====================>........] - ETA: 4:55 - loss: 2.1739 - regression_loss: 1.8314 - classification_loss: 0.3424
1121/1500 [=====================>........] - ETA: 4:54 - loss: 2.1741 - regression_loss: 1.8317 - classification_loss: 0.3424
1122/1500 [=====================>........] - ETA: 4:53 - loss: 2.1746 - regression_loss: 1.8322 - classification_loss: 0.3424
1123/1500 [=====================>........] - ETA: 4:52 - loss: 2.1741 - regression_loss: 1.8318 - classification_loss: 0.3423
1124/1500 [=====================>........] - ETA: 4:51 - loss: 2.1751 - regression_loss: 1.8324 - classification_loss: 0.3426
1125/1500 [=====================>........] - ETA: 4:50 - loss: 2.1745 - regression_loss: 1.8320 - classification_loss: 0.3425
1126/1500 [=====================>........] - ETA: 4:50 - loss: 2.1750 - regression_loss: 1.8320 - classification_loss: 0.3430
1127/1500 [=====================>........] - ETA: 4:49 - loss: 2.1746 - regression_loss: 1.8316 - classification_loss: 0.3429
1128/1500 [=====================>........] - ETA: 4:48 - loss: 2.1743 - regression_loss: 1.8313 - classification_loss: 0.3430
1129/1500 [=====================>........] - ETA: 4:47 - loss: 2.1741 - regression_loss: 1.8312 - classification_loss: 0.3429
1130/1500 [=====================>........] - ETA: 4:46 - loss: 2.1734 - regression_loss: 1.8306 - classification_loss: 0.3428
1131/1500 [=====================>........] - ETA: 4:45 - loss: 2.1735 - regression_loss: 1.8306 - classification_loss: 0.3430
1132/1500 [=====================>........] - ETA: 4:45 - loss: 2.1725 - regression_loss: 1.8297 - classification_loss: 0.3428
1133/1500 [=====================>........] - ETA: 4:44 - loss: 2.1717 - regression_loss: 1.8289 - classification_loss: 0.3427
1134/1500 [=====================>........] - ETA: 4:43 - loss: 2.1709 - regression_loss: 1.8283 - classification_loss: 0.3426
1135/1500 [=====================>........] - ETA: 4:42 - loss: 2.1713 - regression_loss: 1.8285 - classification_loss: 0.3428
1136/1500 [=====================>........] - ETA: 4:41 - loss: 2.1714 - regression_loss: 1.8285 - classification_loss: 0.3429
1137/1500 [=====================>........] - ETA: 4:41 - loss: 2.1712 - regression_loss: 1.8283 - classification_loss: 0.3429
1138/1500 [=====================>........] - ETA: 4:40 - loss: 2.1711 - regression_loss: 1.8283 - classification_loss: 0.3428
1139/1500 [=====================>........] - ETA: 4:39 - loss: 2.1703 - regression_loss: 1.8277 - classification_loss: 0.3427
1140/1500 [=====================>........] - ETA: 4:39 - loss: 2.1699 - regression_loss: 1.8273 - classification_loss: 0.3426
1141/1500 [=====================>........] - ETA: 4:38 - loss: 2.1696 - regression_loss: 1.8270 - classification_loss: 0.3426
1142/1500 [=====================>........] - ETA: 4:37 - loss: 2.1702 - regression_loss: 1.8274 - classification_loss: 0.3428
1143/1500 [=====================>........] - ETA: 4:36 - loss: 2.1697 - regression_loss: 1.8270 - classification_loss: 0.3426
1144/1500 [=====================>........] - ETA: 4:35 - loss: 2.1699 - regression_loss: 1.8273 - classification_loss: 0.3426
1145/1500 [=====================>........] - ETA: 4:34 - loss: 2.1693 - regression_loss: 1.8268 - classification_loss: 0.3425
1146/1500 [=====================>........] - ETA: 4:34 - loss: 2.1699 - regression_loss: 1.8273 - classification_loss: 0.3427
1147/1500 [=====================>........] - ETA: 4:33 - loss: 2.1705 - regression_loss: 1.8279 - classification_loss: 0.3427
1148/1500 [=====================>........] - ETA: 4:32 - loss: 2.1693 - regression_loss: 1.8268 - classification_loss: 0.3425
1149/1500 [=====================>........] - ETA: 4:32 - loss: 2.1690 - regression_loss: 1.8265 - classification_loss: 0.3425
1150/1500 [======================>.......] - ETA: 4:31 - loss: 2.1688 - regression_loss: 1.8262 - classification_loss: 0.3425
1151/1500 [======================>.......] - ETA: 4:30 - loss: 2.1683 - regression_loss: 1.8259 - classification_loss: 0.3424
1152/1500 [======================>.......] - ETA: 4:30 - loss: 2.1687 - regression_loss: 1.8261 - classification_loss: 0.3427
1153/1500 [======================>.......] - ETA: 4:29 - loss: 2.1690 - regression_loss: 1.8263 - classification_loss: 0.3427
1154/1500 [======================>.......] - ETA: 4:28 - loss: 2.1691 - regression_loss: 1.8262 - classification_loss: 0.3429
1155/1500 [======================>.......] - ETA: 4:27 - loss: 2.1684 - regression_loss: 1.8257 - classification_loss: 0.3427
1156/1500 [======================>.......] - ETA: 4:26 - loss: 2.1682 - regression_loss: 1.8255 - classification_loss: 0.3428
1157/1500 [======================>.......] - ETA: 4:26 - loss: 2.1680 - regression_loss: 1.8252 - classification_loss: 0.3428
1158/1500 [======================>.......] - ETA: 4:25 - loss: 2.1682 - regression_loss: 1.8253 - classification_loss: 0.3429
1159/1500 [======================>.......] - ETA: 4:24 - loss: 2.1677 - regression_loss: 1.8249 - classification_loss: 0.3428
1160/1500 [======================>.......] - ETA: 4:24 - loss: 2.1674 - regression_loss: 1.8248 - classification_loss: 0.3426
1161/1500 [======================>.......] - ETA: 4:23 - loss: 2.1705 - regression_loss: 1.8260 - classification_loss: 0.3444
1162/1500 [======================>.......] - ETA: 4:22 - loss: 2.1697 - regression_loss: 1.8254 - classification_loss: 0.3443
1163/1500 [======================>.......] - ETA: 4:21 - loss: 2.1708 - regression_loss: 1.8263 - classification_loss: 0.3445
1164/1500 [======================>.......] - ETA: 4:20 - loss: 2.1700 - regression_loss: 1.8256 - classification_loss: 0.3443
1165/1500 [======================>.......] - ETA: 4:19 - loss: 2.1698 - regression_loss: 1.8255 - classification_loss: 0.3443
1166/1500 [======================>.......] - ETA: 4:19 - loss: 2.1702 - regression_loss: 1.8257 - classification_loss: 0.3445
1167/1500 [======================>.......] - ETA: 4:18 - loss: 2.1713 - regression_loss: 1.8265 - classification_loss: 0.3448
1168/1500 [======================>.......] - ETA: 4:17 - loss: 2.1711 - regression_loss: 1.8264 - classification_loss: 0.3446
1169/1500 [======================>.......] - ETA: 4:16 - loss: 2.1707 - regression_loss: 1.8261 - classification_loss: 0.3445
1170/1500 [======================>.......] - ETA: 4:15 - loss: 2.1697 - regression_loss: 1.8253 - classification_loss: 0.3444
1171/1500 [======================>.......] - ETA: 4:14 - loss: 2.1700 - regression_loss: 1.8257 - classification_loss: 0.3444
1172/1500 [======================>.......] - ETA: 4:13 - loss: 2.1708 - regression_loss: 1.8262 - classification_loss: 0.3446
1173/1500 [======================>.......] - ETA: 4:13 - loss: 2.1714 - regression_loss: 1.8268 - classification_loss: 0.3446
1174/1500 [======================>.......] - ETA: 4:12 - loss: 2.1718 - regression_loss: 1.8271 - classification_loss: 0.3447
1175/1500 [======================>.......] - ETA: 4:11 - loss: 2.1712 - regression_loss: 1.8267 - classification_loss: 0.3445
1176/1500 [======================>.......] - ETA: 4:10 - loss: 2.1721 - regression_loss: 1.8273 - classification_loss: 0.3448
1177/1500 [======================>.......] - ETA: 4:10 - loss: 2.1715 - regression_loss: 1.8269 - classification_loss: 0.3446
1178/1500 [======================>.......] - ETA: 4:09 - loss: 2.1704 - regression_loss: 1.8259 - classification_loss: 0.3445
1179/1500 [======================>.......] - ETA: 4:08 - loss: 2.1702 - regression_loss: 1.8257 - classification_loss: 0.3445
1180/1500 [======================>.......] - ETA: 4:07 - loss: 2.1701 - regression_loss: 1.8256 - classification_loss: 0.3445
1181/1500 [======================>.......] - ETA: 4:06 - loss: 2.1700 - regression_loss: 1.8255 - classification_loss: 0.3445
1182/1500 [======================>.......] - ETA: 4:06 - loss: 2.1705 - regression_loss: 1.8252 - classification_loss: 0.3453
1183/1500 [======================>.......] - ETA: 4:05 - loss: 2.1701 - regression_loss: 1.8249 - classification_loss: 0.3452
1184/1500 [======================>.......] - ETA: 4:04 - loss: 2.1697 - regression_loss: 1.8246 - classification_loss: 0.3451
1185/1500 [======================>.......] - ETA: 4:03 - loss: 2.1694 - regression_loss: 1.8245 - classification_loss: 0.3450
1186/1500 [======================>.......] - ETA: 4:03 - loss: 2.1694 - regression_loss: 1.8244 - classification_loss: 0.3451
1187/1500 [======================>.......] - ETA: 4:02 - loss: 2.1694 - regression_loss: 1.8243 - classification_loss: 0.3451
1188/1500 [======================>.......] - ETA: 4:01 - loss: 2.1699 - regression_loss: 1.8246 - classification_loss: 0.3453
1189/1500 [======================>.......] - ETA: 4:00 - loss: 2.1697 - regression_loss: 1.8244 - classification_loss: 0.3453
1190/1500 [======================>.......] - ETA: 3:59 - loss: 2.1696 - regression_loss: 1.8242 - classification_loss: 0.3454
1191/1500 [======================>.......] - ETA: 3:59 - loss: 2.1695 - regression_loss: 1.8241 - classification_loss: 0.3454
1192/1500 [======================>.......] - ETA: 3:58 - loss: 2.1701 - regression_loss: 1.8245 - classification_loss: 0.3456
1193/1500 [======================>.......] - ETA: 3:57 - loss: 2.1706 - regression_loss: 1.8249 - classification_loss: 0.3457
1194/1500 [======================>.......] - ETA: 3:56 - loss: 2.1706 - regression_loss: 1.8249 - classification_loss: 0.3457
1195/1500 [======================>.......] - ETA: 3:56 - loss: 2.1711 - regression_loss: 1.8252 - classification_loss: 0.3459
1196/1500 [======================>.......] - ETA: 3:55 - loss: 2.1718 - regression_loss: 1.8258 - classification_loss: 0.3460
1197/1500 [======================>.......] - ETA: 3:54 - loss: 2.1715 - regression_loss: 1.8255 - classification_loss: 0.3460
1198/1500 [======================>.......] - ETA: 3:53 - loss: 2.1708 - regression_loss: 1.8249 - classification_loss: 0.3459
1199/1500 [======================>.......] - ETA: 3:53 - loss: 2.1702 - regression_loss: 1.8243 - classification_loss: 0.3459
1200/1500 [=======================>......] - ETA: 3:52 - loss: 2.1698 - regression_loss: 1.8240 - classification_loss: 0.3459
1201/1500 [=======================>......] - ETA: 3:51 - loss: 2.1698 - regression_loss: 1.8239 - classification_loss: 0.3459
1202/1500 [=======================>......] - ETA: 3:50 - loss: 2.1697 - regression_loss: 1.8238 - classification_loss: 0.3459
1203/1500 [=======================>......] - ETA: 3:49 - loss: 2.1694 - regression_loss: 1.8236 - classification_loss: 0.3458
1204/1500 [=======================>......] - ETA: 3:49 - loss: 2.1699 - regression_loss: 1.8240 - classification_loss: 0.3459
1205/1500 [=======================>......] - ETA: 3:48 - loss: 2.1693 - regression_loss: 1.8236 - classification_loss: 0.3458
1206/1500 [=======================>......] - ETA: 3:47 - loss: 2.1692 - regression_loss: 1.8235 - classification_loss: 0.3458
1207/1500 [=======================>......] - ETA: 3:47 - loss: 2.1685 - regression_loss: 1.8229 - classification_loss: 0.3457
1208/1500 [=======================>......] - ETA: 3:46 - loss: 2.1683 - regression_loss: 1.8227 - classification_loss: 0.3456
1209/1500 [=======================>......] - ETA: 3:45 - loss: 2.1681 - regression_loss: 1.8226 - classification_loss: 0.3455
1210/1500 [=======================>......] - ETA: 3:45 - loss: 2.1681 - regression_loss: 1.8227 - classification_loss: 0.3455
1211/1500 [=======================>......] - ETA: 3:44 - loss: 2.1679 - regression_loss: 1.8224 - classification_loss: 0.3455
1212/1500 [=======================>......] - ETA: 3:43 - loss: 2.1685 - regression_loss: 1.8229 - classification_loss: 0.3457
1213/1500 [=======================>......] - ETA: 3:42 - loss: 2.1675 - regression_loss: 1.8220 - classification_loss: 0.3455
1214/1500 [=======================>......] - ETA: 3:41 - loss: 2.1676 - regression_loss: 1.8222 - classification_loss: 0.3455
1215/1500 [=======================>......] - ETA: 3:41 - loss: 2.1678 - regression_loss: 1.8223 - classification_loss: 0.3455
1216/1500 [=======================>......] - ETA: 3:40 - loss: 2.1674 - regression_loss: 1.8221 - classification_loss: 0.3454
1217/1500 [=======================>......] - ETA: 3:39 - loss: 2.1674 - regression_loss: 1.8221 - classification_loss: 0.3453
1218/1500 [=======================>......] - ETA: 3:38 - loss: 2.1669 - regression_loss: 1.8217 - classification_loss: 0.3452
1219/1500 [=======================>......] - ETA: 3:38 - loss: 2.1665 - regression_loss: 1.8214 - classification_loss: 0.3451
1220/1500 [=======================>......] - ETA: 3:37 - loss: 2.1662 - regression_loss: 1.8211 - classification_loss: 0.3451
1221/1500 [=======================>......] - ETA: 3:36 - loss: 2.1671 - regression_loss: 1.8219 - classification_loss: 0.3452
1222/1500 [=======================>......] - ETA: 3:35 - loss: 2.1678 - regression_loss: 1.8226 - classification_loss: 0.3452
1223/1500 [=======================>......] - ETA: 3:35 - loss: 2.1679 - regression_loss: 1.8226 - classification_loss: 0.3453
1224/1500 [=======================>......] - ETA: 3:34 - loss: 2.1683 - regression_loss: 1.8229 - classification_loss: 0.3455
1225/1500 [=======================>......] - ETA: 3:33 - loss: 2.1692 - regression_loss: 1.8235 - classification_loss: 0.3457
1226/1500 [=======================>......] - ETA: 3:32 - loss: 2.1691 - regression_loss: 1.8234 - classification_loss: 0.3457
1227/1500 [=======================>......] - ETA: 3:32 - loss: 2.1685 - regression_loss: 1.8230 - classification_loss: 0.3455
1228/1500 [=======================>......] - ETA: 3:31 - loss: 2.1677 - regression_loss: 1.8223 - classification_loss: 0.3454
1229/1500 [=======================>......] - ETA: 3:30 - loss: 2.1673 - regression_loss: 1.8220 - classification_loss: 0.3452
1230/1500 [=======================>......] - ETA: 3:29 - loss: 2.1671 - regression_loss: 1.8219 - classification_loss: 0.3452
1231/1500 [=======================>......] - ETA: 3:29 - loss: 2.1678 - regression_loss: 1.8224 - classification_loss: 0.3454
1232/1500 [=======================>......] - ETA: 3:28 - loss: 2.1676 - regression_loss: 1.8223 - classification_loss: 0.3453
1233/1500 [=======================>......] - ETA: 3:27 - loss: 2.1671 - regression_loss: 1.8219 - classification_loss: 0.3452
1234/1500 [=======================>......] - ETA: 3:26 - loss: 2.1661 - regression_loss: 1.8210 - classification_loss: 0.3450
1235/1500 [=======================>......] - ETA: 3:25 - loss: 2.1661 - regression_loss: 1.8211 - classification_loss: 0.3450
1236/1500 [=======================>......] - ETA: 3:25 - loss: 2.1667 - regression_loss: 1.8215 - classification_loss: 0.3451
1237/1500 [=======================>......] - ETA: 3:24 - loss: 2.1668 - regression_loss: 1.8214 - classification_loss: 0.3454
1238/1500 [=======================>......] - ETA: 3:23 - loss: 2.1672 - regression_loss: 1.8218 - classification_loss: 0.3455
1239/1500 [=======================>......] - ETA: 3:22 - loss: 2.1669 - regression_loss: 1.8216 - classification_loss: 0.3454
1240/1500 [=======================>......] - ETA: 3:22 - loss: 2.1662 - regression_loss: 1.8210 - classification_loss: 0.3452
1241/1500 [=======================>......] - ETA: 3:21 - loss: 2.1674 - regression_loss: 1.8219 - classification_loss: 0.3456
1242/1500 [=======================>......] - ETA: 3:20 - loss: 2.1678 - regression_loss: 1.8222 - classification_loss: 0.3456
1243/1500 [=======================>......] - ETA: 3:19 - loss: 2.1680 - regression_loss: 1.8224 - classification_loss: 0.3456
1244/1500 [=======================>......] - ETA: 3:18 - loss: 2.1679 - regression_loss: 1.8223 - classification_loss: 0.3456
1245/1500 [=======================>......] - ETA: 3:17 - loss: 2.1685 - regression_loss: 1.8228 - classification_loss: 0.3457
1246/1500 [=======================>......] - ETA: 3:17 - loss: 2.1678 - regression_loss: 1.8223 - classification_loss: 0.3456
1247/1500 [=======================>......] - ETA: 3:16 - loss: 2.1674 - regression_loss: 1.8219 - classification_loss: 0.3455
1248/1500 [=======================>......] - ETA: 3:15 - loss: 2.1673 - regression_loss: 1.8217 - classification_loss: 0.3456
1249/1500 [=======================>......] - ETA: 3:14 - loss: 2.1674 - regression_loss: 1.8217 - classification_loss: 0.3457
1250/1500 [========================>.....] - ETA: 3:13 - loss: 2.1668 - regression_loss: 1.8212 - classification_loss: 0.3456
1251/1500 [========================>.....] - ETA: 3:13 - loss: 2.1672 - regression_loss: 1.8215 - classification_loss: 0.3457
1252/1500 [========================>.....] - ETA: 3:12 - loss: 2.1669 - regression_loss: 1.8214 - classification_loss: 0.3455
1253/1500 [========================>.....] - ETA: 3:11 - loss: 2.1669 - regression_loss: 1.8214 - classification_loss: 0.3455
1254/1500 [========================>.....] - ETA: 3:10 - loss: 2.1673 - regression_loss: 1.8217 - classification_loss: 0.3455
1255/1500 [========================>.....] - ETA: 3:09 - loss: 2.1675 - regression_loss: 1.8219 - classification_loss: 0.3456
1256/1500 [========================>.....] - ETA: 3:09 - loss: 2.1674 - regression_loss: 1.8217 - classification_loss: 0.3456
1257/1500 [========================>.....] - ETA: 3:08 - loss: 2.1683 - regression_loss: 1.8220 - classification_loss: 0.3463
1258/1500 [========================>.....] - ETA: 3:07 - loss: 2.1675 - regression_loss: 1.8214 - classification_loss: 0.3461
1259/1500 [========================>.....] - ETA: 3:06 - loss: 2.1671 - regression_loss: 1.8211 - classification_loss: 0.3460
1260/1500 [========================>.....] - ETA: 3:06 - loss: 2.1675 - regression_loss: 1.8213 - classification_loss: 0.3461
1261/1500 [========================>.....] - ETA: 3:05 - loss: 2.1673 - regression_loss: 1.8213 - classification_loss: 0.3460
1262/1500 [========================>.....] - ETA: 3:04 - loss: 2.1673 - regression_loss: 1.8212 - classification_loss: 0.3461
1263/1500 [========================>.....] - ETA: 3:03 - loss: 2.1669 - regression_loss: 1.8210 - classification_loss: 0.3459
1264/1500 [========================>.....] - ETA: 3:02 - loss: 2.1669 - regression_loss: 1.8210 - classification_loss: 0.3459
1265/1500 [========================>.....] - ETA: 3:02 - loss: 2.1667 - regression_loss: 1.8208 - classification_loss: 0.3459
1266/1500 [========================>.....] - ETA: 3:01 - loss: 2.1666 - regression_loss: 1.8208 - classification_loss: 0.3458
1267/1500 [========================>.....] - ETA: 3:00 - loss: 2.1661 - regression_loss: 1.8204 - classification_loss: 0.3457
1268/1500 [========================>.....] - ETA: 2:59 - loss: 2.1663 - regression_loss: 1.8205 - classification_loss: 0.3458
1269/1500 [========================>.....] - ETA: 2:58 - loss: 2.1656 - regression_loss: 1.8200 - classification_loss: 0.3457
1270/1500 [========================>.....] - ETA: 2:58 - loss: 2.1652 - regression_loss: 1.8197 - classification_loss: 0.3455
1271/1500 [========================>.....] - ETA: 2:57 - loss: 2.1655 - regression_loss: 1.8198 - classification_loss: 0.3456
1272/1500 [========================>.....] - ETA: 2:56 - loss: 2.1646 - regression_loss: 1.8191 - classification_loss: 0.3455
1273/1500 [========================>.....] - ETA: 2:55 - loss: 2.1639 - regression_loss: 1.8186 - classification_loss: 0.3454
1274/1500 [========================>.....] - ETA: 2:55 - loss: 2.1632 - regression_loss: 1.8180 - classification_loss: 0.3452
1275/1500 [========================>.....] - ETA: 2:54 - loss: 2.1635 - regression_loss: 1.8183 - classification_loss: 0.3452
1276/1500 [========================>.....] - ETA: 2:53 - loss: 2.1639 - regression_loss: 1.8188 - classification_loss: 0.3452
1277/1500 [========================>.....] - ETA: 2:52 - loss: 2.1638 - regression_loss: 1.8186 - classification_loss: 0.3452
1278/1500 [========================>.....] - ETA: 2:52 - loss: 2.1644 - regression_loss: 1.8192 - classification_loss: 0.3452
1279/1500 [========================>.....] - ETA: 2:51 - loss: 2.1644 - regression_loss: 1.8191 - classification_loss: 0.3454
1280/1500 [========================>.....] - ETA: 2:50 - loss: 2.1644 - regression_loss: 1.8187 - classification_loss: 0.3457
1281/1500 [========================>.....] - ETA: 2:49 - loss: 2.1634 - regression_loss: 1.8177 - classification_loss: 0.3456
1282/1500 [========================>.....] - ETA: 2:48 - loss: 2.1636 - regression_loss: 1.8180 - classification_loss: 0.3457
1283/1500 [========================>.....] - ETA: 2:48 - loss: 2.1627 - regression_loss: 1.8172 - classification_loss: 0.3455
1284/1500 [========================>.....] - ETA: 2:47 - loss: 2.1617 - regression_loss: 1.8164 - classification_loss: 0.3453
1285/1500 [========================>.....] - ETA: 2:46 - loss: 2.1607 - regression_loss: 1.8156 - classification_loss: 0.3451
1286/1500 [========================>.....] - ETA: 2:45 - loss: 2.1604 - regression_loss: 1.8153 - classification_loss: 0.3450
1287/1500 [========================>.....] - ETA: 2:44 - loss: 2.1598 - regression_loss: 1.8149 - classification_loss: 0.3449
1288/1500 [========================>.....] - ETA: 2:44 - loss: 2.1587 - regression_loss: 1.8140 - classification_loss: 0.3447
1289/1500 [========================>.....] - ETA: 2:43 - loss: 2.1579 - regression_loss: 1.8133 - classification_loss: 0.3445
1290/1500 [========================>.....] - ETA: 2:42 - loss: 2.1571 - regression_loss: 1.8128 - classification_loss: 0.3444
1291/1500 [========================>.....] - ETA: 2:42 - loss: 2.1581 - regression_loss: 1.8138 - classification_loss: 0.3443
1292/1500 [========================>.....] - ETA: 2:41 - loss: 2.1586 - regression_loss: 1.8142 - classification_loss: 0.3445
1293/1500 [========================>.....] - ETA: 2:40 - loss: 2.1590 - regression_loss: 1.8145 - classification_loss: 0.3445
1294/1500 [========================>.....] - ETA: 2:39 - loss: 2.1582 - regression_loss: 1.8139 - classification_loss: 0.3443
1295/1500 [========================>.....] - ETA: 2:38 - loss: 2.1582 - regression_loss: 1.8138 - classification_loss: 0.3444
1296/1500 [========================>.....] - ETA: 2:37 - loss: 2.1579 - regression_loss: 1.8136 - classification_loss: 0.3443
1297/1500 [========================>.....] - ETA: 2:37 - loss: 2.1572 - regression_loss: 1.8131 - classification_loss: 0.3441
1298/1500 [========================>.....] - ETA: 2:36 - loss: 2.1575 - regression_loss: 1.8132 - classification_loss: 0.3443
1299/1500 [========================>.....] - ETA: 2:35 - loss: 2.1569 - regression_loss: 1.8127 - classification_loss: 0.3442
1300/1500 [=========================>....] - ETA: 2:34 - loss: 2.1573 - regression_loss: 1.8130 - classification_loss: 0.3443
1301/1500 [=========================>....] - ETA: 2:34 - loss: 2.1576 - regression_loss: 1.8132 - classification_loss: 0.3444
1302/1500 [=========================>....] - ETA: 2:33 - loss: 2.1580 - regression_loss: 1.8136 - classification_loss: 0.3444
1303/1500 [=========================>....] - ETA: 2:32 - loss: 2.1579 - regression_loss: 1.8135 - classification_loss: 0.3444
1304/1500 [=========================>....] - ETA: 2:31 - loss: 2.1584 - regression_loss: 1.8139 - classification_loss: 0.3445
1305/1500 [=========================>....] - ETA: 2:30 - loss: 2.1590 - regression_loss: 1.8143 - classification_loss: 0.3447
1306/1500 [=========================>....] - ETA: 2:30 - loss: 2.1593 - regression_loss: 1.8147 - classification_loss: 0.3446
1307/1500 [=========================>....] - ETA: 2:29 - loss: 2.1598 - regression_loss: 1.8150 - classification_loss: 0.3448
1308/1500 [=========================>....] - ETA: 2:28 - loss: 2.1601 - regression_loss: 1.8152 - classification_loss: 0.3449
1309/1500 [=========================>....] - ETA: 2:27 - loss: 2.1603 - regression_loss: 1.8153 - classification_loss: 0.3450
1310/1500 [=========================>....] - ETA: 2:26 - loss: 2.1599 - regression_loss: 1.8151 - classification_loss: 0.3448
1311/1500 [=========================>....] - ETA: 2:26 - loss: 2.1600 - regression_loss: 1.8151 - classification_loss: 0.3449
1312/1500 [=========================>....] - ETA: 2:25 - loss: 2.1602 - regression_loss: 1.8153 - classification_loss: 0.3449
1313/1500 [=========================>....] - ETA: 2:24 - loss: 2.1602 - regression_loss: 1.8152 - classification_loss: 0.3449
1314/1500 [=========================>....] - ETA: 2:23 - loss: 2.1608 - regression_loss: 1.8158 - classification_loss: 0.3449
1315/1500 [=========================>....] - ETA: 2:23 - loss: 2.1607 - regression_loss: 1.8158 - classification_loss: 0.3449
1316/1500 [=========================>....] - ETA: 2:22 - loss: 2.1603 - regression_loss: 1.8155 - classification_loss: 0.3448
1317/1500 [=========================>....] - ETA: 2:21 - loss: 2.1600 - regression_loss: 1.8153 - classification_loss: 0.3447
1318/1500 [=========================>....] - ETA: 2:20 - loss: 2.1603 - regression_loss: 1.8155 - classification_loss: 0.3448
1319/1500 [=========================>....] - ETA: 2:20 - loss: 2.1600 - regression_loss: 1.8153 - classification_loss: 0.3447
1320/1500 [=========================>....] - ETA: 2:19 - loss: 2.1593 - regression_loss: 1.8147 - classification_loss: 0.3445
1321/1500 [=========================>....] - ETA: 2:18 - loss: 2.1587 - regression_loss: 1.8143 - classification_loss: 0.3444
1322/1500 [=========================>....] - ETA: 2:17 - loss: 2.1583 - regression_loss: 1.8140 - classification_loss: 0.3442
1323/1500 [=========================>....] - ETA: 2:16 - loss: 2.1577 - regression_loss: 1.8136 - classification_loss: 0.3442
1324/1500 [=========================>....] - ETA: 2:16 - loss: 2.1572 - regression_loss: 1.8132 - classification_loss: 0.3440
1325/1500 [=========================>....] - ETA: 2:15 - loss: 2.1570 - regression_loss: 1.8131 - classification_loss: 0.3439
1326/1500 [=========================>....] - ETA: 2:14 - loss: 2.1571 - regression_loss: 1.8132 - classification_loss: 0.3439
1327/1500 [=========================>....] - ETA: 2:13 - loss: 2.1568 - regression_loss: 1.8131 - classification_loss: 0.3437
1328/1500 [=========================>....] - ETA: 2:12 - loss: 2.1566 - regression_loss: 1.8129 - classification_loss: 0.3437
1329/1500 [=========================>....] - ETA: 2:12 - loss: 2.1566 - regression_loss: 1.8128 - classification_loss: 0.3438
1330/1500 [=========================>....] - ETA: 2:11 - loss: 2.1563 - regression_loss: 1.8125 - classification_loss: 0.3439
1331/1500 [=========================>....] - ETA: 2:10 - loss: 2.1568 - regression_loss: 1.8126 - classification_loss: 0.3442
1332/1500 [=========================>....] - ETA: 2:09 - loss: 2.1572 - regression_loss: 1.8128 - classification_loss: 0.3444
1333/1500 [=========================>....] - ETA: 2:09 - loss: 2.1573 - regression_loss: 1.8130 - classification_loss: 0.3444
1334/1500 [=========================>....] - ETA: 2:08 - loss: 2.1571 - regression_loss: 1.8128 - classification_loss: 0.3443
1335/1500 [=========================>....] - ETA: 2:07 - loss: 2.1569 - regression_loss: 1.8126 - classification_loss: 0.3443
1336/1500 [=========================>....] - ETA: 2:07 - loss: 2.1576 - regression_loss: 1.8132 - classification_loss: 0.3444
1337/1500 [=========================>....] - ETA: 2:06 - loss: 2.1581 - regression_loss: 1.8135 - classification_loss: 0.3445
1338/1500 [=========================>....] - ETA: 2:05 - loss: 2.1583 - regression_loss: 1.8138 - classification_loss: 0.3445
1339/1500 [=========================>....] - ETA: 2:04 - loss: 2.1585 - regression_loss: 1.8140 - classification_loss: 0.3445
1340/1500 [=========================>....] - ETA: 2:03 - loss: 2.1589 - regression_loss: 1.8142 - classification_loss: 0.3447
1341/1500 [=========================>....] - ETA: 2:03 - loss: 2.1582 - regression_loss: 1.8136 - classification_loss: 0.3446
1342/1500 [=========================>....] - ETA: 2:02 - loss: 2.1576 - regression_loss: 1.8132 - classification_loss: 0.3444
1343/1500 [=========================>....] - ETA: 2:01 - loss: 2.1576 - regression_loss: 1.8134 - classification_loss: 0.3443
1344/1500 [=========================>....] - ETA: 2:00 - loss: 2.1581 - regression_loss: 1.8138 - classification_loss: 0.3444
1345/1500 [=========================>....] - ETA: 2:00 - loss: 2.1575 - regression_loss: 1.8132 - classification_loss: 0.3442
1346/1500 [=========================>....] - ETA: 1:59 - loss: 2.1580 - regression_loss: 1.8136 - classification_loss: 0.3444
1347/1500 [=========================>....] - ETA: 1:58 - loss: 2.1577 - regression_loss: 1.8134 - classification_loss: 0.3443
1348/1500 [=========================>....] - ETA: 1:57 - loss: 2.1579 - regression_loss: 1.8134 - classification_loss: 0.3445
1349/1500 [=========================>....] - ETA: 1:56 - loss: 2.1576 - regression_loss: 1.8132 - classification_loss: 0.3444
1350/1500 [==========================>...] - ETA: 1:56 - loss: 2.1569 - regression_loss: 1.8126 - classification_loss: 0.3442
1351/1500 [==========================>...] - ETA: 1:55 - loss: 2.1569 - regression_loss: 1.8128 - classification_loss: 0.3442
1352/1500 [==========================>...] - ETA: 1:54 - loss: 2.1562 - regression_loss: 1.8122 - classification_loss: 0.3440
1353/1500 [==========================>...] - ETA: 1:53 - loss: 2.1556 - regression_loss: 1.8117 - classification_loss: 0.3439
1354/1500 [==========================>...] - ETA: 1:52 - loss: 2.1555 - regression_loss: 1.8116 - classification_loss: 0.3439
1355/1500 [==========================>...] - ETA: 1:52 - loss: 2.1545 - regression_loss: 1.8108 - classification_loss: 0.3438
1356/1500 [==========================>...] - ETA: 1:51 - loss: 2.1547 - regression_loss: 1.8109 - classification_loss: 0.3439
1357/1500 [==========================>...] - ETA: 1:50 - loss: 2.1545 - regression_loss: 1.8108 - classification_loss: 0.3437
1358/1500 [==========================>...] - ETA: 1:50 - loss: 2.1538 - regression_loss: 1.8102 - classification_loss: 0.3436
1359/1500 [==========================>...] - ETA: 1:49 - loss: 2.1531 - regression_loss: 1.8096 - classification_loss: 0.3435
1360/1500 [==========================>...] - ETA: 1:48 - loss: 2.1531 - regression_loss: 1.8096 - classification_loss: 0.3435
1361/1500 [==========================>...] - ETA: 1:47 - loss: 2.1539 - regression_loss: 1.8103 - classification_loss: 0.3437
1362/1500 [==========================>...] - ETA: 1:46 - loss: 2.1544 - regression_loss: 1.8105 - classification_loss: 0.3439
1363/1500 [==========================>...] - ETA: 1:46 - loss: 2.1541 - regression_loss: 1.8103 - classification_loss: 0.3438
1364/1500 [==========================>...] - ETA: 1:45 - loss: 2.1542 - regression_loss: 1.8102 - classification_loss: 0.3439
1365/1500 [==========================>...] - ETA: 1:44 - loss: 2.1533 - regression_loss: 1.8095 - classification_loss: 0.3438
1366/1500 [==========================>...] - ETA: 1:43 - loss: 2.1545 - regression_loss: 1.8101 - classification_loss: 0.3444
1367/1500 [==========================>...] - ETA: 1:43 - loss: 2.1542 - regression_loss: 1.8099 - classification_loss: 0.3443
1368/1500 [==========================>...] - ETA: 1:42 - loss: 2.1539 - regression_loss: 1.8097 - classification_loss: 0.3443
1369/1500 [==========================>...] - ETA: 1:41 - loss: 2.1539 - regression_loss: 1.8097 - classification_loss: 0.3442
1370/1500 [==========================>...] - ETA: 1:40 - loss: 2.1533 - regression_loss: 1.8092 - classification_loss: 0.3441
1371/1500 [==========================>...] - ETA: 1:40 - loss: 2.1535 - regression_loss: 1.8093 - classification_loss: 0.3441
1372/1500 [==========================>...] - ETA: 1:39 - loss: 2.1526 - regression_loss: 1.8086 - classification_loss: 0.3440
1373/1500 [==========================>...] - ETA: 1:38 - loss: 2.1528 - regression_loss: 1.8088 - classification_loss: 0.3440
1374/1500 [==========================>...] - ETA: 1:37 - loss: 2.1531 - regression_loss: 1.8091 - classification_loss: 0.3440
1375/1500 [==========================>...] - ETA: 1:36 - loss: 2.1525 - regression_loss: 1.8085 - classification_loss: 0.3440
1376/1500 [==========================>...] - ETA: 1:36 - loss: 2.1525 - regression_loss: 1.8086 - classification_loss: 0.3439
1377/1500 [==========================>...] - ETA: 1:35 - loss: 2.1520 - regression_loss: 1.8082 - classification_loss: 0.3438
1378/1500 [==========================>...] - ETA: 1:34 - loss: 2.1511 - regression_loss: 1.8075 - classification_loss: 0.3436
1379/1500 [==========================>...] - ETA: 1:33 - loss: 2.1509 - regression_loss: 1.8072 - classification_loss: 0.3437
1380/1500 [==========================>...] - ETA: 1:33 - loss: 2.1502 - regression_loss: 1.8067 - classification_loss: 0.3435
1381/1500 [==========================>...] - ETA: 1:32 - loss: 2.1495 - regression_loss: 1.8061 - classification_loss: 0.3434
1382/1500 [==========================>...] - ETA: 1:31 - loss: 2.1488 - regression_loss: 1.8055 - classification_loss: 0.3433
1383/1500 [==========================>...] - ETA: 1:30 - loss: 2.1482 - regression_loss: 1.8051 - classification_loss: 0.3431
1384/1500 [==========================>...] - ETA: 1:29 - loss: 2.1486 - regression_loss: 1.8054 - classification_loss: 0.3432
1385/1500 [==========================>...] - ETA: 1:29 - loss: 2.1485 - regression_loss: 1.8053 - classification_loss: 0.3432
1386/1500 [==========================>...] - ETA: 1:28 - loss: 2.1485 - regression_loss: 1.8053 - classification_loss: 0.3432
1387/1500 [==========================>...] - ETA: 1:27 - loss: 2.1484 - regression_loss: 1.8054 - classification_loss: 0.3431
1388/1500 [==========================>...] - ETA: 1:26 - loss: 2.1483 - regression_loss: 1.8053 - classification_loss: 0.3430
1389/1500 [==========================>...] - ETA: 1:25 - loss: 2.1483 - regression_loss: 1.8052 - classification_loss: 0.3431
1390/1500 [==========================>...] - ETA: 1:25 - loss: 2.1488 - regression_loss: 1.8056 - classification_loss: 0.3432
1391/1500 [==========================>...] - ETA: 1:24 - loss: 2.1480 - regression_loss: 1.8049 - classification_loss: 0.3431
1392/1500 [==========================>...] - ETA: 1:23 - loss: 2.1478 - regression_loss: 1.8047 - classification_loss: 0.3432
1393/1500 [==========================>...] - ETA: 1:22 - loss: 2.1476 - regression_loss: 1.8045 - classification_loss: 0.3431
1394/1500 [==========================>...] - ETA: 1:22 - loss: 2.1485 - regression_loss: 1.8052 - classification_loss: 0.3432
1395/1500 [==========================>...] - ETA: 1:21 - loss: 2.1483 - regression_loss: 1.8052 - classification_loss: 0.3431
1396/1500 [==========================>...] - ETA: 1:20 - loss: 2.1476 - regression_loss: 1.8046 - classification_loss: 0.3430
1397/1500 [==========================>...] - ETA: 1:19 - loss: 2.1480 - regression_loss: 1.8048 - classification_loss: 0.3431
1398/1500 [==========================>...] - ETA: 1:19 - loss: 2.1479 - regression_loss: 1.8048 - classification_loss: 0.3431
1399/1500 [==========================>...] - ETA: 1:18 - loss: 2.1489 - regression_loss: 1.8056 - classification_loss: 0.3433
1400/1500 [===========================>..] - ETA: 1:17 - loss: 2.1489 - regression_loss: 1.8056 - classification_loss: 0.3434
1401/1500 [===========================>..] - ETA: 1:16 - loss: 2.1491 - regression_loss: 1.8057 - classification_loss: 0.3434
1402/1500 [===========================>..] - ETA: 1:15 - loss: 2.1491 - regression_loss: 1.8058 - classification_loss: 0.3433
1403/1500 [===========================>..] - ETA: 1:15 - loss: 2.1490 - regression_loss: 1.8057 - classification_loss: 0.3433
1404/1500 [===========================>..] - ETA: 1:14 - loss: 2.1488 - regression_loss: 1.8055 - classification_loss: 0.3433
1405/1500 [===========================>..] - ETA: 1:13 - loss: 2.1482 - regression_loss: 1.8051 - classification_loss: 0.3431
1406/1500 [===========================>..] - ETA: 1:12 - loss: 2.1476 - regression_loss: 1.8046 - classification_loss: 0.3430
1407/1500 [===========================>..] - ETA: 1:11 - loss: 2.1475 - regression_loss: 1.8045 - classification_loss: 0.3430
1408/1500 [===========================>..] - ETA: 1:11 - loss: 2.1477 - regression_loss: 1.8046 - classification_loss: 0.3431
1409/1500 [===========================>..] - ETA: 1:10 - loss: 2.1478 - regression_loss: 1.8048 - classification_loss: 0.3430
1410/1500 [===========================>..] - ETA: 1:09 - loss: 2.1474 - regression_loss: 1.8044 - classification_loss: 0.3430
1411/1500 [===========================>..] - ETA: 1:08 - loss: 2.1471 - regression_loss: 1.8041 - classification_loss: 0.3430
1412/1500 [===========================>..] - ETA: 1:07 - loss: 2.1469 - regression_loss: 1.8039 - classification_loss: 0.3430
1413/1500 [===========================>..] - ETA: 1:07 - loss: 2.1464 - regression_loss: 1.8036 - classification_loss: 0.3429
1414/1500 [===========================>..] - ETA: 1:06 - loss: 2.1459 - regression_loss: 1.8030 - classification_loss: 0.3428
1415/1500 [===========================>..] - ETA: 1:05 - loss: 2.1455 - regression_loss: 1.8027 - classification_loss: 0.3429
1416/1500 [===========================>..] - ETA: 1:04 - loss: 2.1456 - regression_loss: 1.8028 - classification_loss: 0.3428
1417/1500 [===========================>..] - ETA: 1:03 - loss: 2.1448 - regression_loss: 1.8022 - classification_loss: 0.3426
1418/1500 [===========================>..] - ETA: 1:03 - loss: 2.1449 - regression_loss: 1.8022 - classification_loss: 0.3427
1419/1500 [===========================>..] - ETA: 1:02 - loss: 2.1448 - regression_loss: 1.8020 - classification_loss: 0.3427
1420/1500 [===========================>..] - ETA: 1:01 - loss: 2.1440 - regression_loss: 1.8014 - classification_loss: 0.3426
1421/1500 [===========================>..] - ETA: 1:00 - loss: 2.1437 - regression_loss: 1.8010 - classification_loss: 0.3427
1422/1500 [===========================>..] - ETA: 1:00 - loss: 2.1438 - regression_loss: 1.8010 - classification_loss: 0.3428
1423/1500 [===========================>..] - ETA: 59s - loss: 2.1445 - regression_loss: 1.8016 - classification_loss: 0.3429 
1424/1500 [===========================>..] - ETA: 58s - loss: 2.1442 - regression_loss: 1.8013 - classification_loss: 0.3430
1425/1500 [===========================>..] - ETA: 57s - loss: 2.1446 - regression_loss: 1.8016 - classification_loss: 0.3430
1426/1500 [===========================>..] - ETA: 57s - loss: 2.1448 - regression_loss: 1.8016 - classification_loss: 0.3432
1427/1500 [===========================>..] - ETA: 56s - loss: 2.1444 - regression_loss: 1.8013 - classification_loss: 0.3431
1428/1500 [===========================>..] - ETA: 55s - loss: 2.1446 - regression_loss: 1.8015 - classification_loss: 0.3432
1429/1500 [===========================>..] - ETA: 54s - loss: 2.1444 - regression_loss: 1.8013 - classification_loss: 0.3431
1430/1500 [===========================>..] - ETA: 54s - loss: 2.1441 - regression_loss: 1.8011 - classification_loss: 0.3431
1431/1500 [===========================>..] - ETA: 53s - loss: 2.1436 - regression_loss: 1.8005 - classification_loss: 0.3431
1432/1500 [===========================>..] - ETA: 52s - loss: 2.1441 - regression_loss: 1.8008 - classification_loss: 0.3433
1433/1500 [===========================>..] - ETA: 51s - loss: 2.1445 - regression_loss: 1.8012 - classification_loss: 0.3433
1434/1500 [===========================>..] - ETA: 51s - loss: 2.1449 - regression_loss: 1.8014 - classification_loss: 0.3435
1435/1500 [===========================>..] - ETA: 50s - loss: 2.1448 - regression_loss: 1.8012 - classification_loss: 0.3436
1436/1500 [===========================>..] - ETA: 49s - loss: 2.1454 - regression_loss: 1.8019 - classification_loss: 0.3436
1437/1500 [===========================>..] - ETA: 48s - loss: 2.1451 - regression_loss: 1.8016 - classification_loss: 0.3435
1438/1500 [===========================>..] - ETA: 48s - loss: 2.1452 - regression_loss: 1.8015 - classification_loss: 0.3436
1439/1500 [===========================>..] - ETA: 47s - loss: 2.1452 - regression_loss: 1.8017 - classification_loss: 0.3435
1440/1500 [===========================>..] - ETA: 46s - loss: 2.1459 - regression_loss: 1.8023 - classification_loss: 0.3435
1441/1500 [===========================>..] - ETA: 45s - loss: 2.1454 - regression_loss: 1.8019 - classification_loss: 0.3435
1442/1500 [===========================>..] - ETA: 44s - loss: 2.1454 - regression_loss: 1.8020 - classification_loss: 0.3435
1443/1500 [===========================>..] - ETA: 44s - loss: 2.1455 - regression_loss: 1.8015 - classification_loss: 0.3440
1444/1500 [===========================>..] - ETA: 43s - loss: 2.1462 - regression_loss: 1.8020 - classification_loss: 0.3442
1445/1500 [===========================>..] - ETA: 42s - loss: 2.1469 - regression_loss: 1.8025 - classification_loss: 0.3444
1446/1500 [===========================>..] - ETA: 41s - loss: 2.1465 - regression_loss: 1.8022 - classification_loss: 0.3443
1447/1500 [===========================>..] - ETA: 41s - loss: 2.1464 - regression_loss: 1.8020 - classification_loss: 0.3444
1448/1500 [===========================>..] - ETA: 40s - loss: 2.1462 - regression_loss: 1.8020 - classification_loss: 0.3443
1449/1500 [===========================>..] - ETA: 39s - loss: 2.1463 - regression_loss: 1.8021 - classification_loss: 0.3442
1450/1500 [============================>.] - ETA: 38s - loss: 2.1456 - regression_loss: 1.8016 - classification_loss: 0.3441
1451/1500 [============================>.] - ETA: 37s - loss: 2.1455 - regression_loss: 1.8014 - classification_loss: 0.3441
1452/1500 [============================>.] - ETA: 37s - loss: 2.1454 - regression_loss: 1.8012 - classification_loss: 0.3442
1453/1500 [============================>.] - ETA: 36s - loss: 2.1447 - regression_loss: 1.8005 - classification_loss: 0.3441
1454/1500 [============================>.] - ETA: 35s - loss: 2.1445 - regression_loss: 1.8004 - classification_loss: 0.3441
1455/1500 [============================>.] - ETA: 34s - loss: 2.1444 - regression_loss: 1.8004 - classification_loss: 0.3440
1456/1500 [============================>.] - ETA: 34s - loss: 2.1439 - regression_loss: 1.8000 - classification_loss: 0.3439
1457/1500 [============================>.] - ETA: 33s - loss: 2.1442 - regression_loss: 1.8002 - classification_loss: 0.3440
1458/1500 [============================>.] - ETA: 32s - loss: 2.1445 - regression_loss: 1.8004 - classification_loss: 0.3441
1459/1500 [============================>.] - ETA: 31s - loss: 2.1449 - regression_loss: 1.8006 - classification_loss: 0.3443
1460/1500 [============================>.] - ETA: 30s - loss: 2.1443 - regression_loss: 1.8001 - classification_loss: 0.3442
1461/1500 [============================>.] - ETA: 30s - loss: 2.1443 - regression_loss: 1.8000 - classification_loss: 0.3443
1462/1500 [============================>.] - ETA: 29s - loss: 2.1439 - regression_loss: 1.7997 - classification_loss: 0.3442
1463/1500 [============================>.] - ETA: 28s - loss: 2.1446 - regression_loss: 1.8002 - classification_loss: 0.3443
1464/1500 [============================>.] - ETA: 27s - loss: 2.1444 - regression_loss: 1.8000 - classification_loss: 0.3443
1465/1500 [============================>.] - ETA: 27s - loss: 2.1436 - regression_loss: 1.7994 - classification_loss: 0.3442
1466/1500 [============================>.] - ETA: 26s - loss: 2.1433 - regression_loss: 1.7991 - classification_loss: 0.3441
1467/1500 [============================>.] - ETA: 25s - loss: 2.1429 - regression_loss: 1.7989 - classification_loss: 0.3440
1468/1500 [============================>.] - ETA: 24s - loss: 2.1422 - regression_loss: 1.7983 - classification_loss: 0.3439
1469/1500 [============================>.] - ETA: 24s - loss: 2.1423 - regression_loss: 1.7984 - classification_loss: 0.3439
1470/1500 [============================>.] - ETA: 23s - loss: 2.1421 - regression_loss: 1.7982 - classification_loss: 0.3439
1471/1500 [============================>.] - ETA: 22s - loss: 2.1425 - regression_loss: 1.7986 - classification_loss: 0.3438
1472/1500 [============================>.] - ETA: 21s - loss: 2.1424 - regression_loss: 1.7986 - classification_loss: 0.3438
1473/1500 [============================>.] - ETA: 20s - loss: 2.1422 - regression_loss: 1.7985 - classification_loss: 0.3438
1474/1500 [============================>.] - ETA: 20s - loss: 2.1425 - regression_loss: 1.7988 - classification_loss: 0.3438
1475/1500 [============================>.] - ETA: 19s - loss: 2.1423 - regression_loss: 1.7987 - classification_loss: 0.3437
1476/1500 [============================>.] - ETA: 18s - loss: 2.1425 - regression_loss: 1.7989 - classification_loss: 0.3436
1477/1500 [============================>.] - ETA: 17s - loss: 2.1424 - regression_loss: 1.7989 - classification_loss: 0.3435
1478/1500 [============================>.] - ETA: 17s - loss: 2.1425 - regression_loss: 1.7991 - classification_loss: 0.3435
1479/1500 [============================>.] - ETA: 16s - loss: 2.1417 - regression_loss: 1.7984 - classification_loss: 0.3433
1480/1500 [============================>.] - ETA: 15s - loss: 2.1414 - regression_loss: 1.7981 - classification_loss: 0.3432
1481/1500 [============================>.] - ETA: 14s - loss: 2.1412 - regression_loss: 1.7981 - classification_loss: 0.3431
1482/1500 [============================>.] - ETA: 13s - loss: 2.1412 - regression_loss: 1.7981 - classification_loss: 0.3431
1483/1500 [============================>.] - ETA: 13s - loss: 2.1414 - regression_loss: 1.7984 - classification_loss: 0.3430
1484/1500 [============================>.] - ETA: 12s - loss: 2.1416 - regression_loss: 1.7986 - classification_loss: 0.3430
1485/1500 [============================>.] - ETA: 11s - loss: 2.1413 - regression_loss: 1.7984 - classification_loss: 0.3429
1486/1500 [============================>.] - ETA: 10s - loss: 2.1419 - regression_loss: 1.7988 - classification_loss: 0.3430
1487/1500 [============================>.] - ETA: 10s - loss: 2.1419 - regression_loss: 1.7989 - classification_loss: 0.3430
1488/1500 [============================>.] - ETA: 9s - loss: 2.1427 - regression_loss: 1.7997 - classification_loss: 0.3430 
1489/1500 [============================>.] - ETA: 8s - loss: 2.1428 - regression_loss: 1.7999 - classification_loss: 0.3429
1490/1500 [============================>.] - ETA: 7s - loss: 2.1426 - regression_loss: 1.7996 - classification_loss: 0.3430
1491/1500 [============================>.] - ETA: 6s - loss: 2.1425 - regression_loss: 1.7996 - classification_loss: 0.3429
1492/1500 [============================>.] - ETA: 6s - loss: 2.1427 - regression_loss: 1.7997 - classification_loss: 0.3430
1493/1500 [============================>.] - ETA: 5s - loss: 2.1430 - regression_loss: 1.7999 - classification_loss: 0.3431
1494/1500 [============================>.] - ETA: 4s - loss: 2.1429 - regression_loss: 1.7998 - classification_loss: 0.3431
1495/1500 [============================>.] - ETA: 3s - loss: 2.1427 - regression_loss: 1.7997 - classification_loss: 0.3431
1496/1500 [============================>.] - ETA: 3s - loss: 2.1427 - regression_loss: 1.7997 - classification_loss: 0.3430
1497/1500 [============================>.] - ETA: 2s - loss: 2.1427 - regression_loss: 1.7997 - classification_loss: 0.3430
1498/1500 [============================>.] - ETA: 1s - loss: 2.1434 - regression_loss: 1.7999 - classification_loss: 0.3435
1499/1500 [============================>.] - ETA: 0s - loss: 2.1437 - regression_loss: 1.8002 - classification_loss: 0.3434
1500/1500 [==============================] - 1160s 773ms/step - loss: 2.1442 - regression_loss: 1.8007 - classification_loss: 0.3435

Epoch 00002: saving model to ./snapshots/resnet50_csv_02.h5
Epoch 3/10

   1/1500 [..............................] - ETA: 10:17 - loss: 1.7497 - regression_loss: 1.5748 - classification_loss: 0.1749
   2/1500 [..............................] - ETA: 14:10 - loss: 2.1186 - regression_loss: 1.7625 - classification_loss: 0.3560
   3/1500 [..............................] - ETA: 16:29 - loss: 2.0608 - regression_loss: 1.7688 - classification_loss: 0.2919
   4/1500 [..............................] - ETA: 15:00 - loss: 1.8307 - regression_loss: 1.5614 - classification_loss: 0.2693
   5/1500 [..............................] - ETA: 14:18 - loss: 1.9385 - regression_loss: 1.6603 - classification_loss: 0.2782
   6/1500 [..............................] - ETA: 18:59 - loss: 2.0312 - regression_loss: 1.7420 - classification_loss: 0.2892
   7/1500 [..............................] - ETA: 24:50 - loss: 2.0666 - regression_loss: 1.7780 - classification_loss: 0.2887
   8/1500 [..............................] - ETA: 24:55 - loss: 2.0310 - regression_loss: 1.7482 - classification_loss: 0.2827
   9/1500 [..............................] - ETA: 23:55 - loss: 1.9642 - regression_loss: 1.6954 - classification_loss: 0.2688
  10/1500 [..............................] - ETA: 25:33 - loss: 1.9988 - regression_loss: 1.7233 - classification_loss: 0.2755
  11/1500 [..............................] - ETA: 25:06 - loss: 1.9432 - regression_loss: 1.6737 - classification_loss: 0.2695
  12/1500 [..............................] - ETA: 26:28 - loss: 1.9904 - regression_loss: 1.7189 - classification_loss: 0.2715
  13/1500 [..............................] - ETA: 26:18 - loss: 2.0224 - regression_loss: 1.7500 - classification_loss: 0.2724
  14/1500 [..............................] - ETA: 25:43 - loss: 2.0153 - regression_loss: 1.7423 - classification_loss: 0.2730
  15/1500 [..............................] - ETA: 25:49 - loss: 2.0564 - regression_loss: 1.7681 - classification_loss: 0.2884
  16/1500 [..............................] - ETA: 24:41 - loss: 2.0784 - regression_loss: 1.7753 - classification_loss: 0.3031
  17/1500 [..............................] - ETA: 24:25 - loss: 2.0338 - regression_loss: 1.7361 - classification_loss: 0.2977
  18/1500 [..............................] - ETA: 23:59 - loss: 2.0588 - regression_loss: 1.7561 - classification_loss: 0.3027
  19/1500 [..............................] - ETA: 23:32 - loss: 2.0768 - regression_loss: 1.7805 - classification_loss: 0.2963
  20/1500 [..............................] - ETA: 23:28 - loss: 2.1085 - regression_loss: 1.7982 - classification_loss: 0.3102
  21/1500 [..............................] - ETA: 24:17 - loss: 2.0990 - regression_loss: 1.7827 - classification_loss: 0.3163
  22/1500 [..............................] - ETA: 24:23 - loss: 2.0688 - regression_loss: 1.7485 - classification_loss: 0.3203
  23/1500 [..............................] - ETA: 24:39 - loss: 2.0842 - regression_loss: 1.7617 - classification_loss: 0.3225
  24/1500 [..............................] - ETA: 25:06 - loss: 2.0665 - regression_loss: 1.7497 - classification_loss: 0.3168
  25/1500 [..............................] - ETA: 24:29 - loss: 2.0736 - regression_loss: 1.7552 - classification_loss: 0.3185
  26/1500 [..............................] - ETA: 23:55 - loss: 2.0591 - regression_loss: 1.7305 - classification_loss: 0.3286
  27/1500 [..............................] - ETA: 23:44 - loss: 2.0347 - regression_loss: 1.7086 - classification_loss: 0.3260
  28/1500 [..............................] - ETA: 23:15 - loss: 2.0228 - regression_loss: 1.7016 - classification_loss: 0.3212
  29/1500 [..............................] - ETA: 22:46 - loss: 2.0250 - regression_loss: 1.6934 - classification_loss: 0.3316
  30/1500 [..............................] - ETA: 22:19 - loss: 1.9951 - regression_loss: 1.6701 - classification_loss: 0.3250
  31/1500 [..............................] - ETA: 21:56 - loss: 1.9859 - regression_loss: 1.6645 - classification_loss: 0.3213
  32/1500 [..............................] - ETA: 21:40 - loss: 1.9643 - regression_loss: 1.6490 - classification_loss: 0.3153
  33/1500 [..............................] - ETA: 21:28 - loss: 1.9512 - regression_loss: 1.6405 - classification_loss: 0.3107
  34/1500 [..............................] - ETA: 21:07 - loss: 1.9439 - regression_loss: 1.6317 - classification_loss: 0.3122
  35/1500 [..............................] - ETA: 21:10 - loss: 1.9542 - regression_loss: 1.6382 - classification_loss: 0.3160
  36/1500 [..............................] - ETA: 21:19 - loss: 1.9701 - regression_loss: 1.6489 - classification_loss: 0.3212
  37/1500 [..............................] - ETA: 21:06 - loss: 1.9598 - regression_loss: 1.6428 - classification_loss: 0.3171
  38/1500 [..............................] - ETA: 20:48 - loss: 1.9534 - regression_loss: 1.6396 - classification_loss: 0.3138
  39/1500 [..............................] - ETA: 20:30 - loss: 1.9222 - regression_loss: 1.6138 - classification_loss: 0.3084
  40/1500 [..............................] - ETA: 20:13 - loss: 1.9002 - regression_loss: 1.5971 - classification_loss: 0.3031
  41/1500 [..............................] - ETA: 20:03 - loss: 1.8816 - regression_loss: 1.5812 - classification_loss: 0.3004
  42/1500 [..............................] - ETA: 20:10 - loss: 1.8953 - regression_loss: 1.5894 - classification_loss: 0.3060
  43/1500 [..............................] - ETA: 21:06 - loss: 1.9000 - regression_loss: 1.5932 - classification_loss: 0.3069
  44/1500 [..............................] - ETA: 21:08 - loss: 1.8908 - regression_loss: 1.5863 - classification_loss: 0.3045
  45/1500 [..............................] - ETA: 20:57 - loss: 1.9039 - regression_loss: 1.5950 - classification_loss: 0.3090
  46/1500 [..............................] - ETA: 20:43 - loss: 1.8904 - regression_loss: 1.5833 - classification_loss: 0.3070
  47/1500 [..............................] - ETA: 20:28 - loss: 1.8791 - regression_loss: 1.5766 - classification_loss: 0.3026
  48/1500 [..............................] - ETA: 20:15 - loss: 1.8750 - regression_loss: 1.5755 - classification_loss: 0.2995
  49/1500 [..............................] - ETA: 20:12 - loss: 1.8652 - regression_loss: 1.5676 - classification_loss: 0.2976
  50/1500 [>.............................] - ETA: 19:59 - loss: 1.8657 - regression_loss: 1.5693 - classification_loss: 0.2964
  51/1500 [>.............................] - ETA: 19:46 - loss: 1.8651 - regression_loss: 1.5693 - classification_loss: 0.2959
  52/1500 [>.............................] - ETA: 19:34 - loss: 1.8732 - regression_loss: 1.5758 - classification_loss: 0.2974
  53/1500 [>.............................] - ETA: 19:25 - loss: 1.8610 - regression_loss: 1.5665 - classification_loss: 0.2944
  54/1500 [>.............................] - ETA: 19:19 - loss: 1.8834 - regression_loss: 1.5818 - classification_loss: 0.3016
  55/1500 [>.............................] - ETA: 19:15 - loss: 1.8951 - regression_loss: 1.5907 - classification_loss: 0.3043
  56/1500 [>.............................] - ETA: 19:29 - loss: 1.8826 - regression_loss: 1.5812 - classification_loss: 0.3014
  57/1500 [>.............................] - ETA: 19:19 - loss: 1.8833 - regression_loss: 1.5825 - classification_loss: 0.3008
  58/1500 [>.............................] - ETA: 19:16 - loss: 1.8728 - regression_loss: 1.5741 - classification_loss: 0.2987
  59/1500 [>.............................] - ETA: 19:07 - loss: 1.8735 - regression_loss: 1.5761 - classification_loss: 0.2974
  60/1500 [>.............................] - ETA: 19:11 - loss: 1.8613 - regression_loss: 1.5670 - classification_loss: 0.2943
  61/1500 [>.............................] - ETA: 19:37 - loss: 1.8630 - regression_loss: 1.5683 - classification_loss: 0.2947
  62/1500 [>.............................] - ETA: 19:31 - loss: 1.8523 - regression_loss: 1.5596 - classification_loss: 0.2927
  63/1500 [>.............................] - ETA: 19:20 - loss: 1.8434 - regression_loss: 1.5511 - classification_loss: 0.2923
  64/1500 [>.............................] - ETA: 19:10 - loss: 1.8312 - regression_loss: 1.5412 - classification_loss: 0.2900
  65/1500 [>.............................] - ETA: 19:12 - loss: 1.8253 - regression_loss: 1.5373 - classification_loss: 0.2880
  66/1500 [>.............................] - ETA: 19:08 - loss: 1.8333 - regression_loss: 1.5453 - classification_loss: 0.2880
  67/1500 [>.............................] - ETA: 18:59 - loss: 1.8296 - regression_loss: 1.5418 - classification_loss: 0.2878
  68/1500 [>.............................] - ETA: 19:00 - loss: 1.8250 - regression_loss: 1.5381 - classification_loss: 0.2869
  69/1500 [>.............................] - ETA: 18:54 - loss: 1.8303 - regression_loss: 1.5433 - classification_loss: 0.2870
  70/1500 [>.............................] - ETA: 18:48 - loss: 1.8425 - regression_loss: 1.5554 - classification_loss: 0.2871
  71/1500 [>.............................] - ETA: 18:40 - loss: 1.8566 - regression_loss: 1.5646 - classification_loss: 0.2920
  72/1500 [>.............................] - ETA: 18:32 - loss: 1.8498 - regression_loss: 1.5599 - classification_loss: 0.2899
  73/1500 [>.............................] - ETA: 18:24 - loss: 1.8530 - regression_loss: 1.5622 - classification_loss: 0.2908
  74/1500 [>.............................] - ETA: 18:15 - loss: 1.8523 - regression_loss: 1.5624 - classification_loss: 0.2899
  75/1500 [>.............................] - ETA: 18:50 - loss: 1.8499 - regression_loss: 1.5600 - classification_loss: 0.2899
  76/1500 [>.............................] - ETA: 18:41 - loss: 1.8445 - regression_loss: 1.5557 - classification_loss: 0.2889
  77/1500 [>.............................] - ETA: 18:33 - loss: 1.8472 - regression_loss: 1.5589 - classification_loss: 0.2883
  78/1500 [>.............................] - ETA: 18:30 - loss: 1.8335 - regression_loss: 1.5473 - classification_loss: 0.2862
  79/1500 [>.............................] - ETA: 18:23 - loss: 1.8240 - regression_loss: 1.5403 - classification_loss: 0.2837
  80/1500 [>.............................] - ETA: 18:24 - loss: 1.8398 - regression_loss: 1.5545 - classification_loss: 0.2853
  81/1500 [>.............................] - ETA: 18:24 - loss: 1.8457 - regression_loss: 1.5593 - classification_loss: 0.2864
  82/1500 [>.............................] - ETA: 18:19 - loss: 1.8640 - regression_loss: 1.5750 - classification_loss: 0.2890
  83/1500 [>.............................] - ETA: 18:17 - loss: 1.8755 - regression_loss: 1.5845 - classification_loss: 0.2910
  84/1500 [>.............................] - ETA: 18:16 - loss: 1.8917 - regression_loss: 1.5989 - classification_loss: 0.2928
  85/1500 [>.............................] - ETA: 18:08 - loss: 1.8858 - regression_loss: 1.5941 - classification_loss: 0.2917
  86/1500 [>.............................] - ETA: 18:03 - loss: 1.8804 - regression_loss: 1.5902 - classification_loss: 0.2902
  87/1500 [>.............................] - ETA: 17:58 - loss: 1.8800 - regression_loss: 1.5907 - classification_loss: 0.2894
  88/1500 [>.............................] - ETA: 17:54 - loss: 1.8821 - regression_loss: 1.5930 - classification_loss: 0.2890
  89/1500 [>.............................] - ETA: 17:57 - loss: 1.8850 - regression_loss: 1.5955 - classification_loss: 0.2895
  90/1500 [>.............................] - ETA: 18:06 - loss: 1.8919 - regression_loss: 1.6017 - classification_loss: 0.2902
  91/1500 [>.............................] - ETA: 18:12 - loss: 1.8865 - regression_loss: 1.5975 - classification_loss: 0.2890
  92/1500 [>.............................] - ETA: 18:05 - loss: 1.8848 - regression_loss: 1.5964 - classification_loss: 0.2884
  93/1500 [>.............................] - ETA: 18:00 - loss: 1.8931 - regression_loss: 1.6038 - classification_loss: 0.2893
  94/1500 [>.............................] - ETA: 17:53 - loss: 1.8932 - regression_loss: 1.6040 - classification_loss: 0.2892
  95/1500 [>.............................] - ETA: 17:48 - loss: 1.8858 - regression_loss: 1.5980 - classification_loss: 0.2878
  96/1500 [>.............................] - ETA: 17:49 - loss: 1.8839 - regression_loss: 1.5969 - classification_loss: 0.2871
  97/1500 [>.............................] - ETA: 17:52 - loss: 1.8783 - regression_loss: 1.5929 - classification_loss: 0.2853
  98/1500 [>.............................] - ETA: 17:46 - loss: 1.8798 - regression_loss: 1.5935 - classification_loss: 0.2863
  99/1500 [>.............................] - ETA: 17:41 - loss: 1.8797 - regression_loss: 1.5928 - classification_loss: 0.2869
 100/1500 [=>............................] - ETA: 17:46 - loss: 1.8881 - regression_loss: 1.6000 - classification_loss: 0.2880
 101/1500 [=>............................] - ETA: 17:51 - loss: 1.8914 - regression_loss: 1.6034 - classification_loss: 0.2880
 102/1500 [=>............................] - ETA: 17:52 - loss: 1.8962 - regression_loss: 1.6069 - classification_loss: 0.2892
 103/1500 [=>............................] - ETA: 17:46 - loss: 1.8931 - regression_loss: 1.6040 - classification_loss: 0.2891
 104/1500 [=>............................] - ETA: 17:51 - loss: 1.8922 - regression_loss: 1.6031 - classification_loss: 0.2890
 105/1500 [=>............................] - ETA: 17:45 - loss: 1.8955 - regression_loss: 1.6059 - classification_loss: 0.2897
 106/1500 [=>............................] - ETA: 18:20 - loss: 1.9026 - regression_loss: 1.6111 - classification_loss: 0.2914
 107/1500 [=>............................] - ETA: 18:18 - loss: 1.9050 - regression_loss: 1.6125 - classification_loss: 0.2925
 108/1500 [=>............................] - ETA: 18:12 - loss: 1.9068 - regression_loss: 1.6139 - classification_loss: 0.2929
 109/1500 [=>............................] - ETA: 18:07 - loss: 1.9128 - regression_loss: 1.6188 - classification_loss: 0.2940
 110/1500 [=>............................] - ETA: 18:14 - loss: 1.9209 - regression_loss: 1.6252 - classification_loss: 0.2957
 111/1500 [=>............................] - ETA: 18:08 - loss: 1.9249 - regression_loss: 1.6283 - classification_loss: 0.2966
 112/1500 [=>............................] - ETA: 18:04 - loss: 1.9228 - regression_loss: 1.6271 - classification_loss: 0.2957
 113/1500 [=>............................] - ETA: 18:10 - loss: 1.9161 - regression_loss: 1.6218 - classification_loss: 0.2943
 114/1500 [=>............................] - ETA: 18:06 - loss: 1.9112 - regression_loss: 1.6177 - classification_loss: 0.2935
 115/1500 [=>............................] - ETA: 18:01 - loss: 1.9133 - regression_loss: 1.6192 - classification_loss: 0.2941
 116/1500 [=>............................] - ETA: 18:00 - loss: 1.9063 - regression_loss: 1.6135 - classification_loss: 0.2927
 117/1500 [=>............................] - ETA: 17:55 - loss: 1.9025 - regression_loss: 1.6099 - classification_loss: 0.2926
 118/1500 [=>............................] - ETA: 17:50 - loss: 1.9037 - regression_loss: 1.6108 - classification_loss: 0.2928
 119/1500 [=>............................] - ETA: 17:45 - loss: 1.8971 - regression_loss: 1.6059 - classification_loss: 0.2912
 120/1500 [=>............................] - ETA: 17:49 - loss: 1.9092 - regression_loss: 1.6150 - classification_loss: 0.2942
 121/1500 [=>............................] - ETA: 17:44 - loss: 1.9148 - regression_loss: 1.6202 - classification_loss: 0.2946
 122/1500 [=>............................] - ETA: 17:43 - loss: 1.9211 - regression_loss: 1.6248 - classification_loss: 0.2963
 123/1500 [=>............................] - ETA: 17:42 - loss: 1.9240 - regression_loss: 1.6259 - classification_loss: 0.2982
 124/1500 [=>............................] - ETA: 17:39 - loss: 1.9222 - regression_loss: 1.6247 - classification_loss: 0.2975
 125/1500 [=>............................] - ETA: 17:34 - loss: 1.9286 - regression_loss: 1.6298 - classification_loss: 0.2988
 126/1500 [=>............................] - ETA: 17:32 - loss: 1.9232 - regression_loss: 1.6253 - classification_loss: 0.2979
 127/1500 [=>............................] - ETA: 17:31 - loss: 1.9206 - regression_loss: 1.6238 - classification_loss: 0.2968
 128/1500 [=>............................] - ETA: 17:27 - loss: 1.9241 - regression_loss: 1.6273 - classification_loss: 0.2968
 129/1500 [=>............................] - ETA: 17:29 - loss: 1.9257 - regression_loss: 1.6280 - classification_loss: 0.2977
 130/1500 [=>............................] - ETA: 17:25 - loss: 1.9213 - regression_loss: 1.6246 - classification_loss: 0.2967
 131/1500 [=>............................] - ETA: 17:20 - loss: 1.9172 - regression_loss: 1.6213 - classification_loss: 0.2959
 132/1500 [=>............................] - ETA: 17:16 - loss: 1.9174 - regression_loss: 1.6218 - classification_loss: 0.2956
 133/1500 [=>............................] - ETA: 17:17 - loss: 1.9137 - regression_loss: 1.6192 - classification_loss: 0.2946
 134/1500 [=>............................] - ETA: 17:24 - loss: 1.9199 - regression_loss: 1.6246 - classification_loss: 0.2953
 135/1500 [=>............................] - ETA: 17:23 - loss: 1.9233 - regression_loss: 1.6276 - classification_loss: 0.2957
 136/1500 [=>............................] - ETA: 17:24 - loss: 1.9148 - regression_loss: 1.6202 - classification_loss: 0.2946
 137/1500 [=>............................] - ETA: 17:23 - loss: 1.9125 - regression_loss: 1.6180 - classification_loss: 0.2945
 138/1500 [=>............................] - ETA: 17:32 - loss: 1.9078 - regression_loss: 1.6136 - classification_loss: 0.2942
 139/1500 [=>............................] - ETA: 17:29 - loss: 1.9079 - regression_loss: 1.6142 - classification_loss: 0.2937
 140/1500 [=>............................] - ETA: 17:25 - loss: 1.9116 - regression_loss: 1.6171 - classification_loss: 0.2945
 141/1500 [=>............................] - ETA: 17:26 - loss: 1.9071 - regression_loss: 1.6136 - classification_loss: 0.2935
 142/1500 [=>............................] - ETA: 17:26 - loss: 1.9060 - regression_loss: 1.6129 - classification_loss: 0.2931
 143/1500 [=>............................] - ETA: 17:21 - loss: 1.9019 - regression_loss: 1.6097 - classification_loss: 0.2922
 144/1500 [=>............................] - ETA: 17:30 - loss: 1.9007 - regression_loss: 1.6092 - classification_loss: 0.2915
 145/1500 [=>............................] - ETA: 17:30 - loss: 1.9061 - regression_loss: 1.6143 - classification_loss: 0.2919
 146/1500 [=>............................] - ETA: 17:29 - loss: 1.9030 - regression_loss: 1.6116 - classification_loss: 0.2914
 147/1500 [=>............................] - ETA: 17:41 - loss: 1.9086 - regression_loss: 1.6164 - classification_loss: 0.2922
 148/1500 [=>............................] - ETA: 17:49 - loss: 1.9150 - regression_loss: 1.6204 - classification_loss: 0.2946
 149/1500 [=>............................] - ETA: 17:50 - loss: 1.9206 - regression_loss: 1.6244 - classification_loss: 0.2962
 150/1500 [==>...........................] - ETA: 17:46 - loss: 1.9270 - regression_loss: 1.6304 - classification_loss: 0.2966
 151/1500 [==>...........................] - ETA: 17:42 - loss: 1.9275 - regression_loss: 1.6314 - classification_loss: 0.2961
 152/1500 [==>...........................] - ETA: 17:37 - loss: 1.9223 - regression_loss: 1.6272 - classification_loss: 0.2951
 153/1500 [==>...........................] - ETA: 17:54 - loss: 1.9285 - regression_loss: 1.6314 - classification_loss: 0.2971
 154/1500 [==>...........................] - ETA: 17:55 - loss: 1.9256 - regression_loss: 1.6288 - classification_loss: 0.2967
 155/1500 [==>...........................] - ETA: 17:56 - loss: 1.9246 - regression_loss: 1.6283 - classification_loss: 0.2963
 156/1500 [==>...........................] - ETA: 17:51 - loss: 1.9248 - regression_loss: 1.6289 - classification_loss: 0.2959
 157/1500 [==>...........................] - ETA: 17:51 - loss: 1.9243 - regression_loss: 1.6282 - classification_loss: 0.2961
 158/1500 [==>...........................] - ETA: 17:49 - loss: 1.9317 - regression_loss: 1.6319 - classification_loss: 0.2998
 159/1500 [==>...........................] - ETA: 17:45 - loss: 1.9287 - regression_loss: 1.6294 - classification_loss: 0.2993
 160/1500 [==>...........................] - ETA: 17:46 - loss: 1.9315 - regression_loss: 1.6280 - classification_loss: 0.3035
 161/1500 [==>...........................] - ETA: 17:42 - loss: 1.9357 - regression_loss: 1.6315 - classification_loss: 0.3042
 162/1500 [==>...........................] - ETA: 17:38 - loss: 1.9388 - regression_loss: 1.6340 - classification_loss: 0.3048
 163/1500 [==>...........................] - ETA: 17:34 - loss: 1.9397 - regression_loss: 1.6345 - classification_loss: 0.3052
 164/1500 [==>...........................] - ETA: 17:31 - loss: 1.9341 - regression_loss: 1.6300 - classification_loss: 0.3041
 165/1500 [==>...........................] - ETA: 17:30 - loss: 1.9360 - regression_loss: 1.6317 - classification_loss: 0.3043
 166/1500 [==>...........................] - ETA: 17:26 - loss: 1.9330 - regression_loss: 1.6294 - classification_loss: 0.3036
 167/1500 [==>...........................] - ETA: 17:23 - loss: 1.9362 - regression_loss: 1.6313 - classification_loss: 0.3049
 168/1500 [==>...........................] - ETA: 17:27 - loss: 1.9391 - regression_loss: 1.6342 - classification_loss: 0.3049
 169/1500 [==>...........................] - ETA: 17:26 - loss: 1.9429 - regression_loss: 1.6384 - classification_loss: 0.3045
 170/1500 [==>...........................] - ETA: 17:22 - loss: 1.9436 - regression_loss: 1.6391 - classification_loss: 0.3045
 171/1500 [==>...........................] - ETA: 17:18 - loss: 1.9392 - regression_loss: 1.6352 - classification_loss: 0.3040
 172/1500 [==>...........................] - ETA: 17:14 - loss: 1.9423 - regression_loss: 1.6380 - classification_loss: 0.3043
 173/1500 [==>...........................] - ETA: 17:11 - loss: 1.9375 - regression_loss: 1.6344 - classification_loss: 0.3031
 174/1500 [==>...........................] - ETA: 17:13 - loss: 1.9426 - regression_loss: 1.6382 - classification_loss: 0.3044
 175/1500 [==>...........................] - ETA: 17:10 - loss: 1.9382 - regression_loss: 1.6348 - classification_loss: 0.3034
 176/1500 [==>...........................] - ETA: 17:06 - loss: 1.9469 - regression_loss: 1.6417 - classification_loss: 0.3052
 177/1500 [==>...........................] - ETA: 17:09 - loss: 1.9475 - regression_loss: 1.6427 - classification_loss: 0.3047
 178/1500 [==>...........................] - ETA: 17:09 - loss: 1.9441 - regression_loss: 1.6393 - classification_loss: 0.3048
 179/1500 [==>...........................] - ETA: 17:17 - loss: 1.9421 - regression_loss: 1.6371 - classification_loss: 0.3050
 180/1500 [==>...........................] - ETA: 17:16 - loss: 1.9466 - regression_loss: 1.6411 - classification_loss: 0.3056
 181/1500 [==>...........................] - ETA: 17:14 - loss: 1.9409 - regression_loss: 1.6364 - classification_loss: 0.3044
 182/1500 [==>...........................] - ETA: 17:14 - loss: 1.9429 - regression_loss: 1.6370 - classification_loss: 0.3059
 183/1500 [==>...........................] - ETA: 17:11 - loss: 1.9450 - regression_loss: 1.6389 - classification_loss: 0.3061
 184/1500 [==>...........................] - ETA: 17:09 - loss: 1.9380 - regression_loss: 1.6330 - classification_loss: 0.3049
 185/1500 [==>...........................] - ETA: 17:10 - loss: 1.9355 - regression_loss: 1.6296 - classification_loss: 0.3059
 186/1500 [==>...........................] - ETA: 17:09 - loss: 1.9340 - regression_loss: 1.6279 - classification_loss: 0.3061
 187/1500 [==>...........................] - ETA: 17:05 - loss: 1.9337 - regression_loss: 1.6280 - classification_loss: 0.3057
 188/1500 [==>...........................] - ETA: 17:04 - loss: 1.9392 - regression_loss: 1.6329 - classification_loss: 0.3063
 189/1500 [==>...........................] - ETA: 17:03 - loss: 1.9445 - regression_loss: 1.6370 - classification_loss: 0.3075
 190/1500 [==>...........................] - ETA: 17:03 - loss: 1.9443 - regression_loss: 1.6372 - classification_loss: 0.3070
 191/1500 [==>...........................] - ETA: 17:00 - loss: 1.9424 - regression_loss: 1.6350 - classification_loss: 0.3073
 192/1500 [==>...........................] - ETA: 16:56 - loss: 1.9473 - regression_loss: 1.6390 - classification_loss: 0.3083
 193/1500 [==>...........................] - ETA: 16:55 - loss: 1.9505 - regression_loss: 1.6419 - classification_loss: 0.3086
 194/1500 [==>...........................] - ETA: 16:54 - loss: 1.9463 - regression_loss: 1.6388 - classification_loss: 0.3075
 195/1500 [==>...........................] - ETA: 16:54 - loss: 1.9627 - regression_loss: 1.6304 - classification_loss: 0.3323
 196/1500 [==>...........................] - ETA: 16:50 - loss: 1.9651 - regression_loss: 1.6316 - classification_loss: 0.3335
 197/1500 [==>...........................] - ETA: 16:55 - loss: 1.9648 - regression_loss: 1.6313 - classification_loss: 0.3335
 198/1500 [==>...........................] - ETA: 16:51 - loss: 1.9714 - regression_loss: 1.6370 - classification_loss: 0.3345
 199/1500 [==>...........................] - ETA: 16:48 - loss: 1.9712 - regression_loss: 1.6366 - classification_loss: 0.3346
 200/1500 [===>..........................] - ETA: 16:57 - loss: 1.9758 - regression_loss: 1.6397 - classification_loss: 0.3361
 201/1500 [===>..........................] - ETA: 16:54 - loss: 1.9758 - regression_loss: 1.6400 - classification_loss: 0.3358
 202/1500 [===>..........................] - ETA: 16:51 - loss: 1.9759 - regression_loss: 1.6407 - classification_loss: 0.3353
 203/1500 [===>..........................] - ETA: 16:53 - loss: 1.9764 - regression_loss: 1.6412 - classification_loss: 0.3352
 204/1500 [===>..........................] - ETA: 16:50 - loss: 1.9765 - regression_loss: 1.6419 - classification_loss: 0.3347
 205/1500 [===>..........................] - ETA: 16:50 - loss: 1.9798 - regression_loss: 1.6450 - classification_loss: 0.3348
 206/1500 [===>..........................] - ETA: 16:50 - loss: 1.9792 - regression_loss: 1.6449 - classification_loss: 0.3343
 207/1500 [===>..........................] - ETA: 16:48 - loss: 1.9765 - regression_loss: 1.6433 - classification_loss: 0.3332
 208/1500 [===>..........................] - ETA: 16:45 - loss: 1.9775 - regression_loss: 1.6435 - classification_loss: 0.3340
 209/1500 [===>..........................] - ETA: 16:42 - loss: 1.9805 - regression_loss: 1.6463 - classification_loss: 0.3343
 210/1500 [===>..........................] - ETA: 16:44 - loss: 1.9802 - regression_loss: 1.6461 - classification_loss: 0.3342
 211/1500 [===>..........................] - ETA: 16:42 - loss: 1.9758 - regression_loss: 1.6423 - classification_loss: 0.3335
 212/1500 [===>..........................] - ETA: 16:39 - loss: 1.9788 - regression_loss: 1.6455 - classification_loss: 0.3332
 213/1500 [===>..........................] - ETA: 16:36 - loss: 1.9752 - regression_loss: 1.6427 - classification_loss: 0.3325
 214/1500 [===>..........................] - ETA: 16:35 - loss: 1.9729 - regression_loss: 1.6413 - classification_loss: 0.3316
 215/1500 [===>..........................] - ETA: 16:35 - loss: 1.9775 - regression_loss: 1.6461 - classification_loss: 0.3315
 216/1500 [===>..........................] - ETA: 16:32 - loss: 1.9820 - regression_loss: 1.6502 - classification_loss: 0.3317
 217/1500 [===>..........................] - ETA: 16:30 - loss: 1.9822 - regression_loss: 1.6503 - classification_loss: 0.3319
 218/1500 [===>..........................] - ETA: 16:32 - loss: 1.9809 - regression_loss: 1.6497 - classification_loss: 0.3312
 219/1500 [===>..........................] - ETA: 16:32 - loss: 1.9799 - regression_loss: 1.6489 - classification_loss: 0.3310
 220/1500 [===>..........................] - ETA: 16:29 - loss: 1.9824 - regression_loss: 1.6509 - classification_loss: 0.3315
 221/1500 [===>..........................] - ETA: 16:26 - loss: 1.9878 - regression_loss: 1.6552 - classification_loss: 0.3326
 222/1500 [===>..........................] - ETA: 16:23 - loss: 1.9865 - regression_loss: 1.6546 - classification_loss: 0.3319
 223/1500 [===>..........................] - ETA: 16:21 - loss: 1.9896 - regression_loss: 1.6573 - classification_loss: 0.3322
 224/1500 [===>..........................] - ETA: 16:18 - loss: 1.9954 - regression_loss: 1.6614 - classification_loss: 0.3339
 225/1500 [===>..........................] - ETA: 16:17 - loss: 2.0017 - regression_loss: 1.6676 - classification_loss: 0.3341
 226/1500 [===>..........................] - ETA: 16:19 - loss: 1.9998 - regression_loss: 1.6665 - classification_loss: 0.3333
 227/1500 [===>..........................] - ETA: 16:30 - loss: 2.0000 - regression_loss: 1.6670 - classification_loss: 0.3330
 228/1500 [===>..........................] - ETA: 16:27 - loss: 1.9978 - regression_loss: 1.6647 - classification_loss: 0.3330
 229/1500 [===>..........................] - ETA: 16:28 - loss: 1.9950 - regression_loss: 1.6628 - classification_loss: 0.3322
 230/1500 [===>..........................] - ETA: 16:26 - loss: 1.9926 - regression_loss: 1.6611 - classification_loss: 0.3315
 231/1500 [===>..........................] - ETA: 16:25 - loss: 1.9944 - regression_loss: 1.6633 - classification_loss: 0.3311
 232/1500 [===>..........................] - ETA: 16:21 - loss: 1.9975 - regression_loss: 1.6666 - classification_loss: 0.3309
 233/1500 [===>..........................] - ETA: 16:25 - loss: 1.9965 - regression_loss: 1.6654 - classification_loss: 0.3311
 234/1500 [===>..........................] - ETA: 16:24 - loss: 2.0022 - regression_loss: 1.6703 - classification_loss: 0.3320
 235/1500 [===>..........................] - ETA: 16:22 - loss: 2.0043 - regression_loss: 1.6726 - classification_loss: 0.3318
 236/1500 [===>..........................] - ETA: 16:26 - loss: 2.0081 - regression_loss: 1.6758 - classification_loss: 0.3323
 237/1500 [===>..........................] - ETA: 16:25 - loss: 2.0055 - regression_loss: 1.6740 - classification_loss: 0.3315
 238/1500 [===>..........................] - ETA: 16:24 - loss: 2.0039 - regression_loss: 1.6729 - classification_loss: 0.3310
 239/1500 [===>..........................] - ETA: 16:21 - loss: 2.0054 - regression_loss: 1.6740 - classification_loss: 0.3314
 240/1500 [===>..........................] - ETA: 16:22 - loss: 2.0082 - regression_loss: 1.6768 - classification_loss: 0.3314
 241/1500 [===>..........................] - ETA: 16:22 - loss: 2.0082 - regression_loss: 1.6770 - classification_loss: 0.3312
 242/1500 [===>..........................] - ETA: 16:19 - loss: 2.0077 - regression_loss: 1.6766 - classification_loss: 0.3311
 243/1500 [===>..........................] - ETA: 16:19 - loss: 2.0046 - regression_loss: 1.6742 - classification_loss: 0.3304
 244/1500 [===>..........................] - ETA: 16:17 - loss: 2.0015 - regression_loss: 1.6717 - classification_loss: 0.3298
 245/1500 [===>..........................] - ETA: 16:14 - loss: 1.9978 - regression_loss: 1.6687 - classification_loss: 0.3291
 246/1500 [===>..........................] - ETA: 16:12 - loss: 2.0034 - regression_loss: 1.6718 - classification_loss: 0.3316
 247/1500 [===>..........................] - ETA: 16:09 - loss: 2.0043 - regression_loss: 1.6731 - classification_loss: 0.3312
 248/1500 [===>..........................] - ETA: 16:08 - loss: 2.0012 - regression_loss: 1.6707 - classification_loss: 0.3306
 249/1500 [===>..........................] - ETA: 16:12 - loss: 1.9985 - regression_loss: 1.6686 - classification_loss: 0.3299
 250/1500 [====>.........................] - ETA: 16:12 - loss: 2.0016 - regression_loss: 1.6711 - classification_loss: 0.3306
 251/1500 [====>.........................] - ETA: 16:09 - loss: 2.0003 - regression_loss: 1.6702 - classification_loss: 0.3301
 252/1500 [====>.........................] - ETA: 16:07 - loss: 2.0019 - regression_loss: 1.6720 - classification_loss: 0.3299
 253/1500 [====>.........................] - ETA: 16:07 - loss: 1.9997 - regression_loss: 1.6703 - classification_loss: 0.3294
 254/1500 [====>.........................] - ETA: 16:05 - loss: 1.9988 - regression_loss: 1.6696 - classification_loss: 0.3292
 255/1500 [====>.........................] - ETA: 16:03 - loss: 1.9982 - regression_loss: 1.6692 - classification_loss: 0.3290
 256/1500 [====>.........................] - ETA: 16:01 - loss: 2.0026 - regression_loss: 1.6724 - classification_loss: 0.3302
 257/1500 [====>.........................] - ETA: 16:03 - loss: 2.0035 - regression_loss: 1.6735 - classification_loss: 0.3299
 258/1500 [====>.........................] - ETA: 16:01 - loss: 2.0018 - regression_loss: 1.6724 - classification_loss: 0.3294
 259/1500 [====>.........................] - ETA: 16:00 - loss: 2.0025 - regression_loss: 1.6729 - classification_loss: 0.3296
 260/1500 [====>.........................] - ETA: 15:59 - loss: 1.9989 - regression_loss: 1.6701 - classification_loss: 0.3287
 261/1500 [====>.........................] - ETA: 16:01 - loss: 2.0023 - regression_loss: 1.6731 - classification_loss: 0.3292
 262/1500 [====>.........................] - ETA: 16:00 - loss: 2.0019 - regression_loss: 1.6728 - classification_loss: 0.3291
 263/1500 [====>.........................] - ETA: 15:57 - loss: 2.0027 - regression_loss: 1.6733 - classification_loss: 0.3294
 264/1500 [====>.........................] - ETA: 15:55 - loss: 2.0037 - regression_loss: 1.6738 - classification_loss: 0.3299
 265/1500 [====>.........................] - ETA: 15:55 - loss: 2.0014 - regression_loss: 1.6720 - classification_loss: 0.3293
 266/1500 [====>.........................] - ETA: 15:55 - loss: 1.9983 - regression_loss: 1.6689 - classification_loss: 0.3293
 267/1500 [====>.........................] - ETA: 15:53 - loss: 1.9949 - regression_loss: 1.6663 - classification_loss: 0.3286
 268/1500 [====>.........................] - ETA: 15:50 - loss: 1.9945 - regression_loss: 1.6659 - classification_loss: 0.3286
 269/1500 [====>.........................] - ETA: 15:51 - loss: 1.9945 - regression_loss: 1.6663 - classification_loss: 0.3283
 270/1500 [====>.........................] - ETA: 15:49 - loss: 1.9932 - regression_loss: 1.6647 - classification_loss: 0.3286
 271/1500 [====>.........................] - ETA: 15:47 - loss: 1.9916 - regression_loss: 1.6633 - classification_loss: 0.3283
 272/1500 [====>.........................] - ETA: 15:44 - loss: 1.9887 - regression_loss: 1.6608 - classification_loss: 0.3279
 273/1500 [====>.........................] - ETA: 15:42 - loss: 1.9873 - regression_loss: 1.6595 - classification_loss: 0.3278
 274/1500 [====>.........................] - ETA: 15:40 - loss: 1.9839 - regression_loss: 1.6565 - classification_loss: 0.3274
 275/1500 [====>.........................] - ETA: 15:40 - loss: 1.9830 - regression_loss: 1.6555 - classification_loss: 0.3274
 276/1500 [====>.........................] - ETA: 15:41 - loss: 1.9815 - regression_loss: 1.6546 - classification_loss: 0.3269
 277/1500 [====>.........................] - ETA: 15:41 - loss: 1.9832 - regression_loss: 1.6565 - classification_loss: 0.3267
 278/1500 [====>.........................] - ETA: 15:43 - loss: 1.9835 - regression_loss: 1.6569 - classification_loss: 0.3266
 279/1500 [====>.........................] - ETA: 15:44 - loss: 1.9801 - regression_loss: 1.6540 - classification_loss: 0.3260
 280/1500 [====>.........................] - ETA: 15:42 - loss: 1.9778 - regression_loss: 1.6523 - classification_loss: 0.3255
 281/1500 [====>.........................] - ETA: 15:40 - loss: 1.9787 - regression_loss: 1.6534 - classification_loss: 0.3253
 282/1500 [====>.........................] - ETA: 15:37 - loss: 1.9778 - regression_loss: 1.6526 - classification_loss: 0.3252
 283/1500 [====>.........................] - ETA: 15:35 - loss: 1.9792 - regression_loss: 1.6532 - classification_loss: 0.3260
 284/1500 [====>.........................] - ETA: 15:33 - loss: 1.9779 - regression_loss: 1.6524 - classification_loss: 0.3255
 285/1500 [====>.........................] - ETA: 15:34 - loss: 1.9742 - regression_loss: 1.6495 - classification_loss: 0.3247
 286/1500 [====>.........................] - ETA: 15:36 - loss: 1.9769 - regression_loss: 1.6519 - classification_loss: 0.3250
 287/1500 [====>.........................] - ETA: 15:37 - loss: 1.9787 - regression_loss: 1.6532 - classification_loss: 0.3255
 288/1500 [====>.........................] - ETA: 15:36 - loss: 1.9763 - regression_loss: 1.6512 - classification_loss: 0.3250
 289/1500 [====>.........................] - ETA: 15:36 - loss: 1.9762 - regression_loss: 1.6514 - classification_loss: 0.3248
 290/1500 [====>.........................] - ETA: 15:34 - loss: 1.9750 - regression_loss: 1.6508 - classification_loss: 0.3242
 291/1500 [====>.........................] - ETA: 15:34 - loss: 1.9771 - regression_loss: 1.6522 - classification_loss: 0.3249
 292/1500 [====>.........................] - ETA: 15:33 - loss: 1.9813 - regression_loss: 1.6551 - classification_loss: 0.3261
 293/1500 [====>.........................] - ETA: 15:31 - loss: 1.9845 - regression_loss: 1.6577 - classification_loss: 0.3268
 294/1500 [====>.........................] - ETA: 15:31 - loss: 1.9873 - regression_loss: 1.6596 - classification_loss: 0.3277
 295/1500 [====>.........................] - ETA: 15:29 - loss: 1.9870 - regression_loss: 1.6596 - classification_loss: 0.3274
 296/1500 [====>.........................] - ETA: 15:29 - loss: 1.9861 - regression_loss: 1.6589 - classification_loss: 0.3272
 297/1500 [====>.........................] - ETA: 15:27 - loss: 1.9882 - regression_loss: 1.6611 - classification_loss: 0.3271
 298/1500 [====>.........................] - ETA: 15:25 - loss: 1.9866 - regression_loss: 1.6599 - classification_loss: 0.3267
 299/1500 [====>.........................] - ETA: 15:23 - loss: 1.9832 - regression_loss: 1.6571 - classification_loss: 0.3261
 300/1500 [=====>........................] - ETA: 15:22 - loss: 1.9820 - regression_loss: 1.6562 - classification_loss: 0.3258
 301/1500 [=====>........................] - ETA: 15:20 - loss: 1.9863 - regression_loss: 1.6565 - classification_loss: 0.3298
 302/1500 [=====>........................] - ETA: 15:18 - loss: 1.9837 - regression_loss: 1.6539 - classification_loss: 0.3298
 303/1500 [=====>........................] - ETA: 15:16 - loss: 1.9838 - regression_loss: 1.6530 - classification_loss: 0.3307
 304/1500 [=====>........................] - ETA: 15:18 - loss: 1.9878 - regression_loss: 1.6566 - classification_loss: 0.3312
 305/1500 [=====>........................] - ETA: 15:16 - loss: 1.9859 - regression_loss: 1.6555 - classification_loss: 0.3305
 306/1500 [=====>........................] - ETA: 15:14 - loss: 1.9860 - regression_loss: 1.6561 - classification_loss: 0.3299
 307/1500 [=====>........................] - ETA: 15:12 - loss: 1.9868 - regression_loss: 1.6563 - classification_loss: 0.3305
 308/1500 [=====>........................] - ETA: 15:10 - loss: 1.9856 - regression_loss: 1.6554 - classification_loss: 0.3302
 309/1500 [=====>........................] - ETA: 15:11 - loss: 1.9876 - regression_loss: 1.6573 - classification_loss: 0.3303
 310/1500 [=====>........................] - ETA: 15:14 - loss: 1.9855 - regression_loss: 1.6559 - classification_loss: 0.3296
 311/1500 [=====>........................] - ETA: 15:14 - loss: 1.9882 - regression_loss: 1.6579 - classification_loss: 0.3302
 312/1500 [=====>........................] - ETA: 15:17 - loss: 1.9908 - regression_loss: 1.6603 - classification_loss: 0.3305
 313/1500 [=====>........................] - ETA: 15:15 - loss: 1.9910 - regression_loss: 1.6602 - classification_loss: 0.3308
 314/1500 [=====>........................] - ETA: 15:13 - loss: 1.9881 - regression_loss: 1.6578 - classification_loss: 0.3303
 315/1500 [=====>........................] - ETA: 15:14 - loss: 1.9883 - regression_loss: 1.6572 - classification_loss: 0.3311
 316/1500 [=====>........................] - ETA: 15:15 - loss: 1.9871 - regression_loss: 1.6565 - classification_loss: 0.3306
 317/1500 [=====>........................] - ETA: 15:13 - loss: 1.9835 - regression_loss: 1.6535 - classification_loss: 0.3300
 318/1500 [=====>........................] - ETA: 15:14 - loss: 1.9870 - regression_loss: 1.6559 - classification_loss: 0.3310
 319/1500 [=====>........................] - ETA: 15:18 - loss: 1.9833 - regression_loss: 1.6529 - classification_loss: 0.3304
 320/1500 [=====>........................] - ETA: 15:19 - loss: 1.9832 - regression_loss: 1.6531 - classification_loss: 0.3301
 321/1500 [=====>........................] - ETA: 15:17 - loss: 1.9840 - regression_loss: 1.6541 - classification_loss: 0.3298
 322/1500 [=====>........................] - ETA: 15:16 - loss: 1.9810 - regression_loss: 1.6519 - classification_loss: 0.3291
 323/1500 [=====>........................] - ETA: 15:14 - loss: 1.9819 - regression_loss: 1.6527 - classification_loss: 0.3293
 324/1500 [=====>........................] - ETA: 15:13 - loss: 1.9832 - regression_loss: 1.6535 - classification_loss: 0.3297
 325/1500 [=====>........................] - ETA: 15:11 - loss: 1.9806 - regression_loss: 1.6515 - classification_loss: 0.3291
 326/1500 [=====>........................] - ETA: 15:13 - loss: 1.9793 - regression_loss: 1.6508 - classification_loss: 0.3286
 327/1500 [=====>........................] - ETA: 15:12 - loss: 1.9807 - regression_loss: 1.6520 - classification_loss: 0.3287
 328/1500 [=====>........................] - ETA: 15:11 - loss: 1.9792 - regression_loss: 1.6511 - classification_loss: 0.3282
 329/1500 [=====>........................] - ETA: 15:09 - loss: 1.9829 - regression_loss: 1.6544 - classification_loss: 0.3285
 330/1500 [=====>........................] - ETA: 15:07 - loss: 1.9804 - regression_loss: 1.6526 - classification_loss: 0.3278
 331/1500 [=====>........................] - ETA: 15:08 - loss: 1.9788 - regression_loss: 1.6514 - classification_loss: 0.3274
 332/1500 [=====>........................] - ETA: 15:12 - loss: 1.9805 - regression_loss: 1.6529 - classification_loss: 0.3276
 333/1500 [=====>........................] - ETA: 15:10 - loss: 1.9801 - regression_loss: 1.6525 - classification_loss: 0.3275
 334/1500 [=====>........................] - ETA: 15:14 - loss: 1.9822 - regression_loss: 1.6543 - classification_loss: 0.3279
 335/1500 [=====>........................] - ETA: 15:15 - loss: 1.9848 - regression_loss: 1.6566 - classification_loss: 0.3282
 336/1500 [=====>........................] - ETA: 15:16 - loss: 1.9842 - regression_loss: 1.6560 - classification_loss: 0.3281
 337/1500 [=====>........................] - ETA: 15:15 - loss: 1.9810 - regression_loss: 1.6536 - classification_loss: 0.3274
 338/1500 [=====>........................] - ETA: 15:15 - loss: 1.9823 - regression_loss: 1.6545 - classification_loss: 0.3278
 339/1500 [=====>........................] - ETA: 15:13 - loss: 1.9801 - regression_loss: 1.6528 - classification_loss: 0.3274
 340/1500 [=====>........................] - ETA: 15:11 - loss: 1.9823 - regression_loss: 1.6533 - classification_loss: 0.3290
 341/1500 [=====>........................] - ETA: 15:09 - loss: 1.9829 - regression_loss: 1.6541 - classification_loss: 0.3288
 342/1500 [=====>........................] - ETA: 15:09 - loss: 1.9840 - regression_loss: 1.6552 - classification_loss: 0.3288
 343/1500 [=====>........................] - ETA: 15:07 - loss: 1.9852 - regression_loss: 1.6561 - classification_loss: 0.3290
 344/1500 [=====>........................] - ETA: 15:09 - loss: 1.9831 - regression_loss: 1.6545 - classification_loss: 0.3286
 345/1500 [=====>........................] - ETA: 15:07 - loss: 1.9819 - regression_loss: 1.6536 - classification_loss: 0.3283
 346/1500 [=====>........................] - ETA: 15:05 - loss: 1.9793 - regression_loss: 1.6514 - classification_loss: 0.3279
 347/1500 [=====>........................] - ETA: 15:03 - loss: 1.9791 - regression_loss: 1.6514 - classification_loss: 0.3276
 348/1500 [=====>........................] - ETA: 15:01 - loss: 1.9808 - regression_loss: 1.6528 - classification_loss: 0.3280
 349/1500 [=====>........................] - ETA: 15:01 - loss: 1.9809 - regression_loss: 1.6528 - classification_loss: 0.3281
 350/1500 [======>.......................] - ETA: 14:59 - loss: 1.9848 - regression_loss: 1.6556 - classification_loss: 0.3292
 351/1500 [======>.......................] - ETA: 14:57 - loss: 1.9884 - regression_loss: 1.6559 - classification_loss: 0.3325
 352/1500 [======>.......................] - ETA: 14:55 - loss: 1.9894 - regression_loss: 1.6567 - classification_loss: 0.3327
 353/1500 [======>.......................] - ETA: 14:55 - loss: 1.9874 - regression_loss: 1.6554 - classification_loss: 0.3320
 354/1500 [======>.......................] - ETA: 14:53 - loss: 1.9861 - regression_loss: 1.6545 - classification_loss: 0.3316
 355/1500 [======>.......................] - ETA: 14:52 - loss: 1.9878 - regression_loss: 1.6559 - classification_loss: 0.3319
 356/1500 [======>.......................] - ETA: 14:51 - loss: 1.9867 - regression_loss: 1.6552 - classification_loss: 0.3315
 357/1500 [======>.......................] - ETA: 14:49 - loss: 1.9855 - regression_loss: 1.6545 - classification_loss: 0.3310
 358/1500 [======>.......................] - ETA: 14:49 - loss: 1.9835 - regression_loss: 1.6529 - classification_loss: 0.3307
 359/1500 [======>.......................] - ETA: 14:50 - loss: 1.9858 - regression_loss: 1.6548 - classification_loss: 0.3310
 360/1500 [======>.......................] - ETA: 14:49 - loss: 1.9839 - regression_loss: 1.6534 - classification_loss: 0.3305
 361/1500 [======>.......................] - ETA: 14:48 - loss: 1.9818 - regression_loss: 1.6517 - classification_loss: 0.3301
 362/1500 [======>.......................] - ETA: 14:46 - loss: 1.9816 - regression_loss: 1.6516 - classification_loss: 0.3300
 363/1500 [======>.......................] - ETA: 14:45 - loss: 1.9802 - regression_loss: 1.6508 - classification_loss: 0.3294
 364/1500 [======>.......................] - ETA: 14:45 - loss: 1.9804 - regression_loss: 1.6509 - classification_loss: 0.3295
 365/1500 [======>.......................] - ETA: 14:43 - loss: 1.9785 - regression_loss: 1.6494 - classification_loss: 0.3291
 366/1500 [======>.......................] - ETA: 14:43 - loss: 1.9803 - regression_loss: 1.6506 - classification_loss: 0.3296
 367/1500 [======>.......................] - ETA: 14:41 - loss: 1.9804 - regression_loss: 1.6509 - classification_loss: 0.3295
 368/1500 [======>.......................] - ETA: 14:39 - loss: 1.9804 - regression_loss: 1.6503 - classification_loss: 0.3300
 369/1500 [======>.......................] - ETA: 14:37 - loss: 1.9795 - regression_loss: 1.6500 - classification_loss: 0.3295
 370/1500 [======>.......................] - ETA: 14:35 - loss: 1.9799 - regression_loss: 1.6503 - classification_loss: 0.3296
 371/1500 [======>.......................] - ETA: 14:36 - loss: 1.9784 - regression_loss: 1.6494 - classification_loss: 0.3291
 372/1500 [======>.......................] - ETA: 14:36 - loss: 1.9778 - regression_loss: 1.6489 - classification_loss: 0.3289
 373/1500 [======>.......................] - ETA: 14:34 - loss: 1.9775 - regression_loss: 1.6486 - classification_loss: 0.3289
 374/1500 [======>.......................] - ETA: 14:34 - loss: 1.9758 - regression_loss: 1.6473 - classification_loss: 0.3285
 375/1500 [======>.......................] - ETA: 14:34 - loss: 1.9772 - regression_loss: 1.6484 - classification_loss: 0.3289
 376/1500 [======>.......................] - ETA: 14:33 - loss: 1.9773 - regression_loss: 1.6485 - classification_loss: 0.3288
 377/1500 [======>.......................] - ETA: 14:31 - loss: 1.9785 - regression_loss: 1.6494 - classification_loss: 0.3291
 378/1500 [======>.......................] - ETA: 14:31 - loss: 1.9803 - regression_loss: 1.6506 - classification_loss: 0.3297
 379/1500 [======>.......................] - ETA: 14:30 - loss: 1.9815 - regression_loss: 1.6514 - classification_loss: 0.3301
 380/1500 [======>.......................] - ETA: 14:28 - loss: 1.9815 - regression_loss: 1.6512 - classification_loss: 0.3303
 381/1500 [======>.......................] - ETA: 14:26 - loss: 1.9798 - regression_loss: 1.6497 - classification_loss: 0.3301
 382/1500 [======>.......................] - ETA: 14:24 - loss: 1.9812 - regression_loss: 1.6510 - classification_loss: 0.3303
 383/1500 [======>.......................] - ETA: 14:22 - loss: 1.9818 - regression_loss: 1.6512 - classification_loss: 0.3305
 384/1500 [======>.......................] - ETA: 14:20 - loss: 1.9856 - regression_loss: 1.6535 - classification_loss: 0.3321
 385/1500 [======>.......................] - ETA: 14:19 - loss: 1.9869 - regression_loss: 1.6550 - classification_loss: 0.3320
 386/1500 [======>.......................] - ETA: 14:17 - loss: 1.9871 - regression_loss: 1.6552 - classification_loss: 0.3319
 387/1500 [======>.......................] - ETA: 14:17 - loss: 1.9873 - regression_loss: 1.6557 - classification_loss: 0.3315
 388/1500 [======>.......................] - ETA: 14:15 - loss: 1.9856 - regression_loss: 1.6545 - classification_loss: 0.3311
 389/1500 [======>.......................] - ETA: 14:13 - loss: 1.9843 - regression_loss: 1.6536 - classification_loss: 0.3307
 390/1500 [======>.......................] - ETA: 14:14 - loss: 1.9833 - regression_loss: 1.6523 - classification_loss: 0.3310
 391/1500 [======>.......................] - ETA: 14:16 - loss: 1.9849 - regression_loss: 1.6539 - classification_loss: 0.3310
 392/1500 [======>.......................] - ETA: 14:18 - loss: 1.9871 - regression_loss: 1.6558 - classification_loss: 0.3313
 393/1500 [======>.......................] - ETA: 14:19 - loss: 1.9876 - regression_loss: 1.6560 - classification_loss: 0.3316
 394/1500 [======>.......................] - ETA: 14:18 - loss: 1.9892 - regression_loss: 1.6575 - classification_loss: 0.3317
 395/1500 [======>.......................] - ETA: 14:17 - loss: 1.9901 - regression_loss: 1.6575 - classification_loss: 0.3325
 396/1500 [======>.......................] - ETA: 14:15 - loss: 1.9886 - regression_loss: 1.6565 - classification_loss: 0.3321
 397/1500 [======>.......................] - ETA: 14:13 - loss: 1.9881 - regression_loss: 1.6558 - classification_loss: 0.3323
 398/1500 [======>.......................] - ETA: 14:13 - loss: 1.9886 - regression_loss: 1.6561 - classification_loss: 0.3325
 399/1500 [======>.......................] - ETA: 14:12 - loss: 1.9876 - regression_loss: 1.6553 - classification_loss: 0.3323
 400/1500 [=======>......................] - ETA: 14:10 - loss: 1.9891 - regression_loss: 1.6564 - classification_loss: 0.3327
 401/1500 [=======>......................] - ETA: 14:09 - loss: 1.9904 - regression_loss: 1.6571 - classification_loss: 0.3333
 402/1500 [=======>......................] - ETA: 14:08 - loss: 1.9928 - regression_loss: 1.6590 - classification_loss: 0.3338
 403/1500 [=======>......................] - ETA: 14:07 - loss: 1.9926 - regression_loss: 1.6587 - classification_loss: 0.3339
 404/1500 [=======>......................] - ETA: 14:06 - loss: 1.9922 - regression_loss: 1.6586 - classification_loss: 0.3336
 405/1500 [=======>......................] - ETA: 14:06 - loss: 1.9910 - regression_loss: 1.6578 - classification_loss: 0.3332
 406/1500 [=======>......................] - ETA: 14:05 - loss: 1.9892 - regression_loss: 1.6564 - classification_loss: 0.3328
 407/1500 [=======>......................] - ETA: 14:06 - loss: 1.9885 - regression_loss: 1.6557 - classification_loss: 0.3328
 408/1500 [=======>......................] - ETA: 14:08 - loss: 1.9872 - regression_loss: 1.6546 - classification_loss: 0.3326
 409/1500 [=======>......................] - ETA: 14:08 - loss: 1.9916 - regression_loss: 1.6588 - classification_loss: 0.3328
 410/1500 [=======>......................] - ETA: 14:07 - loss: 1.9909 - regression_loss: 1.6583 - classification_loss: 0.3327
 411/1500 [=======>......................] - ETA: 14:08 - loss: 1.9902 - regression_loss: 1.6579 - classification_loss: 0.3323
 412/1500 [=======>......................] - ETA: 14:06 - loss: 1.9904 - regression_loss: 1.6583 - classification_loss: 0.3321
 413/1500 [=======>......................] - ETA: 14:05 - loss: 1.9879 - regression_loss: 1.6563 - classification_loss: 0.3316
 414/1500 [=======>......................] - ETA: 14:09 - loss: 1.9886 - regression_loss: 1.6569 - classification_loss: 0.3317
 415/1500 [=======>......................] - ETA: 14:08 - loss: 1.9892 - regression_loss: 1.6576 - classification_loss: 0.3316
 416/1500 [=======>......................] - ETA: 14:07 - loss: 1.9869 - regression_loss: 1.6558 - classification_loss: 0.3311
 417/1500 [=======>......................] - ETA: 14:06 - loss: 1.9847 - regression_loss: 1.6541 - classification_loss: 0.3307
 418/1500 [=======>......................] - ETA: 14:05 - loss: 1.9850 - regression_loss: 1.6546 - classification_loss: 0.3304
 419/1500 [=======>......................] - ETA: 14:03 - loss: 1.9872 - regression_loss: 1.6563 - classification_loss: 0.3309
 420/1500 [=======>......................] - ETA: 14:02 - loss: 1.9856 - regression_loss: 1.6551 - classification_loss: 0.3305
 421/1500 [=======>......................] - ETA: 14:04 - loss: 1.9858 - regression_loss: 1.6553 - classification_loss: 0.3304
 422/1500 [=======>......................] - ETA: 14:04 - loss: 1.9873 - regression_loss: 1.6566 - classification_loss: 0.3307
 423/1500 [=======>......................] - ETA: 14:03 - loss: 1.9898 - regression_loss: 1.6582 - classification_loss: 0.3316
 424/1500 [=======>......................] - ETA: 14:01 - loss: 1.9883 - regression_loss: 1.6571 - classification_loss: 0.3312
 425/1500 [=======>......................] - ETA: 14:00 - loss: 1.9874 - regression_loss: 1.6564 - classification_loss: 0.3311
 426/1500 [=======>......................] - ETA: 14:00 - loss: 1.9896 - regression_loss: 1.6580 - classification_loss: 0.3316
 427/1500 [=======>......................] - ETA: 14:01 - loss: 1.9902 - regression_loss: 1.6587 - classification_loss: 0.3315
 428/1500 [=======>......................] - ETA: 14:00 - loss: 1.9901 - regression_loss: 1.6587 - classification_loss: 0.3314
 429/1500 [=======>......................] - ETA: 14:01 - loss: 1.9897 - regression_loss: 1.6586 - classification_loss: 0.3311
 430/1500 [=======>......................] - ETA: 14:01 - loss: 1.9899 - regression_loss: 1.6586 - classification_loss: 0.3312
 431/1500 [=======>......................] - ETA: 14:03 - loss: 1.9896 - regression_loss: 1.6583 - classification_loss: 0.3312
 432/1500 [=======>......................] - ETA: 14:01 - loss: 1.9877 - regression_loss: 1.6570 - classification_loss: 0.3307
 433/1500 [=======>......................] - ETA: 14:01 - loss: 1.9857 - regression_loss: 1.6554 - classification_loss: 0.3302
 434/1500 [=======>......................] - ETA: 14:02 - loss: 1.9844 - regression_loss: 1.6545 - classification_loss: 0.3299
 435/1500 [=======>......................] - ETA: 14:00 - loss: 1.9823 - regression_loss: 1.6529 - classification_loss: 0.3294
 436/1500 [=======>......................] - ETA: 13:58 - loss: 1.9796 - regression_loss: 1.6507 - classification_loss: 0.3289
 437/1500 [=======>......................] - ETA: 13:56 - loss: 1.9789 - regression_loss: 1.6500 - classification_loss: 0.3289
 438/1500 [=======>......................] - ETA: 13:55 - loss: 1.9800 - regression_loss: 1.6512 - classification_loss: 0.3288
 439/1500 [=======>......................] - ETA: 13:53 - loss: 1.9823 - regression_loss: 1.6526 - classification_loss: 0.3297
 440/1500 [=======>......................] - ETA: 13:53 - loss: 1.9826 - regression_loss: 1.6523 - classification_loss: 0.3302
 441/1500 [=======>......................] - ETA: 13:52 - loss: 1.9824 - regression_loss: 1.6524 - classification_loss: 0.3300
 442/1500 [=======>......................] - ETA: 13:53 - loss: 1.9842 - regression_loss: 1.6539 - classification_loss: 0.3303
 443/1500 [=======>......................] - ETA: 13:51 - loss: 1.9852 - regression_loss: 1.6547 - classification_loss: 0.3305
 444/1500 [=======>......................] - ETA: 13:49 - loss: 1.9861 - regression_loss: 1.6542 - classification_loss: 0.3319
 445/1500 [=======>......................] - ETA: 13:48 - loss: 1.9849 - regression_loss: 1.6533 - classification_loss: 0.3316
 446/1500 [=======>......................] - ETA: 13:47 - loss: 1.9860 - regression_loss: 1.6541 - classification_loss: 0.3319
 447/1500 [=======>......................] - ETA: 13:47 - loss: 1.9858 - regression_loss: 1.6540 - classification_loss: 0.3318
 448/1500 [=======>......................] - ETA: 13:47 - loss: 1.9888 - regression_loss: 1.6568 - classification_loss: 0.3320
 449/1500 [=======>......................] - ETA: 13:45 - loss: 1.9896 - regression_loss: 1.6574 - classification_loss: 0.3322
 450/1500 [========>.....................] - ETA: 13:45 - loss: 1.9897 - regression_loss: 1.6579 - classification_loss: 0.3318
 451/1500 [========>.....................] - ETA: 13:44 - loss: 1.9887 - regression_loss: 1.6572 - classification_loss: 0.3315
 452/1500 [========>.....................] - ETA: 13:42 - loss: 1.9878 - regression_loss: 1.6566 - classification_loss: 0.3312
 453/1500 [========>.....................] - ETA: 13:41 - loss: 1.9902 - regression_loss: 1.6583 - classification_loss: 0.3319
 454/1500 [========>.....................] - ETA: 13:40 - loss: 1.9931 - regression_loss: 1.6612 - classification_loss: 0.3319
 455/1500 [========>.....................] - ETA: 13:40 - loss: 1.9952 - regression_loss: 1.6626 - classification_loss: 0.3325
 456/1500 [========>.....................] - ETA: 13:39 - loss: 1.9962 - regression_loss: 1.6635 - classification_loss: 0.3327
 457/1500 [========>.....................] - ETA: 13:39 - loss: 1.9983 - regression_loss: 1.6649 - classification_loss: 0.3334
 458/1500 [========>.....................] - ETA: 13:38 - loss: 1.9979 - regression_loss: 1.6646 - classification_loss: 0.3333
 459/1500 [========>.....................] - ETA: 13:36 - loss: 1.9968 - regression_loss: 1.6638 - classification_loss: 0.3330
 460/1500 [========>.....................] - ETA: 13:35 - loss: 1.9971 - regression_loss: 1.6643 - classification_loss: 0.3329
 461/1500 [========>.....................] - ETA: 13:33 - loss: 1.9969 - regression_loss: 1.6642 - classification_loss: 0.3327
 462/1500 [========>.....................] - ETA: 13:32 - loss: 1.9967 - regression_loss: 1.6639 - classification_loss: 0.3329
 463/1500 [========>.....................] - ETA: 13:30 - loss: 1.9952 - regression_loss: 1.6627 - classification_loss: 0.3326
 464/1500 [========>.....................] - ETA: 13:29 - loss: 1.9977 - regression_loss: 1.6647 - classification_loss: 0.3329
 465/1500 [========>.....................] - ETA: 13:29 - loss: 1.9986 - regression_loss: 1.6656 - classification_loss: 0.3330
 466/1500 [========>.....................] - ETA: 13:28 - loss: 1.9986 - regression_loss: 1.6659 - classification_loss: 0.3327
 467/1500 [========>.....................] - ETA: 13:26 - loss: 1.9982 - regression_loss: 1.6658 - classification_loss: 0.3325
 468/1500 [========>.....................] - ETA: 13:26 - loss: 1.9982 - regression_loss: 1.6661 - classification_loss: 0.3322
 469/1500 [========>.....................] - ETA: 13:24 - loss: 1.9970 - regression_loss: 1.6650 - classification_loss: 0.3319
 470/1500 [========>.....................] - ETA: 13:26 - loss: 1.9952 - regression_loss: 1.6636 - classification_loss: 0.3316
 471/1500 [========>.....................] - ETA: 13:28 - loss: 1.9965 - regression_loss: 1.6646 - classification_loss: 0.3320
 472/1500 [========>.....................] - ETA: 13:26 - loss: 1.9983 - regression_loss: 1.6659 - classification_loss: 0.3324
 473/1500 [========>.....................] - ETA: 13:25 - loss: 1.9973 - regression_loss: 1.6644 - classification_loss: 0.3330
 474/1500 [========>.....................] - ETA: 13:26 - loss: 1.9972 - regression_loss: 1.6643 - classification_loss: 0.3329
 475/1500 [========>.....................] - ETA: 13:26 - loss: 1.9973 - regression_loss: 1.6645 - classification_loss: 0.3328
 476/1500 [========>.....................] - ETA: 13:25 - loss: 1.9981 - regression_loss: 1.6650 - classification_loss: 0.3331
 477/1500 [========>.....................] - ETA: 13:25 - loss: 1.9966 - regression_loss: 1.6639 - classification_loss: 0.3328
 478/1500 [========>.....................] - ETA: 13:24 - loss: 1.9946 - regression_loss: 1.6618 - classification_loss: 0.3329
 479/1500 [========>.....................] - ETA: 13:25 - loss: 1.9990 - regression_loss: 1.6658 - classification_loss: 0.3332
 480/1500 [========>.....................] - ETA: 13:24 - loss: 1.9986 - regression_loss: 1.6655 - classification_loss: 0.3330
 481/1500 [========>.....................] - ETA: 13:23 - loss: 1.9965 - regression_loss: 1.6638 - classification_loss: 0.3328
 482/1500 [========>.....................] - ETA: 13:23 - loss: 1.9947 - regression_loss: 1.6623 - classification_loss: 0.3325
 483/1500 [========>.....................] - ETA: 13:22 - loss: 1.9954 - regression_loss: 1.6628 - classification_loss: 0.3326
 484/1500 [========>.....................] - ETA: 13:21 - loss: 1.9985 - regression_loss: 1.6655 - classification_loss: 0.3330
 485/1500 [========>.....................] - ETA: 13:24 - loss: 1.9994 - regression_loss: 1.6663 - classification_loss: 0.3332
 486/1500 [========>.....................] - ETA: 13:23 - loss: 1.9990 - regression_loss: 1.6659 - classification_loss: 0.3331
 487/1500 [========>.....................] - ETA: 13:24 - loss: 1.9990 - regression_loss: 1.6657 - classification_loss: 0.3332
 488/1500 [========>.....................] - ETA: 13:23 - loss: 1.9987 - regression_loss: 1.6657 - classification_loss: 0.3330
 489/1500 [========>.....................] - ETA: 13:25 - loss: 1.9990 - regression_loss: 1.6659 - classification_loss: 0.3331
 490/1500 [========>.....................] - ETA: 13:26 - loss: 1.9983 - regression_loss: 1.6654 - classification_loss: 0.3329
 491/1500 [========>.....................] - ETA: 13:26 - loss: 1.9974 - regression_loss: 1.6646 - classification_loss: 0.3328
 492/1500 [========>.....................] - ETA: 13:24 - loss: 1.9969 - regression_loss: 1.6643 - classification_loss: 0.3326
 493/1500 [========>.....................] - ETA: 13:22 - loss: 1.9954 - regression_loss: 1.6632 - classification_loss: 0.3323
 494/1500 [========>.....................] - ETA: 13:21 - loss: 1.9953 - regression_loss: 1.6629 - classification_loss: 0.3324
 495/1500 [========>.....................] - ETA: 13:20 - loss: 1.9951 - regression_loss: 1.6629 - classification_loss: 0.3322
 496/1500 [========>.....................] - ETA: 13:21 - loss: 1.9943 - regression_loss: 1.6619 - classification_loss: 0.3324
 497/1500 [========>.....................] - ETA: 13:21 - loss: 1.9928 - regression_loss: 1.6606 - classification_loss: 0.3322
 498/1500 [========>.....................] - ETA: 13:21 - loss: 1.9932 - regression_loss: 1.6609 - classification_loss: 0.3323
 499/1500 [========>.....................] - ETA: 13:20 - loss: 1.9926 - regression_loss: 1.6606 - classification_loss: 0.3320
 500/1500 [=========>....................] - ETA: 13:18 - loss: 1.9916 - regression_loss: 1.6600 - classification_loss: 0.3316
 501/1500 [=========>....................] - ETA: 13:17 - loss: 1.9916 - regression_loss: 1.6603 - classification_loss: 0.3313
 502/1500 [=========>....................] - ETA: 13:17 - loss: 1.9909 - regression_loss: 1.6599 - classification_loss: 0.3310
 503/1500 [=========>....................] - ETA: 13:15 - loss: 1.9913 - regression_loss: 1.6605 - classification_loss: 0.3308
 504/1500 [=========>....................] - ETA: 13:14 - loss: 1.9921 - regression_loss: 1.6611 - classification_loss: 0.3310
 505/1500 [=========>....................] - ETA: 13:15 - loss: 1.9902 - regression_loss: 1.6594 - classification_loss: 0.3308
 506/1500 [=========>....................] - ETA: 13:14 - loss: 1.9901 - regression_loss: 1.6596 - classification_loss: 0.3306
 507/1500 [=========>....................] - ETA: 13:13 - loss: 1.9899 - regression_loss: 1.6597 - classification_loss: 0.3302
 508/1500 [=========>....................] - ETA: 13:12 - loss: 1.9902 - regression_loss: 1.6597 - classification_loss: 0.3305
 509/1500 [=========>....................] - ETA: 13:11 - loss: 1.9902 - regression_loss: 1.6599 - classification_loss: 0.3303
 510/1500 [=========>....................] - ETA: 13:09 - loss: 1.9901 - regression_loss: 1.6601 - classification_loss: 0.3300
 511/1500 [=========>....................] - ETA: 13:08 - loss: 1.9896 - regression_loss: 1.6597 - classification_loss: 0.3299
 512/1500 [=========>....................] - ETA: 13:11 - loss: 1.9891 - regression_loss: 1.6595 - classification_loss: 0.3296
 513/1500 [=========>....................] - ETA: 13:10 - loss: 1.9878 - regression_loss: 1.6583 - classification_loss: 0.3295
 514/1500 [=========>....................] - ETA: 13:10 - loss: 1.9895 - regression_loss: 1.6600 - classification_loss: 0.3294
 515/1500 [=========>....................] - ETA: 13:08 - loss: 1.9892 - regression_loss: 1.6599 - classification_loss: 0.3293
 516/1500 [=========>....................] - ETA: 13:07 - loss: 1.9888 - regression_loss: 1.6597 - classification_loss: 0.3291
 517/1500 [=========>....................] - ETA: 13:08 - loss: 1.9880 - regression_loss: 1.6591 - classification_loss: 0.3289
 518/1500 [=========>....................] - ETA: 13:07 - loss: 1.9881 - regression_loss: 1.6593 - classification_loss: 0.3288
 519/1500 [=========>....................] - ETA: 13:05 - loss: 1.9889 - regression_loss: 1.6601 - classification_loss: 0.3288
 520/1500 [=========>....................] - ETA: 13:04 - loss: 1.9892 - regression_loss: 1.6605 - classification_loss: 0.3287
 521/1500 [=========>....................] - ETA: 13:02 - loss: 1.9886 - regression_loss: 1.6602 - classification_loss: 0.3284
 522/1500 [=========>....................] - ETA: 13:01 - loss: 1.9885 - regression_loss: 1.6602 - classification_loss: 0.3283
 523/1500 [=========>....................] - ETA: 13:01 - loss: 1.9906 - regression_loss: 1.6617 - classification_loss: 0.3289
 524/1500 [=========>....................] - ETA: 13:01 - loss: 1.9918 - regression_loss: 1.6627 - classification_loss: 0.3291
 525/1500 [=========>....................] - ETA: 13:00 - loss: 1.9915 - regression_loss: 1.6626 - classification_loss: 0.3289
 526/1500 [=========>....................] - ETA: 12:59 - loss: 1.9916 - regression_loss: 1.6629 - classification_loss: 0.3287
 527/1500 [=========>....................] - ETA: 12:57 - loss: 1.9912 - regression_loss: 1.6626 - classification_loss: 0.3286
 528/1500 [=========>....................] - ETA: 12:56 - loss: 1.9895 - regression_loss: 1.6613 - classification_loss: 0.3282
 529/1500 [=========>....................] - ETA: 12:54 - loss: 1.9882 - regression_loss: 1.6602 - classification_loss: 0.3280
 530/1500 [=========>....................] - ETA: 12:52 - loss: 1.9892 - regression_loss: 1.6611 - classification_loss: 0.3281
 531/1500 [=========>....................] - ETA: 12:55 - loss: 1.9908 - regression_loss: 1.6622 - classification_loss: 0.3286
 532/1500 [=========>....................] - ETA: 12:55 - loss: 1.9911 - regression_loss: 1.6628 - classification_loss: 0.3284
 533/1500 [=========>....................] - ETA: 12:54 - loss: 1.9910 - regression_loss: 1.6627 - classification_loss: 0.3283
 534/1500 [=========>....................] - ETA: 12:52 - loss: 1.9889 - regression_loss: 1.6610 - classification_loss: 0.3279
 535/1500 [=========>....................] - ETA: 12:51 - loss: 1.9890 - regression_loss: 1.6613 - classification_loss: 0.3277
 536/1500 [=========>....................] - ETA: 12:49 - loss: 1.9883 - regression_loss: 1.6609 - classification_loss: 0.3275
 537/1500 [=========>....................] - ETA: 12:48 - loss: 1.9861 - regression_loss: 1.6590 - classification_loss: 0.3271
 538/1500 [=========>....................] - ETA: 12:46 - loss: 1.9854 - regression_loss: 1.6586 - classification_loss: 0.3268
 539/1500 [=========>....................] - ETA: 12:45 - loss: 1.9859 - regression_loss: 1.6591 - classification_loss: 0.3268
 540/1500 [=========>....................] - ETA: 12:44 - loss: 1.9863 - regression_loss: 1.6596 - classification_loss: 0.3268
 541/1500 [=========>....................] - ETA: 12:42 - loss: 1.9861 - regression_loss: 1.6595 - classification_loss: 0.3265
 542/1500 [=========>....................] - ETA: 12:41 - loss: 1.9854 - regression_loss: 1.6588 - classification_loss: 0.3265
 543/1500 [=========>....................] - ETA: 12:41 - loss: 1.9843 - regression_loss: 1.6579 - classification_loss: 0.3264
 544/1500 [=========>....................] - ETA: 12:39 - loss: 1.9845 - regression_loss: 1.6582 - classification_loss: 0.3263
 545/1500 [=========>....................] - ETA: 12:38 - loss: 1.9836 - regression_loss: 1.6575 - classification_loss: 0.3261
 546/1500 [=========>....................] - ETA: 12:36 - loss: 1.9824 - regression_loss: 1.6566 - classification_loss: 0.3258
 547/1500 [=========>....................] - ETA: 12:35 - loss: 1.9826 - regression_loss: 1.6566 - classification_loss: 0.3260
 548/1500 [=========>....................] - ETA: 12:33 - loss: 1.9806 - regression_loss: 1.6549 - classification_loss: 0.3257
 549/1500 [=========>....................] - ETA: 12:32 - loss: 1.9803 - regression_loss: 1.6548 - classification_loss: 0.3256
 550/1500 [==========>...................] - ETA: 12:31 - loss: 1.9805 - regression_loss: 1.6548 - classification_loss: 0.3257
 551/1500 [==========>...................] - ETA: 12:29 - loss: 1.9806 - regression_loss: 1.6549 - classification_loss: 0.3257
 552/1500 [==========>...................] - ETA: 12:28 - loss: 1.9803 - regression_loss: 1.6545 - classification_loss: 0.3258
 553/1500 [==========>...................] - ETA: 12:26 - loss: 1.9804 - regression_loss: 1.6548 - classification_loss: 0.3257
 554/1500 [==========>...................] - ETA: 12:26 - loss: 1.9812 - regression_loss: 1.6554 - classification_loss: 0.3258
 555/1500 [==========>...................] - ETA: 12:25 - loss: 1.9795 - regression_loss: 1.6540 - classification_loss: 0.3255
 556/1500 [==========>...................] - ETA: 12:24 - loss: 1.9798 - regression_loss: 1.6542 - classification_loss: 0.3256
 557/1500 [==========>...................] - ETA: 12:23 - loss: 1.9786 - regression_loss: 1.6532 - classification_loss: 0.3254
 558/1500 [==========>...................] - ETA: 12:22 - loss: 1.9786 - regression_loss: 1.6534 - classification_loss: 0.3252
 559/1500 [==========>...................] - ETA: 12:21 - loss: 1.9798 - regression_loss: 1.6545 - classification_loss: 0.3254
 560/1500 [==========>...................] - ETA: 12:19 - loss: 1.9795 - regression_loss: 1.6542 - classification_loss: 0.3252
 561/1500 [==========>...................] - ETA: 12:18 - loss: 1.9788 - regression_loss: 1.6537 - classification_loss: 0.3251
 562/1500 [==========>...................] - ETA: 12:17 - loss: 1.9810 - regression_loss: 1.6559 - classification_loss: 0.3251
 563/1500 [==========>...................] - ETA: 12:15 - loss: 1.9800 - regression_loss: 1.6552 - classification_loss: 0.3248
 564/1500 [==========>...................] - ETA: 12:15 - loss: 1.9789 - regression_loss: 1.6546 - classification_loss: 0.3244
 565/1500 [==========>...................] - ETA: 12:13 - loss: 1.9773 - regression_loss: 1.6532 - classification_loss: 0.3240
 566/1500 [==========>...................] - ETA: 12:13 - loss: 1.9759 - regression_loss: 1.6519 - classification_loss: 0.3240
 567/1500 [==========>...................] - ETA: 12:12 - loss: 1.9746 - regression_loss: 1.6509 - classification_loss: 0.3237
 568/1500 [==========>...................] - ETA: 12:11 - loss: 1.9754 - regression_loss: 1.6518 - classification_loss: 0.3237
 569/1500 [==========>...................] - ETA: 12:10 - loss: 1.9738 - regression_loss: 1.6505 - classification_loss: 0.3233
 570/1500 [==========>...................] - ETA: 12:10 - loss: 1.9724 - regression_loss: 1.6494 - classification_loss: 0.3230
 571/1500 [==========>...................] - ETA: 12:10 - loss: 1.9736 - regression_loss: 1.6505 - classification_loss: 0.3231
 572/1500 [==========>...................] - ETA: 12:09 - loss: 1.9735 - regression_loss: 1.6506 - classification_loss: 0.3230
 573/1500 [==========>...................] - ETA: 12:08 - loss: 1.9738 - regression_loss: 1.6505 - classification_loss: 0.3232
 574/1500 [==========>...................] - ETA: 12:08 - loss: 1.9732 - regression_loss: 1.6502 - classification_loss: 0.3230
 575/1500 [==========>...................] - ETA: 12:07 - loss: 1.9719 - regression_loss: 1.6493 - classification_loss: 0.3226
 576/1500 [==========>...................] - ETA: 12:06 - loss: 1.9729 - regression_loss: 1.6504 - classification_loss: 0.3225
 577/1500 [==========>...................] - ETA: 12:04 - loss: 1.9731 - regression_loss: 1.6505 - classification_loss: 0.3226
 578/1500 [==========>...................] - ETA: 12:04 - loss: 1.9747 - regression_loss: 1.6522 - classification_loss: 0.3225
 579/1500 [==========>...................] - ETA: 12:03 - loss: 1.9759 - regression_loss: 1.6532 - classification_loss: 0.3227
 580/1500 [==========>...................] - ETA: 12:01 - loss: 1.9751 - regression_loss: 1.6526 - classification_loss: 0.3225
 581/1500 [==========>...................] - ETA: 12:00 - loss: 1.9750 - regression_loss: 1.6525 - classification_loss: 0.3225
 582/1500 [==========>...................] - ETA: 11:59 - loss: 1.9744 - regression_loss: 1.6521 - classification_loss: 0.3223
 583/1500 [==========>...................] - ETA: 11:58 - loss: 1.9728 - regression_loss: 1.6508 - classification_loss: 0.3220
 584/1500 [==========>...................] - ETA: 11:56 - loss: 1.9725 - regression_loss: 1.6507 - classification_loss: 0.3218
 585/1500 [==========>...................] - ETA: 11:55 - loss: 1.9715 - regression_loss: 1.6499 - classification_loss: 0.3216
 586/1500 [==========>...................] - ETA: 11:54 - loss: 1.9708 - regression_loss: 1.6495 - classification_loss: 0.3213
 587/1500 [==========>...................] - ETA: 11:52 - loss: 1.9708 - regression_loss: 1.6496 - classification_loss: 0.3212
 588/1500 [==========>...................] - ETA: 11:52 - loss: 1.9698 - regression_loss: 1.6488 - classification_loss: 0.3210
 589/1500 [==========>...................] - ETA: 11:50 - loss: 1.9692 - regression_loss: 1.6484 - classification_loss: 0.3209
 590/1500 [==========>...................] - ETA: 11:50 - loss: 1.9707 - regression_loss: 1.6494 - classification_loss: 0.3213
 591/1500 [==========>...................] - ETA: 11:49 - loss: 1.9697 - regression_loss: 1.6487 - classification_loss: 0.3210
 592/1500 [==========>...................] - ETA: 11:48 - loss: 1.9681 - regression_loss: 1.6473 - classification_loss: 0.3208
 593/1500 [==========>...................] - ETA: 11:46 - loss: 1.9690 - regression_loss: 1.6479 - classification_loss: 0.3211
 594/1500 [==========>...................] - ETA: 11:45 - loss: 1.9673 - regression_loss: 1.6466 - classification_loss: 0.3207
 595/1500 [==========>...................] - ETA: 11:44 - loss: 1.9665 - regression_loss: 1.6458 - classification_loss: 0.3207
 596/1500 [==========>...................] - ETA: 11:43 - loss: 1.9664 - regression_loss: 1.6453 - classification_loss: 0.3211
 597/1500 [==========>...................] - ETA: 11:42 - loss: 1.9665 - regression_loss: 1.6455 - classification_loss: 0.3210
 598/1500 [==========>...................] - ETA: 11:41 - loss: 1.9668 - regression_loss: 1.6458 - classification_loss: 0.3210
 599/1500 [==========>...................] - ETA: 11:39 - loss: 1.9652 - regression_loss: 1.6445 - classification_loss: 0.3207
 600/1500 [===========>..................] - ETA: 11:39 - loss: 1.9646 - regression_loss: 1.6442 - classification_loss: 0.3204
 601/1500 [===========>..................] - ETA: 11:38 - loss: 1.9645 - regression_loss: 1.6442 - classification_loss: 0.3203
 602/1500 [===========>..................] - ETA: 11:37 - loss: 1.9659 - regression_loss: 1.6452 - classification_loss: 0.3206
 603/1500 [===========>..................] - ETA: 11:38 - loss: 1.9660 - regression_loss: 1.6453 - classification_loss: 0.3207
 604/1500 [===========>..................] - ETA: 11:37 - loss: 1.9647 - regression_loss: 1.6444 - classification_loss: 0.3204
 605/1500 [===========>..................] - ETA: 11:36 - loss: 1.9645 - regression_loss: 1.6442 - classification_loss: 0.3202
 606/1500 [===========>..................] - ETA: 11:35 - loss: 1.9641 - regression_loss: 1.6440 - classification_loss: 0.3201
 607/1500 [===========>..................] - ETA: 11:35 - loss: 1.9626 - regression_loss: 1.6427 - classification_loss: 0.3198
 608/1500 [===========>..................] - ETA: 11:34 - loss: 1.9624 - regression_loss: 1.6426 - classification_loss: 0.3198
 609/1500 [===========>..................] - ETA: 11:32 - loss: 1.9610 - regression_loss: 1.6415 - classification_loss: 0.3196
 610/1500 [===========>..................] - ETA: 11:31 - loss: 1.9596 - regression_loss: 1.6402 - classification_loss: 0.3194
 611/1500 [===========>..................] - ETA: 11:30 - loss: 1.9608 - regression_loss: 1.6413 - classification_loss: 0.3195
 612/1500 [===========>..................] - ETA: 11:30 - loss: 1.9623 - regression_loss: 1.6423 - classification_loss: 0.3200
 613/1500 [===========>..................] - ETA: 11:29 - loss: 1.9633 - regression_loss: 1.6429 - classification_loss: 0.3204
 614/1500 [===========>..................] - ETA: 11:28 - loss: 1.9622 - regression_loss: 1.6418 - classification_loss: 0.3203
 615/1500 [===========>..................] - ETA: 11:28 - loss: 1.9623 - regression_loss: 1.6420 - classification_loss: 0.3203
 616/1500 [===========>..................] - ETA: 11:26 - loss: 1.9621 - regression_loss: 1.6417 - classification_loss: 0.3203
 617/1500 [===========>..................] - ETA: 11:25 - loss: 1.9626 - regression_loss: 1.6426 - classification_loss: 0.3201
 618/1500 [===========>..................] - ETA: 11:24 - loss: 1.9637 - regression_loss: 1.6436 - classification_loss: 0.3201
 619/1500 [===========>..................] - ETA: 11:23 - loss: 1.9640 - regression_loss: 1.6441 - classification_loss: 0.3200
 620/1500 [===========>..................] - ETA: 11:22 - loss: 1.9644 - regression_loss: 1.6445 - classification_loss: 0.3199
 621/1500 [===========>..................] - ETA: 11:20 - loss: 1.9649 - regression_loss: 1.6450 - classification_loss: 0.3199
 622/1500 [===========>..................] - ETA: 11:19 - loss: 1.9654 - regression_loss: 1.6455 - classification_loss: 0.3199
 623/1500 [===========>..................] - ETA: 11:18 - loss: 1.9642 - regression_loss: 1.6447 - classification_loss: 0.3196
 624/1500 [===========>..................] - ETA: 11:17 - loss: 1.9643 - regression_loss: 1.6449 - classification_loss: 0.3195
 625/1500 [===========>..................] - ETA: 11:16 - loss: 1.9638 - regression_loss: 1.6445 - classification_loss: 0.3192
 626/1500 [===========>..................] - ETA: 11:15 - loss: 1.9635 - regression_loss: 1.6443 - classification_loss: 0.3192
 627/1500 [===========>..................] - ETA: 11:15 - loss: 1.9634 - regression_loss: 1.6440 - classification_loss: 0.3194
 628/1500 [===========>..................] - ETA: 11:14 - loss: 1.9634 - regression_loss: 1.6441 - classification_loss: 0.3193
 629/1500 [===========>..................] - ETA: 11:13 - loss: 1.9635 - regression_loss: 1.6442 - classification_loss: 0.3193
 630/1500 [===========>..................] - ETA: 11:11 - loss: 1.9653 - regression_loss: 1.6454 - classification_loss: 0.3199
 631/1500 [===========>..................] - ETA: 11:10 - loss: 1.9653 - regression_loss: 1.6453 - classification_loss: 0.3200
 632/1500 [===========>..................] - ETA: 11:09 - loss: 1.9647 - regression_loss: 1.6449 - classification_loss: 0.3198
 633/1500 [===========>..................] - ETA: 11:08 - loss: 1.9655 - regression_loss: 1.6456 - classification_loss: 0.3199
 634/1500 [===========>..................] - ETA: 11:07 - loss: 1.9636 - regression_loss: 1.6441 - classification_loss: 0.3195
 635/1500 [===========>..................] - ETA: 11:06 - loss: 1.9631 - regression_loss: 1.6438 - classification_loss: 0.3193
 636/1500 [===========>..................] - ETA: 11:04 - loss: 1.9634 - regression_loss: 1.6441 - classification_loss: 0.3193
 637/1500 [===========>..................] - ETA: 11:03 - loss: 1.9632 - regression_loss: 1.6440 - classification_loss: 0.3192
 638/1500 [===========>..................] - ETA: 11:02 - loss: 1.9619 - regression_loss: 1.6429 - classification_loss: 0.3190
 639/1500 [===========>..................] - ETA: 11:01 - loss: 1.9609 - regression_loss: 1.6422 - classification_loss: 0.3187
 640/1500 [===========>..................] - ETA: 11:00 - loss: 1.9610 - regression_loss: 1.6424 - classification_loss: 0.3187
 641/1500 [===========>..................] - ETA: 11:00 - loss: 1.9617 - regression_loss: 1.6429 - classification_loss: 0.3189
 642/1500 [===========>..................] - ETA: 10:59 - loss: 1.9623 - regression_loss: 1.6435 - classification_loss: 0.3188
 643/1500 [===========>..................] - ETA: 10:58 - loss: 1.9613 - regression_loss: 1.6428 - classification_loss: 0.3185
 644/1500 [===========>..................] - ETA: 10:59 - loss: 1.9602 - regression_loss: 1.6418 - classification_loss: 0.3184
 645/1500 [===========>..................] - ETA: 10:57 - loss: 1.9588 - regression_loss: 1.6406 - classification_loss: 0.3182
 646/1500 [===========>..................] - ETA: 10:58 - loss: 1.9593 - regression_loss: 1.6411 - classification_loss: 0.3182
 647/1500 [===========>..................] - ETA: 10:56 - loss: 1.9605 - regression_loss: 1.6424 - classification_loss: 0.3181
 648/1500 [===========>..................] - ETA: 10:55 - loss: 1.9593 - regression_loss: 1.6412 - classification_loss: 0.3180
 649/1500 [===========>..................] - ETA: 10:56 - loss: 1.9602 - regression_loss: 1.6419 - classification_loss: 0.3182
 650/1500 [============>.................] - ETA: 10:55 - loss: 1.9593 - regression_loss: 1.6414 - classification_loss: 0.3180
 651/1500 [============>.................] - ETA: 10:54 - loss: 1.9606 - regression_loss: 1.6425 - classification_loss: 0.3182
 652/1500 [============>.................] - ETA: 10:53 - loss: 1.9620 - regression_loss: 1.6438 - classification_loss: 0.3182
 653/1500 [============>.................] - ETA: 10:52 - loss: 1.9613 - regression_loss: 1.6431 - classification_loss: 0.3181
 654/1500 [============>.................] - ETA: 10:52 - loss: 1.9606 - regression_loss: 1.6427 - classification_loss: 0.3179
 655/1500 [============>.................] - ETA: 10:51 - loss: 1.9593 - regression_loss: 1.6416 - classification_loss: 0.3176
 656/1500 [============>.................] - ETA: 10:49 - loss: 1.9587 - regression_loss: 1.6413 - classification_loss: 0.3174
 657/1500 [============>.................] - ETA: 10:48 - loss: 1.9573 - regression_loss: 1.6402 - classification_loss: 0.3171
 658/1500 [============>.................] - ETA: 10:47 - loss: 1.9568 - regression_loss: 1.6398 - classification_loss: 0.3169
 659/1500 [============>.................] - ETA: 10:46 - loss: 1.9559 - regression_loss: 1.6390 - classification_loss: 0.3169
 660/1500 [============>.................] - ETA: 10:45 - loss: 1.9565 - regression_loss: 1.6393 - classification_loss: 0.3172
 661/1500 [============>.................] - ETA: 10:44 - loss: 1.9563 - regression_loss: 1.6393 - classification_loss: 0.3170
 662/1500 [============>.................] - ETA: 10:44 - loss: 1.9552 - regression_loss: 1.6384 - classification_loss: 0.3168
 663/1500 [============>.................] - ETA: 10:43 - loss: 1.9556 - regression_loss: 1.6387 - classification_loss: 0.3169
 664/1500 [============>.................] - ETA: 10:43 - loss: 1.9548 - regression_loss: 1.6381 - classification_loss: 0.3167
 665/1500 [============>.................] - ETA: 10:42 - loss: 1.9539 - regression_loss: 1.6372 - classification_loss: 0.3167
 666/1500 [============>.................] - ETA: 10:42 - loss: 1.9527 - regression_loss: 1.6362 - classification_loss: 0.3165
 667/1500 [============>.................] - ETA: 10:43 - loss: 1.9537 - regression_loss: 1.6368 - classification_loss: 0.3168
 668/1500 [============>.................] - ETA: 10:42 - loss: 1.9545 - regression_loss: 1.6378 - classification_loss: 0.3167
 669/1500 [============>.................] - ETA: 10:41 - loss: 1.9536 - regression_loss: 1.6372 - classification_loss: 0.3164
 670/1500 [============>.................] - ETA: 10:41 - loss: 1.9545 - regression_loss: 1.6380 - classification_loss: 0.3165
 671/1500 [============>.................] - ETA: 10:40 - loss: 1.9537 - regression_loss: 1.6375 - classification_loss: 0.3162
 672/1500 [============>.................] - ETA: 10:39 - loss: 1.9526 - regression_loss: 1.6367 - classification_loss: 0.3159
 673/1500 [============>.................] - ETA: 10:38 - loss: 1.9539 - regression_loss: 1.6378 - classification_loss: 0.3161
 674/1500 [============>.................] - ETA: 10:39 - loss: 1.9547 - regression_loss: 1.6383 - classification_loss: 0.3164
 675/1500 [============>.................] - ETA: 10:38 - loss: 1.9552 - regression_loss: 1.6388 - classification_loss: 0.3164
 676/1500 [============>.................] - ETA: 10:37 - loss: 1.9547 - regression_loss: 1.6384 - classification_loss: 0.3163
 677/1500 [============>.................] - ETA: 10:37 - loss: 1.9527 - regression_loss: 1.6367 - classification_loss: 0.3159
 678/1500 [============>.................] - ETA: 10:36 - loss: 1.9514 - regression_loss: 1.6357 - classification_loss: 0.3157
 679/1500 [============>.................] - ETA: 10:37 - loss: 1.9519 - regression_loss: 1.6361 - classification_loss: 0.3158
 680/1500 [============>.................] - ETA: 10:36 - loss: 1.9520 - regression_loss: 1.6362 - classification_loss: 0.3158
 681/1500 [============>.................] - ETA: 10:36 - loss: 1.9514 - regression_loss: 1.6355 - classification_loss: 0.3160
 682/1500 [============>.................] - ETA: 10:35 - loss: 1.9499 - regression_loss: 1.6342 - classification_loss: 0.3157
 683/1500 [============>.................] - ETA: 10:35 - loss: 1.9513 - regression_loss: 1.6355 - classification_loss: 0.3159
 684/1500 [============>.................] - ETA: 10:34 - loss: 1.9514 - regression_loss: 1.6355 - classification_loss: 0.3158
 685/1500 [============>.................] - ETA: 10:33 - loss: 1.9515 - regression_loss: 1.6356 - classification_loss: 0.3159
 686/1500 [============>.................] - ETA: 10:31 - loss: 1.9506 - regression_loss: 1.6349 - classification_loss: 0.3157
 687/1500 [============>.................] - ETA: 10:30 - loss: 1.9497 - regression_loss: 1.6340 - classification_loss: 0.3157
 688/1500 [============>.................] - ETA: 10:29 - loss: 1.9490 - regression_loss: 1.6334 - classification_loss: 0.3155
 689/1500 [============>.................] - ETA: 10:28 - loss: 1.9499 - regression_loss: 1.6343 - classification_loss: 0.3157
 690/1500 [============>.................] - ETA: 10:28 - loss: 1.9514 - regression_loss: 1.6354 - classification_loss: 0.3161
 691/1500 [============>.................] - ETA: 10:27 - loss: 1.9523 - regression_loss: 1.6359 - classification_loss: 0.3164
 692/1500 [============>.................] - ETA: 10:26 - loss: 1.9512 - regression_loss: 1.6350 - classification_loss: 0.3163
 693/1500 [============>.................] - ETA: 10:25 - loss: 1.9498 - regression_loss: 1.6339 - classification_loss: 0.3160
 694/1500 [============>.................] - ETA: 10:24 - loss: 1.9497 - regression_loss: 1.6339 - classification_loss: 0.3159
 695/1500 [============>.................] - ETA: 10:23 - loss: 1.9489 - regression_loss: 1.6331 - classification_loss: 0.3158
 696/1500 [============>.................] - ETA: 10:24 - loss: 1.9502 - regression_loss: 1.6342 - classification_loss: 0.3161
 697/1500 [============>.................] - ETA: 10:24 - loss: 1.9494 - regression_loss: 1.6335 - classification_loss: 0.3158
 698/1500 [============>.................] - ETA: 10:23 - loss: 1.9500 - regression_loss: 1.6339 - classification_loss: 0.3161
 699/1500 [============>.................] - ETA: 10:22 - loss: 1.9489 - regression_loss: 1.6330 - classification_loss: 0.3159
 700/1500 [=============>................] - ETA: 10:22 - loss: 1.9497 - regression_loss: 1.6336 - classification_loss: 0.3162
 701/1500 [=============>................] - ETA: 10:21 - loss: 1.9500 - regression_loss: 1.6338 - classification_loss: 0.3162
 702/1500 [=============>................] - ETA: 10:21 - loss: 1.9495 - regression_loss: 1.6333 - classification_loss: 0.3161
 703/1500 [=============>................] - ETA: 10:19 - loss: 1.9494 - regression_loss: 1.6333 - classification_loss: 0.3161
 704/1500 [=============>................] - ETA: 10:19 - loss: 1.9497 - regression_loss: 1.6337 - classification_loss: 0.3160
 705/1500 [=============>................] - ETA: 10:18 - loss: 1.9491 - regression_loss: 1.6333 - classification_loss: 0.3158
 706/1500 [=============>................] - ETA: 10:18 - loss: 1.9485 - regression_loss: 1.6330 - classification_loss: 0.3155
 707/1500 [=============>................] - ETA: 10:17 - loss: 1.9492 - regression_loss: 1.6336 - classification_loss: 0.3156
 708/1500 [=============>................] - ETA: 10:17 - loss: 1.9484 - regression_loss: 1.6328 - classification_loss: 0.3156
 709/1500 [=============>................] - ETA: 10:16 - loss: 1.9475 - regression_loss: 1.6318 - classification_loss: 0.3158
 710/1500 [=============>................] - ETA: 10:15 - loss: 1.9486 - regression_loss: 1.6329 - classification_loss: 0.3157
 711/1500 [=============>................] - ETA: 10:15 - loss: 1.9487 - regression_loss: 1.6329 - classification_loss: 0.3158
 712/1500 [=============>................] - ETA: 10:15 - loss: 1.9493 - regression_loss: 1.6336 - classification_loss: 0.3157
 713/1500 [=============>................] - ETA: 10:15 - loss: 1.9493 - regression_loss: 1.6334 - classification_loss: 0.3159
 714/1500 [=============>................] - ETA: 10:13 - loss: 1.9477 - regression_loss: 1.6321 - classification_loss: 0.3156
 715/1500 [=============>................] - ETA: 10:12 - loss: 1.9478 - regression_loss: 1.6321 - classification_loss: 0.3157
 716/1500 [=============>................] - ETA: 10:11 - loss: 1.9483 - regression_loss: 1.6324 - classification_loss: 0.3159
 717/1500 [=============>................] - ETA: 10:10 - loss: 1.9467 - regression_loss: 1.6311 - classification_loss: 0.3155
 718/1500 [=============>................] - ETA: 10:09 - loss: 1.9455 - regression_loss: 1.6301 - classification_loss: 0.3153
 719/1500 [=============>................] - ETA: 10:09 - loss: 1.9452 - regression_loss: 1.6300 - classification_loss: 0.3151
 720/1500 [=============>................] - ETA: 10:09 - loss: 1.9448 - regression_loss: 1.6297 - classification_loss: 0.3150
 721/1500 [=============>................] - ETA: 10:09 - loss: 1.9463 - regression_loss: 1.6310 - classification_loss: 0.3154
 722/1500 [=============>................] - ETA: 10:08 - loss: 1.9455 - regression_loss: 1.6304 - classification_loss: 0.3152
 723/1500 [=============>................] - ETA: 10:07 - loss: 1.9450 - regression_loss: 1.6300 - classification_loss: 0.3150
 724/1500 [=============>................] - ETA: 10:06 - loss: 1.9429 - regression_loss: 1.6282 - classification_loss: 0.3147
 725/1500 [=============>................] - ETA: 10:04 - loss: 1.9431 - regression_loss: 1.6285 - classification_loss: 0.3146
 726/1500 [=============>................] - ETA: 10:04 - loss: 1.9439 - regression_loss: 1.6292 - classification_loss: 0.3146
 727/1500 [=============>................] - ETA: 10:03 - loss: 1.9425 - regression_loss: 1.6281 - classification_loss: 0.3144
 728/1500 [=============>................] - ETA: 10:02 - loss: 1.9421 - regression_loss: 1.6278 - classification_loss: 0.3143
 729/1500 [=============>................] - ETA: 10:01 - loss: 1.9411 - regression_loss: 1.6269 - classification_loss: 0.3142
 730/1500 [=============>................] - ETA: 10:01 - loss: 1.9402 - regression_loss: 1.6262 - classification_loss: 0.3140
 731/1500 [=============>................] - ETA: 10:00 - loss: 1.9398 - regression_loss: 1.6260 - classification_loss: 0.3138
 732/1500 [=============>................] - ETA: 9:59 - loss: 1.9387 - regression_loss: 1.6251 - classification_loss: 0.3136 
 733/1500 [=============>................] - ETA: 9:58 - loss: 1.9392 - regression_loss: 1.6255 - classification_loss: 0.3137
 734/1500 [=============>................] - ETA: 9:59 - loss: 1.9381 - regression_loss: 1.6247 - classification_loss: 0.3134
 735/1500 [=============>................] - ETA: 9:59 - loss: 1.9389 - regression_loss: 1.6253 - classification_loss: 0.3136
 736/1500 [=============>................] - ETA: 9:58 - loss: 1.9377 - regression_loss: 1.6244 - classification_loss: 0.3133
 737/1500 [=============>................] - ETA: 9:57 - loss: 1.9368 - regression_loss: 1.6237 - classification_loss: 0.3131
 738/1500 [=============>................] - ETA: 9:57 - loss: 1.9374 - regression_loss: 1.6241 - classification_loss: 0.3134
 739/1500 [=============>................] - ETA: 9:57 - loss: 1.9380 - regression_loss: 1.6247 - classification_loss: 0.3133
 740/1500 [=============>................] - ETA: 9:55 - loss: 1.9375 - regression_loss: 1.6244 - classification_loss: 0.3131
 741/1500 [=============>................] - ETA: 9:55 - loss: 1.9387 - regression_loss: 1.6255 - classification_loss: 0.3132
 742/1500 [=============>................] - ETA: 9:54 - loss: 1.9379 - regression_loss: 1.6249 - classification_loss: 0.3130
 743/1500 [=============>................] - ETA: 9:54 - loss: 1.9362 - regression_loss: 1.6235 - classification_loss: 0.3127
 744/1500 [=============>................] - ETA: 9:53 - loss: 1.9360 - regression_loss: 1.6235 - classification_loss: 0.3126
 745/1500 [=============>................] - ETA: 9:52 - loss: 1.9353 - regression_loss: 1.6229 - classification_loss: 0.3125
 746/1500 [=============>................] - ETA: 9:52 - loss: 1.9343 - regression_loss: 1.6220 - classification_loss: 0.3123
 747/1500 [=============>................] - ETA: 9:50 - loss: 1.9346 - regression_loss: 1.6223 - classification_loss: 0.3123
 748/1500 [=============>................] - ETA: 9:50 - loss: 1.9335 - regression_loss: 1.6214 - classification_loss: 0.3121
 749/1500 [=============>................] - ETA: 9:49 - loss: 1.9333 - regression_loss: 1.6212 - classification_loss: 0.3121
 750/1500 [==============>...............] - ETA: 9:47 - loss: 1.9333 - regression_loss: 1.6212 - classification_loss: 0.3121
 751/1500 [==============>...............] - ETA: 9:46 - loss: 1.9321 - regression_loss: 1.6202 - classification_loss: 0.3119
 752/1500 [==============>...............] - ETA: 9:45 - loss: 1.9323 - regression_loss: 1.6203 - classification_loss: 0.3120
 753/1500 [==============>...............] - ETA: 9:44 - loss: 1.9315 - regression_loss: 1.6197 - classification_loss: 0.3118
 754/1500 [==============>...............] - ETA: 9:43 - loss: 1.9305 - regression_loss: 1.6188 - classification_loss: 0.3117
 755/1500 [==============>...............] - ETA: 9:42 - loss: 1.9291 - regression_loss: 1.6176 - classification_loss: 0.3114
 756/1500 [==============>...............] - ETA: 9:41 - loss: 1.9300 - regression_loss: 1.6183 - classification_loss: 0.3117
 757/1500 [==============>...............] - ETA: 9:43 - loss: 1.9300 - regression_loss: 1.6185 - classification_loss: 0.3116
 758/1500 [==============>...............] - ETA: 9:42 - loss: 1.9293 - regression_loss: 1.6176 - classification_loss: 0.3116
 759/1500 [==============>...............] - ETA: 9:41 - loss: 1.9302 - regression_loss: 1.6186 - classification_loss: 0.3116
 760/1500 [==============>...............] - ETA: 9:41 - loss: 1.9318 - regression_loss: 1.6202 - classification_loss: 0.3116
 761/1500 [==============>...............] - ETA: 9:40 - loss: 1.9333 - regression_loss: 1.6213 - classification_loss: 0.3120
 762/1500 [==============>...............] - ETA: 9:39 - loss: 1.9329 - regression_loss: 1.6205 - classification_loss: 0.3124
 763/1500 [==============>...............] - ETA: 9:38 - loss: 1.9328 - regression_loss: 1.6205 - classification_loss: 0.3123
 764/1500 [==============>...............] - ETA: 9:37 - loss: 1.9345 - regression_loss: 1.6217 - classification_loss: 0.3128
 765/1500 [==============>...............] - ETA: 9:36 - loss: 1.9333 - regression_loss: 1.6207 - classification_loss: 0.3126
 766/1500 [==============>...............] - ETA: 9:35 - loss: 1.9336 - regression_loss: 1.6211 - classification_loss: 0.3126
 767/1500 [==============>...............] - ETA: 9:34 - loss: 1.9335 - regression_loss: 1.6210 - classification_loss: 0.3124
 768/1500 [==============>...............] - ETA: 9:33 - loss: 1.9334 - regression_loss: 1.6210 - classification_loss: 0.3124
 769/1500 [==============>...............] - ETA: 9:32 - loss: 1.9326 - regression_loss: 1.6204 - classification_loss: 0.3122
 770/1500 [==============>...............] - ETA: 9:31 - loss: 1.9328 - regression_loss: 1.6207 - classification_loss: 0.3121
 771/1500 [==============>...............] - ETA: 9:29 - loss: 1.9327 - regression_loss: 1.6206 - classification_loss: 0.3121
 772/1500 [==============>...............] - ETA: 9:28 - loss: 1.9337 - regression_loss: 1.6215 - classification_loss: 0.3123
 773/1500 [==============>...............] - ETA: 9:27 - loss: 1.9324 - regression_loss: 1.6204 - classification_loss: 0.3120
 774/1500 [==============>...............] - ETA: 9:27 - loss: 1.9321 - regression_loss: 1.6202 - classification_loss: 0.3119
 775/1500 [==============>...............] - ETA: 9:26 - loss: 1.9321 - regression_loss: 1.6203 - classification_loss: 0.3118
 776/1500 [==============>...............] - ETA: 9:25 - loss: 1.9317 - regression_loss: 1.6199 - classification_loss: 0.3119
 777/1500 [==============>...............] - ETA: 9:25 - loss: 1.9328 - regression_loss: 1.6209 - classification_loss: 0.3120
 778/1500 [==============>...............] - ETA: 9:23 - loss: 1.9324 - regression_loss: 1.6206 - classification_loss: 0.3118
 779/1500 [==============>...............] - ETA: 9:22 - loss: 1.9321 - regression_loss: 1.6204 - classification_loss: 0.3118
 780/1500 [==============>...............] - ETA: 9:21 - loss: 1.9312 - regression_loss: 1.6197 - classification_loss: 0.3116
 781/1500 [==============>...............] - ETA: 9:21 - loss: 1.9323 - regression_loss: 1.6206 - classification_loss: 0.3117
 782/1500 [==============>...............] - ETA: 9:20 - loss: 1.9312 - regression_loss: 1.6197 - classification_loss: 0.3115
 783/1500 [==============>...............] - ETA: 9:19 - loss: 1.9305 - regression_loss: 1.6192 - classification_loss: 0.3113
 784/1500 [==============>...............] - ETA: 9:18 - loss: 1.9295 - regression_loss: 1.6185 - classification_loss: 0.3110
 785/1500 [==============>...............] - ETA: 9:18 - loss: 1.9303 - regression_loss: 1.6190 - classification_loss: 0.3114
 786/1500 [==============>...............] - ETA: 9:17 - loss: 1.9310 - regression_loss: 1.6187 - classification_loss: 0.3124
 787/1500 [==============>...............] - ETA: 9:16 - loss: 1.9306 - regression_loss: 1.6184 - classification_loss: 0.3122
 788/1500 [==============>...............] - ETA: 9:16 - loss: 1.9315 - regression_loss: 1.6193 - classification_loss: 0.3123
 789/1500 [==============>...............] - ETA: 9:15 - loss: 1.9312 - regression_loss: 1.6189 - classification_loss: 0.3122
 790/1500 [==============>...............] - ETA: 9:14 - loss: 1.9296 - regression_loss: 1.6176 - classification_loss: 0.3119
 791/1500 [==============>...............] - ETA: 9:13 - loss: 1.9293 - regression_loss: 1.6173 - classification_loss: 0.3120
 792/1500 [==============>...............] - ETA: 9:12 - loss: 1.9290 - regression_loss: 1.6172 - classification_loss: 0.3118
 793/1500 [==============>...............] - ETA: 9:12 - loss: 1.9286 - regression_loss: 1.6167 - classification_loss: 0.3119
 794/1500 [==============>...............] - ETA: 9:11 - loss: 1.9279 - regression_loss: 1.6161 - classification_loss: 0.3118
 795/1500 [==============>...............] - ETA: 9:10 - loss: 1.9277 - regression_loss: 1.6160 - classification_loss: 0.3117
 796/1500 [==============>...............] - ETA: 9:10 - loss: 1.9271 - regression_loss: 1.6155 - classification_loss: 0.3116
 797/1500 [==============>...............] - ETA: 9:11 - loss: 1.9275 - regression_loss: 1.6160 - classification_loss: 0.3115
 798/1500 [==============>...............] - ETA: 9:10 - loss: 1.9278 - regression_loss: 1.6164 - classification_loss: 0.3114
 799/1500 [==============>...............] - ETA: 9:09 - loss: 1.9275 - regression_loss: 1.6162 - classification_loss: 0.3112
 800/1500 [===============>..............] - ETA: 9:08 - loss: 1.9274 - regression_loss: 1.6162 - classification_loss: 0.3112
 801/1500 [===============>..............] - ETA: 9:08 - loss: 1.9278 - regression_loss: 1.6167 - classification_loss: 0.3111
 802/1500 [===============>..............] - ETA: 9:07 - loss: 1.9270 - regression_loss: 1.6162 - classification_loss: 0.3108
 803/1500 [===============>..............] - ETA: 9:06 - loss: 1.9279 - regression_loss: 1.6171 - classification_loss: 0.3109
 804/1500 [===============>..............] - ETA: 9:06 - loss: 1.9273 - regression_loss: 1.6162 - classification_loss: 0.3110
 805/1500 [===============>..............] - ETA: 9:05 - loss: 1.9265 - regression_loss: 1.6156 - classification_loss: 0.3109
 806/1500 [===============>..............] - ETA: 9:04 - loss: 1.9257 - regression_loss: 1.6150 - classification_loss: 0.3107
 807/1500 [===============>..............] - ETA: 9:05 - loss: 1.9258 - regression_loss: 1.6150 - classification_loss: 0.3108
 808/1500 [===============>..............] - ETA: 9:04 - loss: 1.9257 - regression_loss: 1.6149 - classification_loss: 0.3108
 809/1500 [===============>..............] - ETA: 9:03 - loss: 1.9252 - regression_loss: 1.6145 - classification_loss: 0.3107
 810/1500 [===============>..............] - ETA: 9:02 - loss: 1.9242 - regression_loss: 1.6137 - classification_loss: 0.3105
 811/1500 [===============>..............] - ETA: 9:01 - loss: 1.9249 - regression_loss: 1.6143 - classification_loss: 0.3106
 812/1500 [===============>..............] - ETA: 8:59 - loss: 1.9256 - regression_loss: 1.6149 - classification_loss: 0.3107
 813/1500 [===============>..............] - ETA: 8:59 - loss: 1.9259 - regression_loss: 1.6151 - classification_loss: 0.3108
 814/1500 [===============>..............] - ETA: 8:58 - loss: 1.9260 - regression_loss: 1.6152 - classification_loss: 0.3109
 815/1500 [===============>..............] - ETA: 8:56 - loss: 1.9250 - regression_loss: 1.6144 - classification_loss: 0.3107
 816/1500 [===============>..............] - ETA: 8:55 - loss: 1.9242 - regression_loss: 1.6137 - classification_loss: 0.3105
 817/1500 [===============>..............] - ETA: 8:54 - loss: 1.9243 - regression_loss: 1.6137 - classification_loss: 0.3106
 818/1500 [===============>..............] - ETA: 8:53 - loss: 1.9249 - regression_loss: 1.6139 - classification_loss: 0.3110
 819/1500 [===============>..............] - ETA: 8:52 - loss: 1.9239 - regression_loss: 1.6131 - classification_loss: 0.3108
 820/1500 [===============>..............] - ETA: 8:51 - loss: 1.9232 - regression_loss: 1.6126 - classification_loss: 0.3106
 821/1500 [===============>..............] - ETA: 8:50 - loss: 1.9221 - regression_loss: 1.6117 - classification_loss: 0.3104
 822/1500 [===============>..............] - ETA: 8:49 - loss: 1.9224 - regression_loss: 1.6119 - classification_loss: 0.3105
 823/1500 [===============>..............] - ETA: 8:48 - loss: 1.9219 - regression_loss: 1.6115 - classification_loss: 0.3103
 824/1500 [===============>..............] - ETA: 8:48 - loss: 1.9226 - regression_loss: 1.6120 - classification_loss: 0.3106
 825/1500 [===============>..............] - ETA: 8:47 - loss: 1.9234 - regression_loss: 1.6125 - classification_loss: 0.3109
 826/1500 [===============>..............] - ETA: 8:47 - loss: 1.9235 - regression_loss: 1.6125 - classification_loss: 0.3110
 827/1500 [===============>..............] - ETA: 8:46 - loss: 1.9218 - regression_loss: 1.6111 - classification_loss: 0.3107
 828/1500 [===============>..............] - ETA: 8:46 - loss: 1.9210 - regression_loss: 1.6105 - classification_loss: 0.3105
 829/1500 [===============>..............] - ETA: 8:45 - loss: 1.9208 - regression_loss: 1.6102 - classification_loss: 0.3105
 830/1500 [===============>..............] - ETA: 8:44 - loss: 1.9216 - regression_loss: 1.6110 - classification_loss: 0.3107
 831/1500 [===============>..............] - ETA: 8:44 - loss: 1.9206 - regression_loss: 1.6102 - classification_loss: 0.3105
 832/1500 [===============>..............] - ETA: 8:42 - loss: 1.9202 - regression_loss: 1.6098 - classification_loss: 0.3104
 833/1500 [===============>..............] - ETA: 8:41 - loss: 1.9196 - regression_loss: 1.6093 - classification_loss: 0.3103
 834/1500 [===============>..............] - ETA: 8:41 - loss: 1.9182 - regression_loss: 1.6074 - classification_loss: 0.3108
 835/1500 [===============>..............] - ETA: 8:40 - loss: 1.9172 - regression_loss: 1.6065 - classification_loss: 0.3107
 836/1500 [===============>..............] - ETA: 8:39 - loss: 1.9182 - regression_loss: 1.6074 - classification_loss: 0.3108
 837/1500 [===============>..............] - ETA: 8:38 - loss: 1.9182 - regression_loss: 1.6074 - classification_loss: 0.3107
 838/1500 [===============>..............] - ETA: 8:38 - loss: 1.9181 - regression_loss: 1.6075 - classification_loss: 0.3106
 839/1500 [===============>..............] - ETA: 8:37 - loss: 1.9174 - regression_loss: 1.6069 - classification_loss: 0.3105
 840/1500 [===============>..............] - ETA: 8:36 - loss: 1.9165 - regression_loss: 1.6063 - classification_loss: 0.3103
 841/1500 [===============>..............] - ETA: 8:35 - loss: 1.9167 - regression_loss: 1.6064 - classification_loss: 0.3102
 842/1500 [===============>..............] - ETA: 8:34 - loss: 1.9157 - regression_loss: 1.6057 - classification_loss: 0.3100
 843/1500 [===============>..............] - ETA: 8:33 - loss: 1.9171 - regression_loss: 1.6069 - classification_loss: 0.3102
 844/1500 [===============>..............] - ETA: 8:32 - loss: 1.9161 - regression_loss: 1.6061 - classification_loss: 0.3100
 845/1500 [===============>..............] - ETA: 8:31 - loss: 1.9158 - regression_loss: 1.6058 - classification_loss: 0.3100
 846/1500 [===============>..............] - ETA: 8:30 - loss: 1.9174 - regression_loss: 1.6072 - classification_loss: 0.3102
 847/1500 [===============>..............] - ETA: 8:29 - loss: 1.9167 - regression_loss: 1.6067 - classification_loss: 0.3100
 848/1500 [===============>..............] - ETA: 8:28 - loss: 1.9166 - regression_loss: 1.6066 - classification_loss: 0.3100
 849/1500 [===============>..............] - ETA: 8:28 - loss: 1.9179 - regression_loss: 1.6075 - classification_loss: 0.3104
 850/1500 [================>.............] - ETA: 8:28 - loss: 1.9179 - regression_loss: 1.6075 - classification_loss: 0.3104
 851/1500 [================>.............] - ETA: 8:27 - loss: 1.9172 - regression_loss: 1.6071 - classification_loss: 0.3102
 852/1500 [================>.............] - ETA: 8:26 - loss: 1.9166 - regression_loss: 1.6065 - classification_loss: 0.3101
 853/1500 [================>.............] - ETA: 8:25 - loss: 1.9165 - regression_loss: 1.6064 - classification_loss: 0.3101
 854/1500 [================>.............] - ETA: 8:24 - loss: 1.9169 - regression_loss: 1.6068 - classification_loss: 0.3101
 855/1500 [================>.............] - ETA: 8:24 - loss: 1.9166 - regression_loss: 1.6067 - classification_loss: 0.3099
 856/1500 [================>.............] - ETA: 8:23 - loss: 1.9175 - regression_loss: 1.6076 - classification_loss: 0.3100
 857/1500 [================>.............] - ETA: 8:23 - loss: 1.9164 - regression_loss: 1.6066 - classification_loss: 0.3099
 858/1500 [================>.............] - ETA: 8:22 - loss: 1.9166 - regression_loss: 1.6068 - classification_loss: 0.3098
 859/1500 [================>.............] - ETA: 8:22 - loss: 1.9155 - regression_loss: 1.6060 - classification_loss: 0.3095
 860/1500 [================>.............] - ETA: 8:21 - loss: 1.9146 - regression_loss: 1.6052 - classification_loss: 0.3094
 861/1500 [================>.............] - ETA: 8:20 - loss: 1.9156 - regression_loss: 1.6061 - classification_loss: 0.3095
 862/1500 [================>.............] - ETA: 8:19 - loss: 1.9155 - regression_loss: 1.6059 - classification_loss: 0.3096
 863/1500 [================>.............] - ETA: 8:18 - loss: 1.9154 - regression_loss: 1.6060 - classification_loss: 0.3094
 864/1500 [================>.............] - ETA: 8:18 - loss: 1.9146 - regression_loss: 1.6054 - classification_loss: 0.3092
 865/1500 [================>.............] - ETA: 8:17 - loss: 1.9144 - regression_loss: 1.6054 - classification_loss: 0.3091
 866/1500 [================>.............] - ETA: 8:16 - loss: 1.9141 - regression_loss: 1.6051 - classification_loss: 0.3090
 867/1500 [================>.............] - ETA: 8:15 - loss: 1.9132 - regression_loss: 1.6044 - classification_loss: 0.3089
 868/1500 [================>.............] - ETA: 8:14 - loss: 1.9130 - regression_loss: 1.6041 - classification_loss: 0.3089
 869/1500 [================>.............] - ETA: 8:13 - loss: 1.9128 - regression_loss: 1.6039 - classification_loss: 0.3089
 870/1500 [================>.............] - ETA: 8:13 - loss: 1.9122 - regression_loss: 1.6036 - classification_loss: 0.3086
 871/1500 [================>.............] - ETA: 8:12 - loss: 1.9108 - regression_loss: 1.6024 - classification_loss: 0.3084
 872/1500 [================>.............] - ETA: 8:12 - loss: 1.9120 - regression_loss: 1.6033 - classification_loss: 0.3087
 873/1500 [================>.............] - ETA: 8:11 - loss: 1.9113 - regression_loss: 1.6027 - classification_loss: 0.3086
 874/1500 [================>.............] - ETA: 8:10 - loss: 1.9131 - regression_loss: 1.6042 - classification_loss: 0.3089
 875/1500 [================>.............] - ETA: 8:09 - loss: 1.9123 - regression_loss: 1.6037 - classification_loss: 0.3087
 876/1500 [================>.............] - ETA: 8:08 - loss: 1.9120 - regression_loss: 1.6032 - classification_loss: 0.3088
 877/1500 [================>.............] - ETA: 8:07 - loss: 1.9111 - regression_loss: 1.6025 - classification_loss: 0.3086
 878/1500 [================>.............] - ETA: 8:06 - loss: 1.9099 - regression_loss: 1.6015 - classification_loss: 0.3084
 879/1500 [================>.............] - ETA: 8:05 - loss: 1.9093 - regression_loss: 1.6011 - classification_loss: 0.3083
 880/1500 [================>.............] - ETA: 8:05 - loss: 1.9101 - regression_loss: 1.6017 - classification_loss: 0.3084
 881/1500 [================>.............] - ETA: 8:04 - loss: 1.9094 - regression_loss: 1.6011 - classification_loss: 0.3083
 882/1500 [================>.............] - ETA: 8:04 - loss: 1.9092 - regression_loss: 1.6010 - classification_loss: 0.3083
 883/1500 [================>.............] - ETA: 8:03 - loss: 1.9097 - regression_loss: 1.6013 - classification_loss: 0.3084
 884/1500 [================>.............] - ETA: 8:02 - loss: 1.9108 - regression_loss: 1.6021 - classification_loss: 0.3086
 885/1500 [================>.............] - ETA: 8:02 - loss: 1.9104 - regression_loss: 1.6018 - classification_loss: 0.3086
 886/1500 [================>.............] - ETA: 8:01 - loss: 1.9109 - regression_loss: 1.6022 - classification_loss: 0.3087
 887/1500 [================>.............] - ETA: 8:00 - loss: 1.9122 - regression_loss: 1.6034 - classification_loss: 0.3088
 888/1500 [================>.............] - ETA: 7:59 - loss: 1.9125 - regression_loss: 1.6037 - classification_loss: 0.3088
 889/1500 [================>.............] - ETA: 7:58 - loss: 1.9125 - regression_loss: 1.6038 - classification_loss: 0.3087
 890/1500 [================>.............] - ETA: 7:57 - loss: 1.9140 - regression_loss: 1.6051 - classification_loss: 0.3088
 891/1500 [================>.............] - ETA: 7:57 - loss: 1.9128 - regression_loss: 1.6042 - classification_loss: 0.3087
 892/1500 [================>.............] - ETA: 7:56 - loss: 1.9132 - regression_loss: 1.6045 - classification_loss: 0.3087
 893/1500 [================>.............] - ETA: 7:55 - loss: 1.9145 - regression_loss: 1.6054 - classification_loss: 0.3091
 894/1500 [================>.............] - ETA: 7:55 - loss: 1.9155 - regression_loss: 1.6062 - classification_loss: 0.3094
 895/1500 [================>.............] - ETA: 7:54 - loss: 1.9155 - regression_loss: 1.6061 - classification_loss: 0.3094
 896/1500 [================>.............] - ETA: 7:54 - loss: 1.9169 - regression_loss: 1.6057 - classification_loss: 0.3113
 897/1500 [================>.............] - ETA: 7:53 - loss: 1.9156 - regression_loss: 1.6045 - classification_loss: 0.3111
 898/1500 [================>.............] - ETA: 7:52 - loss: 1.9160 - regression_loss: 1.6049 - classification_loss: 0.3111
 899/1500 [================>.............] - ETA: 7:51 - loss: 1.9170 - regression_loss: 1.6057 - classification_loss: 0.3113
 900/1500 [=================>............] - ETA: 7:50 - loss: 1.9174 - regression_loss: 1.6059 - classification_loss: 0.3114
 901/1500 [=================>............] - ETA: 7:49 - loss: 1.9171 - regression_loss: 1.6057 - classification_loss: 0.3114
 902/1500 [=================>............] - ETA: 7:49 - loss: 1.9171 - regression_loss: 1.6056 - classification_loss: 0.3115
 903/1500 [=================>............] - ETA: 7:48 - loss: 1.9161 - regression_loss: 1.6048 - classification_loss: 0.3113
 904/1500 [=================>............] - ETA: 7:48 - loss: 1.9157 - regression_loss: 1.6045 - classification_loss: 0.3112
 905/1500 [=================>............] - ETA: 7:47 - loss: 1.9161 - regression_loss: 1.6049 - classification_loss: 0.3111
 906/1500 [=================>............] - ETA: 7:46 - loss: 1.9153 - regression_loss: 1.6043 - classification_loss: 0.3110
 907/1500 [=================>............] - ETA: 7:45 - loss: 1.9144 - regression_loss: 1.6036 - classification_loss: 0.3108
 908/1500 [=================>............] - ETA: 7:45 - loss: 1.9149 - regression_loss: 1.6039 - classification_loss: 0.3110
 909/1500 [=================>............] - ETA: 7:45 - loss: 1.9141 - regression_loss: 1.6033 - classification_loss: 0.3108
 910/1500 [=================>............] - ETA: 7:44 - loss: 1.9148 - regression_loss: 1.6038 - classification_loss: 0.3111
 911/1500 [=================>............] - ETA: 7:43 - loss: 1.9146 - regression_loss: 1.6036 - classification_loss: 0.3110
 912/1500 [=================>............] - ETA: 7:42 - loss: 1.9138 - regression_loss: 1.6028 - classification_loss: 0.3110
 913/1500 [=================>............] - ETA: 7:41 - loss: 1.9137 - regression_loss: 1.6027 - classification_loss: 0.3110
 914/1500 [=================>............] - ETA: 7:41 - loss: 1.9149 - regression_loss: 1.6038 - classification_loss: 0.3111
 915/1500 [=================>............] - ETA: 7:40 - loss: 1.9141 - regression_loss: 1.6032 - classification_loss: 0.3108
 916/1500 [=================>............] - ETA: 7:39 - loss: 1.9138 - regression_loss: 1.6030 - classification_loss: 0.3108
 917/1500 [=================>............] - ETA: 7:38 - loss: 1.9133 - regression_loss: 1.6026 - classification_loss: 0.3107
 918/1500 [=================>............] - ETA: 7:37 - loss: 1.9137 - regression_loss: 1.6029 - classification_loss: 0.3108
 919/1500 [=================>............] - ETA: 7:36 - loss: 1.9152 - regression_loss: 1.6040 - classification_loss: 0.3112
 920/1500 [=================>............] - ETA: 7:35 - loss: 1.9154 - regression_loss: 1.6042 - classification_loss: 0.3112
 921/1500 [=================>............] - ETA: 7:34 - loss: 1.9153 - regression_loss: 1.6043 - classification_loss: 0.3111
 922/1500 [=================>............] - ETA: 7:33 - loss: 1.9148 - regression_loss: 1.6039 - classification_loss: 0.3109
 923/1500 [=================>............] - ETA: 7:33 - loss: 1.9156 - regression_loss: 1.6045 - classification_loss: 0.3110
 924/1500 [=================>............] - ETA: 7:32 - loss: 1.9150 - regression_loss: 1.6041 - classification_loss: 0.3108
 925/1500 [=================>............] - ETA: 7:31 - loss: 1.9152 - regression_loss: 1.6044 - classification_loss: 0.3108
 926/1500 [=================>............] - ETA: 7:30 - loss: 1.9141 - regression_loss: 1.6035 - classification_loss: 0.3106
 927/1500 [=================>............] - ETA: 7:30 - loss: 1.9141 - regression_loss: 1.6034 - classification_loss: 0.3106
 928/1500 [=================>............] - ETA: 7:29 - loss: 1.9133 - regression_loss: 1.6028 - classification_loss: 0.3105
 929/1500 [=================>............] - ETA: 7:28 - loss: 1.9138 - regression_loss: 1.6033 - classification_loss: 0.3105
 930/1500 [=================>............] - ETA: 7:28 - loss: 1.9146 - regression_loss: 1.6037 - classification_loss: 0.3110
 931/1500 [=================>............] - ETA: 7:27 - loss: 1.9143 - regression_loss: 1.6034 - classification_loss: 0.3109
 932/1500 [=================>............] - ETA: 7:26 - loss: 1.9140 - regression_loss: 1.6031 - classification_loss: 0.3109
 933/1500 [=================>............] - ETA: 7:25 - loss: 1.9138 - regression_loss: 1.6029 - classification_loss: 0.3108
 934/1500 [=================>............] - ETA: 7:24 - loss: 1.9137 - regression_loss: 1.6029 - classification_loss: 0.3108
 935/1500 [=================>............] - ETA: 7:23 - loss: 1.9139 - regression_loss: 1.6030 - classification_loss: 0.3109
 936/1500 [=================>............] - ETA: 7:23 - loss: 1.9143 - regression_loss: 1.6035 - classification_loss: 0.3109
 937/1500 [=================>............] - ETA: 7:22 - loss: 1.9133 - regression_loss: 1.6026 - classification_loss: 0.3107
 938/1500 [=================>............] - ETA: 7:22 - loss: 1.9134 - regression_loss: 1.6027 - classification_loss: 0.3106
 939/1500 [=================>............] - ETA: 7:21 - loss: 1.9139 - regression_loss: 1.6033 - classification_loss: 0.3105
 940/1500 [=================>............] - ETA: 7:20 - loss: 1.9131 - regression_loss: 1.6027 - classification_loss: 0.3104
 941/1500 [=================>............] - ETA: 7:19 - loss: 1.9121 - regression_loss: 1.6019 - classification_loss: 0.3102
 942/1500 [=================>............] - ETA: 7:19 - loss: 1.9114 - regression_loss: 1.6012 - classification_loss: 0.3102
 943/1500 [=================>............] - ETA: 7:18 - loss: 1.9105 - regression_loss: 1.6005 - classification_loss: 0.3100
 944/1500 [=================>............] - ETA: 7:17 - loss: 1.9102 - regression_loss: 1.6001 - classification_loss: 0.3101
 945/1500 [=================>............] - ETA: 7:16 - loss: 1.9104 - regression_loss: 1.6003 - classification_loss: 0.3101
 946/1500 [=================>............] - ETA: 7:15 - loss: 1.9102 - regression_loss: 1.6001 - classification_loss: 0.3100
 947/1500 [=================>............] - ETA: 7:14 - loss: 1.9112 - regression_loss: 1.6008 - classification_loss: 0.3103
 948/1500 [=================>............] - ETA: 7:13 - loss: 1.9105 - regression_loss: 1.6004 - classification_loss: 0.3101
 949/1500 [=================>............] - ETA: 7:12 - loss: 1.9101 - regression_loss: 1.6000 - classification_loss: 0.3102
 950/1500 [==================>...........] - ETA: 7:11 - loss: 1.9106 - regression_loss: 1.6001 - classification_loss: 0.3105
 951/1500 [==================>...........] - ETA: 7:10 - loss: 1.9107 - regression_loss: 1.6002 - classification_loss: 0.3104
 952/1500 [==================>...........] - ETA: 7:10 - loss: 1.9096 - regression_loss: 1.5994 - classification_loss: 0.3102
 953/1500 [==================>...........] - ETA: 7:09 - loss: 1.9092 - regression_loss: 1.5989 - classification_loss: 0.3102
 954/1500 [==================>...........] - ETA: 7:08 - loss: 1.9084 - regression_loss: 1.5983 - classification_loss: 0.3100
 955/1500 [==================>...........] - ETA: 7:08 - loss: 1.9096 - regression_loss: 1.5995 - classification_loss: 0.3101
 956/1500 [==================>...........] - ETA: 7:07 - loss: 1.9084 - regression_loss: 1.5984 - classification_loss: 0.3099
 957/1500 [==================>...........] - ETA: 7:06 - loss: 1.9090 - regression_loss: 1.5989 - classification_loss: 0.3101
 958/1500 [==================>...........] - ETA: 7:05 - loss: 1.9098 - regression_loss: 1.5995 - classification_loss: 0.3103
 959/1500 [==================>...........] - ETA: 7:04 - loss: 1.9094 - regression_loss: 1.5992 - classification_loss: 0.3102
 960/1500 [==================>...........] - ETA: 7:03 - loss: 1.9087 - regression_loss: 1.5987 - classification_loss: 0.3100
 961/1500 [==================>...........] - ETA: 7:03 - loss: 1.9085 - regression_loss: 1.5986 - classification_loss: 0.3099
 962/1500 [==================>...........] - ETA: 7:02 - loss: 1.9096 - regression_loss: 1.5996 - classification_loss: 0.3100
 963/1500 [==================>...........] - ETA: 7:01 - loss: 1.9097 - regression_loss: 1.5999 - classification_loss: 0.3098
 964/1500 [==================>...........] - ETA: 7:00 - loss: 1.9086 - regression_loss: 1.5989 - classification_loss: 0.3096
 965/1500 [==================>...........] - ETA: 6:59 - loss: 1.9091 - regression_loss: 1.5994 - classification_loss: 0.3097
 966/1500 [==================>...........] - ETA: 6:59 - loss: 1.9097 - regression_loss: 1.5999 - classification_loss: 0.3098
 967/1500 [==================>...........] - ETA: 6:58 - loss: 1.9102 - regression_loss: 1.6003 - classification_loss: 0.3098
 968/1500 [==================>...........] - ETA: 6:57 - loss: 1.9104 - regression_loss: 1.6005 - classification_loss: 0.3099
 969/1500 [==================>...........] - ETA: 6:57 - loss: 1.9108 - regression_loss: 1.6008 - classification_loss: 0.3100
 970/1500 [==================>...........] - ETA: 6:56 - loss: 1.9105 - regression_loss: 1.6005 - classification_loss: 0.3100
 971/1500 [==================>...........] - ETA: 6:55 - loss: 1.9098 - regression_loss: 1.6000 - classification_loss: 0.3099
 972/1500 [==================>...........] - ETA: 6:54 - loss: 1.9085 - regression_loss: 1.5989 - classification_loss: 0.3096
 973/1500 [==================>...........] - ETA: 6:54 - loss: 1.9084 - regression_loss: 1.5988 - classification_loss: 0.3096
 974/1500 [==================>...........] - ETA: 6:53 - loss: 1.9073 - regression_loss: 1.5980 - classification_loss: 0.3094
 975/1500 [==================>...........] - ETA: 6:52 - loss: 1.9076 - regression_loss: 1.5983 - classification_loss: 0.3094
 976/1500 [==================>...........] - ETA: 6:51 - loss: 1.9067 - regression_loss: 1.5975 - classification_loss: 0.3092
 977/1500 [==================>...........] - ETA: 6:50 - loss: 1.9072 - regression_loss: 1.5980 - classification_loss: 0.3092
 978/1500 [==================>...........] - ETA: 6:49 - loss: 1.9064 - regression_loss: 1.5973 - classification_loss: 0.3091
 979/1500 [==================>...........] - ETA: 6:48 - loss: 1.9062 - regression_loss: 1.5973 - classification_loss: 0.3090
 980/1500 [==================>...........] - ETA: 6:47 - loss: 1.9060 - regression_loss: 1.5971 - classification_loss: 0.3088
 981/1500 [==================>...........] - ETA: 6:46 - loss: 1.9059 - regression_loss: 1.5972 - classification_loss: 0.3088
 982/1500 [==================>...........] - ETA: 6:45 - loss: 1.9062 - regression_loss: 1.5974 - classification_loss: 0.3088
 983/1500 [==================>...........] - ETA: 6:45 - loss: 1.9053 - regression_loss: 1.5966 - classification_loss: 0.3087
 984/1500 [==================>...........] - ETA: 6:44 - loss: 1.9055 - regression_loss: 1.5968 - classification_loss: 0.3087
 985/1500 [==================>...........] - ETA: 6:43 - loss: 1.9065 - regression_loss: 1.5975 - classification_loss: 0.3090
 986/1500 [==================>...........] - ETA: 6:42 - loss: 1.9074 - regression_loss: 1.5983 - classification_loss: 0.3091
 987/1500 [==================>...........] - ETA: 6:41 - loss: 1.9085 - regression_loss: 1.5991 - classification_loss: 0.3094
 988/1500 [==================>...........] - ETA: 6:40 - loss: 1.9083 - regression_loss: 1.5990 - classification_loss: 0.3093
 989/1500 [==================>...........] - ETA: 6:39 - loss: 1.9075 - regression_loss: 1.5983 - classification_loss: 0.3092
 990/1500 [==================>...........] - ETA: 6:38 - loss: 1.9084 - regression_loss: 1.5991 - classification_loss: 0.3093
 991/1500 [==================>...........] - ETA: 6:37 - loss: 1.9085 - regression_loss: 1.5993 - classification_loss: 0.3092
 992/1500 [==================>...........] - ETA: 6:37 - loss: 1.9082 - regression_loss: 1.5991 - classification_loss: 0.3091
 993/1500 [==================>...........] - ETA: 6:36 - loss: 1.9079 - regression_loss: 1.5989 - classification_loss: 0.3090
 994/1500 [==================>...........] - ETA: 6:35 - loss: 1.9079 - regression_loss: 1.5989 - classification_loss: 0.3090
 995/1500 [==================>...........] - ETA: 6:34 - loss: 1.9072 - regression_loss: 1.5983 - classification_loss: 0.3089
 996/1500 [==================>...........] - ETA: 6:34 - loss: 1.9071 - regression_loss: 1.5981 - classification_loss: 0.3089
 997/1500 [==================>...........] - ETA: 6:33 - loss: 1.9077 - regression_loss: 1.5988 - classification_loss: 0.3089
 998/1500 [==================>...........] - ETA: 6:32 - loss: 1.9081 - regression_loss: 1.5990 - classification_loss: 0.3091
 999/1500 [==================>...........] - ETA: 6:32 - loss: 1.9085 - regression_loss: 1.5994 - classification_loss: 0.3091
1000/1500 [===================>..........] - ETA: 6:31 - loss: 1.9082 - regression_loss: 1.5992 - classification_loss: 0.3090
1001/1500 [===================>..........] - ETA: 6:30 - loss: 1.9077 - regression_loss: 1.5988 - classification_loss: 0.3089
1002/1500 [===================>..........] - ETA: 6:29 - loss: 1.9069 - regression_loss: 1.5981 - classification_loss: 0.3088
1003/1500 [===================>..........] - ETA: 6:28 - loss: 1.9064 - regression_loss: 1.5977 - classification_loss: 0.3087
1004/1500 [===================>..........] - ETA: 6:27 - loss: 1.9059 - regression_loss: 1.5973 - classification_loss: 0.3086
1005/1500 [===================>..........] - ETA: 6:26 - loss: 1.9065 - regression_loss: 1.5977 - classification_loss: 0.3088
1006/1500 [===================>..........] - ETA: 6:25 - loss: 1.9072 - regression_loss: 1.5985 - classification_loss: 0.3088
1007/1500 [===================>..........] - ETA: 6:24 - loss: 1.9068 - regression_loss: 1.5981 - classification_loss: 0.3087
1008/1500 [===================>..........] - ETA: 6:24 - loss: 1.9058 - regression_loss: 1.5972 - classification_loss: 0.3085
1009/1500 [===================>..........] - ETA: 6:23 - loss: 1.9056 - regression_loss: 1.5970 - classification_loss: 0.3086
1010/1500 [===================>..........] - ETA: 6:22 - loss: 1.9051 - regression_loss: 1.5967 - classification_loss: 0.3084
1011/1500 [===================>..........] - ETA: 6:21 - loss: 1.9059 - regression_loss: 1.5973 - classification_loss: 0.3086
1012/1500 [===================>..........] - ETA: 6:20 - loss: 1.9061 - regression_loss: 1.5974 - classification_loss: 0.3087
1013/1500 [===================>..........] - ETA: 6:19 - loss: 1.9058 - regression_loss: 1.5971 - classification_loss: 0.3088
1014/1500 [===================>..........] - ETA: 6:18 - loss: 1.9055 - regression_loss: 1.5968 - classification_loss: 0.3087
1015/1500 [===================>..........] - ETA: 6:18 - loss: 1.9062 - regression_loss: 1.5975 - classification_loss: 0.3087
1016/1500 [===================>..........] - ETA: 6:17 - loss: 1.9068 - regression_loss: 1.5979 - classification_loss: 0.3089
1017/1500 [===================>..........] - ETA: 6:16 - loss: 1.9064 - regression_loss: 1.5975 - classification_loss: 0.3089
1018/1500 [===================>..........] - ETA: 6:16 - loss: 1.9064 - regression_loss: 1.5976 - classification_loss: 0.3089
1019/1500 [===================>..........] - ETA: 6:15 - loss: 1.9070 - regression_loss: 1.5981 - classification_loss: 0.3089
1020/1500 [===================>..........] - ETA: 6:15 - loss: 1.9069 - regression_loss: 1.5980 - classification_loss: 0.3089
1021/1500 [===================>..........] - ETA: 6:14 - loss: 1.9077 - regression_loss: 1.5986 - classification_loss: 0.3090
1022/1500 [===================>..........] - ETA: 6:13 - loss: 1.9094 - regression_loss: 1.6002 - classification_loss: 0.3092
1023/1500 [===================>..........] - ETA: 6:13 - loss: 1.9096 - regression_loss: 1.6004 - classification_loss: 0.3091
1024/1500 [===================>..........] - ETA: 6:12 - loss: 1.9086 - regression_loss: 1.5996 - classification_loss: 0.3089
1025/1500 [===================>..........] - ETA: 6:11 - loss: 1.9093 - regression_loss: 1.6002 - classification_loss: 0.3091
1026/1500 [===================>..........] - ETA: 6:10 - loss: 1.9097 - regression_loss: 1.6005 - classification_loss: 0.3092
1027/1500 [===================>..........] - ETA: 6:10 - loss: 1.9100 - regression_loss: 1.6007 - classification_loss: 0.3093
1028/1500 [===================>..........] - ETA: 6:09 - loss: 1.9096 - regression_loss: 1.6005 - classification_loss: 0.3091
1029/1500 [===================>..........] - ETA: 6:08 - loss: 1.9089 - regression_loss: 1.6000 - classification_loss: 0.3089
1030/1500 [===================>..........] - ETA: 6:08 - loss: 1.9088 - regression_loss: 1.5998 - classification_loss: 0.3090
1031/1500 [===================>..........] - ETA: 6:07 - loss: 1.9082 - regression_loss: 1.5993 - classification_loss: 0.3088
1032/1500 [===================>..........] - ETA: 6:06 - loss: 1.9077 - regression_loss: 1.5990 - classification_loss: 0.3087
1033/1500 [===================>..........] - ETA: 6:05 - loss: 1.9080 - regression_loss: 1.5993 - classification_loss: 0.3087
1034/1500 [===================>..........] - ETA: 6:05 - loss: 1.9072 - regression_loss: 1.5987 - classification_loss: 0.3085
1035/1500 [===================>..........] - ETA: 6:05 - loss: 1.9071 - regression_loss: 1.5985 - classification_loss: 0.3085
1036/1500 [===================>..........] - ETA: 6:04 - loss: 1.9070 - regression_loss: 1.5985 - classification_loss: 0.3085
1037/1500 [===================>..........] - ETA: 6:03 - loss: 1.9063 - regression_loss: 1.5979 - classification_loss: 0.3084
1038/1500 [===================>..........] - ETA: 6:02 - loss: 1.9059 - regression_loss: 1.5977 - classification_loss: 0.3083
1039/1500 [===================>..........] - ETA: 6:01 - loss: 1.9066 - regression_loss: 1.5984 - classification_loss: 0.3082
1040/1500 [===================>..........] - ETA: 6:01 - loss: 1.9069 - regression_loss: 1.5987 - classification_loss: 0.3082
1041/1500 [===================>..........] - ETA: 6:00 - loss: 1.9067 - regression_loss: 1.5986 - classification_loss: 0.3081
1042/1500 [===================>..........] - ETA: 5:59 - loss: 1.9067 - regression_loss: 1.5985 - classification_loss: 0.3082
1043/1500 [===================>..........] - ETA: 5:59 - loss: 1.9074 - regression_loss: 1.5992 - classification_loss: 0.3081
1044/1500 [===================>..........] - ETA: 5:58 - loss: 1.9069 - regression_loss: 1.5990 - classification_loss: 0.3079
1045/1500 [===================>..........] - ETA: 5:57 - loss: 1.9061 - regression_loss: 1.5983 - classification_loss: 0.3078
1046/1500 [===================>..........] - ETA: 5:57 - loss: 1.9058 - regression_loss: 1.5978 - classification_loss: 0.3080
1047/1500 [===================>..........] - ETA: 5:56 - loss: 1.9052 - regression_loss: 1.5973 - classification_loss: 0.3079
1048/1500 [===================>..........] - ETA: 5:55 - loss: 1.9046 - regression_loss: 1.5969 - classification_loss: 0.3077
1049/1500 [===================>..........] - ETA: 5:54 - loss: 1.9049 - regression_loss: 1.5972 - classification_loss: 0.3077
1050/1500 [====================>.........] - ETA: 5:53 - loss: 1.9049 - regression_loss: 1.5973 - classification_loss: 0.3076
1051/1500 [====================>.........] - ETA: 5:52 - loss: 1.9045 - regression_loss: 1.5968 - classification_loss: 0.3077
1052/1500 [====================>.........] - ETA: 5:52 - loss: 1.9048 - regression_loss: 1.5972 - classification_loss: 0.3075
1053/1500 [====================>.........] - ETA: 5:51 - loss: 1.9048 - regression_loss: 1.5973 - classification_loss: 0.3075
1054/1500 [====================>.........] - ETA: 5:50 - loss: 1.9042 - regression_loss: 1.5968 - classification_loss: 0.3073
1055/1500 [====================>.........] - ETA: 5:49 - loss: 1.9038 - regression_loss: 1.5967 - classification_loss: 0.3072
1056/1500 [====================>.........] - ETA: 5:48 - loss: 1.9036 - regression_loss: 1.5965 - classification_loss: 0.3070
1057/1500 [====================>.........] - ETA: 5:47 - loss: 1.9032 - regression_loss: 1.5962 - classification_loss: 0.3070
1058/1500 [====================>.........] - ETA: 5:46 - loss: 1.9037 - regression_loss: 1.5966 - classification_loss: 0.3071
1059/1500 [====================>.........] - ETA: 5:45 - loss: 1.9038 - regression_loss: 1.5966 - classification_loss: 0.3073
1060/1500 [====================>.........] - ETA: 5:46 - loss: 1.9051 - regression_loss: 1.5977 - classification_loss: 0.3074
1061/1500 [====================>.........] - ETA: 5:45 - loss: 1.9058 - regression_loss: 1.5982 - classification_loss: 0.3076
1062/1500 [====================>.........] - ETA: 5:44 - loss: 1.9062 - regression_loss: 1.5985 - classification_loss: 0.3076
1063/1500 [====================>.........] - ETA: 5:43 - loss: 1.9062 - regression_loss: 1.5986 - classification_loss: 0.3076
1064/1500 [====================>.........] - ETA: 5:42 - loss: 1.9064 - regression_loss: 1.5987 - classification_loss: 0.3077
1065/1500 [====================>.........] - ETA: 5:42 - loss: 1.9066 - regression_loss: 1.5990 - classification_loss: 0.3076
1066/1500 [====================>.........] - ETA: 5:41 - loss: 1.9068 - regression_loss: 1.5992 - classification_loss: 0.3076
1067/1500 [====================>.........] - ETA: 5:40 - loss: 1.9074 - regression_loss: 1.5996 - classification_loss: 0.3078
1068/1500 [====================>.........] - ETA: 5:39 - loss: 1.9073 - regression_loss: 1.5994 - classification_loss: 0.3079
1069/1500 [====================>.........] - ETA: 5:38 - loss: 1.9063 - regression_loss: 1.5985 - classification_loss: 0.3077
1070/1500 [====================>.........] - ETA: 5:37 - loss: 1.9064 - regression_loss: 1.5985 - classification_loss: 0.3079
1071/1500 [====================>.........] - ETA: 5:37 - loss: 1.9059 - regression_loss: 1.5981 - classification_loss: 0.3077
1072/1500 [====================>.........] - ETA: 5:36 - loss: 1.9057 - regression_loss: 1.5981 - classification_loss: 0.3076
1073/1500 [====================>.........] - ETA: 5:36 - loss: 1.9064 - regression_loss: 1.5987 - classification_loss: 0.3077
1074/1500 [====================>.........] - ETA: 5:35 - loss: 1.9057 - regression_loss: 1.5980 - classification_loss: 0.3076
1075/1500 [====================>.........] - ETA: 5:34 - loss: 1.9062 - regression_loss: 1.5985 - classification_loss: 0.3077
1076/1500 [====================>.........] - ETA: 5:33 - loss: 1.9066 - regression_loss: 1.5989 - classification_loss: 0.3077
1077/1500 [====================>.........] - ETA: 5:32 - loss: 1.9068 - regression_loss: 1.5990 - classification_loss: 0.3078
1078/1500 [====================>.........] - ETA: 5:32 - loss: 1.9059 - regression_loss: 1.5983 - classification_loss: 0.3076
1079/1500 [====================>.........] - ETA: 5:31 - loss: 1.9065 - regression_loss: 1.5988 - classification_loss: 0.3077
1080/1500 [====================>.........] - ETA: 5:30 - loss: 1.9058 - regression_loss: 1.5981 - classification_loss: 0.3077
1081/1500 [====================>.........] - ETA: 5:29 - loss: 1.9049 - regression_loss: 1.5974 - classification_loss: 0.3075
1082/1500 [====================>.........] - ETA: 5:28 - loss: 1.9042 - regression_loss: 1.5968 - classification_loss: 0.3074
1083/1500 [====================>.........] - ETA: 5:28 - loss: 1.9050 - regression_loss: 1.5976 - classification_loss: 0.3074
1084/1500 [====================>.........] - ETA: 5:27 - loss: 1.9048 - regression_loss: 1.5975 - classification_loss: 0.3073
1085/1500 [====================>.........] - ETA: 5:26 - loss: 1.9044 - regression_loss: 1.5971 - classification_loss: 0.3074
1086/1500 [====================>.........] - ETA: 5:25 - loss: 1.9033 - regression_loss: 1.5962 - classification_loss: 0.3072
1087/1500 [====================>.........] - ETA: 5:24 - loss: 1.9049 - regression_loss: 1.5975 - classification_loss: 0.3074
1088/1500 [====================>.........] - ETA: 5:23 - loss: 1.9048 - regression_loss: 1.5973 - classification_loss: 0.3075
1089/1500 [====================>.........] - ETA: 5:23 - loss: 1.9041 - regression_loss: 1.5968 - classification_loss: 0.3074
1090/1500 [====================>.........] - ETA: 5:22 - loss: 1.9040 - regression_loss: 1.5967 - classification_loss: 0.3073
1091/1500 [====================>.........] - ETA: 5:21 - loss: 1.9042 - regression_loss: 1.5968 - classification_loss: 0.3073
1092/1500 [====================>.........] - ETA: 5:20 - loss: 1.9037 - regression_loss: 1.5965 - classification_loss: 0.3072
1093/1500 [====================>.........] - ETA: 5:20 - loss: 1.9044 - regression_loss: 1.5967 - classification_loss: 0.3078
1094/1500 [====================>.........] - ETA: 5:19 - loss: 1.9042 - regression_loss: 1.5966 - classification_loss: 0.3076
1095/1500 [====================>.........] - ETA: 5:18 - loss: 1.9044 - regression_loss: 1.5969 - classification_loss: 0.3075
1096/1500 [====================>.........] - ETA: 5:17 - loss: 1.9035 - regression_loss: 1.5961 - classification_loss: 0.3074
1097/1500 [====================>.........] - ETA: 5:16 - loss: 1.9027 - regression_loss: 1.5955 - classification_loss: 0.3072
1098/1500 [====================>.........] - ETA: 5:15 - loss: 1.9019 - regression_loss: 1.5948 - classification_loss: 0.3070
1099/1500 [====================>.........] - ETA: 5:14 - loss: 1.9019 - regression_loss: 1.5950 - classification_loss: 0.3069
1100/1500 [=====================>........] - ETA: 5:14 - loss: 1.9013 - regression_loss: 1.5945 - classification_loss: 0.3068
1101/1500 [=====================>........] - ETA: 5:13 - loss: 1.9002 - regression_loss: 1.5936 - classification_loss: 0.3066
1102/1500 [=====================>........] - ETA: 5:12 - loss: 1.9015 - regression_loss: 1.5945 - classification_loss: 0.3070
1103/1500 [=====================>........] - ETA: 5:11 - loss: 1.9025 - regression_loss: 1.5952 - classification_loss: 0.3072
1104/1500 [=====================>........] - ETA: 5:10 - loss: 1.9029 - regression_loss: 1.5957 - classification_loss: 0.3073
1105/1500 [=====================>........] - ETA: 5:09 - loss: 1.9030 - regression_loss: 1.5956 - classification_loss: 0.3073
1106/1500 [=====================>........] - ETA: 5:08 - loss: 1.9020 - regression_loss: 1.5948 - classification_loss: 0.3071
1107/1500 [=====================>........] - ETA: 5:08 - loss: 1.9013 - regression_loss: 1.5943 - classification_loss: 0.3069
1108/1500 [=====================>........] - ETA: 5:07 - loss: 1.9008 - regression_loss: 1.5941 - classification_loss: 0.3068
1109/1500 [=====================>........] - ETA: 5:06 - loss: 1.9000 - regression_loss: 1.5934 - classification_loss: 0.3067
1110/1500 [=====================>........] - ETA: 5:05 - loss: 1.9004 - regression_loss: 1.5938 - classification_loss: 0.3067
1111/1500 [=====================>........] - ETA: 5:04 - loss: 1.9004 - regression_loss: 1.5937 - classification_loss: 0.3066
1112/1500 [=====================>........] - ETA: 5:04 - loss: 1.8994 - regression_loss: 1.5929 - classification_loss: 0.3065
1113/1500 [=====================>........] - ETA: 5:03 - loss: 1.8988 - regression_loss: 1.5925 - classification_loss: 0.3063
1114/1500 [=====================>........] - ETA: 5:02 - loss: 1.8986 - regression_loss: 1.5923 - classification_loss: 0.3063
1115/1500 [=====================>........] - ETA: 5:01 - loss: 1.8986 - regression_loss: 1.5923 - classification_loss: 0.3063
1116/1500 [=====================>........] - ETA: 5:00 - loss: 1.8990 - regression_loss: 1.5927 - classification_loss: 0.3063
1117/1500 [=====================>........] - ETA: 5:00 - loss: 1.8996 - regression_loss: 1.5931 - classification_loss: 0.3064
1118/1500 [=====================>........] - ETA: 4:59 - loss: 1.8993 - regression_loss: 1.5927 - classification_loss: 0.3066
1119/1500 [=====================>........] - ETA: 4:59 - loss: 1.9002 - regression_loss: 1.5933 - classification_loss: 0.3070
1120/1500 [=====================>........] - ETA: 4:58 - loss: 1.9009 - regression_loss: 1.5935 - classification_loss: 0.3074
1121/1500 [=====================>........] - ETA: 4:58 - loss: 1.9004 - regression_loss: 1.5931 - classification_loss: 0.3073
1122/1500 [=====================>........] - ETA: 4:57 - loss: 1.9005 - regression_loss: 1.5932 - classification_loss: 0.3074
1123/1500 [=====================>........] - ETA: 4:57 - loss: 1.8998 - regression_loss: 1.5926 - classification_loss: 0.3073
1124/1500 [=====================>........] - ETA: 4:56 - loss: 1.8994 - regression_loss: 1.5922 - classification_loss: 0.3072
1125/1500 [=====================>........] - ETA: 4:55 - loss: 1.8994 - regression_loss: 1.5922 - classification_loss: 0.3072
1126/1500 [=====================>........] - ETA: 4:54 - loss: 1.8995 - regression_loss: 1.5922 - classification_loss: 0.3073
1127/1500 [=====================>........] - ETA: 4:53 - loss: 1.9001 - regression_loss: 1.5927 - classification_loss: 0.3074
1128/1500 [=====================>........] - ETA: 4:52 - loss: 1.9001 - regression_loss: 1.5927 - classification_loss: 0.3074
1129/1500 [=====================>........] - ETA: 4:52 - loss: 1.8996 - regression_loss: 1.5923 - classification_loss: 0.3073
1130/1500 [=====================>........] - ETA: 4:51 - loss: 1.8999 - regression_loss: 1.5926 - classification_loss: 0.3073
1131/1500 [=====================>........] - ETA: 4:50 - loss: 1.9006 - regression_loss: 1.5932 - classification_loss: 0.3074
1132/1500 [=====================>........] - ETA: 4:49 - loss: 1.9014 - regression_loss: 1.5938 - classification_loss: 0.3076
1133/1500 [=====================>........] - ETA: 4:48 - loss: 1.9013 - regression_loss: 1.5936 - classification_loss: 0.3077
1134/1500 [=====================>........] - ETA: 4:48 - loss: 1.9002 - regression_loss: 1.5927 - classification_loss: 0.3075
1135/1500 [=====================>........] - ETA: 4:47 - loss: 1.8996 - regression_loss: 1.5922 - classification_loss: 0.3074
1136/1500 [=====================>........] - ETA: 4:46 - loss: 1.8996 - regression_loss: 1.5924 - classification_loss: 0.3073
1137/1500 [=====================>........] - ETA: 4:45 - loss: 1.8987 - regression_loss: 1.5916 - classification_loss: 0.3072
1138/1500 [=====================>........] - ETA: 4:45 - loss: 1.8981 - regression_loss: 1.5910 - classification_loss: 0.3071
1139/1500 [=====================>........] - ETA: 4:44 - loss: 1.8984 - regression_loss: 1.5914 - classification_loss: 0.3070
1140/1500 [=====================>........] - ETA: 4:43 - loss: 1.8985 - regression_loss: 1.5915 - classification_loss: 0.3069
1141/1500 [=====================>........] - ETA: 4:42 - loss: 1.8974 - regression_loss: 1.5906 - classification_loss: 0.3067
1142/1500 [=====================>........] - ETA: 4:41 - loss: 1.8974 - regression_loss: 1.5906 - classification_loss: 0.3068
1143/1500 [=====================>........] - ETA: 4:40 - loss: 1.8976 - regression_loss: 1.5908 - classification_loss: 0.3068
1144/1500 [=====================>........] - ETA: 4:40 - loss: 1.8972 - regression_loss: 1.5904 - classification_loss: 0.3067
1145/1500 [=====================>........] - ETA: 4:39 - loss: 1.8979 - regression_loss: 1.5910 - classification_loss: 0.3068
1146/1500 [=====================>........] - ETA: 4:38 - loss: 1.8985 - regression_loss: 1.5915 - classification_loss: 0.3070
1147/1500 [=====================>........] - ETA: 4:37 - loss: 1.8987 - regression_loss: 1.5917 - classification_loss: 0.3069
1148/1500 [=====================>........] - ETA: 4:36 - loss: 1.8993 - regression_loss: 1.5922 - classification_loss: 0.3071
1149/1500 [=====================>........] - ETA: 4:35 - loss: 1.9003 - regression_loss: 1.5929 - classification_loss: 0.3073
1150/1500 [======================>.......] - ETA: 4:35 - loss: 1.9010 - regression_loss: 1.5935 - classification_loss: 0.3075
1151/1500 [======================>.......] - ETA: 4:34 - loss: 1.9010 - regression_loss: 1.5936 - classification_loss: 0.3075
1152/1500 [======================>.......] - ETA: 4:33 - loss: 1.9017 - regression_loss: 1.5942 - classification_loss: 0.3075
1153/1500 [======================>.......] - ETA: 4:33 - loss: 1.9012 - regression_loss: 1.5938 - classification_loss: 0.3074
1154/1500 [======================>.......] - ETA: 4:32 - loss: 1.9006 - regression_loss: 1.5932 - classification_loss: 0.3074
1155/1500 [======================>.......] - ETA: 4:31 - loss: 1.9010 - regression_loss: 1.5935 - classification_loss: 0.3075
1156/1500 [======================>.......] - ETA: 4:30 - loss: 1.9005 - regression_loss: 1.5931 - classification_loss: 0.3074
1157/1500 [======================>.......] - ETA: 4:29 - loss: 1.8998 - regression_loss: 1.5926 - classification_loss: 0.3072
1158/1500 [======================>.......] - ETA: 4:28 - loss: 1.8988 - regression_loss: 1.5918 - classification_loss: 0.3070
1159/1500 [======================>.......] - ETA: 4:27 - loss: 1.8987 - regression_loss: 1.5917 - classification_loss: 0.3070
1160/1500 [======================>.......] - ETA: 4:26 - loss: 1.8978 - regression_loss: 1.5909 - classification_loss: 0.3068
1161/1500 [======================>.......] - ETA: 4:26 - loss: 1.8985 - regression_loss: 1.5914 - classification_loss: 0.3071
1162/1500 [======================>.......] - ETA: 4:25 - loss: 1.8981 - regression_loss: 1.5910 - classification_loss: 0.3070
1163/1500 [======================>.......] - ETA: 4:24 - loss: 1.8986 - regression_loss: 1.5915 - classification_loss: 0.3071
1164/1500 [======================>.......] - ETA: 4:24 - loss: 1.8986 - regression_loss: 1.5916 - classification_loss: 0.3070
1165/1500 [======================>.......] - ETA: 4:23 - loss: 1.8978 - regression_loss: 1.5909 - classification_loss: 0.3068
1166/1500 [======================>.......] - ETA: 4:22 - loss: 1.8979 - regression_loss: 1.5912 - classification_loss: 0.3067
1167/1500 [======================>.......] - ETA: 4:21 - loss: 1.8982 - regression_loss: 1.5915 - classification_loss: 0.3067
1168/1500 [======================>.......] - ETA: 4:20 - loss: 1.8973 - regression_loss: 1.5908 - classification_loss: 0.3065
1169/1500 [======================>.......] - ETA: 4:19 - loss: 1.8967 - regression_loss: 1.5902 - classification_loss: 0.3064
1170/1500 [======================>.......] - ETA: 4:18 - loss: 1.8969 - regression_loss: 1.5905 - classification_loss: 0.3064
1171/1500 [======================>.......] - ETA: 4:18 - loss: 1.8964 - regression_loss: 1.5902 - classification_loss: 0.3062
1172/1500 [======================>.......] - ETA: 4:17 - loss: 1.8964 - regression_loss: 1.5902 - classification_loss: 0.3062
1173/1500 [======================>.......] - ETA: 4:16 - loss: 1.8969 - regression_loss: 1.5906 - classification_loss: 0.3063
1174/1500 [======================>.......] - ETA: 4:15 - loss: 1.8968 - regression_loss: 1.5905 - classification_loss: 0.3062
1175/1500 [======================>.......] - ETA: 4:14 - loss: 1.8966 - regression_loss: 1.5904 - classification_loss: 0.3062
1176/1500 [======================>.......] - ETA: 4:14 - loss: 1.8976 - regression_loss: 1.5910 - classification_loss: 0.3066
1177/1500 [======================>.......] - ETA: 4:13 - loss: 1.8976 - regression_loss: 1.5911 - classification_loss: 0.3065
1178/1500 [======================>.......] - ETA: 4:13 - loss: 1.8978 - regression_loss: 1.5913 - classification_loss: 0.3065
1179/1500 [======================>.......] - ETA: 4:12 - loss: 1.8971 - regression_loss: 1.5908 - classification_loss: 0.3064
1180/1500 [======================>.......] - ETA: 4:11 - loss: 1.8968 - regression_loss: 1.5906 - classification_loss: 0.3062
1181/1500 [======================>.......] - ETA: 4:10 - loss: 1.8971 - regression_loss: 1.5908 - classification_loss: 0.3063
1182/1500 [======================>.......] - ETA: 4:09 - loss: 1.8968 - regression_loss: 1.5905 - classification_loss: 0.3062
1183/1500 [======================>.......] - ETA: 4:09 - loss: 1.8964 - regression_loss: 1.5902 - classification_loss: 0.3062
1184/1500 [======================>.......] - ETA: 4:08 - loss: 1.8970 - regression_loss: 1.5907 - classification_loss: 0.3063
1185/1500 [======================>.......] - ETA: 4:07 - loss: 1.8972 - regression_loss: 1.5909 - classification_loss: 0.3064
1186/1500 [======================>.......] - ETA: 4:06 - loss: 1.8968 - regression_loss: 1.5904 - classification_loss: 0.3064
1187/1500 [======================>.......] - ETA: 4:05 - loss: 1.8964 - regression_loss: 1.5901 - classification_loss: 0.3063
1188/1500 [======================>.......] - ETA: 4:05 - loss: 1.8971 - regression_loss: 1.5901 - classification_loss: 0.3070
1189/1500 [======================>.......] - ETA: 4:04 - loss: 1.8980 - regression_loss: 1.5909 - classification_loss: 0.3071
1190/1500 [======================>.......] - ETA: 4:03 - loss: 1.8980 - regression_loss: 1.5910 - classification_loss: 0.3070
1191/1500 [======================>.......] - ETA: 4:03 - loss: 1.8974 - regression_loss: 1.5905 - classification_loss: 0.3068
1192/1500 [======================>.......] - ETA: 4:02 - loss: 1.8970 - regression_loss: 1.5903 - classification_loss: 0.3067
1193/1500 [======================>.......] - ETA: 4:01 - loss: 1.8969 - regression_loss: 1.5904 - classification_loss: 0.3066
1194/1500 [======================>.......] - ETA: 4:00 - loss: 1.8973 - regression_loss: 1.5907 - classification_loss: 0.3066
1195/1500 [======================>.......] - ETA: 4:00 - loss: 1.8974 - regression_loss: 1.5908 - classification_loss: 0.3066
1196/1500 [======================>.......] - ETA: 3:59 - loss: 1.8974 - regression_loss: 1.5909 - classification_loss: 0.3065
1197/1500 [======================>.......] - ETA: 3:58 - loss: 1.8972 - regression_loss: 1.5907 - classification_loss: 0.3065
1198/1500 [======================>.......] - ETA: 3:57 - loss: 1.8974 - regression_loss: 1.5909 - classification_loss: 0.3066
1199/1500 [======================>.......] - ETA: 3:56 - loss: 1.8976 - regression_loss: 1.5910 - classification_loss: 0.3067
1200/1500 [=======================>......] - ETA: 3:56 - loss: 1.8977 - regression_loss: 1.5910 - classification_loss: 0.3067
1201/1500 [=======================>......] - ETA: 3:55 - loss: 1.8987 - regression_loss: 1.5917 - classification_loss: 0.3070
1202/1500 [=======================>......] - ETA: 3:54 - loss: 1.8996 - regression_loss: 1.5923 - classification_loss: 0.3073
1203/1500 [=======================>......] - ETA: 3:53 - loss: 1.8992 - regression_loss: 1.5920 - classification_loss: 0.3072
1204/1500 [=======================>......] - ETA: 3:53 - loss: 1.8997 - regression_loss: 1.5924 - classification_loss: 0.3073
1205/1500 [=======================>......] - ETA: 3:52 - loss: 1.8995 - regression_loss: 1.5922 - classification_loss: 0.3073
1206/1500 [=======================>......] - ETA: 3:51 - loss: 1.8995 - regression_loss: 1.5923 - classification_loss: 0.3072
1207/1500 [=======================>......] - ETA: 3:50 - loss: 1.9000 - regression_loss: 1.5925 - classification_loss: 0.3074
1208/1500 [=======================>......] - ETA: 3:49 - loss: 1.8995 - regression_loss: 1.5921 - classification_loss: 0.3074
1209/1500 [=======================>......] - ETA: 3:49 - loss: 1.8993 - regression_loss: 1.5919 - classification_loss: 0.3074
1210/1500 [=======================>......] - ETA: 3:48 - loss: 1.8989 - regression_loss: 1.5916 - classification_loss: 0.3073
1211/1500 [=======================>......] - ETA: 3:47 - loss: 1.9000 - regression_loss: 1.5923 - classification_loss: 0.3077
1212/1500 [=======================>......] - ETA: 3:46 - loss: 1.8996 - regression_loss: 1.5920 - classification_loss: 0.3075
1213/1500 [=======================>......] - ETA: 3:46 - loss: 1.8995 - regression_loss: 1.5921 - classification_loss: 0.3074
1214/1500 [=======================>......] - ETA: 3:45 - loss: 1.8990 - regression_loss: 1.5917 - classification_loss: 0.3073
1215/1500 [=======================>......] - ETA: 3:44 - loss: 1.8987 - regression_loss: 1.5915 - classification_loss: 0.3072
1216/1500 [=======================>......] - ETA: 3:43 - loss: 1.8990 - regression_loss: 1.5917 - classification_loss: 0.3072
1217/1500 [=======================>......] - ETA: 3:42 - loss: 1.8990 - regression_loss: 1.5918 - classification_loss: 0.3072
1218/1500 [=======================>......] - ETA: 3:42 - loss: 1.8983 - regression_loss: 1.5912 - classification_loss: 0.3071
1219/1500 [=======================>......] - ETA: 3:41 - loss: 1.8986 - regression_loss: 1.5916 - classification_loss: 0.3071
1220/1500 [=======================>......] - ETA: 3:40 - loss: 1.8984 - regression_loss: 1.5914 - classification_loss: 0.3070
1221/1500 [=======================>......] - ETA: 3:39 - loss: 1.8975 - regression_loss: 1.5907 - classification_loss: 0.3068
1222/1500 [=======================>......] - ETA: 3:39 - loss: 1.8968 - regression_loss: 1.5901 - classification_loss: 0.3067
1223/1500 [=======================>......] - ETA: 3:38 - loss: 1.8964 - regression_loss: 1.5899 - classification_loss: 0.3066
1224/1500 [=======================>......] - ETA: 3:37 - loss: 1.8958 - regression_loss: 1.5894 - classification_loss: 0.3064
1225/1500 [=======================>......] - ETA: 3:37 - loss: 1.8964 - regression_loss: 1.5899 - classification_loss: 0.3065
1226/1500 [=======================>......] - ETA: 3:36 - loss: 1.8956 - regression_loss: 1.5892 - classification_loss: 0.3065
1227/1500 [=======================>......] - ETA: 3:35 - loss: 1.8946 - regression_loss: 1.5882 - classification_loss: 0.3063
1228/1500 [=======================>......] - ETA: 3:34 - loss: 1.8948 - regression_loss: 1.5884 - classification_loss: 0.3064
1229/1500 [=======================>......] - ETA: 3:34 - loss: 1.8962 - regression_loss: 1.5897 - classification_loss: 0.3066
1230/1500 [=======================>......] - ETA: 3:33 - loss: 1.8956 - regression_loss: 1.5891 - classification_loss: 0.3064
1231/1500 [=======================>......] - ETA: 3:32 - loss: 1.8953 - regression_loss: 1.5890 - classification_loss: 0.3063
1232/1500 [=======================>......] - ETA: 3:31 - loss: 1.8949 - regression_loss: 1.5888 - classification_loss: 0.3061
1233/1500 [=======================>......] - ETA: 3:30 - loss: 1.8945 - regression_loss: 1.5885 - classification_loss: 0.3060
1234/1500 [=======================>......] - ETA: 3:30 - loss: 1.8950 - regression_loss: 1.5890 - classification_loss: 0.3060
1235/1500 [=======================>......] - ETA: 3:29 - loss: 1.8951 - regression_loss: 1.5891 - classification_loss: 0.3060
1236/1500 [=======================>......] - ETA: 3:28 - loss: 1.8956 - regression_loss: 1.5895 - classification_loss: 0.3062
1237/1500 [=======================>......] - ETA: 3:27 - loss: 1.8961 - regression_loss: 1.5899 - classification_loss: 0.3063
1238/1500 [=======================>......] - ETA: 3:26 - loss: 1.8953 - regression_loss: 1.5893 - classification_loss: 0.3061
1239/1500 [=======================>......] - ETA: 3:26 - loss: 1.8959 - regression_loss: 1.5897 - classification_loss: 0.3061
1240/1500 [=======================>......] - ETA: 3:25 - loss: 1.8954 - regression_loss: 1.5894 - classification_loss: 0.3060
1241/1500 [=======================>......] - ETA: 3:24 - loss: 1.8955 - regression_loss: 1.5895 - classification_loss: 0.3060
1242/1500 [=======================>......] - ETA: 3:23 - loss: 1.8956 - regression_loss: 1.5895 - classification_loss: 0.3061
1243/1500 [=======================>......] - ETA: 3:23 - loss: 1.8954 - regression_loss: 1.5894 - classification_loss: 0.3060
1244/1500 [=======================>......] - ETA: 3:22 - loss: 1.8951 - regression_loss: 1.5892 - classification_loss: 0.3059
1245/1500 [=======================>......] - ETA: 3:21 - loss: 1.8946 - regression_loss: 1.5888 - classification_loss: 0.3059
1246/1500 [=======================>......] - ETA: 3:21 - loss: 1.8943 - regression_loss: 1.5884 - classification_loss: 0.3059
1247/1500 [=======================>......] - ETA: 3:20 - loss: 1.8952 - regression_loss: 1.5891 - classification_loss: 0.3061
1248/1500 [=======================>......] - ETA: 3:19 - loss: 1.8957 - regression_loss: 1.5895 - classification_loss: 0.3062
1249/1500 [=======================>......] - ETA: 3:19 - loss: 1.8963 - regression_loss: 1.5901 - classification_loss: 0.3062
1250/1500 [========================>.....] - ETA: 3:18 - loss: 1.8965 - regression_loss: 1.5902 - classification_loss: 0.3063
1251/1500 [========================>.....] - ETA: 3:17 - loss: 1.8972 - regression_loss: 1.5909 - classification_loss: 0.3063
1252/1500 [========================>.....] - ETA: 3:16 - loss: 1.8965 - regression_loss: 1.5902 - classification_loss: 0.3063
1253/1500 [========================>.....] - ETA: 3:15 - loss: 1.8962 - regression_loss: 1.5901 - classification_loss: 0.3062
1254/1500 [========================>.....] - ETA: 3:14 - loss: 1.8964 - regression_loss: 1.5902 - classification_loss: 0.3062
1255/1500 [========================>.....] - ETA: 3:14 - loss: 1.8967 - regression_loss: 1.5905 - classification_loss: 0.3062
1256/1500 [========================>.....] - ETA: 3:13 - loss: 1.8963 - regression_loss: 1.5903 - classification_loss: 0.3060
1257/1500 [========================>.....] - ETA: 3:12 - loss: 1.8961 - regression_loss: 1.5901 - classification_loss: 0.3060
1258/1500 [========================>.....] - ETA: 3:11 - loss: 1.8959 - regression_loss: 1.5897 - classification_loss: 0.3062
1259/1500 [========================>.....] - ETA: 3:10 - loss: 1.8950 - regression_loss: 1.5890 - classification_loss: 0.3060
1260/1500 [========================>.....] - ETA: 3:09 - loss: 1.8946 - regression_loss: 1.5887 - classification_loss: 0.3059
1261/1500 [========================>.....] - ETA: 3:09 - loss: 1.8953 - regression_loss: 1.5891 - classification_loss: 0.3063
1262/1500 [========================>.....] - ETA: 3:08 - loss: 1.8946 - regression_loss: 1.5885 - classification_loss: 0.3061
1263/1500 [========================>.....] - ETA: 3:07 - loss: 1.8948 - regression_loss: 1.5886 - classification_loss: 0.3062
1264/1500 [========================>.....] - ETA: 3:06 - loss: 1.8958 - regression_loss: 1.5894 - classification_loss: 0.3064
1265/1500 [========================>.....] - ETA: 3:05 - loss: 1.8954 - regression_loss: 1.5891 - classification_loss: 0.3063
1266/1500 [========================>.....] - ETA: 3:05 - loss: 1.8954 - regression_loss: 1.5890 - classification_loss: 0.3063
1267/1500 [========================>.....] - ETA: 3:04 - loss: 1.8965 - regression_loss: 1.5898 - classification_loss: 0.3067
1268/1500 [========================>.....] - ETA: 3:03 - loss: 1.8963 - regression_loss: 1.5896 - classification_loss: 0.3067
1269/1500 [========================>.....] - ETA: 3:02 - loss: 1.8959 - regression_loss: 1.5893 - classification_loss: 0.3066
1270/1500 [========================>.....] - ETA: 3:01 - loss: 1.8957 - regression_loss: 1.5889 - classification_loss: 0.3068
1271/1500 [========================>.....] - ETA: 3:01 - loss: 1.8951 - regression_loss: 1.5885 - classification_loss: 0.3067
1272/1500 [========================>.....] - ETA: 3:00 - loss: 1.8948 - regression_loss: 1.5883 - classification_loss: 0.3065
1273/1500 [========================>.....] - ETA: 2:59 - loss: 1.8948 - regression_loss: 1.5883 - classification_loss: 0.3065
1274/1500 [========================>.....] - ETA: 2:59 - loss: 1.8952 - regression_loss: 1.5887 - classification_loss: 0.3065
1275/1500 [========================>.....] - ETA: 2:58 - loss: 1.8955 - regression_loss: 1.5889 - classification_loss: 0.3066
1276/1500 [========================>.....] - ETA: 2:57 - loss: 1.8959 - regression_loss: 1.5892 - classification_loss: 0.3068
1277/1500 [========================>.....] - ETA: 2:56 - loss: 1.8967 - regression_loss: 1.5897 - classification_loss: 0.3070
1278/1500 [========================>.....] - ETA: 2:55 - loss: 1.8966 - regression_loss: 1.5897 - classification_loss: 0.3068
1279/1500 [========================>.....] - ETA: 2:55 - loss: 1.8976 - regression_loss: 1.5904 - classification_loss: 0.3071
1280/1500 [========================>.....] - ETA: 2:54 - loss: 1.8979 - regression_loss: 1.5907 - classification_loss: 0.3072
1281/1500 [========================>.....] - ETA: 2:53 - loss: 1.8977 - regression_loss: 1.5905 - classification_loss: 0.3072
1282/1500 [========================>.....] - ETA: 2:52 - loss: 1.8982 - regression_loss: 1.5909 - classification_loss: 0.3074
1283/1500 [========================>.....] - ETA: 2:51 - loss: 1.8976 - regression_loss: 1.5903 - classification_loss: 0.3073
1284/1500 [========================>.....] - ETA: 2:51 - loss: 1.8974 - regression_loss: 1.5902 - classification_loss: 0.3072
1285/1500 [========================>.....] - ETA: 2:50 - loss: 1.8968 - regression_loss: 1.5896 - classification_loss: 0.3072
1286/1500 [========================>.....] - ETA: 2:49 - loss: 1.8969 - regression_loss: 1.5898 - classification_loss: 0.3071
1287/1500 [========================>.....] - ETA: 2:48 - loss: 1.8965 - regression_loss: 1.5893 - classification_loss: 0.3071
1288/1500 [========================>.....] - ETA: 2:47 - loss: 1.8960 - regression_loss: 1.5890 - classification_loss: 0.3071
1289/1500 [========================>.....] - ETA: 2:46 - loss: 1.8963 - regression_loss: 1.5893 - classification_loss: 0.3070
1290/1500 [========================>.....] - ETA: 2:46 - loss: 1.8974 - regression_loss: 1.5903 - classification_loss: 0.3071
1291/1500 [========================>.....] - ETA: 2:45 - loss: 1.8974 - regression_loss: 1.5902 - classification_loss: 0.3072
1292/1500 [========================>.....] - ETA: 2:44 - loss: 1.8966 - regression_loss: 1.5896 - classification_loss: 0.3070
1293/1500 [========================>.....] - ETA: 2:43 - loss: 1.8971 - regression_loss: 1.5899 - classification_loss: 0.3072
1294/1500 [========================>.....] - ETA: 2:42 - loss: 1.8962 - regression_loss: 1.5891 - classification_loss: 0.3071
1295/1500 [========================>.....] - ETA: 2:42 - loss: 1.8974 - regression_loss: 1.5897 - classification_loss: 0.3077
1296/1500 [========================>.....] - ETA: 2:41 - loss: 1.8978 - regression_loss: 1.5900 - classification_loss: 0.3078
1297/1500 [========================>.....] - ETA: 2:40 - loss: 1.8983 - regression_loss: 1.5904 - classification_loss: 0.3079
1298/1500 [========================>.....] - ETA: 2:39 - loss: 1.8983 - regression_loss: 1.5904 - classification_loss: 0.3079
1299/1500 [========================>.....] - ETA: 2:38 - loss: 1.8986 - regression_loss: 1.5907 - classification_loss: 0.3079
1300/1500 [=========================>....] - ETA: 2:37 - loss: 1.8987 - regression_loss: 1.5907 - classification_loss: 0.3080
1301/1500 [=========================>....] - ETA: 2:37 - loss: 1.8984 - regression_loss: 1.5904 - classification_loss: 0.3080
1302/1500 [=========================>....] - ETA: 2:36 - loss: 1.8987 - regression_loss: 1.5907 - classification_loss: 0.3080
1303/1500 [=========================>....] - ETA: 2:35 - loss: 1.8983 - regression_loss: 1.5904 - classification_loss: 0.3079
1304/1500 [=========================>....] - ETA: 2:34 - loss: 1.8985 - regression_loss: 1.5906 - classification_loss: 0.3079
1305/1500 [=========================>....] - ETA: 2:33 - loss: 1.8987 - regression_loss: 1.5908 - classification_loss: 0.3079
1306/1500 [=========================>....] - ETA: 2:33 - loss: 1.8982 - regression_loss: 1.5904 - classification_loss: 0.3078
1307/1500 [=========================>....] - ETA: 2:32 - loss: 1.8982 - regression_loss: 1.5904 - classification_loss: 0.3078
1308/1500 [=========================>....] - ETA: 2:31 - loss: 1.8981 - regression_loss: 1.5903 - classification_loss: 0.3078
1309/1500 [=========================>....] - ETA: 2:30 - loss: 1.8979 - regression_loss: 1.5901 - classification_loss: 0.3077
1310/1500 [=========================>....] - ETA: 2:29 - loss: 1.8978 - regression_loss: 1.5902 - classification_loss: 0.3077
1311/1500 [=========================>....] - ETA: 2:29 - loss: 1.8974 - regression_loss: 1.5899 - classification_loss: 0.3075
1312/1500 [=========================>....] - ETA: 2:28 - loss: 1.8977 - regression_loss: 1.5902 - classification_loss: 0.3075
1313/1500 [=========================>....] - ETA: 2:27 - loss: 1.8974 - regression_loss: 1.5901 - classification_loss: 0.3074
1314/1500 [=========================>....] - ETA: 2:26 - loss: 1.8970 - regression_loss: 1.5897 - classification_loss: 0.3073
1315/1500 [=========================>....] - ETA: 2:26 - loss: 1.8968 - regression_loss: 1.5896 - classification_loss: 0.3072
1316/1500 [=========================>....] - ETA: 2:25 - loss: 1.8965 - regression_loss: 1.5891 - classification_loss: 0.3074
1317/1500 [=========================>....] - ETA: 2:24 - loss: 1.8966 - regression_loss: 1.5893 - classification_loss: 0.3073
1318/1500 [=========================>....] - ETA: 2:23 - loss: 1.8966 - regression_loss: 1.5893 - classification_loss: 0.3073
1319/1500 [=========================>....] - ETA: 2:22 - loss: 1.8961 - regression_loss: 1.5889 - classification_loss: 0.3072
1320/1500 [=========================>....] - ETA: 2:22 - loss: 1.8956 - regression_loss: 1.5885 - classification_loss: 0.3071
1321/1500 [=========================>....] - ETA: 2:21 - loss: 1.8949 - regression_loss: 1.5880 - classification_loss: 0.3069
1322/1500 [=========================>....] - ETA: 2:20 - loss: 1.8952 - regression_loss: 1.5881 - classification_loss: 0.3070
1323/1500 [=========================>....] - ETA: 2:19 - loss: 1.8945 - regression_loss: 1.5876 - classification_loss: 0.3069
1324/1500 [=========================>....] - ETA: 2:18 - loss: 1.8937 - regression_loss: 1.5870 - classification_loss: 0.3067
1325/1500 [=========================>....] - ETA: 2:18 - loss: 1.8937 - regression_loss: 1.5870 - classification_loss: 0.3067
1326/1500 [=========================>....] - ETA: 2:17 - loss: 1.8941 - regression_loss: 1.5874 - classification_loss: 0.3067
1327/1500 [=========================>....] - ETA: 2:16 - loss: 1.8941 - regression_loss: 1.5874 - classification_loss: 0.3066
1328/1500 [=========================>....] - ETA: 2:15 - loss: 1.8942 - regression_loss: 1.5875 - classification_loss: 0.3067
1329/1500 [=========================>....] - ETA: 2:15 - loss: 1.8951 - regression_loss: 1.5883 - classification_loss: 0.3067
1330/1500 [=========================>....] - ETA: 2:14 - loss: 1.8950 - regression_loss: 1.5883 - classification_loss: 0.3066
1331/1500 [=========================>....] - ETA: 2:13 - loss: 1.8952 - regression_loss: 1.5885 - classification_loss: 0.3066
1332/1500 [=========================>....] - ETA: 2:12 - loss: 1.8949 - regression_loss: 1.5884 - classification_loss: 0.3065
1333/1500 [=========================>....] - ETA: 2:11 - loss: 1.8953 - regression_loss: 1.5888 - classification_loss: 0.3065
1334/1500 [=========================>....] - ETA: 2:11 - loss: 1.8949 - regression_loss: 1.5885 - classification_loss: 0.3064
1335/1500 [=========================>....] - ETA: 2:10 - loss: 1.8946 - regression_loss: 1.5883 - classification_loss: 0.3063
1336/1500 [=========================>....] - ETA: 2:09 - loss: 1.8940 - regression_loss: 1.5878 - classification_loss: 0.3062
1337/1500 [=========================>....] - ETA: 2:08 - loss: 1.8945 - regression_loss: 1.5881 - classification_loss: 0.3064
1338/1500 [=========================>....] - ETA: 2:08 - loss: 1.8952 - regression_loss: 1.5886 - classification_loss: 0.3065
1339/1500 [=========================>....] - ETA: 2:07 - loss: 1.8956 - regression_loss: 1.5891 - classification_loss: 0.3065
1340/1500 [=========================>....] - ETA: 2:06 - loss: 1.8952 - regression_loss: 1.5887 - classification_loss: 0.3065
1341/1500 [=========================>....] - ETA: 2:05 - loss: 1.8955 - regression_loss: 1.5889 - classification_loss: 0.3066
1342/1500 [=========================>....] - ETA: 2:05 - loss: 1.8955 - regression_loss: 1.5888 - classification_loss: 0.3067
1343/1500 [=========================>....] - ETA: 2:04 - loss: 1.8949 - regression_loss: 1.5883 - classification_loss: 0.3066
1344/1500 [=========================>....] - ETA: 2:03 - loss: 1.8952 - regression_loss: 1.5885 - classification_loss: 0.3067
1345/1500 [=========================>....] - ETA: 2:02 - loss: 1.8954 - regression_loss: 1.5886 - classification_loss: 0.3067
1346/1500 [=========================>....] - ETA: 2:01 - loss: 1.8954 - regression_loss: 1.5887 - classification_loss: 0.3067
1347/1500 [=========================>....] - ETA: 2:01 - loss: 1.8957 - regression_loss: 1.5890 - classification_loss: 0.3067
1348/1500 [=========================>....] - ETA: 2:00 - loss: 1.8950 - regression_loss: 1.5885 - classification_loss: 0.3065
1349/1500 [=========================>....] - ETA: 1:59 - loss: 1.8942 - regression_loss: 1.5879 - classification_loss: 0.3064
1350/1500 [==========================>...] - ETA: 1:58 - loss: 1.8946 - regression_loss: 1.5881 - classification_loss: 0.3065
1351/1500 [==========================>...] - ETA: 1:57 - loss: 1.8938 - regression_loss: 1.5875 - classification_loss: 0.3064
1352/1500 [==========================>...] - ETA: 1:57 - loss: 1.8940 - regression_loss: 1.5876 - classification_loss: 0.3065
1353/1500 [==========================>...] - ETA: 1:56 - loss: 1.8942 - regression_loss: 1.5877 - classification_loss: 0.3065
1354/1500 [==========================>...] - ETA: 1:55 - loss: 1.8934 - regression_loss: 1.5871 - classification_loss: 0.3063
1355/1500 [==========================>...] - ETA: 1:55 - loss: 1.8930 - regression_loss: 1.5868 - classification_loss: 0.3062
1356/1500 [==========================>...] - ETA: 1:54 - loss: 1.8935 - regression_loss: 1.5872 - classification_loss: 0.3063
1357/1500 [==========================>...] - ETA: 1:53 - loss: 1.8930 - regression_loss: 1.5868 - classification_loss: 0.3063
1358/1500 [==========================>...] - ETA: 1:52 - loss: 1.8934 - regression_loss: 1.5871 - classification_loss: 0.3064
1359/1500 [==========================>...] - ETA: 1:51 - loss: 1.8938 - regression_loss: 1.5873 - classification_loss: 0.3064
1360/1500 [==========================>...] - ETA: 1:51 - loss: 1.8932 - regression_loss: 1.5869 - classification_loss: 0.3063
1361/1500 [==========================>...] - ETA: 1:50 - loss: 1.8929 - regression_loss: 1.5867 - classification_loss: 0.3062
1362/1500 [==========================>...] - ETA: 1:49 - loss: 1.8920 - regression_loss: 1.5860 - classification_loss: 0.3060
1363/1500 [==========================>...] - ETA: 1:48 - loss: 1.8916 - regression_loss: 1.5858 - classification_loss: 0.3059
1364/1500 [==========================>...] - ETA: 1:47 - loss: 1.8920 - regression_loss: 1.5860 - classification_loss: 0.3060
1365/1500 [==========================>...] - ETA: 1:47 - loss: 1.8927 - regression_loss: 1.5865 - classification_loss: 0.3062
1366/1500 [==========================>...] - ETA: 1:46 - loss: 1.8932 - regression_loss: 1.5871 - classification_loss: 0.3061
1367/1500 [==========================>...] - ETA: 1:45 - loss: 1.8936 - regression_loss: 1.5871 - classification_loss: 0.3065
1368/1500 [==========================>...] - ETA: 1:44 - loss: 1.8936 - regression_loss: 1.5870 - classification_loss: 0.3066
1369/1500 [==========================>...] - ETA: 1:43 - loss: 1.8936 - regression_loss: 1.5869 - classification_loss: 0.3067
1370/1500 [==========================>...] - ETA: 1:42 - loss: 1.8935 - regression_loss: 1.5869 - classification_loss: 0.3067
1371/1500 [==========================>...] - ETA: 1:42 - loss: 1.8930 - regression_loss: 1.5865 - classification_loss: 0.3065
1372/1500 [==========================>...] - ETA: 1:41 - loss: 1.8929 - regression_loss: 1.5865 - classification_loss: 0.3064
1373/1500 [==========================>...] - ETA: 1:40 - loss: 1.8934 - regression_loss: 1.5865 - classification_loss: 0.3069
1374/1500 [==========================>...] - ETA: 1:39 - loss: 1.8932 - regression_loss: 1.5863 - classification_loss: 0.3068
1375/1500 [==========================>...] - ETA: 1:39 - loss: 1.8925 - regression_loss: 1.5858 - classification_loss: 0.3067
1376/1500 [==========================>...] - ETA: 1:38 - loss: 1.8926 - regression_loss: 1.5859 - classification_loss: 0.3067
1377/1500 [==========================>...] - ETA: 1:37 - loss: 1.8920 - regression_loss: 1.5854 - classification_loss: 0.3066
1378/1500 [==========================>...] - ETA: 1:36 - loss: 1.8911 - regression_loss: 1.5847 - classification_loss: 0.3064
1379/1500 [==========================>...] - ETA: 1:35 - loss: 1.8906 - regression_loss: 1.5843 - classification_loss: 0.3063
1380/1500 [==========================>...] - ETA: 1:35 - loss: 1.8911 - regression_loss: 1.5846 - classification_loss: 0.3065
1381/1500 [==========================>...] - ETA: 1:34 - loss: 1.8910 - regression_loss: 1.5845 - classification_loss: 0.3065
1382/1500 [==========================>...] - ETA: 1:33 - loss: 1.8912 - regression_loss: 1.5846 - classification_loss: 0.3065
1383/1500 [==========================>...] - ETA: 1:32 - loss: 1.8907 - regression_loss: 1.5843 - classification_loss: 0.3064
1384/1500 [==========================>...] - ETA: 1:31 - loss: 1.8914 - regression_loss: 1.5847 - classification_loss: 0.3066
1385/1500 [==========================>...] - ETA: 1:31 - loss: 1.8909 - regression_loss: 1.5843 - classification_loss: 0.3066
1386/1500 [==========================>...] - ETA: 1:30 - loss: 1.8909 - regression_loss: 1.5843 - classification_loss: 0.3065
1387/1500 [==========================>...] - ETA: 1:29 - loss: 1.8907 - regression_loss: 1.5842 - classification_loss: 0.3065
1388/1500 [==========================>...] - ETA: 1:28 - loss: 1.8905 - regression_loss: 1.5841 - classification_loss: 0.3064
1389/1500 [==========================>...] - ETA: 1:28 - loss: 1.8900 - regression_loss: 1.5837 - classification_loss: 0.3063
1390/1500 [==========================>...] - ETA: 1:27 - loss: 1.8900 - regression_loss: 1.5836 - classification_loss: 0.3063
1391/1500 [==========================>...] - ETA: 1:26 - loss: 1.8898 - regression_loss: 1.5835 - classification_loss: 0.3063
1392/1500 [==========================>...] - ETA: 1:25 - loss: 1.8894 - regression_loss: 1.5833 - classification_loss: 0.3061
1393/1500 [==========================>...] - ETA: 1:24 - loss: 1.8889 - regression_loss: 1.5829 - classification_loss: 0.3060
1394/1500 [==========================>...] - ETA: 1:23 - loss: 1.8895 - regression_loss: 1.5834 - classification_loss: 0.3062
1395/1500 [==========================>...] - ETA: 1:23 - loss: 1.8891 - regression_loss: 1.5830 - classification_loss: 0.3061
1396/1500 [==========================>...] - ETA: 1:22 - loss: 1.8885 - regression_loss: 1.5825 - classification_loss: 0.3060
1397/1500 [==========================>...] - ETA: 1:21 - loss: 1.8879 - regression_loss: 1.5821 - classification_loss: 0.3059
1398/1500 [==========================>...] - ETA: 1:20 - loss: 1.8880 - regression_loss: 1.5822 - classification_loss: 0.3058
1399/1500 [==========================>...] - ETA: 1:19 - loss: 1.8883 - regression_loss: 1.5824 - classification_loss: 0.3059
1400/1500 [===========================>..] - ETA: 1:19 - loss: 1.8885 - regression_loss: 1.5827 - classification_loss: 0.3058
1401/1500 [===========================>..] - ETA: 1:18 - loss: 1.8892 - regression_loss: 1.5831 - classification_loss: 0.3061
1402/1500 [===========================>..] - ETA: 1:17 - loss: 1.8905 - regression_loss: 1.5841 - classification_loss: 0.3065
1403/1500 [===========================>..] - ETA: 1:16 - loss: 1.8902 - regression_loss: 1.5838 - classification_loss: 0.3064
1404/1500 [===========================>..] - ETA: 1:15 - loss: 1.8901 - regression_loss: 1.5837 - classification_loss: 0.3064
1405/1500 [===========================>..] - ETA: 1:15 - loss: 1.8900 - regression_loss: 1.5835 - classification_loss: 0.3065
1406/1500 [===========================>..] - ETA: 1:14 - loss: 1.8896 - regression_loss: 1.5832 - classification_loss: 0.3064
1407/1500 [===========================>..] - ETA: 1:13 - loss: 1.8890 - regression_loss: 1.5827 - classification_loss: 0.3063
1408/1500 [===========================>..] - ETA: 1:12 - loss: 1.8887 - regression_loss: 1.5824 - classification_loss: 0.3063
1409/1500 [===========================>..] - ETA: 1:11 - loss: 1.8884 - regression_loss: 1.5822 - classification_loss: 0.3062
1410/1500 [===========================>..] - ETA: 1:11 - loss: 1.8878 - regression_loss: 1.5817 - classification_loss: 0.3061
1411/1500 [===========================>..] - ETA: 1:10 - loss: 1.8873 - regression_loss: 1.5814 - classification_loss: 0.3060
1412/1500 [===========================>..] - ETA: 1:09 - loss: 1.8866 - regression_loss: 1.5807 - classification_loss: 0.3058
1413/1500 [===========================>..] - ETA: 1:08 - loss: 1.8863 - regression_loss: 1.5806 - classification_loss: 0.3057
1414/1500 [===========================>..] - ETA: 1:07 - loss: 1.8864 - regression_loss: 1.5807 - classification_loss: 0.3057
1415/1500 [===========================>..] - ETA: 1:07 - loss: 1.8858 - regression_loss: 1.5802 - classification_loss: 0.3056
1416/1500 [===========================>..] - ETA: 1:06 - loss: 1.8850 - regression_loss: 1.5795 - classification_loss: 0.3055
1417/1500 [===========================>..] - ETA: 1:05 - loss: 1.8843 - regression_loss: 1.5789 - classification_loss: 0.3054
1418/1500 [===========================>..] - ETA: 1:04 - loss: 1.8849 - regression_loss: 1.5793 - classification_loss: 0.3056
1419/1500 [===========================>..] - ETA: 1:03 - loss: 1.8849 - regression_loss: 1.5793 - classification_loss: 0.3056
1420/1500 [===========================>..] - ETA: 1:03 - loss: 1.8848 - regression_loss: 1.5792 - classification_loss: 0.3056
1421/1500 [===========================>..] - ETA: 1:02 - loss: 1.8851 - regression_loss: 1.5795 - classification_loss: 0.3056
1422/1500 [===========================>..] - ETA: 1:01 - loss: 1.8860 - regression_loss: 1.5801 - classification_loss: 0.3059
1423/1500 [===========================>..] - ETA: 1:00 - loss: 1.8860 - regression_loss: 1.5801 - classification_loss: 0.3059
1424/1500 [===========================>..] - ETA: 59s - loss: 1.8864 - regression_loss: 1.5806 - classification_loss: 0.3058 
1425/1500 [===========================>..] - ETA: 59s - loss: 1.8867 - regression_loss: 1.5808 - classification_loss: 0.3059
1426/1500 [===========================>..] - ETA: 58s - loss: 1.8865 - regression_loss: 1.5805 - classification_loss: 0.3059
1427/1500 [===========================>..] - ETA: 57s - loss: 1.8861 - regression_loss: 1.5803 - classification_loss: 0.3058
1428/1500 [===========================>..] - ETA: 56s - loss: 1.8859 - regression_loss: 1.5801 - classification_loss: 0.3058
1429/1500 [===========================>..] - ETA: 55s - loss: 1.8860 - regression_loss: 1.5803 - classification_loss: 0.3057
1430/1500 [===========================>..] - ETA: 55s - loss: 1.8855 - regression_loss: 1.5800 - classification_loss: 0.3055
1431/1500 [===========================>..] - ETA: 54s - loss: 1.8863 - regression_loss: 1.5808 - classification_loss: 0.3055
1432/1500 [===========================>..] - ETA: 53s - loss: 1.8872 - regression_loss: 1.5814 - classification_loss: 0.3058
1433/1500 [===========================>..] - ETA: 52s - loss: 1.8867 - regression_loss: 1.5811 - classification_loss: 0.3056
1434/1500 [===========================>..] - ETA: 51s - loss: 1.8861 - regression_loss: 1.5806 - classification_loss: 0.3055
1435/1500 [===========================>..] - ETA: 51s - loss: 1.8857 - regression_loss: 1.5803 - classification_loss: 0.3054
1436/1500 [===========================>..] - ETA: 50s - loss: 1.8856 - regression_loss: 1.5802 - classification_loss: 0.3054
1437/1500 [===========================>..] - ETA: 49s - loss: 1.8852 - regression_loss: 1.5800 - classification_loss: 0.3053
1438/1500 [===========================>..] - ETA: 48s - loss: 1.8855 - regression_loss: 1.5802 - classification_loss: 0.3053
1439/1500 [===========================>..] - ETA: 48s - loss: 1.8855 - regression_loss: 1.5803 - classification_loss: 0.3053
1440/1500 [===========================>..] - ETA: 47s - loss: 1.8854 - regression_loss: 1.5802 - classification_loss: 0.3052
1441/1500 [===========================>..] - ETA: 46s - loss: 1.8853 - regression_loss: 1.5801 - classification_loss: 0.3051
1442/1500 [===========================>..] - ETA: 45s - loss: 1.8851 - regression_loss: 1.5800 - classification_loss: 0.3051
1443/1500 [===========================>..] - ETA: 44s - loss: 1.8850 - regression_loss: 1.5800 - classification_loss: 0.3050
1444/1500 [===========================>..] - ETA: 44s - loss: 1.8842 - regression_loss: 1.5794 - classification_loss: 0.3049
1445/1500 [===========================>..] - ETA: 43s - loss: 1.8837 - regression_loss: 1.5790 - classification_loss: 0.3048
1446/1500 [===========================>..] - ETA: 42s - loss: 1.8833 - regression_loss: 1.5787 - classification_loss: 0.3046
1447/1500 [===========================>..] - ETA: 41s - loss: 1.8835 - regression_loss: 1.5789 - classification_loss: 0.3046
1448/1500 [===========================>..] - ETA: 40s - loss: 1.8837 - regression_loss: 1.5791 - classification_loss: 0.3045
1449/1500 [===========================>..] - ETA: 40s - loss: 1.8832 - regression_loss: 1.5788 - classification_loss: 0.3044
1450/1500 [============================>.] - ETA: 39s - loss: 1.8840 - regression_loss: 1.5794 - classification_loss: 0.3046
1451/1500 [============================>.] - ETA: 38s - loss: 1.8832 - regression_loss: 1.5787 - classification_loss: 0.3044
1452/1500 [============================>.] - ETA: 37s - loss: 1.8831 - regression_loss: 1.5787 - classification_loss: 0.3044
1453/1500 [============================>.] - ETA: 37s - loss: 1.8839 - regression_loss: 1.5794 - classification_loss: 0.3046
1454/1500 [============================>.] - ETA: 36s - loss: 1.8839 - regression_loss: 1.5794 - classification_loss: 0.3045
1455/1500 [============================>.] - ETA: 35s - loss: 1.8842 - regression_loss: 1.5797 - classification_loss: 0.3045
1456/1500 [============================>.] - ETA: 34s - loss: 1.8838 - regression_loss: 1.5794 - classification_loss: 0.3044
1457/1500 [============================>.] - ETA: 33s - loss: 1.8836 - regression_loss: 1.5792 - classification_loss: 0.3044
1458/1500 [============================>.] - ETA: 33s - loss: 1.8829 - regression_loss: 1.5787 - classification_loss: 0.3042
1459/1500 [============================>.] - ETA: 32s - loss: 1.8828 - regression_loss: 1.5787 - classification_loss: 0.3042
1460/1500 [============================>.] - ETA: 31s - loss: 1.8854 - regression_loss: 1.5791 - classification_loss: 0.3063
1461/1500 [============================>.] - ETA: 30s - loss: 1.8857 - regression_loss: 1.5794 - classification_loss: 0.3063
1462/1500 [============================>.] - ETA: 29s - loss: 1.8857 - regression_loss: 1.5795 - classification_loss: 0.3062
1463/1500 [============================>.] - ETA: 29s - loss: 1.8856 - regression_loss: 1.5793 - classification_loss: 0.3063
1464/1500 [============================>.] - ETA: 28s - loss: 1.8861 - regression_loss: 1.5797 - classification_loss: 0.3064
1465/1500 [============================>.] - ETA: 27s - loss: 1.8865 - regression_loss: 1.5801 - classification_loss: 0.3064
1466/1500 [============================>.] - ETA: 26s - loss: 1.8861 - regression_loss: 1.5798 - classification_loss: 0.3063
1467/1500 [============================>.] - ETA: 26s - loss: 1.8865 - regression_loss: 1.5802 - classification_loss: 0.3062
1468/1500 [============================>.] - ETA: 25s - loss: 1.8860 - regression_loss: 1.5799 - classification_loss: 0.3061
1469/1500 [============================>.] - ETA: 24s - loss: 1.8862 - regression_loss: 1.5801 - classification_loss: 0.3061
1470/1500 [============================>.] - ETA: 23s - loss: 1.8855 - regression_loss: 1.5796 - classification_loss: 0.3059
1471/1500 [============================>.] - ETA: 22s - loss: 1.8854 - regression_loss: 1.5796 - classification_loss: 0.3059
1472/1500 [============================>.] - ETA: 22s - loss: 1.8845 - regression_loss: 1.5788 - classification_loss: 0.3057
1473/1500 [============================>.] - ETA: 21s - loss: 1.8839 - regression_loss: 1.5783 - classification_loss: 0.3056
1474/1500 [============================>.] - ETA: 20s - loss: 1.8833 - regression_loss: 1.5778 - classification_loss: 0.3055
1475/1500 [============================>.] - ETA: 19s - loss: 1.8833 - regression_loss: 1.5777 - classification_loss: 0.3055
1476/1500 [============================>.] - ETA: 18s - loss: 1.8831 - regression_loss: 1.5776 - classification_loss: 0.3055
1477/1500 [============================>.] - ETA: 18s - loss: 1.8831 - regression_loss: 1.5777 - classification_loss: 0.3054
1478/1500 [============================>.] - ETA: 17s - loss: 1.8830 - regression_loss: 1.5776 - classification_loss: 0.3054
1479/1500 [============================>.] - ETA: 16s - loss: 1.8825 - regression_loss: 1.5772 - classification_loss: 0.3053
1480/1500 [============================>.] - ETA: 15s - loss: 1.8822 - regression_loss: 1.5770 - classification_loss: 0.3052
1481/1500 [============================>.] - ETA: 14s - loss: 1.8821 - regression_loss: 1.5770 - classification_loss: 0.3051
1482/1500 [============================>.] - ETA: 14s - loss: 1.8820 - regression_loss: 1.5770 - classification_loss: 0.3050
1483/1500 [============================>.] - ETA: 13s - loss: 1.8820 - regression_loss: 1.5771 - classification_loss: 0.3049
1484/1500 [============================>.] - ETA: 12s - loss: 1.8814 - regression_loss: 1.5765 - classification_loss: 0.3048
1485/1500 [============================>.] - ETA: 11s - loss: 1.8815 - regression_loss: 1.5766 - classification_loss: 0.3049
1486/1500 [============================>.] - ETA: 11s - loss: 1.8820 - regression_loss: 1.5769 - classification_loss: 0.3051
1487/1500 [============================>.] - ETA: 10s - loss: 1.8821 - regression_loss: 1.5771 - classification_loss: 0.3051
1488/1500 [============================>.] - ETA: 9s - loss: 1.8826 - regression_loss: 1.5774 - classification_loss: 0.3052 
1489/1500 [============================>.] - ETA: 8s - loss: 1.8832 - regression_loss: 1.5778 - classification_loss: 0.3054
1490/1500 [============================>.] - ETA: 7s - loss: 1.8827 - regression_loss: 1.5774 - classification_loss: 0.3053
1491/1500 [============================>.] - ETA: 7s - loss: 1.8833 - regression_loss: 1.5780 - classification_loss: 0.3054
1492/1500 [============================>.] - ETA: 6s - loss: 1.8825 - regression_loss: 1.5772 - classification_loss: 0.3053
1493/1500 [============================>.] - ETA: 5s - loss: 1.8830 - regression_loss: 1.5775 - classification_loss: 0.3055
1494/1500 [============================>.] - ETA: 4s - loss: 1.8828 - regression_loss: 1.5774 - classification_loss: 0.3055
1495/1500 [============================>.] - ETA: 3s - loss: 1.8832 - regression_loss: 1.5777 - classification_loss: 0.3055
1496/1500 [============================>.] - ETA: 3s - loss: 1.8827 - regression_loss: 1.5773 - classification_loss: 0.3054
1497/1500 [============================>.] - ETA: 2s - loss: 1.8831 - regression_loss: 1.5775 - classification_loss: 0.3056
1498/1500 [============================>.] - ETA: 1s - loss: 1.8837 - regression_loss: 1.5779 - classification_loss: 0.3058
1499/1500 [============================>.] - ETA: 0s - loss: 1.8842 - regression_loss: 1.5782 - classification_loss: 0.3059
1500/1500 [==============================] - 1182s 788ms/step - loss: 1.8836 - regression_loss: 1.5778 - classification_loss: 0.3059

Epoch 00003: saving model to ./snapshots/resnet50_csv_03.h5
Epoch 4/10

   1/1500 [..............................] - ETA: 10:20 - loss: 1.0351 - regression_loss: 0.9288 - classification_loss: 0.1064
   2/1500 [..............................] - ETA: 9:21 - loss: 1.1438 - regression_loss: 1.0369 - classification_loss: 0.1069 
   3/1500 [..............................] - ETA: 9:45 - loss: 1.4886 - regression_loss: 1.3327 - classification_loss: 0.1560
   4/1500 [..............................] - ETA: 9:48 - loss: 1.4589 - regression_loss: 1.3103 - classification_loss: 0.1486
   5/1500 [..............................] - ETA: 12:46 - loss: 1.5418 - regression_loss: 1.3803 - classification_loss: 0.1615
   6/1500 [..............................] - ETA: 14:15 - loss: 1.6073 - regression_loss: 1.4211 - classification_loss: 0.1862
   7/1500 [..............................] - ETA: 15:41 - loss: 1.6540 - regression_loss: 1.4577 - classification_loss: 0.1963
   8/1500 [..............................] - ETA: 14:56 - loss: 1.6325 - regression_loss: 1.4278 - classification_loss: 0.2046
   9/1500 [..............................] - ETA: 18:13 - loss: 1.6067 - regression_loss: 1.4076 - classification_loss: 0.1991
  10/1500 [..............................] - ETA: 20:01 - loss: 1.6933 - regression_loss: 1.4690 - classification_loss: 0.2242
  11/1500 [..............................] - ETA: 19:06 - loss: 1.7776 - regression_loss: 1.5217 - classification_loss: 0.2558
  12/1500 [..............................] - ETA: 22:09 - loss: 1.8034 - regression_loss: 1.5394 - classification_loss: 0.2640
  13/1500 [..............................] - ETA: 21:58 - loss: 1.8171 - regression_loss: 1.5544 - classification_loss: 0.2627
  14/1500 [..............................] - ETA: 21:27 - loss: 1.8801 - regression_loss: 1.6037 - classification_loss: 0.2765
  15/1500 [..............................] - ETA: 20:39 - loss: 1.8217 - regression_loss: 1.5556 - classification_loss: 0.2661
  16/1500 [..............................] - ETA: 19:59 - loss: 1.8311 - regression_loss: 1.5663 - classification_loss: 0.2648
  17/1500 [..............................] - ETA: 19:23 - loss: 1.7786 - regression_loss: 1.5212 - classification_loss: 0.2575
  18/1500 [..............................] - ETA: 20:32 - loss: 1.7586 - regression_loss: 1.5086 - classification_loss: 0.2501
  19/1500 [..............................] - ETA: 20:49 - loss: 1.7284 - regression_loss: 1.4805 - classification_loss: 0.2479
  20/1500 [..............................] - ETA: 21:54 - loss: 1.7352 - regression_loss: 1.4810 - classification_loss: 0.2542
  21/1500 [..............................] - ETA: 21:20 - loss: 1.7067 - regression_loss: 1.4590 - classification_loss: 0.2476
  22/1500 [..............................] - ETA: 22:05 - loss: 1.7425 - regression_loss: 1.4905 - classification_loss: 0.2520
  23/1500 [..............................] - ETA: 22:14 - loss: 1.7469 - regression_loss: 1.4936 - classification_loss: 0.2532
  24/1500 [..............................] - ETA: 21:44 - loss: 1.7458 - regression_loss: 1.4887 - classification_loss: 0.2571
  25/1500 [..............................] - ETA: 21:24 - loss: 1.7308 - regression_loss: 1.4761 - classification_loss: 0.2547
  26/1500 [..............................] - ETA: 21:19 - loss: 1.7375 - regression_loss: 1.4821 - classification_loss: 0.2554
  27/1500 [..............................] - ETA: 21:51 - loss: 1.7524 - regression_loss: 1.4974 - classification_loss: 0.2550
  28/1500 [..............................] - ETA: 21:26 - loss: 1.7342 - regression_loss: 1.4807 - classification_loss: 0.2535
  29/1500 [..............................] - ETA: 21:44 - loss: 1.7264 - regression_loss: 1.4743 - classification_loss: 0.2521
  30/1500 [..............................] - ETA: 21:33 - loss: 1.7216 - regression_loss: 1.4678 - classification_loss: 0.2537
  31/1500 [..............................] - ETA: 21:10 - loss: 1.7027 - regression_loss: 1.4525 - classification_loss: 0.2502
  32/1500 [..............................] - ETA: 20:49 - loss: 1.6745 - regression_loss: 1.4278 - classification_loss: 0.2466
  33/1500 [..............................] - ETA: 20:30 - loss: 1.6520 - regression_loss: 1.4097 - classification_loss: 0.2423
  34/1500 [..............................] - ETA: 20:10 - loss: 1.6642 - regression_loss: 1.4225 - classification_loss: 0.2417
  35/1500 [..............................] - ETA: 20:49 - loss: 1.6535 - regression_loss: 1.4147 - classification_loss: 0.2388
  36/1500 [..............................] - ETA: 20:32 - loss: 1.6623 - regression_loss: 1.4211 - classification_loss: 0.2411
  37/1500 [..............................] - ETA: 20:27 - loss: 1.6539 - regression_loss: 1.4128 - classification_loss: 0.2411
  38/1500 [..............................] - ETA: 20:21 - loss: 1.6657 - regression_loss: 1.4199 - classification_loss: 0.2458
  39/1500 [..............................] - ETA: 20:44 - loss: 1.6879 - regression_loss: 1.4374 - classification_loss: 0.2505
  40/1500 [..............................] - ETA: 20:44 - loss: 1.7112 - regression_loss: 1.4556 - classification_loss: 0.2556
  41/1500 [..............................] - ETA: 20:26 - loss: 1.7258 - regression_loss: 1.4682 - classification_loss: 0.2576
  42/1500 [..............................] - ETA: 20:34 - loss: 1.7233 - regression_loss: 1.4661 - classification_loss: 0.2572
  43/1500 [..............................] - ETA: 20:45 - loss: 1.7151 - regression_loss: 1.4597 - classification_loss: 0.2554
  44/1500 [..............................] - ETA: 20:35 - loss: 1.7147 - regression_loss: 1.4606 - classification_loss: 0.2541
  45/1500 [..............................] - ETA: 20:44 - loss: 1.6958 - regression_loss: 1.4455 - classification_loss: 0.2504
  46/1500 [..............................] - ETA: 20:32 - loss: 1.6780 - regression_loss: 1.4314 - classification_loss: 0.2465
  47/1500 [..............................] - ETA: 20:17 - loss: 1.6824 - regression_loss: 1.4383 - classification_loss: 0.2441
  48/1500 [..............................] - ETA: 20:01 - loss: 1.6736 - regression_loss: 1.4297 - classification_loss: 0.2439
  49/1500 [..............................] - ETA: 19:48 - loss: 1.6628 - regression_loss: 1.4221 - classification_loss: 0.2407
  50/1500 [>.............................] - ETA: 19:50 - loss: 1.6730 - regression_loss: 1.4319 - classification_loss: 0.2411
  51/1500 [>.............................] - ETA: 19:37 - loss: 1.6651 - regression_loss: 1.4237 - classification_loss: 0.2414
  52/1500 [>.............................] - ETA: 19:38 - loss: 1.6536 - regression_loss: 1.4144 - classification_loss: 0.2393
  53/1500 [>.............................] - ETA: 19:27 - loss: 1.6528 - regression_loss: 1.4103 - classification_loss: 0.2425
  54/1500 [>.............................] - ETA: 19:35 - loss: 1.6556 - regression_loss: 1.4121 - classification_loss: 0.2435
  55/1500 [>.............................] - ETA: 19:27 - loss: 1.6690 - regression_loss: 1.4222 - classification_loss: 0.2468
  56/1500 [>.............................] - ETA: 19:44 - loss: 1.6942 - regression_loss: 1.4397 - classification_loss: 0.2545
  57/1500 [>.............................] - ETA: 19:33 - loss: 1.6945 - regression_loss: 1.4420 - classification_loss: 0.2525
  58/1500 [>.............................] - ETA: 19:51 - loss: 1.7032 - regression_loss: 1.4485 - classification_loss: 0.2547
  59/1500 [>.............................] - ETA: 20:36 - loss: 1.7047 - regression_loss: 1.4455 - classification_loss: 0.2592
  60/1500 [>.............................] - ETA: 20:48 - loss: 1.6997 - regression_loss: 1.4411 - classification_loss: 0.2585
  61/1500 [>.............................] - ETA: 21:07 - loss: 1.7071 - regression_loss: 1.4467 - classification_loss: 0.2603
  62/1500 [>.............................] - ETA: 21:12 - loss: 1.7022 - regression_loss: 1.4410 - classification_loss: 0.2612
  63/1500 [>.............................] - ETA: 21:10 - loss: 1.6933 - regression_loss: 1.4349 - classification_loss: 0.2584
  64/1500 [>.............................] - ETA: 20:58 - loss: 1.6978 - regression_loss: 1.4387 - classification_loss: 0.2592
  65/1500 [>.............................] - ETA: 20:49 - loss: 1.6921 - regression_loss: 1.4345 - classification_loss: 0.2577
  66/1500 [>.............................] - ETA: 20:38 - loss: 1.6889 - regression_loss: 1.4321 - classification_loss: 0.2568
  67/1500 [>.............................] - ETA: 20:28 - loss: 1.6738 - regression_loss: 1.4191 - classification_loss: 0.2547
  68/1500 [>.............................] - ETA: 20:17 - loss: 1.6644 - regression_loss: 1.4110 - classification_loss: 0.2534
  69/1500 [>.............................] - ETA: 20:06 - loss: 1.6517 - regression_loss: 1.4006 - classification_loss: 0.2511
  70/1500 [>.............................] - ETA: 19:56 - loss: 1.6439 - regression_loss: 1.3934 - classification_loss: 0.2505
  71/1500 [>.............................] - ETA: 20:05 - loss: 1.6502 - regression_loss: 1.3982 - classification_loss: 0.2521
  72/1500 [>.............................] - ETA: 20:12 - loss: 1.6617 - regression_loss: 1.4076 - classification_loss: 0.2541
  73/1500 [>.............................] - ETA: 20:14 - loss: 1.6610 - regression_loss: 1.4059 - classification_loss: 0.2551
  74/1500 [>.............................] - ETA: 20:04 - loss: 1.6676 - regression_loss: 1.4121 - classification_loss: 0.2555
  75/1500 [>.............................] - ETA: 19:55 - loss: 1.6614 - regression_loss: 1.4072 - classification_loss: 0.2543
  76/1500 [>.............................] - ETA: 19:46 - loss: 1.6530 - regression_loss: 1.4010 - classification_loss: 0.2519
  77/1500 [>.............................] - ETA: 19:38 - loss: 1.6474 - regression_loss: 1.3978 - classification_loss: 0.2497
  78/1500 [>.............................] - ETA: 19:44 - loss: 1.6426 - regression_loss: 1.3938 - classification_loss: 0.2488
  79/1500 [>.............................] - ETA: 19:42 - loss: 1.6297 - regression_loss: 1.3833 - classification_loss: 0.2465
  80/1500 [>.............................] - ETA: 20:12 - loss: 1.6340 - regression_loss: 1.3861 - classification_loss: 0.2478
  81/1500 [>.............................] - ETA: 20:25 - loss: 1.6291 - regression_loss: 1.3821 - classification_loss: 0.2470
  82/1500 [>.............................] - ETA: 20:15 - loss: 1.6229 - regression_loss: 1.3775 - classification_loss: 0.2454
  83/1500 [>.............................] - ETA: 20:06 - loss: 1.6338 - regression_loss: 1.3846 - classification_loss: 0.2492
  84/1500 [>.............................] - ETA: 20:03 - loss: 1.6305 - regression_loss: 1.3822 - classification_loss: 0.2483
  85/1500 [>.............................] - ETA: 19:58 - loss: 1.6189 - regression_loss: 1.3726 - classification_loss: 0.2464
  86/1500 [>.............................] - ETA: 19:50 - loss: 1.6263 - regression_loss: 1.3782 - classification_loss: 0.2481
  87/1500 [>.............................] - ETA: 19:43 - loss: 1.6310 - regression_loss: 1.3833 - classification_loss: 0.2477
  88/1500 [>.............................] - ETA: 19:36 - loss: 1.6283 - regression_loss: 1.3797 - classification_loss: 0.2486
  89/1500 [>.............................] - ETA: 19:29 - loss: 1.6364 - regression_loss: 1.3860 - classification_loss: 0.2504
  90/1500 [>.............................] - ETA: 19:23 - loss: 1.6344 - regression_loss: 1.3846 - classification_loss: 0.2498
  91/1500 [>.............................] - ETA: 19:17 - loss: 1.6409 - regression_loss: 1.3895 - classification_loss: 0.2513
  92/1500 [>.............................] - ETA: 19:10 - loss: 1.6343 - regression_loss: 1.3843 - classification_loss: 0.2500
  93/1500 [>.............................] - ETA: 19:08 - loss: 1.6448 - regression_loss: 1.3931 - classification_loss: 0.2516
  94/1500 [>.............................] - ETA: 19:12 - loss: 1.6496 - regression_loss: 1.3977 - classification_loss: 0.2519
  95/1500 [>.............................] - ETA: 19:05 - loss: 1.6540 - regression_loss: 1.4018 - classification_loss: 0.2522
  96/1500 [>.............................] - ETA: 19:03 - loss: 1.6634 - regression_loss: 1.4078 - classification_loss: 0.2555
  97/1500 [>.............................] - ETA: 19:02 - loss: 1.6647 - regression_loss: 1.4083 - classification_loss: 0.2564
  98/1500 [>.............................] - ETA: 18:57 - loss: 1.6622 - regression_loss: 1.4064 - classification_loss: 0.2559
  99/1500 [>.............................] - ETA: 19:04 - loss: 1.6597 - regression_loss: 1.4043 - classification_loss: 0.2554
 100/1500 [=>............................] - ETA: 18:59 - loss: 1.6547 - regression_loss: 1.4011 - classification_loss: 0.2536
 101/1500 [=>............................] - ETA: 18:53 - loss: 1.6595 - regression_loss: 1.4046 - classification_loss: 0.2549
 102/1500 [=>............................] - ETA: 18:51 - loss: 1.6790 - regression_loss: 1.4121 - classification_loss: 0.2668
 103/1500 [=>............................] - ETA: 18:48 - loss: 1.6823 - regression_loss: 1.4144 - classification_loss: 0.2680
 104/1500 [=>............................] - ETA: 18:42 - loss: 1.6822 - regression_loss: 1.4152 - classification_loss: 0.2669
 105/1500 [=>............................] - ETA: 18:55 - loss: 1.6747 - regression_loss: 1.4092 - classification_loss: 0.2655
 106/1500 [=>............................] - ETA: 19:02 - loss: 1.6795 - regression_loss: 1.4135 - classification_loss: 0.2660
 107/1500 [=>............................] - ETA: 19:03 - loss: 1.6726 - regression_loss: 1.4072 - classification_loss: 0.2655
 108/1500 [=>............................] - ETA: 18:57 - loss: 1.6731 - regression_loss: 1.4073 - classification_loss: 0.2659
 109/1500 [=>............................] - ETA: 19:03 - loss: 1.6816 - regression_loss: 1.4139 - classification_loss: 0.2677
 110/1500 [=>............................] - ETA: 18:57 - loss: 1.6915 - regression_loss: 1.4217 - classification_loss: 0.2698
 111/1500 [=>............................] - ETA: 18:56 - loss: 1.6849 - regression_loss: 1.4151 - classification_loss: 0.2698
 112/1500 [=>............................] - ETA: 18:58 - loss: 1.6911 - regression_loss: 1.4196 - classification_loss: 0.2715
 113/1500 [=>............................] - ETA: 19:11 - loss: 1.6951 - regression_loss: 1.4223 - classification_loss: 0.2728
 114/1500 [=>............................] - ETA: 19:20 - loss: 1.7048 - regression_loss: 1.4302 - classification_loss: 0.2746
 115/1500 [=>............................] - ETA: 19:20 - loss: 1.6979 - regression_loss: 1.4242 - classification_loss: 0.2737
 116/1500 [=>............................] - ETA: 19:19 - loss: 1.6973 - regression_loss: 1.4225 - classification_loss: 0.2748
 117/1500 [=>............................] - ETA: 19:23 - loss: 1.6987 - regression_loss: 1.4236 - classification_loss: 0.2751
 118/1500 [=>............................] - ETA: 19:20 - loss: 1.7043 - regression_loss: 1.4291 - classification_loss: 0.2752
 119/1500 [=>............................] - ETA: 19:20 - loss: 1.7128 - regression_loss: 1.4350 - classification_loss: 0.2778
 120/1500 [=>............................] - ETA: 19:14 - loss: 1.7097 - regression_loss: 1.4321 - classification_loss: 0.2775
 121/1500 [=>............................] - ETA: 19:27 - loss: 1.7098 - regression_loss: 1.4326 - classification_loss: 0.2772
 122/1500 [=>............................] - ETA: 19:27 - loss: 1.7074 - regression_loss: 1.4315 - classification_loss: 0.2760
 123/1500 [=>............................] - ETA: 19:22 - loss: 1.7018 - regression_loss: 1.4271 - classification_loss: 0.2747
 124/1500 [=>............................] - ETA: 19:24 - loss: 1.7126 - regression_loss: 1.4345 - classification_loss: 0.2781
 125/1500 [=>............................] - ETA: 19:18 - loss: 1.7169 - regression_loss: 1.4384 - classification_loss: 0.2785
 126/1500 [=>............................] - ETA: 19:15 - loss: 1.7181 - regression_loss: 1.4398 - classification_loss: 0.2783
 127/1500 [=>............................] - ETA: 19:09 - loss: 1.7139 - regression_loss: 1.4363 - classification_loss: 0.2775
 128/1500 [=>............................] - ETA: 19:04 - loss: 1.7147 - regression_loss: 1.4352 - classification_loss: 0.2795
 129/1500 [=>............................] - ETA: 19:11 - loss: 1.7209 - regression_loss: 1.4394 - classification_loss: 0.2815
 130/1500 [=>............................] - ETA: 19:05 - loss: 1.7231 - regression_loss: 1.4405 - classification_loss: 0.2827
 131/1500 [=>............................] - ETA: 19:08 - loss: 1.7273 - regression_loss: 1.4444 - classification_loss: 0.2830
 132/1500 [=>............................] - ETA: 19:07 - loss: 1.7348 - regression_loss: 1.4504 - classification_loss: 0.2844
 133/1500 [=>............................] - ETA: 19:09 - loss: 1.7371 - regression_loss: 1.4519 - classification_loss: 0.2852
 134/1500 [=>............................] - ETA: 19:11 - loss: 1.7357 - regression_loss: 1.4493 - classification_loss: 0.2864
 135/1500 [=>............................] - ETA: 19:05 - loss: 1.7338 - regression_loss: 1.4482 - classification_loss: 0.2856
 136/1500 [=>............................] - ETA: 19:08 - loss: 1.7262 - regression_loss: 1.4419 - classification_loss: 0.2842
 137/1500 [=>............................] - ETA: 19:03 - loss: 1.7202 - regression_loss: 1.4368 - classification_loss: 0.2834
 138/1500 [=>............................] - ETA: 18:58 - loss: 1.7220 - regression_loss: 1.4378 - classification_loss: 0.2842
 139/1500 [=>............................] - ETA: 18:59 - loss: 1.7195 - regression_loss: 1.4335 - classification_loss: 0.2860
 140/1500 [=>............................] - ETA: 19:00 - loss: 1.7143 - regression_loss: 1.4296 - classification_loss: 0.2847
 141/1500 [=>............................] - ETA: 19:11 - loss: 1.7133 - regression_loss: 1.4282 - classification_loss: 0.2851
 142/1500 [=>............................] - ETA: 19:06 - loss: 1.7118 - regression_loss: 1.4278 - classification_loss: 0.2840
 143/1500 [=>............................] - ETA: 19:02 - loss: 1.7136 - regression_loss: 1.4293 - classification_loss: 0.2843
 144/1500 [=>............................] - ETA: 19:03 - loss: 1.7077 - regression_loss: 1.4241 - classification_loss: 0.2835
 145/1500 [=>............................] - ETA: 18:57 - loss: 1.7016 - regression_loss: 1.4190 - classification_loss: 0.2826
 146/1500 [=>............................] - ETA: 18:56 - loss: 1.7002 - regression_loss: 1.4181 - classification_loss: 0.2821
 147/1500 [=>............................] - ETA: 18:54 - loss: 1.7084 - regression_loss: 1.4240 - classification_loss: 0.2843
 148/1500 [=>............................] - ETA: 18:49 - loss: 1.7108 - regression_loss: 1.4235 - classification_loss: 0.2874
 149/1500 [=>............................] - ETA: 18:55 - loss: 1.7132 - regression_loss: 1.4247 - classification_loss: 0.2885
 150/1500 [==>...........................] - ETA: 18:50 - loss: 1.7239 - regression_loss: 1.4348 - classification_loss: 0.2891
 151/1500 [==>...........................] - ETA: 18:46 - loss: 1.7241 - regression_loss: 1.4356 - classification_loss: 0.2886
 152/1500 [==>...........................] - ETA: 18:41 - loss: 1.7187 - regression_loss: 1.4313 - classification_loss: 0.2874
 153/1500 [==>...........................] - ETA: 18:40 - loss: 1.7217 - regression_loss: 1.4342 - classification_loss: 0.2875
 154/1500 [==>...........................] - ETA: 18:41 - loss: 1.7259 - regression_loss: 1.4371 - classification_loss: 0.2888
 155/1500 [==>...........................] - ETA: 18:36 - loss: 1.7263 - regression_loss: 1.4383 - classification_loss: 0.2880
 156/1500 [==>...........................] - ETA: 18:32 - loss: 1.7203 - regression_loss: 1.4333 - classification_loss: 0.2870
 157/1500 [==>...........................] - ETA: 18:27 - loss: 1.7211 - regression_loss: 1.4343 - classification_loss: 0.2867
 158/1500 [==>...........................] - ETA: 18:29 - loss: 1.7289 - regression_loss: 1.4398 - classification_loss: 0.2890
 159/1500 [==>...........................] - ETA: 18:29 - loss: 1.7311 - regression_loss: 1.4415 - classification_loss: 0.2896
 160/1500 [==>...........................] - ETA: 18:31 - loss: 1.7293 - regression_loss: 1.4399 - classification_loss: 0.2894
 161/1500 [==>...........................] - ETA: 18:28 - loss: 1.7254 - regression_loss: 1.4371 - classification_loss: 0.2883
 162/1500 [==>...........................] - ETA: 18:25 - loss: 1.7268 - regression_loss: 1.4391 - classification_loss: 0.2877
 163/1500 [==>...........................] - ETA: 18:20 - loss: 1.7269 - regression_loss: 1.4384 - classification_loss: 0.2885
 164/1500 [==>...........................] - ETA: 18:16 - loss: 1.7222 - regression_loss: 1.4349 - classification_loss: 0.2873
 165/1500 [==>...........................] - ETA: 18:12 - loss: 1.7206 - regression_loss: 1.4341 - classification_loss: 0.2865
 166/1500 [==>...........................] - ETA: 18:16 - loss: 1.7227 - regression_loss: 1.4350 - classification_loss: 0.2877
 167/1500 [==>...........................] - ETA: 18:23 - loss: 1.7190 - regression_loss: 1.4318 - classification_loss: 0.2871
 168/1500 [==>...........................] - ETA: 18:20 - loss: 1.7140 - regression_loss: 1.4276 - classification_loss: 0.2864
 169/1500 [==>...........................] - ETA: 18:19 - loss: 1.7209 - regression_loss: 1.4336 - classification_loss: 0.2873
 170/1500 [==>...........................] - ETA: 18:15 - loss: 1.7198 - regression_loss: 1.4331 - classification_loss: 0.2867
 171/1500 [==>...........................] - ETA: 18:10 - loss: 1.7202 - regression_loss: 1.4335 - classification_loss: 0.2867
 172/1500 [==>...........................] - ETA: 18:06 - loss: 1.7231 - regression_loss: 1.4355 - classification_loss: 0.2876
 173/1500 [==>...........................] - ETA: 18:03 - loss: 1.7247 - regression_loss: 1.4362 - classification_loss: 0.2885
 174/1500 [==>...........................] - ETA: 18:04 - loss: 1.7232 - regression_loss: 1.4351 - classification_loss: 0.2881
 175/1500 [==>...........................] - ETA: 18:00 - loss: 1.7235 - regression_loss: 1.4355 - classification_loss: 0.2880
 176/1500 [==>...........................] - ETA: 17:56 - loss: 1.7224 - regression_loss: 1.4349 - classification_loss: 0.2875
 177/1500 [==>...........................] - ETA: 17:53 - loss: 1.7247 - regression_loss: 1.4369 - classification_loss: 0.2878
 178/1500 [==>...........................] - ETA: 18:01 - loss: 1.7215 - regression_loss: 1.4348 - classification_loss: 0.2867
 179/1500 [==>...........................] - ETA: 17:58 - loss: 1.7222 - regression_loss: 1.4355 - classification_loss: 0.2866
 180/1500 [==>...........................] - ETA: 17:56 - loss: 1.7214 - regression_loss: 1.4347 - classification_loss: 0.2867
 181/1500 [==>...........................] - ETA: 17:52 - loss: 1.7242 - regression_loss: 1.4376 - classification_loss: 0.2866
 182/1500 [==>...........................] - ETA: 17:48 - loss: 1.7239 - regression_loss: 1.4374 - classification_loss: 0.2865
 183/1500 [==>...........................] - ETA: 17:45 - loss: 1.7213 - regression_loss: 1.4353 - classification_loss: 0.2859
 184/1500 [==>...........................] - ETA: 17:41 - loss: 1.7220 - regression_loss: 1.4355 - classification_loss: 0.2865
 185/1500 [==>...........................] - ETA: 17:37 - loss: 1.7243 - regression_loss: 1.4375 - classification_loss: 0.2869
 186/1500 [==>...........................] - ETA: 17:35 - loss: 1.7309 - regression_loss: 1.4422 - classification_loss: 0.2886
 187/1500 [==>...........................] - ETA: 17:34 - loss: 1.7255 - regression_loss: 1.4379 - classification_loss: 0.2876
 188/1500 [==>...........................] - ETA: 17:32 - loss: 1.7220 - regression_loss: 1.4354 - classification_loss: 0.2865
 189/1500 [==>...........................] - ETA: 17:35 - loss: 1.7185 - regression_loss: 1.4329 - classification_loss: 0.2856
 190/1500 [==>...........................] - ETA: 17:36 - loss: 1.7132 - regression_loss: 1.4287 - classification_loss: 0.2846
 191/1500 [==>...........................] - ETA: 17:38 - loss: 1.7103 - regression_loss: 1.4264 - classification_loss: 0.2839
 192/1500 [==>...........................] - ETA: 17:39 - loss: 1.7135 - regression_loss: 1.4290 - classification_loss: 0.2846
 193/1500 [==>...........................] - ETA: 17:38 - loss: 1.7132 - regression_loss: 1.4286 - classification_loss: 0.2846
 194/1500 [==>...........................] - ETA: 17:34 - loss: 1.7143 - regression_loss: 1.4299 - classification_loss: 0.2843
 195/1500 [==>...........................] - ETA: 17:38 - loss: 1.7087 - regression_loss: 1.4254 - classification_loss: 0.2833
 196/1500 [==>...........................] - ETA: 17:41 - loss: 1.7091 - regression_loss: 1.4261 - classification_loss: 0.2830
 197/1500 [==>...........................] - ETA: 17:38 - loss: 1.7076 - regression_loss: 1.4252 - classification_loss: 0.2824
 198/1500 [==>...........................] - ETA: 17:34 - loss: 1.7091 - regression_loss: 1.4265 - classification_loss: 0.2825
 199/1500 [==>...........................] - ETA: 17:36 - loss: 1.7043 - regression_loss: 1.4227 - classification_loss: 0.2816
 200/1500 [===>..........................] - ETA: 17:36 - loss: 1.7056 - regression_loss: 1.4237 - classification_loss: 0.2818
 201/1500 [===>..........................] - ETA: 17:32 - loss: 1.7035 - regression_loss: 1.4213 - classification_loss: 0.2821
 202/1500 [===>..........................] - ETA: 17:33 - loss: 1.7013 - regression_loss: 1.4197 - classification_loss: 0.2816
 203/1500 [===>..........................] - ETA: 17:33 - loss: 1.6992 - regression_loss: 1.4183 - classification_loss: 0.2809
 204/1500 [===>..........................] - ETA: 17:36 - loss: 1.6960 - regression_loss: 1.4159 - classification_loss: 0.2801
 205/1500 [===>..........................] - ETA: 17:36 - loss: 1.7026 - regression_loss: 1.4210 - classification_loss: 0.2816
 206/1500 [===>..........................] - ETA: 17:38 - loss: 1.7019 - regression_loss: 1.4199 - classification_loss: 0.2820
 207/1500 [===>..........................] - ETA: 17:36 - loss: 1.7028 - regression_loss: 1.4209 - classification_loss: 0.2819
 208/1500 [===>..........................] - ETA: 17:41 - loss: 1.6998 - regression_loss: 1.4187 - classification_loss: 0.2811
 209/1500 [===>..........................] - ETA: 17:38 - loss: 1.7048 - regression_loss: 1.4223 - classification_loss: 0.2824
 210/1500 [===>..........................] - ETA: 17:35 - loss: 1.7029 - regression_loss: 1.4209 - classification_loss: 0.2820
 211/1500 [===>..........................] - ETA: 17:32 - loss: 1.7019 - regression_loss: 1.4206 - classification_loss: 0.2813
 212/1500 [===>..........................] - ETA: 17:31 - loss: 1.7019 - regression_loss: 1.4196 - classification_loss: 0.2823
 213/1500 [===>..........................] - ETA: 17:28 - loss: 1.7026 - regression_loss: 1.4203 - classification_loss: 0.2823
 214/1500 [===>..........................] - ETA: 17:24 - loss: 1.7049 - regression_loss: 1.4222 - classification_loss: 0.2827
 215/1500 [===>..........................] - ETA: 17:26 - loss: 1.7083 - regression_loss: 1.4253 - classification_loss: 0.2830
 216/1500 [===>..........................] - ETA: 17:22 - loss: 1.7086 - regression_loss: 1.4261 - classification_loss: 0.2825
 217/1500 [===>..........................] - ETA: 17:19 - loss: 1.7057 - regression_loss: 1.4234 - classification_loss: 0.2823
 218/1500 [===>..........................] - ETA: 17:20 - loss: 1.7042 - regression_loss: 1.4222 - classification_loss: 0.2819
 219/1500 [===>..........................] - ETA: 17:20 - loss: 1.7008 - regression_loss: 1.4195 - classification_loss: 0.2814
 220/1500 [===>..........................] - ETA: 17:30 - loss: 1.6995 - regression_loss: 1.4184 - classification_loss: 0.2811
 221/1500 [===>..........................] - ETA: 17:26 - loss: 1.6992 - regression_loss: 1.4189 - classification_loss: 0.2803
 222/1500 [===>..........................] - ETA: 17:25 - loss: 1.6984 - regression_loss: 1.4189 - classification_loss: 0.2794
 223/1500 [===>..........................] - ETA: 17:23 - loss: 1.7024 - regression_loss: 1.4227 - classification_loss: 0.2797
 224/1500 [===>..........................] - ETA: 17:21 - loss: 1.7027 - regression_loss: 1.4230 - classification_loss: 0.2797
 225/1500 [===>..........................] - ETA: 17:18 - loss: 1.7016 - regression_loss: 1.4226 - classification_loss: 0.2789
 226/1500 [===>..........................] - ETA: 17:18 - loss: 1.7043 - regression_loss: 1.4254 - classification_loss: 0.2789
 227/1500 [===>..........................] - ETA: 17:16 - loss: 1.7032 - regression_loss: 1.4248 - classification_loss: 0.2784
 228/1500 [===>..........................] - ETA: 17:14 - loss: 1.7047 - regression_loss: 1.4256 - classification_loss: 0.2791
 229/1500 [===>..........................] - ETA: 17:12 - loss: 1.7048 - regression_loss: 1.4260 - classification_loss: 0.2788
 230/1500 [===>..........................] - ETA: 17:09 - loss: 1.7030 - regression_loss: 1.4247 - classification_loss: 0.2783
 231/1500 [===>..........................] - ETA: 17:09 - loss: 1.7008 - regression_loss: 1.4233 - classification_loss: 0.2775
 232/1500 [===>..........................] - ETA: 17:09 - loss: 1.6969 - regression_loss: 1.4200 - classification_loss: 0.2769
 233/1500 [===>..........................] - ETA: 17:06 - loss: 1.6935 - regression_loss: 1.4171 - classification_loss: 0.2764
 234/1500 [===>..........................] - ETA: 17:03 - loss: 1.6904 - regression_loss: 1.4146 - classification_loss: 0.2758
 235/1500 [===>..........................] - ETA: 17:00 - loss: 1.6933 - regression_loss: 1.4167 - classification_loss: 0.2766
 236/1500 [===>..........................] - ETA: 16:59 - loss: 1.6910 - regression_loss: 1.4153 - classification_loss: 0.2757
 237/1500 [===>..........................] - ETA: 16:57 - loss: 1.6909 - regression_loss: 1.4153 - classification_loss: 0.2756
 238/1500 [===>..........................] - ETA: 16:57 - loss: 1.6904 - regression_loss: 1.4149 - classification_loss: 0.2755
 239/1500 [===>..........................] - ETA: 16:54 - loss: 1.6911 - regression_loss: 1.4156 - classification_loss: 0.2755
 240/1500 [===>..........................] - ETA: 16:56 - loss: 1.6900 - regression_loss: 1.4149 - classification_loss: 0.2751
 241/1500 [===>..........................] - ETA: 16:53 - loss: 1.6889 - regression_loss: 1.4144 - classification_loss: 0.2745
 242/1500 [===>..........................] - ETA: 16:52 - loss: 1.6876 - regression_loss: 1.4137 - classification_loss: 0.2739
 243/1500 [===>..........................] - ETA: 16:49 - loss: 1.6852 - regression_loss: 1.4119 - classification_loss: 0.2733
 244/1500 [===>..........................] - ETA: 16:46 - loss: 1.6918 - regression_loss: 1.4179 - classification_loss: 0.2739
 245/1500 [===>..........................] - ETA: 16:43 - loss: 1.6927 - regression_loss: 1.4184 - classification_loss: 0.2743
 246/1500 [===>..........................] - ETA: 16:42 - loss: 1.6949 - regression_loss: 1.4199 - classification_loss: 0.2749
 247/1500 [===>..........................] - ETA: 16:39 - loss: 1.6929 - regression_loss: 1.4184 - classification_loss: 0.2746
 248/1500 [===>..........................] - ETA: 16:41 - loss: 1.6911 - regression_loss: 1.4164 - classification_loss: 0.2746
 249/1500 [===>..........................] - ETA: 16:38 - loss: 1.6954 - regression_loss: 1.4204 - classification_loss: 0.2750
 250/1500 [====>.........................] - ETA: 16:36 - loss: 1.6969 - regression_loss: 1.4221 - classification_loss: 0.2747
 251/1500 [====>.........................] - ETA: 16:36 - loss: 1.6956 - regression_loss: 1.4208 - classification_loss: 0.2748
 252/1500 [====>.........................] - ETA: 16:41 - loss: 1.6962 - regression_loss: 1.4218 - classification_loss: 0.2745
 253/1500 [====>.........................] - ETA: 16:41 - loss: 1.6947 - regression_loss: 1.4206 - classification_loss: 0.2741
 254/1500 [====>.........................] - ETA: 16:40 - loss: 1.6947 - regression_loss: 1.4206 - classification_loss: 0.2742
 255/1500 [====>.........................] - ETA: 16:39 - loss: 1.6952 - regression_loss: 1.4205 - classification_loss: 0.2746
 256/1500 [====>.........................] - ETA: 16:41 - loss: 1.6936 - regression_loss: 1.4195 - classification_loss: 0.2741
 257/1500 [====>.........................] - ETA: 16:38 - loss: 1.6936 - regression_loss: 1.4191 - classification_loss: 0.2745
 258/1500 [====>.........................] - ETA: 16:38 - loss: 1.6916 - regression_loss: 1.4178 - classification_loss: 0.2738
 259/1500 [====>.........................] - ETA: 16:39 - loss: 1.6931 - regression_loss: 1.4194 - classification_loss: 0.2737
 260/1500 [====>.........................] - ETA: 16:39 - loss: 1.6905 - regression_loss: 1.4174 - classification_loss: 0.2732
 261/1500 [====>.........................] - ETA: 16:38 - loss: 1.6880 - regression_loss: 1.4154 - classification_loss: 0.2727
 262/1500 [====>.........................] - ETA: 16:35 - loss: 1.6884 - regression_loss: 1.4159 - classification_loss: 0.2725
 263/1500 [====>.........................] - ETA: 16:32 - loss: 1.6907 - regression_loss: 1.4177 - classification_loss: 0.2730
 264/1500 [====>.........................] - ETA: 16:36 - loss: 1.6885 - regression_loss: 1.4161 - classification_loss: 0.2724
 265/1500 [====>.........................] - ETA: 16:33 - loss: 1.6881 - regression_loss: 1.4154 - classification_loss: 0.2727
 266/1500 [====>.........................] - ETA: 16:38 - loss: 1.6911 - regression_loss: 1.4181 - classification_loss: 0.2730
 267/1500 [====>.........................] - ETA: 16:37 - loss: 1.6883 - regression_loss: 1.4157 - classification_loss: 0.2726
 268/1500 [====>.........................] - ETA: 16:41 - loss: 1.6881 - regression_loss: 1.4156 - classification_loss: 0.2725
 269/1500 [====>.........................] - ETA: 16:40 - loss: 1.6888 - regression_loss: 1.4163 - classification_loss: 0.2725
 270/1500 [====>.........................] - ETA: 16:38 - loss: 1.6871 - regression_loss: 1.4147 - classification_loss: 0.2724
 271/1500 [====>.........................] - ETA: 16:35 - loss: 1.6879 - regression_loss: 1.4155 - classification_loss: 0.2723
 272/1500 [====>.........................] - ETA: 16:35 - loss: 1.6876 - regression_loss: 1.4156 - classification_loss: 0.2719
 273/1500 [====>.........................] - ETA: 16:34 - loss: 1.6916 - regression_loss: 1.4187 - classification_loss: 0.2729
 274/1500 [====>.........................] - ETA: 16:32 - loss: 1.6914 - regression_loss: 1.4180 - classification_loss: 0.2733
 275/1500 [====>.........................] - ETA: 16:29 - loss: 1.6897 - regression_loss: 1.4168 - classification_loss: 0.2729
 276/1500 [====>.........................] - ETA: 16:26 - loss: 1.6950 - regression_loss: 1.4214 - classification_loss: 0.2736
 277/1500 [====>.........................] - ETA: 16:24 - loss: 1.6930 - regression_loss: 1.4198 - classification_loss: 0.2732
 278/1500 [====>.........................] - ETA: 16:22 - loss: 1.6964 - regression_loss: 1.4226 - classification_loss: 0.2738
 279/1500 [====>.........................] - ETA: 16:21 - loss: 1.6934 - regression_loss: 1.4202 - classification_loss: 0.2732
 280/1500 [====>.........................] - ETA: 16:26 - loss: 1.6966 - regression_loss: 1.4233 - classification_loss: 0.2733
 281/1500 [====>.........................] - ETA: 16:27 - loss: 1.6978 - regression_loss: 1.4249 - classification_loss: 0.2728
 282/1500 [====>.........................] - ETA: 16:27 - loss: 1.6979 - regression_loss: 1.4252 - classification_loss: 0.2727
 283/1500 [====>.........................] - ETA: 16:24 - loss: 1.6995 - regression_loss: 1.4268 - classification_loss: 0.2727
 284/1500 [====>.........................] - ETA: 16:22 - loss: 1.7026 - regression_loss: 1.4296 - classification_loss: 0.2730
 285/1500 [====>.........................] - ETA: 16:23 - loss: 1.7019 - regression_loss: 1.4293 - classification_loss: 0.2726
 286/1500 [====>.........................] - ETA: 16:21 - loss: 1.7076 - regression_loss: 1.4342 - classification_loss: 0.2735
 287/1500 [====>.........................] - ETA: 16:20 - loss: 1.7069 - regression_loss: 1.4339 - classification_loss: 0.2730
 288/1500 [====>.........................] - ETA: 16:19 - loss: 1.7088 - regression_loss: 1.4358 - classification_loss: 0.2730
 289/1500 [====>.........................] - ETA: 16:21 - loss: 1.7112 - regression_loss: 1.4378 - classification_loss: 0.2735
 290/1500 [====>.........................] - ETA: 16:19 - loss: 1.7095 - regression_loss: 1.4364 - classification_loss: 0.2731
 291/1500 [====>.........................] - ETA: 16:16 - loss: 1.7103 - regression_loss: 1.4370 - classification_loss: 0.2733
 292/1500 [====>.........................] - ETA: 16:16 - loss: 1.7127 - regression_loss: 1.4365 - classification_loss: 0.2762
 293/1500 [====>.........................] - ETA: 16:15 - loss: 1.7101 - regression_loss: 1.4343 - classification_loss: 0.2758
 294/1500 [====>.........................] - ETA: 16:13 - loss: 1.7123 - regression_loss: 1.4366 - classification_loss: 0.2757
 295/1500 [====>.........................] - ETA: 16:10 - loss: 1.7139 - regression_loss: 1.4379 - classification_loss: 0.2760
 296/1500 [====>.........................] - ETA: 16:07 - loss: 1.7152 - regression_loss: 1.4394 - classification_loss: 0.2757
 297/1500 [====>.........................] - ETA: 16:08 - loss: 1.7157 - regression_loss: 1.4394 - classification_loss: 0.2763
 298/1500 [====>.........................] - ETA: 16:06 - loss: 1.7147 - regression_loss: 1.4386 - classification_loss: 0.2761
 299/1500 [====>.........................] - ETA: 16:05 - loss: 1.7141 - regression_loss: 1.4381 - classification_loss: 0.2760
 300/1500 [=====>........................] - ETA: 16:06 - loss: 1.7161 - regression_loss: 1.4393 - classification_loss: 0.2767
 301/1500 [=====>........................] - ETA: 16:06 - loss: 1.7146 - regression_loss: 1.4382 - classification_loss: 0.2765
 302/1500 [=====>........................] - ETA: 16:04 - loss: 1.7175 - regression_loss: 1.4401 - classification_loss: 0.2773
 303/1500 [=====>........................] - ETA: 16:01 - loss: 1.7144 - regression_loss: 1.4377 - classification_loss: 0.2767
 304/1500 [=====>........................] - ETA: 15:59 - loss: 1.7168 - regression_loss: 1.4394 - classification_loss: 0.2773
 305/1500 [=====>........................] - ETA: 15:58 - loss: 1.7140 - regression_loss: 1.4371 - classification_loss: 0.2769
 306/1500 [=====>........................] - ETA: 15:55 - loss: 1.7116 - regression_loss: 1.4352 - classification_loss: 0.2764
 307/1500 [=====>........................] - ETA: 15:54 - loss: 1.7144 - regression_loss: 1.4376 - classification_loss: 0.2768
 308/1500 [=====>........................] - ETA: 15:52 - loss: 1.7130 - regression_loss: 1.4366 - classification_loss: 0.2764
 309/1500 [=====>........................] - ETA: 15:49 - loss: 1.7115 - regression_loss: 1.4355 - classification_loss: 0.2760
 310/1500 [=====>........................] - ETA: 15:47 - loss: 1.7110 - regression_loss: 1.4352 - classification_loss: 0.2758
 311/1500 [=====>........................] - ETA: 15:45 - loss: 1.7129 - regression_loss: 1.4367 - classification_loss: 0.2762
 312/1500 [=====>........................] - ETA: 15:43 - loss: 1.7122 - regression_loss: 1.4352 - classification_loss: 0.2770
 313/1500 [=====>........................] - ETA: 15:41 - loss: 1.7114 - regression_loss: 1.4346 - classification_loss: 0.2768
 314/1500 [=====>........................] - ETA: 15:38 - loss: 1.7124 - regression_loss: 1.4354 - classification_loss: 0.2770
 315/1500 [=====>........................] - ETA: 15:36 - loss: 1.7150 - regression_loss: 1.4380 - classification_loss: 0.2770
 316/1500 [=====>........................] - ETA: 15:34 - loss: 1.7137 - regression_loss: 1.4371 - classification_loss: 0.2766
 317/1500 [=====>........................] - ETA: 15:32 - loss: 1.7122 - regression_loss: 1.4360 - classification_loss: 0.2762
 318/1500 [=====>........................] - ETA: 15:30 - loss: 1.7117 - regression_loss: 1.4354 - classification_loss: 0.2763
 319/1500 [=====>........................] - ETA: 15:29 - loss: 1.7117 - regression_loss: 1.4355 - classification_loss: 0.2762
 320/1500 [=====>........................] - ETA: 15:33 - loss: 1.7157 - regression_loss: 1.4384 - classification_loss: 0.2773
 321/1500 [=====>........................] - ETA: 15:35 - loss: 1.7163 - regression_loss: 1.4390 - classification_loss: 0.2774
 322/1500 [=====>........................] - ETA: 15:36 - loss: 1.7141 - regression_loss: 1.4371 - classification_loss: 0.2770
 323/1500 [=====>........................] - ETA: 15:34 - loss: 1.7128 - regression_loss: 1.4362 - classification_loss: 0.2766
 324/1500 [=====>........................] - ETA: 15:32 - loss: 1.7117 - regression_loss: 1.4355 - classification_loss: 0.2761
 325/1500 [=====>........................] - ETA: 15:32 - loss: 1.7140 - regression_loss: 1.4369 - classification_loss: 0.2771
 326/1500 [=====>........................] - ETA: 15:32 - loss: 1.7125 - regression_loss: 1.4357 - classification_loss: 0.2768
 327/1500 [=====>........................] - ETA: 15:31 - loss: 1.7111 - regression_loss: 1.4348 - classification_loss: 0.2762
 328/1500 [=====>........................] - ETA: 15:29 - loss: 1.7098 - regression_loss: 1.4340 - classification_loss: 0.2758
 329/1500 [=====>........................] - ETA: 15:27 - loss: 1.7086 - regression_loss: 1.4332 - classification_loss: 0.2754
 330/1500 [=====>........................] - ETA: 15:25 - loss: 1.7081 - regression_loss: 1.4329 - classification_loss: 0.2752
 331/1500 [=====>........................] - ETA: 15:24 - loss: 1.7064 - regression_loss: 1.4315 - classification_loss: 0.2749
 332/1500 [=====>........................] - ETA: 15:23 - loss: 1.7067 - regression_loss: 1.4316 - classification_loss: 0.2750
 333/1500 [=====>........................] - ETA: 15:20 - loss: 1.7050 - regression_loss: 1.4304 - classification_loss: 0.2746
 334/1500 [=====>........................] - ETA: 15:18 - loss: 1.7053 - regression_loss: 1.4308 - classification_loss: 0.2745
 335/1500 [=====>........................] - ETA: 15:17 - loss: 1.7043 - regression_loss: 1.4302 - classification_loss: 0.2741
 336/1500 [=====>........................] - ETA: 15:17 - loss: 1.7051 - regression_loss: 1.4306 - classification_loss: 0.2745
 337/1500 [=====>........................] - ETA: 15:17 - loss: 1.7053 - regression_loss: 1.4310 - classification_loss: 0.2743
 338/1500 [=====>........................] - ETA: 15:15 - loss: 1.7062 - regression_loss: 1.4311 - classification_loss: 0.2751
 339/1500 [=====>........................] - ETA: 15:15 - loss: 1.7066 - regression_loss: 1.4317 - classification_loss: 0.2749
 340/1500 [=====>........................] - ETA: 15:13 - loss: 1.7087 - regression_loss: 1.4334 - classification_loss: 0.2752
 341/1500 [=====>........................] - ETA: 15:13 - loss: 1.7106 - regression_loss: 1.4347 - classification_loss: 0.2759
 342/1500 [=====>........................] - ETA: 15:14 - loss: 1.7095 - regression_loss: 1.4340 - classification_loss: 0.2756
 343/1500 [=====>........................] - ETA: 15:14 - loss: 1.7098 - regression_loss: 1.4341 - classification_loss: 0.2757
 344/1500 [=====>........................] - ETA: 15:12 - loss: 1.7099 - regression_loss: 1.4344 - classification_loss: 0.2754
 345/1500 [=====>........................] - ETA: 15:12 - loss: 1.7085 - regression_loss: 1.4333 - classification_loss: 0.2752
 346/1500 [=====>........................] - ETA: 15:20 - loss: 1.7094 - regression_loss: 1.4341 - classification_loss: 0.2752
 347/1500 [=====>........................] - ETA: 15:20 - loss: 1.7089 - regression_loss: 1.4337 - classification_loss: 0.2752
 348/1500 [=====>........................] - ETA: 15:18 - loss: 1.7094 - regression_loss: 1.4344 - classification_loss: 0.2750
 349/1500 [=====>........................] - ETA: 15:18 - loss: 1.7094 - regression_loss: 1.4346 - classification_loss: 0.2749
 350/1500 [======>.......................] - ETA: 15:16 - loss: 1.7091 - regression_loss: 1.4343 - classification_loss: 0.2748
 351/1500 [======>.......................] - ETA: 15:15 - loss: 1.7092 - regression_loss: 1.4341 - classification_loss: 0.2751
 352/1500 [======>.......................] - ETA: 15:13 - loss: 1.7095 - regression_loss: 1.4344 - classification_loss: 0.2750
 353/1500 [======>.......................] - ETA: 15:12 - loss: 1.7092 - regression_loss: 1.4343 - classification_loss: 0.2748
 354/1500 [======>.......................] - ETA: 15:11 - loss: 1.7094 - regression_loss: 1.4335 - classification_loss: 0.2759
 355/1500 [======>.......................] - ETA: 15:09 - loss: 1.7062 - regression_loss: 1.4309 - classification_loss: 0.2753
 356/1500 [======>.......................] - ETA: 15:07 - loss: 1.7046 - regression_loss: 1.4297 - classification_loss: 0.2749
 357/1500 [======>.......................] - ETA: 15:07 - loss: 1.7028 - regression_loss: 1.4284 - classification_loss: 0.2743
 358/1500 [======>.......................] - ETA: 15:07 - loss: 1.7038 - regression_loss: 1.4292 - classification_loss: 0.2746
 359/1500 [======>.......................] - ETA: 15:05 - loss: 1.7013 - regression_loss: 1.4273 - classification_loss: 0.2741
 360/1500 [======>.......................] - ETA: 15:03 - loss: 1.6998 - regression_loss: 1.4261 - classification_loss: 0.2737
 361/1500 [======>.......................] - ETA: 15:08 - loss: 1.7004 - regression_loss: 1.4264 - classification_loss: 0.2741
 362/1500 [======>.......................] - ETA: 15:07 - loss: 1.7003 - regression_loss: 1.4263 - classification_loss: 0.2740
 363/1500 [======>.......................] - ETA: 15:05 - loss: 1.6987 - regression_loss: 1.4248 - classification_loss: 0.2739
 364/1500 [======>.......................] - ETA: 15:03 - loss: 1.6964 - regression_loss: 1.4230 - classification_loss: 0.2734
 365/1500 [======>.......................] - ETA: 15:03 - loss: 1.6943 - regression_loss: 1.4213 - classification_loss: 0.2730
 366/1500 [======>.......................] - ETA: 15:01 - loss: 1.6955 - regression_loss: 1.4224 - classification_loss: 0.2732
 367/1500 [======>.......................] - ETA: 15:03 - loss: 1.6946 - regression_loss: 1.4216 - classification_loss: 0.2729
 368/1500 [======>.......................] - ETA: 15:04 - loss: 1.6923 - regression_loss: 1.4197 - classification_loss: 0.2726
 369/1500 [======>.......................] - ETA: 15:02 - loss: 1.6897 - regression_loss: 1.4176 - classification_loss: 0.2721
 370/1500 [======>.......................] - ETA: 15:01 - loss: 1.6897 - regression_loss: 1.4174 - classification_loss: 0.2723
 371/1500 [======>.......................] - ETA: 14:59 - loss: 1.6892 - regression_loss: 1.4171 - classification_loss: 0.2720
 372/1500 [======>.......................] - ETA: 14:57 - loss: 1.6892 - regression_loss: 1.4172 - classification_loss: 0.2720
 373/1500 [======>.......................] - ETA: 14:55 - loss: 1.6901 - regression_loss: 1.4179 - classification_loss: 0.2722
 374/1500 [======>.......................] - ETA: 14:53 - loss: 1.6911 - regression_loss: 1.4190 - classification_loss: 0.2721
 375/1500 [======>.......................] - ETA: 14:54 - loss: 1.6923 - regression_loss: 1.4199 - classification_loss: 0.2724
 376/1500 [======>.......................] - ETA: 14:52 - loss: 1.6952 - regression_loss: 1.4222 - classification_loss: 0.2730
 377/1500 [======>.......................] - ETA: 14:54 - loss: 1.6947 - regression_loss: 1.4211 - classification_loss: 0.2737
 378/1500 [======>.......................] - ETA: 14:54 - loss: 1.6959 - regression_loss: 1.4219 - classification_loss: 0.2739
 379/1500 [======>.......................] - ETA: 14:54 - loss: 1.6937 - regression_loss: 1.4201 - classification_loss: 0.2736
 380/1500 [======>.......................] - ETA: 14:53 - loss: 1.6963 - regression_loss: 1.4219 - classification_loss: 0.2744
 381/1500 [======>.......................] - ETA: 14:54 - loss: 1.6983 - regression_loss: 1.4234 - classification_loss: 0.2749
 382/1500 [======>.......................] - ETA: 14:58 - loss: 1.6985 - regression_loss: 1.4237 - classification_loss: 0.2748
 383/1500 [======>.......................] - ETA: 14:56 - loss: 1.6981 - regression_loss: 1.4230 - classification_loss: 0.2751
 384/1500 [======>.......................] - ETA: 14:56 - loss: 1.6983 - regression_loss: 1.4229 - classification_loss: 0.2754
 385/1500 [======>.......................] - ETA: 14:56 - loss: 1.6982 - regression_loss: 1.4228 - classification_loss: 0.2754
 386/1500 [======>.......................] - ETA: 14:56 - loss: 1.6978 - regression_loss: 1.4227 - classification_loss: 0.2752
 387/1500 [======>.......................] - ETA: 14:56 - loss: 1.6966 - regression_loss: 1.4217 - classification_loss: 0.2749
 388/1500 [======>.......................] - ETA: 14:56 - loss: 1.6957 - regression_loss: 1.4209 - classification_loss: 0.2748
 389/1500 [======>.......................] - ETA: 14:54 - loss: 1.6944 - regression_loss: 1.4199 - classification_loss: 0.2745
 390/1500 [======>.......................] - ETA: 14:55 - loss: 1.6955 - regression_loss: 1.4211 - classification_loss: 0.2744
 391/1500 [======>.......................] - ETA: 14:53 - loss: 1.6954 - regression_loss: 1.4211 - classification_loss: 0.2743
 392/1500 [======>.......................] - ETA: 14:51 - loss: 1.6960 - regression_loss: 1.4220 - classification_loss: 0.2740
 393/1500 [======>.......................] - ETA: 14:49 - loss: 1.6958 - regression_loss: 1.4222 - classification_loss: 0.2736
 394/1500 [======>.......................] - ETA: 14:47 - loss: 1.6943 - regression_loss: 1.4210 - classification_loss: 0.2733
 395/1500 [======>.......................] - ETA: 14:45 - loss: 1.6945 - regression_loss: 1.4213 - classification_loss: 0.2733
 396/1500 [======>.......................] - ETA: 14:44 - loss: 1.6925 - regression_loss: 1.4194 - classification_loss: 0.2731
 397/1500 [======>.......................] - ETA: 14:42 - loss: 1.6939 - regression_loss: 1.4210 - classification_loss: 0.2730
 398/1500 [======>.......................] - ETA: 14:41 - loss: 1.6971 - regression_loss: 1.4237 - classification_loss: 0.2733
 399/1500 [======>.......................] - ETA: 14:40 - loss: 1.6974 - regression_loss: 1.4242 - classification_loss: 0.2731
 400/1500 [=======>......................] - ETA: 14:41 - loss: 1.6956 - regression_loss: 1.4228 - classification_loss: 0.2728
 401/1500 [=======>......................] - ETA: 14:39 - loss: 1.6950 - regression_loss: 1.4224 - classification_loss: 0.2726
 402/1500 [=======>......................] - ETA: 14:38 - loss: 1.6934 - regression_loss: 1.4209 - classification_loss: 0.2724
 403/1500 [=======>......................] - ETA: 14:36 - loss: 1.6919 - regression_loss: 1.4200 - classification_loss: 0.2720
 404/1500 [=======>......................] - ETA: 14:39 - loss: 1.6914 - regression_loss: 1.4196 - classification_loss: 0.2717
 405/1500 [=======>......................] - ETA: 14:38 - loss: 1.6898 - regression_loss: 1.4183 - classification_loss: 0.2715
 406/1500 [=======>......................] - ETA: 14:40 - loss: 1.6941 - regression_loss: 1.4220 - classification_loss: 0.2721
 407/1500 [=======>......................] - ETA: 14:39 - loss: 1.6973 - regression_loss: 1.4246 - classification_loss: 0.2727
 408/1500 [=======>......................] - ETA: 14:38 - loss: 1.6988 - regression_loss: 1.4262 - classification_loss: 0.2727
 409/1500 [=======>......................] - ETA: 14:37 - loss: 1.7014 - regression_loss: 1.4282 - classification_loss: 0.2732
 410/1500 [=======>......................] - ETA: 14:38 - loss: 1.7030 - regression_loss: 1.4295 - classification_loss: 0.2735
 411/1500 [=======>......................] - ETA: 14:40 - loss: 1.7035 - regression_loss: 1.4301 - classification_loss: 0.2735
 412/1500 [=======>......................] - ETA: 14:41 - loss: 1.7022 - regression_loss: 1.4288 - classification_loss: 0.2734
 413/1500 [=======>......................] - ETA: 14:39 - loss: 1.7020 - regression_loss: 1.4283 - classification_loss: 0.2737
 414/1500 [=======>......................] - ETA: 14:38 - loss: 1.7013 - regression_loss: 1.4279 - classification_loss: 0.2734
 415/1500 [=======>......................] - ETA: 14:36 - loss: 1.7053 - regression_loss: 1.4308 - classification_loss: 0.2745
 416/1500 [=======>......................] - ETA: 14:35 - loss: 1.7046 - regression_loss: 1.4302 - classification_loss: 0.2743
 417/1500 [=======>......................] - ETA: 14:34 - loss: 1.7050 - regression_loss: 1.4305 - classification_loss: 0.2745
 418/1500 [=======>......................] - ETA: 14:32 - loss: 1.7054 - regression_loss: 1.4312 - classification_loss: 0.2742
 419/1500 [=======>......................] - ETA: 14:31 - loss: 1.7054 - regression_loss: 1.4309 - classification_loss: 0.2745
 420/1500 [=======>......................] - ETA: 14:29 - loss: 1.7062 - regression_loss: 1.4317 - classification_loss: 0.2745
 421/1500 [=======>......................] - ETA: 14:28 - loss: 1.7072 - regression_loss: 1.4330 - classification_loss: 0.2742
 422/1500 [=======>......................] - ETA: 14:27 - loss: 1.7077 - regression_loss: 1.4335 - classification_loss: 0.2743
 423/1500 [=======>......................] - ETA: 14:26 - loss: 1.7062 - regression_loss: 1.4324 - classification_loss: 0.2738
 424/1500 [=======>......................] - ETA: 14:24 - loss: 1.7066 - regression_loss: 1.4326 - classification_loss: 0.2741
 425/1500 [=======>......................] - ETA: 14:25 - loss: 1.7141 - regression_loss: 1.4316 - classification_loss: 0.2825
 426/1500 [=======>......................] - ETA: 14:23 - loss: 1.7116 - regression_loss: 1.4296 - classification_loss: 0.2820
 427/1500 [=======>......................] - ETA: 14:21 - loss: 1.7109 - regression_loss: 1.4291 - classification_loss: 0.2818
 428/1500 [=======>......................] - ETA: 14:20 - loss: 1.7098 - regression_loss: 1.4284 - classification_loss: 0.2814
 429/1500 [=======>......................] - ETA: 14:18 - loss: 1.7128 - regression_loss: 1.4308 - classification_loss: 0.2821
 430/1500 [=======>......................] - ETA: 14:16 - loss: 1.7116 - regression_loss: 1.4297 - classification_loss: 0.2819
 431/1500 [=======>......................] - ETA: 14:14 - loss: 1.7118 - regression_loss: 1.4298 - classification_loss: 0.2820
 432/1500 [=======>......................] - ETA: 14:14 - loss: 1.7099 - regression_loss: 1.4282 - classification_loss: 0.2816
 433/1500 [=======>......................] - ETA: 14:12 - loss: 1.7121 - regression_loss: 1.4304 - classification_loss: 0.2817
 434/1500 [=======>......................] - ETA: 14:11 - loss: 1.7136 - regression_loss: 1.4316 - classification_loss: 0.2820
 435/1500 [=======>......................] - ETA: 14:10 - loss: 1.7134 - regression_loss: 1.4314 - classification_loss: 0.2819
 436/1500 [=======>......................] - ETA: 14:08 - loss: 1.7125 - regression_loss: 1.4309 - classification_loss: 0.2816
 437/1500 [=======>......................] - ETA: 14:06 - loss: 1.7130 - regression_loss: 1.4313 - classification_loss: 0.2818
 438/1500 [=======>......................] - ETA: 14:05 - loss: 1.7123 - regression_loss: 1.4305 - classification_loss: 0.2818
 439/1500 [=======>......................] - ETA: 14:03 - loss: 1.7116 - regression_loss: 1.4301 - classification_loss: 0.2815
 440/1500 [=======>......................] - ETA: 14:01 - loss: 1.7127 - regression_loss: 1.4309 - classification_loss: 0.2818
 441/1500 [=======>......................] - ETA: 14:00 - loss: 1.7126 - regression_loss: 1.4310 - classification_loss: 0.2817
 442/1500 [=======>......................] - ETA: 13:58 - loss: 1.7131 - regression_loss: 1.4315 - classification_loss: 0.2816
 443/1500 [=======>......................] - ETA: 13:57 - loss: 1.7125 - regression_loss: 1.4312 - classification_loss: 0.2813
 444/1500 [=======>......................] - ETA: 13:56 - loss: 1.7128 - regression_loss: 1.4318 - classification_loss: 0.2810
 445/1500 [=======>......................] - ETA: 13:55 - loss: 1.7117 - regression_loss: 1.4310 - classification_loss: 0.2806
 446/1500 [=======>......................] - ETA: 13:53 - loss: 1.7114 - regression_loss: 1.4309 - classification_loss: 0.2806
 447/1500 [=======>......................] - ETA: 13:51 - loss: 1.7108 - regression_loss: 1.4304 - classification_loss: 0.2804
 448/1500 [=======>......................] - ETA: 13:49 - loss: 1.7123 - regression_loss: 1.4317 - classification_loss: 0.2806
 449/1500 [=======>......................] - ETA: 13:48 - loss: 1.7132 - regression_loss: 1.4326 - classification_loss: 0.2805
 450/1500 [========>.....................] - ETA: 13:46 - loss: 1.7119 - regression_loss: 1.4316 - classification_loss: 0.2802
 451/1500 [========>.....................] - ETA: 13:44 - loss: 1.7104 - regression_loss: 1.4304 - classification_loss: 0.2800
 452/1500 [========>.....................] - ETA: 13:43 - loss: 1.7084 - regression_loss: 1.4288 - classification_loss: 0.2795
 453/1500 [========>.....................] - ETA: 13:41 - loss: 1.7075 - regression_loss: 1.4283 - classification_loss: 0.2792
 454/1500 [========>.....................] - ETA: 13:40 - loss: 1.7080 - regression_loss: 1.4290 - classification_loss: 0.2790
 455/1500 [========>.....................] - ETA: 13:41 - loss: 1.7105 - regression_loss: 1.4307 - classification_loss: 0.2798
 456/1500 [========>.....................] - ETA: 13:40 - loss: 1.7107 - regression_loss: 1.4311 - classification_loss: 0.2796
 457/1500 [========>.....................] - ETA: 13:38 - loss: 1.7111 - regression_loss: 1.4310 - classification_loss: 0.2802
 458/1500 [========>.....................] - ETA: 13:37 - loss: 1.7127 - regression_loss: 1.4319 - classification_loss: 0.2807
 459/1500 [========>.....................] - ETA: 13:35 - loss: 1.7114 - regression_loss: 1.4309 - classification_loss: 0.2805
 460/1500 [========>.....................] - ETA: 13:34 - loss: 1.7102 - regression_loss: 1.4298 - classification_loss: 0.2804
 461/1500 [========>.....................] - ETA: 13:33 - loss: 1.7099 - regression_loss: 1.4297 - classification_loss: 0.2802
 462/1500 [========>.....................] - ETA: 13:32 - loss: 1.7092 - regression_loss: 1.4293 - classification_loss: 0.2799
 463/1500 [========>.....................] - ETA: 13:32 - loss: 1.7102 - regression_loss: 1.4300 - classification_loss: 0.2802
 464/1500 [========>.....................] - ETA: 13:31 - loss: 1.7123 - regression_loss: 1.4320 - classification_loss: 0.2803
 465/1500 [========>.....................] - ETA: 13:29 - loss: 1.7109 - regression_loss: 1.4310 - classification_loss: 0.2799
 466/1500 [========>.....................] - ETA: 13:28 - loss: 1.7114 - regression_loss: 1.4315 - classification_loss: 0.2799
 467/1500 [========>.....................] - ETA: 13:27 - loss: 1.7096 - regression_loss: 1.4300 - classification_loss: 0.2796
 468/1500 [========>.....................] - ETA: 13:26 - loss: 1.7128 - regression_loss: 1.4324 - classification_loss: 0.2804
 469/1500 [========>.....................] - ETA: 13:25 - loss: 1.7128 - regression_loss: 1.4325 - classification_loss: 0.2804
 470/1500 [========>.....................] - ETA: 13:23 - loss: 1.7131 - regression_loss: 1.4327 - classification_loss: 0.2804
 471/1500 [========>.....................] - ETA: 13:22 - loss: 1.7116 - regression_loss: 1.4315 - classification_loss: 0.2801
 472/1500 [========>.....................] - ETA: 13:21 - loss: 1.7108 - regression_loss: 1.4308 - classification_loss: 0.2800
 473/1500 [========>.....................] - ETA: 13:19 - loss: 1.7105 - regression_loss: 1.4308 - classification_loss: 0.2797
 474/1500 [========>.....................] - ETA: 13:20 - loss: 1.7108 - regression_loss: 1.4313 - classification_loss: 0.2795
 475/1500 [========>.....................] - ETA: 13:20 - loss: 1.7112 - regression_loss: 1.4319 - classification_loss: 0.2793
 476/1500 [========>.....................] - ETA: 13:18 - loss: 1.7100 - regression_loss: 1.4308 - classification_loss: 0.2792
 477/1500 [========>.....................] - ETA: 13:17 - loss: 1.7086 - regression_loss: 1.4296 - classification_loss: 0.2790
 478/1500 [========>.....................] - ETA: 13:15 - loss: 1.7084 - regression_loss: 1.4294 - classification_loss: 0.2789
 479/1500 [========>.....................] - ETA: 13:14 - loss: 1.7092 - regression_loss: 1.4304 - classification_loss: 0.2788
 480/1500 [========>.....................] - ETA: 13:13 - loss: 1.7084 - regression_loss: 1.4298 - classification_loss: 0.2787
 481/1500 [========>.....................] - ETA: 13:13 - loss: 1.7081 - regression_loss: 1.4296 - classification_loss: 0.2785
 482/1500 [========>.....................] - ETA: 13:11 - loss: 1.7071 - regression_loss: 1.4286 - classification_loss: 0.2784
 483/1500 [========>.....................] - ETA: 13:09 - loss: 1.7098 - regression_loss: 1.4307 - classification_loss: 0.2791
 484/1500 [========>.....................] - ETA: 13:08 - loss: 1.7077 - regression_loss: 1.4290 - classification_loss: 0.2787
 485/1500 [========>.....................] - ETA: 13:07 - loss: 1.7069 - regression_loss: 1.4284 - classification_loss: 0.2785
 486/1500 [========>.....................] - ETA: 13:07 - loss: 1.7090 - regression_loss: 1.4305 - classification_loss: 0.2785
 487/1500 [========>.....................] - ETA: 13:05 - loss: 1.7089 - regression_loss: 1.4301 - classification_loss: 0.2788
 488/1500 [========>.....................] - ETA: 13:04 - loss: 1.7102 - regression_loss: 1.4312 - classification_loss: 0.2791
 489/1500 [========>.....................] - ETA: 13:04 - loss: 1.7105 - regression_loss: 1.4313 - classification_loss: 0.2792
 490/1500 [========>.....................] - ETA: 13:03 - loss: 1.7085 - regression_loss: 1.4297 - classification_loss: 0.2788
 491/1500 [========>.....................] - ETA: 13:02 - loss: 1.7086 - regression_loss: 1.4297 - classification_loss: 0.2789
 492/1500 [========>.....................] - ETA: 13:01 - loss: 1.7092 - regression_loss: 1.4305 - classification_loss: 0.2788
 493/1500 [========>.....................] - ETA: 13:00 - loss: 1.7084 - regression_loss: 1.4296 - classification_loss: 0.2788
 494/1500 [========>.....................] - ETA: 12:59 - loss: 1.7104 - regression_loss: 1.4313 - classification_loss: 0.2791
 495/1500 [========>.....................] - ETA: 12:58 - loss: 1.7110 - regression_loss: 1.4318 - classification_loss: 0.2792
 496/1500 [========>.....................] - ETA: 12:59 - loss: 1.7109 - regression_loss: 1.4315 - classification_loss: 0.2793
 497/1500 [========>.....................] - ETA: 12:59 - loss: 1.7090 - regression_loss: 1.4300 - classification_loss: 0.2790
 498/1500 [========>.....................] - ETA: 12:57 - loss: 1.7092 - regression_loss: 1.4304 - classification_loss: 0.2788
 499/1500 [========>.....................] - ETA: 12:56 - loss: 1.7076 - regression_loss: 1.4292 - classification_loss: 0.2783
 500/1500 [=========>....................] - ETA: 12:55 - loss: 1.7080 - regression_loss: 1.4295 - classification_loss: 0.2785
 501/1500 [=========>....................] - ETA: 12:54 - loss: 1.7076 - regression_loss: 1.4289 - classification_loss: 0.2787
 502/1500 [=========>....................] - ETA: 12:53 - loss: 1.7080 - regression_loss: 1.4295 - classification_loss: 0.2785
 503/1500 [=========>....................] - ETA: 12:52 - loss: 1.7082 - regression_loss: 1.4297 - classification_loss: 0.2785
 504/1500 [=========>....................] - ETA: 12:50 - loss: 1.7094 - regression_loss: 1.4304 - classification_loss: 0.2790
 505/1500 [=========>....................] - ETA: 12:49 - loss: 1.7090 - regression_loss: 1.4300 - classification_loss: 0.2790
 506/1500 [=========>....................] - ETA: 12:47 - loss: 1.7111 - regression_loss: 1.4312 - classification_loss: 0.2800
 507/1500 [=========>....................] - ETA: 12:46 - loss: 1.7107 - regression_loss: 1.4308 - classification_loss: 0.2799
 508/1500 [=========>....................] - ETA: 12:45 - loss: 1.7102 - regression_loss: 1.4302 - classification_loss: 0.2800
 509/1500 [=========>....................] - ETA: 12:45 - loss: 1.7115 - regression_loss: 1.4313 - classification_loss: 0.2802
 510/1500 [=========>....................] - ETA: 12:43 - loss: 1.7135 - regression_loss: 1.4331 - classification_loss: 0.2805
 511/1500 [=========>....................] - ETA: 12:45 - loss: 1.7133 - regression_loss: 1.4330 - classification_loss: 0.2803
 512/1500 [=========>....................] - ETA: 12:44 - loss: 1.7155 - regression_loss: 1.4351 - classification_loss: 0.2804
 513/1500 [=========>....................] - ETA: 12:44 - loss: 1.7143 - regression_loss: 1.4341 - classification_loss: 0.2802
 514/1500 [=========>....................] - ETA: 12:46 - loss: 1.7147 - regression_loss: 1.4341 - classification_loss: 0.2806
 515/1500 [=========>....................] - ETA: 12:46 - loss: 1.7151 - regression_loss: 1.4345 - classification_loss: 0.2806
 516/1500 [=========>....................] - ETA: 12:45 - loss: 1.7145 - regression_loss: 1.4339 - classification_loss: 0.2806
 517/1500 [=========>....................] - ETA: 12:46 - loss: 1.7137 - regression_loss: 1.4333 - classification_loss: 0.2804
 518/1500 [=========>....................] - ETA: 12:44 - loss: 1.7137 - regression_loss: 1.4331 - classification_loss: 0.2805
 519/1500 [=========>....................] - ETA: 12:43 - loss: 1.7125 - regression_loss: 1.4322 - classification_loss: 0.2803
 520/1500 [=========>....................] - ETA: 12:41 - loss: 1.7121 - regression_loss: 1.4318 - classification_loss: 0.2803
 521/1500 [=========>....................] - ETA: 12:40 - loss: 1.7111 - regression_loss: 1.4307 - classification_loss: 0.2804
 522/1500 [=========>....................] - ETA: 12:44 - loss: 1.7113 - regression_loss: 1.4309 - classification_loss: 0.2804
 523/1500 [=========>....................] - ETA: 12:42 - loss: 1.7130 - regression_loss: 1.4322 - classification_loss: 0.2807
 524/1500 [=========>....................] - ETA: 12:41 - loss: 1.7148 - regression_loss: 1.4338 - classification_loss: 0.2811
 525/1500 [=========>....................] - ETA: 12:40 - loss: 1.7145 - regression_loss: 1.4336 - classification_loss: 0.2810
 526/1500 [=========>....................] - ETA: 12:39 - loss: 1.7160 - regression_loss: 1.4348 - classification_loss: 0.2812
 527/1500 [=========>....................] - ETA: 12:40 - loss: 1.7156 - regression_loss: 1.4345 - classification_loss: 0.2811
 528/1500 [=========>....................] - ETA: 12:40 - loss: 1.7151 - regression_loss: 1.4341 - classification_loss: 0.2810
 529/1500 [=========>....................] - ETA: 12:39 - loss: 1.7164 - regression_loss: 1.4351 - classification_loss: 0.2813
 530/1500 [=========>....................] - ETA: 12:38 - loss: 1.7175 - regression_loss: 1.4359 - classification_loss: 0.2816
 531/1500 [=========>....................] - ETA: 12:36 - loss: 1.7191 - regression_loss: 1.4372 - classification_loss: 0.2818
 532/1500 [=========>....................] - ETA: 12:36 - loss: 1.7212 - regression_loss: 1.4389 - classification_loss: 0.2823
 533/1500 [=========>....................] - ETA: 12:35 - loss: 1.7234 - regression_loss: 1.4406 - classification_loss: 0.2828
 534/1500 [=========>....................] - ETA: 12:34 - loss: 1.7237 - regression_loss: 1.4411 - classification_loss: 0.2826
 535/1500 [=========>....................] - ETA: 12:33 - loss: 1.7249 - regression_loss: 1.4420 - classification_loss: 0.2829
 536/1500 [=========>....................] - ETA: 12:31 - loss: 1.7242 - regression_loss: 1.4415 - classification_loss: 0.2828
 537/1500 [=========>....................] - ETA: 12:30 - loss: 1.7234 - regression_loss: 1.4407 - classification_loss: 0.2826
 538/1500 [=========>....................] - ETA: 12:28 - loss: 1.7222 - regression_loss: 1.4399 - classification_loss: 0.2823
 539/1500 [=========>....................] - ETA: 12:27 - loss: 1.7220 - regression_loss: 1.4398 - classification_loss: 0.2822
 540/1500 [=========>....................] - ETA: 12:25 - loss: 1.7229 - regression_loss: 1.4406 - classification_loss: 0.2823
 541/1500 [=========>....................] - ETA: 12:25 - loss: 1.7242 - regression_loss: 1.4419 - classification_loss: 0.2823
 542/1500 [=========>....................] - ETA: 12:23 - loss: 1.7258 - regression_loss: 1.4433 - classification_loss: 0.2825
 543/1500 [=========>....................] - ETA: 12:22 - loss: 1.7254 - regression_loss: 1.4428 - classification_loss: 0.2826
 544/1500 [=========>....................] - ETA: 12:22 - loss: 1.7267 - regression_loss: 1.4430 - classification_loss: 0.2837
 545/1500 [=========>....................] - ETA: 12:22 - loss: 1.7274 - regression_loss: 1.4437 - classification_loss: 0.2838
 546/1500 [=========>....................] - ETA: 12:23 - loss: 1.7280 - regression_loss: 1.4440 - classification_loss: 0.2841
 547/1500 [=========>....................] - ETA: 12:22 - loss: 1.7277 - regression_loss: 1.4437 - classification_loss: 0.2840
 548/1500 [=========>....................] - ETA: 12:21 - loss: 1.7262 - regression_loss: 1.4426 - classification_loss: 0.2836
 549/1500 [=========>....................] - ETA: 12:19 - loss: 1.7278 - regression_loss: 1.4437 - classification_loss: 0.2840
 550/1500 [==========>...................] - ETA: 12:18 - loss: 1.7275 - regression_loss: 1.4431 - classification_loss: 0.2844
 551/1500 [==========>...................] - ETA: 12:17 - loss: 1.7282 - regression_loss: 1.4436 - classification_loss: 0.2846
 552/1500 [==========>...................] - ETA: 12:16 - loss: 1.7309 - regression_loss: 1.4460 - classification_loss: 0.2849
 553/1500 [==========>...................] - ETA: 12:15 - loss: 1.7317 - regression_loss: 1.4465 - classification_loss: 0.2852
 554/1500 [==========>...................] - ETA: 12:15 - loss: 1.7306 - regression_loss: 1.4457 - classification_loss: 0.2849
 555/1500 [==========>...................] - ETA: 12:16 - loss: 1.7317 - regression_loss: 1.4466 - classification_loss: 0.2851
 556/1500 [==========>...................] - ETA: 12:15 - loss: 1.7315 - regression_loss: 1.4465 - classification_loss: 0.2851
 557/1500 [==========>...................] - ETA: 12:14 - loss: 1.7298 - regression_loss: 1.4451 - classification_loss: 0.2847
 558/1500 [==========>...................] - ETA: 12:13 - loss: 1.7290 - regression_loss: 1.4443 - classification_loss: 0.2846
 559/1500 [==========>...................] - ETA: 12:12 - loss: 1.7282 - regression_loss: 1.4438 - classification_loss: 0.2844
 560/1500 [==========>...................] - ETA: 12:12 - loss: 1.7268 - regression_loss: 1.4426 - classification_loss: 0.2842
 561/1500 [==========>...................] - ETA: 12:12 - loss: 1.7270 - regression_loss: 1.4427 - classification_loss: 0.2843
 562/1500 [==========>...................] - ETA: 12:12 - loss: 1.7273 - regression_loss: 1.4431 - classification_loss: 0.2842
 563/1500 [==========>...................] - ETA: 12:11 - loss: 1.7264 - regression_loss: 1.4424 - classification_loss: 0.2840
 564/1500 [==========>...................] - ETA: 12:10 - loss: 1.7285 - regression_loss: 1.4439 - classification_loss: 0.2846
 565/1500 [==========>...................] - ETA: 12:09 - loss: 1.7305 - regression_loss: 1.4457 - classification_loss: 0.2847
 566/1500 [==========>...................] - ETA: 12:09 - loss: 1.7293 - regression_loss: 1.4448 - classification_loss: 0.2845
 567/1500 [==========>...................] - ETA: 12:08 - loss: 1.7290 - regression_loss: 1.4447 - classification_loss: 0.2843
 568/1500 [==========>...................] - ETA: 12:07 - loss: 1.7293 - regression_loss: 1.4450 - classification_loss: 0.2844
 569/1500 [==========>...................] - ETA: 12:05 - loss: 1.7286 - regression_loss: 1.4444 - classification_loss: 0.2842
 570/1500 [==========>...................] - ETA: 12:06 - loss: 1.7287 - regression_loss: 1.4446 - classification_loss: 0.2841
 571/1500 [==========>...................] - ETA: 12:04 - loss: 1.7313 - regression_loss: 1.4463 - classification_loss: 0.2849
 572/1500 [==========>...................] - ETA: 12:04 - loss: 1.7310 - regression_loss: 1.4461 - classification_loss: 0.2849
 573/1500 [==========>...................] - ETA: 12:02 - loss: 1.7322 - regression_loss: 1.4470 - classification_loss: 0.2852
 574/1500 [==========>...................] - ETA: 12:01 - loss: 1.7330 - regression_loss: 1.4476 - classification_loss: 0.2854
 575/1500 [==========>...................] - ETA: 12:01 - loss: 1.7323 - regression_loss: 1.4470 - classification_loss: 0.2853
 576/1500 [==========>...................] - ETA: 12:00 - loss: 1.7309 - regression_loss: 1.4460 - classification_loss: 0.2850
 577/1500 [==========>...................] - ETA: 12:01 - loss: 1.7304 - regression_loss: 1.4457 - classification_loss: 0.2847
 578/1500 [==========>...................] - ETA: 12:00 - loss: 1.7302 - regression_loss: 1.4457 - classification_loss: 0.2845
 579/1500 [==========>...................] - ETA: 11:59 - loss: 1.7300 - regression_loss: 1.4457 - classification_loss: 0.2843
 580/1500 [==========>...................] - ETA: 11:59 - loss: 1.7318 - regression_loss: 1.4471 - classification_loss: 0.2847
 581/1500 [==========>...................] - ETA: 11:58 - loss: 1.7322 - regression_loss: 1.4462 - classification_loss: 0.2860
 582/1500 [==========>...................] - ETA: 11:57 - loss: 1.7322 - regression_loss: 1.4462 - classification_loss: 0.2860
 583/1500 [==========>...................] - ETA: 11:57 - loss: 1.7346 - regression_loss: 1.4481 - classification_loss: 0.2866
 584/1500 [==========>...................] - ETA: 11:57 - loss: 1.7335 - regression_loss: 1.4472 - classification_loss: 0.2863
 585/1500 [==========>...................] - ETA: 11:56 - loss: 1.7349 - regression_loss: 1.4482 - classification_loss: 0.2867
 586/1500 [==========>...................] - ETA: 11:55 - loss: 1.7375 - regression_loss: 1.4499 - classification_loss: 0.2876
 587/1500 [==========>...................] - ETA: 11:54 - loss: 1.7360 - regression_loss: 1.4486 - classification_loss: 0.2874
 588/1500 [==========>...................] - ETA: 11:54 - loss: 1.7378 - regression_loss: 1.4501 - classification_loss: 0.2876
 589/1500 [==========>...................] - ETA: 11:52 - loss: 1.7371 - regression_loss: 1.4497 - classification_loss: 0.2874
 590/1500 [==========>...................] - ETA: 11:51 - loss: 1.7370 - regression_loss: 1.4498 - classification_loss: 0.2873
 591/1500 [==========>...................] - ETA: 11:51 - loss: 1.7378 - regression_loss: 1.4507 - classification_loss: 0.2871
 592/1500 [==========>...................] - ETA: 11:50 - loss: 1.7377 - regression_loss: 1.4509 - classification_loss: 0.2869
 593/1500 [==========>...................] - ETA: 11:48 - loss: 1.7365 - regression_loss: 1.4499 - classification_loss: 0.2866
 594/1500 [==========>...................] - ETA: 11:47 - loss: 1.7359 - regression_loss: 1.4495 - classification_loss: 0.2864
 595/1500 [==========>...................] - ETA: 11:46 - loss: 1.7370 - regression_loss: 1.4503 - classification_loss: 0.2867
 596/1500 [==========>...................] - ETA: 11:44 - loss: 1.7357 - regression_loss: 1.4492 - classification_loss: 0.2865
 597/1500 [==========>...................] - ETA: 11:43 - loss: 1.7351 - regression_loss: 1.4488 - classification_loss: 0.2863
 598/1500 [==========>...................] - ETA: 11:42 - loss: 1.7357 - regression_loss: 1.4493 - classification_loss: 0.2864
 599/1500 [==========>...................] - ETA: 11:41 - loss: 1.7363 - regression_loss: 1.4498 - classification_loss: 0.2865
 600/1500 [===========>..................] - ETA: 11:40 - loss: 1.7397 - regression_loss: 1.4518 - classification_loss: 0.2879
 601/1500 [===========>..................] - ETA: 11:38 - loss: 1.7409 - regression_loss: 1.4529 - classification_loss: 0.2880
 602/1500 [===========>..................] - ETA: 11:37 - loss: 1.7412 - regression_loss: 1.4532 - classification_loss: 0.2880
 603/1500 [===========>..................] - ETA: 11:36 - loss: 1.7397 - regression_loss: 1.4519 - classification_loss: 0.2878
 604/1500 [===========>..................] - ETA: 11:34 - loss: 1.7404 - regression_loss: 1.4525 - classification_loss: 0.2879
 605/1500 [===========>..................] - ETA: 11:34 - loss: 1.7404 - regression_loss: 1.4526 - classification_loss: 0.2878
 606/1500 [===========>..................] - ETA: 11:32 - loss: 1.7394 - regression_loss: 1.4519 - classification_loss: 0.2875
 607/1500 [===========>..................] - ETA: 11:31 - loss: 1.7389 - regression_loss: 1.4516 - classification_loss: 0.2873
 608/1500 [===========>..................] - ETA: 11:30 - loss: 1.7387 - regression_loss: 1.4515 - classification_loss: 0.2872
 609/1500 [===========>..................] - ETA: 11:31 - loss: 1.7386 - regression_loss: 1.4514 - classification_loss: 0.2872
 610/1500 [===========>..................] - ETA: 11:29 - loss: 1.7377 - regression_loss: 1.4503 - classification_loss: 0.2874
 611/1500 [===========>..................] - ETA: 11:29 - loss: 1.7375 - regression_loss: 1.4500 - classification_loss: 0.2874
 612/1500 [===========>..................] - ETA: 11:27 - loss: 1.7379 - regression_loss: 1.4505 - classification_loss: 0.2874
 613/1500 [===========>..................] - ETA: 11:26 - loss: 1.7371 - regression_loss: 1.4498 - classification_loss: 0.2873
 614/1500 [===========>..................] - ETA: 11:25 - loss: 1.7378 - regression_loss: 1.4496 - classification_loss: 0.2882
 615/1500 [===========>..................] - ETA: 11:24 - loss: 1.7366 - regression_loss: 1.4487 - classification_loss: 0.2880
 616/1500 [===========>..................] - ETA: 11:23 - loss: 1.7365 - regression_loss: 1.4485 - classification_loss: 0.2880
 617/1500 [===========>..................] - ETA: 11:21 - loss: 1.7374 - regression_loss: 1.4492 - classification_loss: 0.2882
 618/1500 [===========>..................] - ETA: 11:20 - loss: 1.7363 - regression_loss: 1.4484 - classification_loss: 0.2879
 619/1500 [===========>..................] - ETA: 11:19 - loss: 1.7371 - regression_loss: 1.4492 - classification_loss: 0.2878
 620/1500 [===========>..................] - ETA: 11:18 - loss: 1.7376 - regression_loss: 1.4499 - classification_loss: 0.2877
 621/1500 [===========>..................] - ETA: 11:17 - loss: 1.7373 - regression_loss: 1.4498 - classification_loss: 0.2875
 622/1500 [===========>..................] - ETA: 11:18 - loss: 1.7366 - regression_loss: 1.4492 - classification_loss: 0.2874
 623/1500 [===========>..................] - ETA: 11:17 - loss: 1.7366 - regression_loss: 1.4493 - classification_loss: 0.2873
 624/1500 [===========>..................] - ETA: 11:15 - loss: 1.7364 - regression_loss: 1.4491 - classification_loss: 0.2873
 625/1500 [===========>..................] - ETA: 11:14 - loss: 1.7357 - regression_loss: 1.4485 - classification_loss: 0.2872
 626/1500 [===========>..................] - ETA: 11:13 - loss: 1.7375 - regression_loss: 1.4499 - classification_loss: 0.2877
 627/1500 [===========>..................] - ETA: 11:12 - loss: 1.7366 - regression_loss: 1.4491 - classification_loss: 0.2874
 628/1500 [===========>..................] - ETA: 11:12 - loss: 1.7356 - regression_loss: 1.4484 - classification_loss: 0.2872
 629/1500 [===========>..................] - ETA: 11:11 - loss: 1.7351 - regression_loss: 1.4480 - classification_loss: 0.2871
 630/1500 [===========>..................] - ETA: 11:10 - loss: 1.7363 - regression_loss: 1.4490 - classification_loss: 0.2872
 631/1500 [===========>..................] - ETA: 11:09 - loss: 1.7361 - regression_loss: 1.4490 - classification_loss: 0.2871
 632/1500 [===========>..................] - ETA: 11:08 - loss: 1.7354 - regression_loss: 1.4484 - classification_loss: 0.2869
 633/1500 [===========>..................] - ETA: 11:07 - loss: 1.7359 - regression_loss: 1.4490 - classification_loss: 0.2869
 634/1500 [===========>..................] - ETA: 11:08 - loss: 1.7349 - regression_loss: 1.4482 - classification_loss: 0.2867
 635/1500 [===========>..................] - ETA: 11:09 - loss: 1.7354 - regression_loss: 1.4487 - classification_loss: 0.2867
 636/1500 [===========>..................] - ETA: 11:07 - loss: 1.7353 - regression_loss: 1.4486 - classification_loss: 0.2866
 637/1500 [===========>..................] - ETA: 11:06 - loss: 1.7339 - regression_loss: 1.4475 - classification_loss: 0.2864
 638/1500 [===========>..................] - ETA: 11:05 - loss: 1.7336 - regression_loss: 1.4474 - classification_loss: 0.2862
 639/1500 [===========>..................] - ETA: 11:04 - loss: 1.7336 - regression_loss: 1.4476 - classification_loss: 0.2860
 640/1500 [===========>..................] - ETA: 11:03 - loss: 1.7351 - regression_loss: 1.4487 - classification_loss: 0.2864
 641/1500 [===========>..................] - ETA: 11:03 - loss: 1.7349 - regression_loss: 1.4481 - classification_loss: 0.2868
 642/1500 [===========>..................] - ETA: 11:01 - loss: 1.7340 - regression_loss: 1.4474 - classification_loss: 0.2866
 643/1500 [===========>..................] - ETA: 11:02 - loss: 1.7337 - regression_loss: 1.4472 - classification_loss: 0.2865
 644/1500 [===========>..................] - ETA: 11:01 - loss: 1.7325 - regression_loss: 1.4464 - classification_loss: 0.2862
 645/1500 [===========>..................] - ETA: 11:00 - loss: 1.7328 - regression_loss: 1.4466 - classification_loss: 0.2862
 646/1500 [===========>..................] - ETA: 10:59 - loss: 1.7347 - regression_loss: 1.4483 - classification_loss: 0.2864
 647/1500 [===========>..................] - ETA: 10:58 - loss: 1.7357 - regression_loss: 1.4494 - classification_loss: 0.2863
 648/1500 [===========>..................] - ETA: 10:57 - loss: 1.7370 - regression_loss: 1.4504 - classification_loss: 0.2865
 649/1500 [===========>..................] - ETA: 10:56 - loss: 1.7362 - regression_loss: 1.4499 - classification_loss: 0.2863
 650/1500 [============>.................] - ETA: 10:55 - loss: 1.7359 - regression_loss: 1.4498 - classification_loss: 0.2862
 651/1500 [============>.................] - ETA: 10:54 - loss: 1.7351 - regression_loss: 1.4491 - classification_loss: 0.2859
 652/1500 [============>.................] - ETA: 10:54 - loss: 1.7353 - regression_loss: 1.4493 - classification_loss: 0.2860
 653/1500 [============>.................] - ETA: 10:52 - loss: 1.7362 - regression_loss: 1.4500 - classification_loss: 0.2862
 654/1500 [============>.................] - ETA: 10:52 - loss: 1.7364 - regression_loss: 1.4500 - classification_loss: 0.2864
 655/1500 [============>.................] - ETA: 10:51 - loss: 1.7363 - regression_loss: 1.4496 - classification_loss: 0.2867
 656/1500 [============>.................] - ETA: 10:50 - loss: 1.7352 - regression_loss: 1.4487 - classification_loss: 0.2865
 657/1500 [============>.................] - ETA: 10:49 - loss: 1.7371 - regression_loss: 1.4506 - classification_loss: 0.2865
 658/1500 [============>.................] - ETA: 10:48 - loss: 1.7376 - regression_loss: 1.4511 - classification_loss: 0.2865
 659/1500 [============>.................] - ETA: 10:47 - loss: 1.7395 - regression_loss: 1.4530 - classification_loss: 0.2865
 660/1500 [============>.................] - ETA: 10:46 - loss: 1.7388 - regression_loss: 1.4525 - classification_loss: 0.2863
 661/1500 [============>.................] - ETA: 10:45 - loss: 1.7385 - regression_loss: 1.4522 - classification_loss: 0.2863
 662/1500 [============>.................] - ETA: 10:44 - loss: 1.7379 - regression_loss: 1.4518 - classification_loss: 0.2861
 663/1500 [============>.................] - ETA: 10:43 - loss: 1.7378 - regression_loss: 1.4519 - classification_loss: 0.2860
 664/1500 [============>.................] - ETA: 10:42 - loss: 1.7395 - regression_loss: 1.4530 - classification_loss: 0.2864
 665/1500 [============>.................] - ETA: 10:44 - loss: 1.7390 - regression_loss: 1.4527 - classification_loss: 0.2863
 666/1500 [============>.................] - ETA: 10:43 - loss: 1.7408 - regression_loss: 1.4540 - classification_loss: 0.2868
 667/1500 [============>.................] - ETA: 10:42 - loss: 1.7397 - regression_loss: 1.4531 - classification_loss: 0.2866
 668/1500 [============>.................] - ETA: 10:42 - loss: 1.7380 - regression_loss: 1.4517 - classification_loss: 0.2863
 669/1500 [============>.................] - ETA: 10:41 - loss: 1.7386 - regression_loss: 1.4520 - classification_loss: 0.2865
 670/1500 [============>.................] - ETA: 10:41 - loss: 1.7375 - regression_loss: 1.4512 - classification_loss: 0.2863
 671/1500 [============>.................] - ETA: 10:41 - loss: 1.7379 - regression_loss: 1.4516 - classification_loss: 0.2863
 672/1500 [============>.................] - ETA: 10:41 - loss: 1.7361 - regression_loss: 1.4501 - classification_loss: 0.2860
 673/1500 [============>.................] - ETA: 10:41 - loss: 1.7360 - regression_loss: 1.4500 - classification_loss: 0.2860
 674/1500 [============>.................] - ETA: 10:40 - loss: 1.7363 - regression_loss: 1.4505 - classification_loss: 0.2858
 675/1500 [============>.................] - ETA: 10:40 - loss: 1.7380 - regression_loss: 1.4518 - classification_loss: 0.2861
 676/1500 [============>.................] - ETA: 10:40 - loss: 1.7389 - regression_loss: 1.4525 - classification_loss: 0.2864
 677/1500 [============>.................] - ETA: 10:39 - loss: 1.7375 - regression_loss: 1.4514 - classification_loss: 0.2861
 678/1500 [============>.................] - ETA: 10:39 - loss: 1.7369 - regression_loss: 1.4511 - classification_loss: 0.2858
 679/1500 [============>.................] - ETA: 10:38 - loss: 1.7374 - regression_loss: 1.4516 - classification_loss: 0.2857
 680/1500 [============>.................] - ETA: 10:37 - loss: 1.7382 - regression_loss: 1.4516 - classification_loss: 0.2866
 681/1500 [============>.................] - ETA: 10:35 - loss: 1.7371 - regression_loss: 1.4505 - classification_loss: 0.2866
 682/1500 [============>.................] - ETA: 10:34 - loss: 1.7366 - regression_loss: 1.4501 - classification_loss: 0.2865
 683/1500 [============>.................] - ETA: 10:33 - loss: 1.7363 - regression_loss: 1.4499 - classification_loss: 0.2864
 684/1500 [============>.................] - ETA: 10:32 - loss: 1.7375 - regression_loss: 1.4507 - classification_loss: 0.2868
 685/1500 [============>.................] - ETA: 10:31 - loss: 1.7388 - regression_loss: 1.4517 - classification_loss: 0.2871
 686/1500 [============>.................] - ETA: 10:31 - loss: 1.7383 - regression_loss: 1.4512 - classification_loss: 0.2870
 687/1500 [============>.................] - ETA: 10:30 - loss: 1.7368 - regression_loss: 1.4500 - classification_loss: 0.2868
 688/1500 [============>.................] - ETA: 10:29 - loss: 1.7375 - regression_loss: 1.4506 - classification_loss: 0.2869
 689/1500 [============>.................] - ETA: 10:29 - loss: 1.7362 - regression_loss: 1.4494 - classification_loss: 0.2868
 690/1500 [============>.................] - ETA: 10:27 - loss: 1.7360 - regression_loss: 1.4492 - classification_loss: 0.2867
 691/1500 [============>.................] - ETA: 10:27 - loss: 1.7357 - regression_loss: 1.4490 - classification_loss: 0.2868
 692/1500 [============>.................] - ETA: 10:26 - loss: 1.7360 - regression_loss: 1.4491 - classification_loss: 0.2869
 693/1500 [============>.................] - ETA: 10:25 - loss: 1.7351 - regression_loss: 1.4485 - classification_loss: 0.2867
 694/1500 [============>.................] - ETA: 10:24 - loss: 1.7354 - regression_loss: 1.4487 - classification_loss: 0.2867
 695/1500 [============>.................] - ETA: 10:23 - loss: 1.7357 - regression_loss: 1.4489 - classification_loss: 0.2868
 696/1500 [============>.................] - ETA: 10:23 - loss: 1.7358 - regression_loss: 1.4490 - classification_loss: 0.2869
 697/1500 [============>.................] - ETA: 10:23 - loss: 1.7353 - regression_loss: 1.4485 - classification_loss: 0.2868
 698/1500 [============>.................] - ETA: 10:23 - loss: 1.7347 - regression_loss: 1.4474 - classification_loss: 0.2873
 699/1500 [============>.................] - ETA: 10:22 - loss: 1.7344 - regression_loss: 1.4472 - classification_loss: 0.2872
 700/1500 [=============>................] - ETA: 10:21 - loss: 1.7351 - regression_loss: 1.4478 - classification_loss: 0.2872
 701/1500 [=============>................] - ETA: 10:21 - loss: 1.7361 - regression_loss: 1.4488 - classification_loss: 0.2874
 702/1500 [=============>................] - ETA: 10:21 - loss: 1.7375 - regression_loss: 1.4499 - classification_loss: 0.2876
 703/1500 [=============>................] - ETA: 10:21 - loss: 1.7394 - regression_loss: 1.4513 - classification_loss: 0.2881
 704/1500 [=============>................] - ETA: 10:20 - loss: 1.7401 - regression_loss: 1.4519 - classification_loss: 0.2882
 705/1500 [=============>................] - ETA: 10:19 - loss: 1.7407 - regression_loss: 1.4526 - classification_loss: 0.2881
 706/1500 [=============>................] - ETA: 10:19 - loss: 1.7419 - regression_loss: 1.4536 - classification_loss: 0.2883
 707/1500 [=============>................] - ETA: 10:18 - loss: 1.7414 - regression_loss: 1.4531 - classification_loss: 0.2883
 708/1500 [=============>................] - ETA: 10:17 - loss: 1.7414 - regression_loss: 1.4530 - classification_loss: 0.2884
 709/1500 [=============>................] - ETA: 10:16 - loss: 1.7405 - regression_loss: 1.4522 - classification_loss: 0.2883
 710/1500 [=============>................] - ETA: 10:15 - loss: 1.7392 - regression_loss: 1.4512 - classification_loss: 0.2880
 711/1500 [=============>................] - ETA: 10:15 - loss: 1.7401 - regression_loss: 1.4519 - classification_loss: 0.2882
 712/1500 [=============>................] - ETA: 10:15 - loss: 1.7400 - regression_loss: 1.4518 - classification_loss: 0.2882
 713/1500 [=============>................] - ETA: 10:15 - loss: 1.7392 - regression_loss: 1.4513 - classification_loss: 0.2879
 714/1500 [=============>................] - ETA: 10:15 - loss: 1.7398 - regression_loss: 1.4518 - classification_loss: 0.2880
 715/1500 [=============>................] - ETA: 10:14 - loss: 1.7392 - regression_loss: 1.4512 - classification_loss: 0.2880
 716/1500 [=============>................] - ETA: 10:13 - loss: 1.7403 - regression_loss: 1.4519 - classification_loss: 0.2884
 717/1500 [=============>................] - ETA: 10:12 - loss: 1.7403 - regression_loss: 1.4520 - classification_loss: 0.2884
 718/1500 [=============>................] - ETA: 10:11 - loss: 1.7397 - regression_loss: 1.4515 - classification_loss: 0.2882
 719/1500 [=============>................] - ETA: 10:10 - loss: 1.7395 - regression_loss: 1.4514 - classification_loss: 0.2880
 720/1500 [=============>................] - ETA: 10:09 - loss: 1.7397 - regression_loss: 1.4516 - classification_loss: 0.2880
 721/1500 [=============>................] - ETA: 10:08 - loss: 1.7408 - regression_loss: 1.4528 - classification_loss: 0.2880
 722/1500 [=============>................] - ETA: 10:08 - loss: 1.7419 - regression_loss: 1.4537 - classification_loss: 0.2882
 723/1500 [=============>................] - ETA: 10:07 - loss: 1.7430 - regression_loss: 1.4547 - classification_loss: 0.2883
 724/1500 [=============>................] - ETA: 10:07 - loss: 1.7429 - regression_loss: 1.4546 - classification_loss: 0.2883
 725/1500 [=============>................] - ETA: 10:06 - loss: 1.7423 - regression_loss: 1.4542 - classification_loss: 0.2882
 726/1500 [=============>................] - ETA: 10:05 - loss: 1.7424 - regression_loss: 1.4544 - classification_loss: 0.2881
 727/1500 [=============>................] - ETA: 10:04 - loss: 1.7415 - regression_loss: 1.4536 - classification_loss: 0.2878
 728/1500 [=============>................] - ETA: 10:03 - loss: 1.7413 - regression_loss: 1.4533 - classification_loss: 0.2881
 729/1500 [=============>................] - ETA: 10:01 - loss: 1.7408 - regression_loss: 1.4528 - classification_loss: 0.2880
 730/1500 [=============>................] - ETA: 10:00 - loss: 1.7406 - regression_loss: 1.4528 - classification_loss: 0.2878
 731/1500 [=============>................] - ETA: 9:59 - loss: 1.7400 - regression_loss: 1.4524 - classification_loss: 0.2877 
 732/1500 [=============>................] - ETA: 9:58 - loss: 1.7420 - regression_loss: 1.4539 - classification_loss: 0.2880
 733/1500 [=============>................] - ETA: 9:57 - loss: 1.7413 - regression_loss: 1.4534 - classification_loss: 0.2879
 734/1500 [=============>................] - ETA: 9:56 - loss: 1.7429 - regression_loss: 1.4546 - classification_loss: 0.2883
 735/1500 [=============>................] - ETA: 9:55 - loss: 1.7425 - regression_loss: 1.4542 - classification_loss: 0.2882
 736/1500 [=============>................] - ETA: 9:54 - loss: 1.7418 - regression_loss: 1.4538 - classification_loss: 0.2880
 737/1500 [=============>................] - ETA: 9:53 - loss: 1.7413 - regression_loss: 1.4534 - classification_loss: 0.2879
 738/1500 [=============>................] - ETA: 9:53 - loss: 1.7420 - regression_loss: 1.4543 - classification_loss: 0.2877
 739/1500 [=============>................] - ETA: 9:52 - loss: 1.7416 - regression_loss: 1.4540 - classification_loss: 0.2876
 740/1500 [=============>................] - ETA: 9:52 - loss: 1.7422 - regression_loss: 1.4545 - classification_loss: 0.2876
 741/1500 [=============>................] - ETA: 9:52 - loss: 1.7415 - regression_loss: 1.4540 - classification_loss: 0.2875
 742/1500 [=============>................] - ETA: 9:51 - loss: 1.7413 - regression_loss: 1.4539 - classification_loss: 0.2875
 743/1500 [=============>................] - ETA: 9:49 - loss: 1.7417 - regression_loss: 1.4542 - classification_loss: 0.2875
 744/1500 [=============>................] - ETA: 9:49 - loss: 1.7403 - regression_loss: 1.4530 - classification_loss: 0.2873
 745/1500 [=============>................] - ETA: 9:48 - loss: 1.7420 - regression_loss: 1.4540 - classification_loss: 0.2881
 746/1500 [=============>................] - ETA: 9:47 - loss: 1.7413 - regression_loss: 1.4534 - classification_loss: 0.2879
 747/1500 [=============>................] - ETA: 9:46 - loss: 1.7403 - regression_loss: 1.4526 - classification_loss: 0.2877
 748/1500 [=============>................] - ETA: 9:45 - loss: 1.7397 - regression_loss: 1.4522 - classification_loss: 0.2874
 749/1500 [=============>................] - ETA: 9:44 - loss: 1.7412 - regression_loss: 1.4537 - classification_loss: 0.2875
 750/1500 [==============>...............] - ETA: 9:43 - loss: 1.7417 - regression_loss: 1.4542 - classification_loss: 0.2875
 751/1500 [==============>...............] - ETA: 9:42 - loss: 1.7427 - regression_loss: 1.4552 - classification_loss: 0.2875
 752/1500 [==============>...............] - ETA: 9:41 - loss: 1.7419 - regression_loss: 1.4544 - classification_loss: 0.2875
 753/1500 [==============>...............] - ETA: 9:40 - loss: 1.7423 - regression_loss: 1.4546 - classification_loss: 0.2877
 754/1500 [==============>...............] - ETA: 9:39 - loss: 1.7415 - regression_loss: 1.4540 - classification_loss: 0.2875
 755/1500 [==============>...............] - ETA: 9:38 - loss: 1.7418 - regression_loss: 1.4544 - classification_loss: 0.2875
 756/1500 [==============>...............] - ETA: 9:38 - loss: 1.7417 - regression_loss: 1.4541 - classification_loss: 0.2876
 757/1500 [==============>...............] - ETA: 9:37 - loss: 1.7424 - regression_loss: 1.4545 - classification_loss: 0.2878
 758/1500 [==============>...............] - ETA: 9:36 - loss: 1.7421 - regression_loss: 1.4545 - classification_loss: 0.2876
 759/1500 [==============>...............] - ETA: 9:35 - loss: 1.7417 - regression_loss: 1.4543 - classification_loss: 0.2874
 760/1500 [==============>...............] - ETA: 9:34 - loss: 1.7405 - regression_loss: 1.4533 - classification_loss: 0.2872
 761/1500 [==============>...............] - ETA: 9:33 - loss: 1.7406 - regression_loss: 1.4534 - classification_loss: 0.2872
 762/1500 [==============>...............] - ETA: 9:32 - loss: 1.7402 - regression_loss: 1.4530 - classification_loss: 0.2872
 763/1500 [==============>...............] - ETA: 9:33 - loss: 1.7413 - regression_loss: 1.4538 - classification_loss: 0.2875
 764/1500 [==============>...............] - ETA: 9:32 - loss: 1.7411 - regression_loss: 1.4536 - classification_loss: 0.2875
 765/1500 [==============>...............] - ETA: 9:31 - loss: 1.7399 - regression_loss: 1.4526 - classification_loss: 0.2873
 766/1500 [==============>...............] - ETA: 9:30 - loss: 1.7406 - regression_loss: 1.4534 - classification_loss: 0.2873
 767/1500 [==============>...............] - ETA: 9:30 - loss: 1.7412 - regression_loss: 1.4539 - classification_loss: 0.2873
 768/1500 [==============>...............] - ETA: 9:29 - loss: 1.7416 - regression_loss: 1.4542 - classification_loss: 0.2874
 769/1500 [==============>...............] - ETA: 9:28 - loss: 1.7414 - regression_loss: 1.4537 - classification_loss: 0.2877
 770/1500 [==============>...............] - ETA: 9:28 - loss: 1.7425 - regression_loss: 1.4547 - classification_loss: 0.2878
 771/1500 [==============>...............] - ETA: 9:27 - loss: 1.7430 - regression_loss: 1.4552 - classification_loss: 0.2878
 772/1500 [==============>...............] - ETA: 9:26 - loss: 1.7433 - regression_loss: 1.4555 - classification_loss: 0.2878
 773/1500 [==============>...............] - ETA: 9:26 - loss: 1.7437 - regression_loss: 1.4549 - classification_loss: 0.2887
 774/1500 [==============>...............] - ETA: 9:25 - loss: 1.7434 - regression_loss: 1.4546 - classification_loss: 0.2888
 775/1500 [==============>...............] - ETA: 9:24 - loss: 1.7432 - regression_loss: 1.4545 - classification_loss: 0.2887
 776/1500 [==============>...............] - ETA: 9:25 - loss: 1.7440 - regression_loss: 1.4552 - classification_loss: 0.2887
 777/1500 [==============>...............] - ETA: 9:25 - loss: 1.7450 - regression_loss: 1.4561 - classification_loss: 0.2889
 778/1500 [==============>...............] - ETA: 9:24 - loss: 1.7456 - regression_loss: 1.4566 - classification_loss: 0.2891
 779/1500 [==============>...............] - ETA: 9:24 - loss: 1.7472 - regression_loss: 1.4579 - classification_loss: 0.2894
 780/1500 [==============>...............] - ETA: 9:23 - loss: 1.7464 - regression_loss: 1.4572 - classification_loss: 0.2892
 781/1500 [==============>...............] - ETA: 9:22 - loss: 1.7467 - regression_loss: 1.4576 - classification_loss: 0.2891
 782/1500 [==============>...............] - ETA: 9:20 - loss: 1.7464 - regression_loss: 1.4572 - classification_loss: 0.2891
 783/1500 [==============>...............] - ETA: 9:19 - loss: 1.7456 - regression_loss: 1.4566 - classification_loss: 0.2889
 784/1500 [==============>...............] - ETA: 9:18 - loss: 1.7453 - regression_loss: 1.4564 - classification_loss: 0.2890
 785/1500 [==============>...............] - ETA: 9:17 - loss: 1.7458 - regression_loss: 1.4568 - classification_loss: 0.2891
 786/1500 [==============>...............] - ETA: 9:16 - loss: 1.7454 - regression_loss: 1.4566 - classification_loss: 0.2889
 787/1500 [==============>...............] - ETA: 9:15 - loss: 1.7450 - regression_loss: 1.4562 - classification_loss: 0.2888
 788/1500 [==============>...............] - ETA: 9:16 - loss: 1.7442 - regression_loss: 1.4555 - classification_loss: 0.2886
 789/1500 [==============>...............] - ETA: 9:15 - loss: 1.7427 - regression_loss: 1.4544 - classification_loss: 0.2884
 790/1500 [==============>...............] - ETA: 9:14 - loss: 1.7430 - regression_loss: 1.4546 - classification_loss: 0.2884
 791/1500 [==============>...............] - ETA: 9:13 - loss: 1.7434 - regression_loss: 1.4551 - classification_loss: 0.2883
 792/1500 [==============>...............] - ETA: 9:12 - loss: 1.7429 - regression_loss: 1.4547 - classification_loss: 0.2882
 793/1500 [==============>...............] - ETA: 9:11 - loss: 1.7418 - regression_loss: 1.4538 - classification_loss: 0.2880
 794/1500 [==============>...............] - ETA: 9:11 - loss: 1.7427 - regression_loss: 1.4543 - classification_loss: 0.2884
 795/1500 [==============>...............] - ETA: 9:10 - loss: 1.7421 - regression_loss: 1.4539 - classification_loss: 0.2883
 796/1500 [==============>...............] - ETA: 9:09 - loss: 1.7407 - regression_loss: 1.4527 - classification_loss: 0.2880
 797/1500 [==============>...............] - ETA: 9:08 - loss: 1.7400 - regression_loss: 1.4520 - classification_loss: 0.2880
 798/1500 [==============>...............] - ETA: 9:07 - loss: 1.7387 - regression_loss: 1.4510 - classification_loss: 0.2878
 799/1500 [==============>...............] - ETA: 9:06 - loss: 1.7381 - regression_loss: 1.4505 - classification_loss: 0.2876
 800/1500 [===============>..............] - ETA: 9:05 - loss: 1.7378 - regression_loss: 1.4504 - classification_loss: 0.2874
 801/1500 [===============>..............] - ETA: 9:06 - loss: 1.7378 - regression_loss: 1.4503 - classification_loss: 0.2875
 802/1500 [===============>..............] - ETA: 9:05 - loss: 1.7384 - regression_loss: 1.4506 - classification_loss: 0.2878
 803/1500 [===============>..............] - ETA: 9:05 - loss: 1.7393 - regression_loss: 1.4513 - classification_loss: 0.2880
 804/1500 [===============>..............] - ETA: 9:04 - loss: 1.7404 - regression_loss: 1.4521 - classification_loss: 0.2883
 805/1500 [===============>..............] - ETA: 9:03 - loss: 1.7411 - regression_loss: 1.4525 - classification_loss: 0.2886
 806/1500 [===============>..............] - ETA: 9:02 - loss: 1.7408 - regression_loss: 1.4523 - classification_loss: 0.2885
 807/1500 [===============>..............] - ETA: 9:02 - loss: 1.7401 - regression_loss: 1.4518 - classification_loss: 0.2883
 808/1500 [===============>..............] - ETA: 9:01 - loss: 1.7396 - regression_loss: 1.4513 - classification_loss: 0.2883
 809/1500 [===============>..............] - ETA: 9:00 - loss: 1.7395 - regression_loss: 1.4513 - classification_loss: 0.2882
 810/1500 [===============>..............] - ETA: 9:00 - loss: 1.7408 - regression_loss: 1.4523 - classification_loss: 0.2885
 811/1500 [===============>..............] - ETA: 9:00 - loss: 1.7401 - regression_loss: 1.4516 - classification_loss: 0.2884
 812/1500 [===============>..............] - ETA: 8:59 - loss: 1.7403 - regression_loss: 1.4520 - classification_loss: 0.2883
 813/1500 [===============>..............] - ETA: 8:58 - loss: 1.7412 - regression_loss: 1.4528 - classification_loss: 0.2884
 814/1500 [===============>..............] - ETA: 8:58 - loss: 1.7400 - regression_loss: 1.4518 - classification_loss: 0.2882
 815/1500 [===============>..............] - ETA: 8:58 - loss: 1.7408 - regression_loss: 1.4523 - classification_loss: 0.2885
 816/1500 [===============>..............] - ETA: 8:57 - loss: 1.7409 - regression_loss: 1.4520 - classification_loss: 0.2889
 817/1500 [===============>..............] - ETA: 8:56 - loss: 1.7404 - regression_loss: 1.4517 - classification_loss: 0.2887
 818/1500 [===============>..............] - ETA: 8:55 - loss: 1.7408 - regression_loss: 1.4514 - classification_loss: 0.2893
 819/1500 [===============>..............] - ETA: 8:54 - loss: 1.7397 - regression_loss: 1.4506 - classification_loss: 0.2892
 820/1500 [===============>..............] - ETA: 8:53 - loss: 1.7386 - regression_loss: 1.4496 - classification_loss: 0.2890
 821/1500 [===============>..............] - ETA: 8:53 - loss: 1.7380 - regression_loss: 1.4492 - classification_loss: 0.2888
 822/1500 [===============>..............] - ETA: 8:52 - loss: 1.7384 - regression_loss: 1.4496 - classification_loss: 0.2888
 823/1500 [===============>..............] - ETA: 8:51 - loss: 1.7397 - regression_loss: 1.4498 - classification_loss: 0.2899
 824/1500 [===============>..............] - ETA: 8:51 - loss: 1.7396 - regression_loss: 1.4497 - classification_loss: 0.2899
 825/1500 [===============>..............] - ETA: 8:50 - loss: 1.7393 - regression_loss: 1.4494 - classification_loss: 0.2899
 826/1500 [===============>..............] - ETA: 8:49 - loss: 1.7397 - regression_loss: 1.4498 - classification_loss: 0.2899
 827/1500 [===============>..............] - ETA: 8:48 - loss: 1.7391 - regression_loss: 1.4494 - classification_loss: 0.2898
 828/1500 [===============>..............] - ETA: 8:47 - loss: 1.7395 - regression_loss: 1.4497 - classification_loss: 0.2898
 829/1500 [===============>..............] - ETA: 8:47 - loss: 1.7394 - regression_loss: 1.4495 - classification_loss: 0.2898
 830/1500 [===============>..............] - ETA: 8:45 - loss: 1.7400 - regression_loss: 1.4503 - classification_loss: 0.2897
 831/1500 [===============>..............] - ETA: 8:44 - loss: 1.7397 - regression_loss: 1.4499 - classification_loss: 0.2897
 832/1500 [===============>..............] - ETA: 8:44 - loss: 1.7398 - regression_loss: 1.4499 - classification_loss: 0.2899
 833/1500 [===============>..............] - ETA: 8:43 - loss: 1.7392 - regression_loss: 1.4495 - classification_loss: 0.2897
 834/1500 [===============>..............] - ETA: 8:42 - loss: 1.7380 - regression_loss: 1.4485 - classification_loss: 0.2895
 835/1500 [===============>..............] - ETA: 8:41 - loss: 1.7379 - regression_loss: 1.4485 - classification_loss: 0.2894
 836/1500 [===============>..............] - ETA: 8:41 - loss: 1.7370 - regression_loss: 1.4479 - classification_loss: 0.2891
 837/1500 [===============>..............] - ETA: 8:40 - loss: 1.7384 - regression_loss: 1.4490 - classification_loss: 0.2894
 838/1500 [===============>..............] - ETA: 8:39 - loss: 1.7372 - regression_loss: 1.4480 - classification_loss: 0.2892
 839/1500 [===============>..............] - ETA: 8:38 - loss: 1.7375 - regression_loss: 1.4482 - classification_loss: 0.2893
 840/1500 [===============>..............] - ETA: 8:37 - loss: 1.7365 - regression_loss: 1.4473 - classification_loss: 0.2892
 841/1500 [===============>..............] - ETA: 8:37 - loss: 1.7360 - regression_loss: 1.4468 - classification_loss: 0.2892
 842/1500 [===============>..............] - ETA: 8:37 - loss: 1.7360 - regression_loss: 1.4468 - classification_loss: 0.2891
 843/1500 [===============>..............] - ETA: 8:37 - loss: 1.7362 - regression_loss: 1.4471 - classification_loss: 0.2891
 844/1500 [===============>..............] - ETA: 8:36 - loss: 1.7352 - regression_loss: 1.4462 - classification_loss: 0.2890
 845/1500 [===============>..............] - ETA: 8:35 - loss: 1.7344 - regression_loss: 1.4457 - classification_loss: 0.2888
 846/1500 [===============>..............] - ETA: 8:34 - loss: 1.7338 - regression_loss: 1.4452 - classification_loss: 0.2886
 847/1500 [===============>..............] - ETA: 8:33 - loss: 1.7330 - regression_loss: 1.4445 - classification_loss: 0.2884
 848/1500 [===============>..............] - ETA: 8:32 - loss: 1.7318 - regression_loss: 1.4434 - classification_loss: 0.2883
 849/1500 [===============>..............] - ETA: 8:31 - loss: 1.7323 - regression_loss: 1.4439 - classification_loss: 0.2884
 850/1500 [================>.............] - ETA: 8:30 - loss: 1.7317 - regression_loss: 1.4433 - classification_loss: 0.2884
 851/1500 [================>.............] - ETA: 8:29 - loss: 1.7327 - regression_loss: 1.4443 - classification_loss: 0.2884
 852/1500 [================>.............] - ETA: 8:28 - loss: 1.7322 - regression_loss: 1.4440 - classification_loss: 0.2883
 853/1500 [================>.............] - ETA: 8:26 - loss: 1.7313 - regression_loss: 1.4433 - classification_loss: 0.2880
 854/1500 [================>.............] - ETA: 8:25 - loss: 1.7322 - regression_loss: 1.4441 - classification_loss: 0.2882
 855/1500 [================>.............] - ETA: 8:25 - loss: 1.7326 - regression_loss: 1.4446 - classification_loss: 0.2881
 856/1500 [================>.............] - ETA: 8:23 - loss: 1.7322 - regression_loss: 1.4441 - classification_loss: 0.2880
 857/1500 [================>.............] - ETA: 8:22 - loss: 1.7318 - regression_loss: 1.4439 - classification_loss: 0.2879
 858/1500 [================>.............] - ETA: 8:21 - loss: 1.7318 - regression_loss: 1.4440 - classification_loss: 0.2877
 859/1500 [================>.............] - ETA: 8:20 - loss: 1.7309 - regression_loss: 1.4435 - classification_loss: 0.2875
 860/1500 [================>.............] - ETA: 8:19 - loss: 1.7317 - regression_loss: 1.4441 - classification_loss: 0.2876
 861/1500 [================>.............] - ETA: 8:18 - loss: 1.7308 - regression_loss: 1.4434 - classification_loss: 0.2874
 862/1500 [================>.............] - ETA: 8:17 - loss: 1.7298 - regression_loss: 1.4426 - classification_loss: 0.2872
 863/1500 [================>.............] - ETA: 8:17 - loss: 1.7292 - regression_loss: 1.4423 - classification_loss: 0.2870
 864/1500 [================>.............] - ETA: 8:16 - loss: 1.7290 - regression_loss: 1.4421 - classification_loss: 0.2869
 865/1500 [================>.............] - ETA: 8:15 - loss: 1.7287 - regression_loss: 1.4417 - classification_loss: 0.2870
 866/1500 [================>.............] - ETA: 8:14 - loss: 1.7287 - regression_loss: 1.4419 - classification_loss: 0.2868
 867/1500 [================>.............] - ETA: 8:14 - loss: 1.7275 - regression_loss: 1.4408 - classification_loss: 0.2866
 868/1500 [================>.............] - ETA: 8:13 - loss: 1.7271 - regression_loss: 1.4406 - classification_loss: 0.2865
 869/1500 [================>.............] - ETA: 8:13 - loss: 1.7264 - regression_loss: 1.4401 - classification_loss: 0.2863
 870/1500 [================>.............] - ETA: 8:13 - loss: 1.7272 - regression_loss: 1.4406 - classification_loss: 0.2867
 871/1500 [================>.............] - ETA: 8:12 - loss: 1.7268 - regression_loss: 1.4403 - classification_loss: 0.2865
 872/1500 [================>.............] - ETA: 8:11 - loss: 1.7274 - regression_loss: 1.4407 - classification_loss: 0.2867
 873/1500 [================>.............] - ETA: 8:10 - loss: 1.7285 - regression_loss: 1.4414 - classification_loss: 0.2871
 874/1500 [================>.............] - ETA: 8:09 - loss: 1.7285 - regression_loss: 1.4413 - classification_loss: 0.2872
 875/1500 [================>.............] - ETA: 8:08 - loss: 1.7292 - regression_loss: 1.4420 - classification_loss: 0.2872
 876/1500 [================>.............] - ETA: 8:08 - loss: 1.7290 - regression_loss: 1.4419 - classification_loss: 0.2871
 877/1500 [================>.............] - ETA: 8:07 - loss: 1.7291 - regression_loss: 1.4420 - classification_loss: 0.2871
 878/1500 [================>.............] - ETA: 8:06 - loss: 1.7298 - regression_loss: 1.4425 - classification_loss: 0.2873
 879/1500 [================>.............] - ETA: 8:05 - loss: 1.7308 - regression_loss: 1.4432 - classification_loss: 0.2876
 880/1500 [================>.............] - ETA: 8:04 - loss: 1.7309 - regression_loss: 1.4433 - classification_loss: 0.2877
 881/1500 [================>.............] - ETA: 8:03 - loss: 1.7303 - regression_loss: 1.4428 - classification_loss: 0.2875
 882/1500 [================>.............] - ETA: 8:03 - loss: 1.7299 - regression_loss: 1.4425 - classification_loss: 0.2874
 883/1500 [================>.............] - ETA: 8:02 - loss: 1.7289 - regression_loss: 1.4418 - classification_loss: 0.2871
 884/1500 [================>.............] - ETA: 8:02 - loss: 1.7292 - regression_loss: 1.4420 - classification_loss: 0.2872
 885/1500 [================>.............] - ETA: 8:01 - loss: 1.7286 - regression_loss: 1.4417 - classification_loss: 0.2870
 886/1500 [================>.............] - ETA: 8:00 - loss: 1.7276 - regression_loss: 1.4408 - classification_loss: 0.2868
 887/1500 [================>.............] - ETA: 7:59 - loss: 1.7270 - regression_loss: 1.4404 - classification_loss: 0.2866
 888/1500 [================>.............] - ETA: 7:58 - loss: 1.7266 - regression_loss: 1.4402 - classification_loss: 0.2865
 889/1500 [================>.............] - ETA: 7:58 - loss: 1.7260 - regression_loss: 1.4397 - classification_loss: 0.2863
 890/1500 [================>.............] - ETA: 7:57 - loss: 1.7257 - regression_loss: 1.4394 - classification_loss: 0.2864
 891/1500 [================>.............] - ETA: 7:56 - loss: 1.7249 - regression_loss: 1.4387 - classification_loss: 0.2862
 892/1500 [================>.............] - ETA: 7:55 - loss: 1.7237 - regression_loss: 1.4378 - classification_loss: 0.2860
 893/1500 [================>.............] - ETA: 7:55 - loss: 1.7230 - regression_loss: 1.4372 - classification_loss: 0.2858
 894/1500 [================>.............] - ETA: 7:54 - loss: 1.7231 - regression_loss: 1.4373 - classification_loss: 0.2858
 895/1500 [================>.............] - ETA: 7:53 - loss: 1.7241 - regression_loss: 1.4381 - classification_loss: 0.2860
 896/1500 [================>.............] - ETA: 7:52 - loss: 1.7239 - regression_loss: 1.4380 - classification_loss: 0.2859
 897/1500 [================>.............] - ETA: 7:51 - loss: 1.7239 - regression_loss: 1.4380 - classification_loss: 0.2859
 898/1500 [================>.............] - ETA: 7:50 - loss: 1.7229 - regression_loss: 1.4372 - classification_loss: 0.2857
 899/1500 [================>.............] - ETA: 7:50 - loss: 1.7232 - regression_loss: 1.4375 - classification_loss: 0.2857
 900/1500 [=================>............] - ETA: 7:49 - loss: 1.7232 - regression_loss: 1.4376 - classification_loss: 0.2857
 901/1500 [=================>............] - ETA: 7:48 - loss: 1.7226 - regression_loss: 1.4371 - classification_loss: 0.2855
 902/1500 [=================>............] - ETA: 7:47 - loss: 1.7218 - regression_loss: 1.4365 - classification_loss: 0.2854
 903/1500 [=================>............] - ETA: 7:47 - loss: 1.7204 - regression_loss: 1.4353 - classification_loss: 0.2851
 904/1500 [=================>............] - ETA: 7:46 - loss: 1.7200 - regression_loss: 1.4349 - classification_loss: 0.2850
 905/1500 [=================>............] - ETA: 7:44 - loss: 1.7192 - regression_loss: 1.4343 - classification_loss: 0.2849
 906/1500 [=================>............] - ETA: 7:43 - loss: 1.7189 - regression_loss: 1.4342 - classification_loss: 0.2848
 907/1500 [=================>............] - ETA: 7:42 - loss: 1.7180 - regression_loss: 1.4334 - classification_loss: 0.2846
 908/1500 [=================>............] - ETA: 7:42 - loss: 1.7175 - regression_loss: 1.4331 - classification_loss: 0.2844
 909/1500 [=================>............] - ETA: 7:41 - loss: 1.7171 - regression_loss: 1.4328 - classification_loss: 0.2843
 910/1500 [=================>............] - ETA: 7:40 - loss: 1.7166 - regression_loss: 1.4324 - classification_loss: 0.2842
 911/1500 [=================>............] - ETA: 7:39 - loss: 1.7164 - regression_loss: 1.4323 - classification_loss: 0.2840
 912/1500 [=================>............] - ETA: 7:38 - loss: 1.7162 - regression_loss: 1.4323 - classification_loss: 0.2839
 913/1500 [=================>............] - ETA: 7:37 - loss: 1.7151 - regression_loss: 1.4314 - classification_loss: 0.2837
 914/1500 [=================>............] - ETA: 7:36 - loss: 1.7140 - regression_loss: 1.4304 - classification_loss: 0.2836
 915/1500 [=================>............] - ETA: 7:35 - loss: 1.7138 - regression_loss: 1.4303 - classification_loss: 0.2835
 916/1500 [=================>............] - ETA: 7:34 - loss: 1.7126 - regression_loss: 1.4294 - classification_loss: 0.2833
 917/1500 [=================>............] - ETA: 7:34 - loss: 1.7120 - regression_loss: 1.4288 - classification_loss: 0.2831
 918/1500 [=================>............] - ETA: 7:33 - loss: 1.7115 - regression_loss: 1.4284 - classification_loss: 0.2830
 919/1500 [=================>............] - ETA: 7:33 - loss: 1.7118 - regression_loss: 1.4287 - classification_loss: 0.2830
 920/1500 [=================>............] - ETA: 7:32 - loss: 1.7116 - regression_loss: 1.4287 - classification_loss: 0.2829
 921/1500 [=================>............] - ETA: 7:31 - loss: 1.7111 - regression_loss: 1.4283 - classification_loss: 0.2828
 922/1500 [=================>............] - ETA: 7:30 - loss: 1.7104 - regression_loss: 1.4278 - classification_loss: 0.2827
 923/1500 [=================>............] - ETA: 7:30 - loss: 1.7109 - regression_loss: 1.4282 - classification_loss: 0.2826
 924/1500 [=================>............] - ETA: 7:29 - loss: 1.7109 - regression_loss: 1.4283 - classification_loss: 0.2827
 925/1500 [=================>............] - ETA: 7:28 - loss: 1.7101 - regression_loss: 1.4276 - classification_loss: 0.2825
 926/1500 [=================>............] - ETA: 7:27 - loss: 1.7095 - regression_loss: 1.4272 - classification_loss: 0.2824
 927/1500 [=================>............] - ETA: 7:26 - loss: 1.7091 - regression_loss: 1.4268 - classification_loss: 0.2823
 928/1500 [=================>............] - ETA: 7:25 - loss: 1.7081 - regression_loss: 1.4261 - classification_loss: 0.2821
 929/1500 [=================>............] - ETA: 7:24 - loss: 1.7081 - regression_loss: 1.4261 - classification_loss: 0.2820
 930/1500 [=================>............] - ETA: 7:23 - loss: 1.7076 - regression_loss: 1.4257 - classification_loss: 0.2819
 931/1500 [=================>............] - ETA: 7:22 - loss: 1.7068 - regression_loss: 1.4252 - classification_loss: 0.2817
 932/1500 [=================>............] - ETA: 7:22 - loss: 1.7066 - regression_loss: 1.4251 - classification_loss: 0.2816
 933/1500 [=================>............] - ETA: 7:21 - loss: 1.7066 - regression_loss: 1.4251 - classification_loss: 0.2815
 934/1500 [=================>............] - ETA: 7:21 - loss: 1.7072 - regression_loss: 1.4257 - classification_loss: 0.2815
 935/1500 [=================>............] - ETA: 7:20 - loss: 1.7061 - regression_loss: 1.4249 - classification_loss: 0.2812
 936/1500 [=================>............] - ETA: 7:19 - loss: 1.7052 - regression_loss: 1.4242 - classification_loss: 0.2810
 937/1500 [=================>............] - ETA: 7:19 - loss: 1.7039 - regression_loss: 1.4230 - classification_loss: 0.2809
 938/1500 [=================>............] - ETA: 7:18 - loss: 1.7036 - regression_loss: 1.4227 - classification_loss: 0.2809
 939/1500 [=================>............] - ETA: 7:17 - loss: 1.7023 - regression_loss: 1.4215 - classification_loss: 0.2807
 940/1500 [=================>............] - ETA: 7:16 - loss: 1.7034 - regression_loss: 1.4224 - classification_loss: 0.2810
 941/1500 [=================>............] - ETA: 7:15 - loss: 1.7027 - regression_loss: 1.4217 - classification_loss: 0.2810
 942/1500 [=================>............] - ETA: 7:14 - loss: 1.7019 - regression_loss: 1.4211 - classification_loss: 0.2808
 943/1500 [=================>............] - ETA: 7:14 - loss: 1.7027 - regression_loss: 1.4217 - classification_loss: 0.2810
 944/1500 [=================>............] - ETA: 7:14 - loss: 1.7035 - regression_loss: 1.4224 - classification_loss: 0.2811
 945/1500 [=================>............] - ETA: 7:13 - loss: 1.7044 - regression_loss: 1.4231 - classification_loss: 0.2813
 946/1500 [=================>............] - ETA: 7:12 - loss: 1.7049 - regression_loss: 1.4235 - classification_loss: 0.2815
 947/1500 [=================>............] - ETA: 7:11 - loss: 1.7050 - regression_loss: 1.4235 - classification_loss: 0.2814
 948/1500 [=================>............] - ETA: 7:10 - loss: 1.7049 - regression_loss: 1.4234 - classification_loss: 0.2815
 949/1500 [=================>............] - ETA: 7:09 - loss: 1.7054 - regression_loss: 1.4239 - classification_loss: 0.2816
 950/1500 [==================>...........] - ETA: 7:08 - loss: 1.7060 - regression_loss: 1.4244 - classification_loss: 0.2816
 951/1500 [==================>...........] - ETA: 7:07 - loss: 1.7052 - regression_loss: 1.4238 - classification_loss: 0.2815
 952/1500 [==================>...........] - ETA: 7:06 - loss: 1.7057 - regression_loss: 1.4241 - classification_loss: 0.2816
 953/1500 [==================>...........] - ETA: 7:06 - loss: 1.7062 - regression_loss: 1.4247 - classification_loss: 0.2815
 954/1500 [==================>...........] - ETA: 7:05 - loss: 1.7055 - regression_loss: 1.4240 - classification_loss: 0.2815
 955/1500 [==================>...........] - ETA: 7:04 - loss: 1.7055 - regression_loss: 1.4240 - classification_loss: 0.2815
 956/1500 [==================>...........] - ETA: 7:04 - loss: 1.7046 - regression_loss: 1.4233 - classification_loss: 0.2813
 957/1500 [==================>...........] - ETA: 7:03 - loss: 1.7041 - regression_loss: 1.4229 - classification_loss: 0.2812
 958/1500 [==================>...........] - ETA: 7:02 - loss: 1.7047 - regression_loss: 1.4236 - classification_loss: 0.2811
 959/1500 [==================>...........] - ETA: 7:01 - loss: 1.7055 - regression_loss: 1.4242 - classification_loss: 0.2812
 960/1500 [==================>...........] - ETA: 7:01 - loss: 1.7047 - regression_loss: 1.4237 - classification_loss: 0.2811
 961/1500 [==================>...........] - ETA: 7:00 - loss: 1.7043 - regression_loss: 1.4233 - classification_loss: 0.2809
 962/1500 [==================>...........] - ETA: 6:59 - loss: 1.7033 - regression_loss: 1.4225 - classification_loss: 0.2807
 963/1500 [==================>...........] - ETA: 6:58 - loss: 1.7039 - regression_loss: 1.4231 - classification_loss: 0.2808
 964/1500 [==================>...........] - ETA: 6:57 - loss: 1.7031 - regression_loss: 1.4224 - classification_loss: 0.2807
 965/1500 [==================>...........] - ETA: 6:57 - loss: 1.7024 - regression_loss: 1.4219 - classification_loss: 0.2806
 966/1500 [==================>...........] - ETA: 6:56 - loss: 1.7031 - regression_loss: 1.4224 - classification_loss: 0.2807
 967/1500 [==================>...........] - ETA: 6:55 - loss: 1.7035 - regression_loss: 1.4226 - classification_loss: 0.2809
 968/1500 [==================>...........] - ETA: 6:54 - loss: 1.7037 - regression_loss: 1.4228 - classification_loss: 0.2809
 969/1500 [==================>...........] - ETA: 6:53 - loss: 1.7042 - regression_loss: 1.4231 - classification_loss: 0.2811
 970/1500 [==================>...........] - ETA: 6:53 - loss: 1.7033 - regression_loss: 1.4224 - classification_loss: 0.2809
 971/1500 [==================>...........] - ETA: 6:53 - loss: 1.7048 - regression_loss: 1.4235 - classification_loss: 0.2813
 972/1500 [==================>...........] - ETA: 6:52 - loss: 1.7041 - regression_loss: 1.4229 - classification_loss: 0.2812
 973/1500 [==================>...........] - ETA: 6:51 - loss: 1.7033 - regression_loss: 1.4223 - classification_loss: 0.2810
 974/1500 [==================>...........] - ETA: 6:50 - loss: 1.7027 - regression_loss: 1.4218 - classification_loss: 0.2809
 975/1500 [==================>...........] - ETA: 6:49 - loss: 1.7022 - regression_loss: 1.4214 - classification_loss: 0.2808
 976/1500 [==================>...........] - ETA: 6:48 - loss: 1.7019 - regression_loss: 1.4209 - classification_loss: 0.2809
 977/1500 [==================>...........] - ETA: 6:47 - loss: 1.7012 - regression_loss: 1.4204 - classification_loss: 0.2808
 978/1500 [==================>...........] - ETA: 6:46 - loss: 1.7015 - regression_loss: 1.4208 - classification_loss: 0.2807
 979/1500 [==================>...........] - ETA: 6:45 - loss: 1.7011 - regression_loss: 1.4205 - classification_loss: 0.2806
 980/1500 [==================>...........] - ETA: 6:44 - loss: 1.7010 - regression_loss: 1.4201 - classification_loss: 0.2809
 981/1500 [==================>...........] - ETA: 6:44 - loss: 1.7003 - regression_loss: 1.4196 - classification_loss: 0.2807
 982/1500 [==================>...........] - ETA: 6:44 - loss: 1.7001 - regression_loss: 1.4195 - classification_loss: 0.2806
 983/1500 [==================>...........] - ETA: 6:43 - loss: 1.6997 - regression_loss: 1.4191 - classification_loss: 0.2806
 984/1500 [==================>...........] - ETA: 6:42 - loss: 1.6990 - regression_loss: 1.4185 - classification_loss: 0.2805
 985/1500 [==================>...........] - ETA: 6:41 - loss: 1.6983 - regression_loss: 1.4180 - classification_loss: 0.2803
 986/1500 [==================>...........] - ETA: 6:40 - loss: 1.6997 - regression_loss: 1.4194 - classification_loss: 0.2803
 987/1500 [==================>...........] - ETA: 6:39 - loss: 1.6992 - regression_loss: 1.4190 - classification_loss: 0.2802
 988/1500 [==================>...........] - ETA: 6:38 - loss: 1.6994 - regression_loss: 1.4193 - classification_loss: 0.2801
 989/1500 [==================>...........] - ETA: 6:38 - loss: 1.6993 - regression_loss: 1.4191 - classification_loss: 0.2802
 990/1500 [==================>...........] - ETA: 6:37 - loss: 1.6991 - regression_loss: 1.4189 - classification_loss: 0.2801
 991/1500 [==================>...........] - ETA: 6:36 - loss: 1.6998 - regression_loss: 1.4194 - classification_loss: 0.2803
 992/1500 [==================>...........] - ETA: 6:35 - loss: 1.7007 - regression_loss: 1.4202 - classification_loss: 0.2805
 993/1500 [==================>...........] - ETA: 6:35 - loss: 1.7010 - regression_loss: 1.4204 - classification_loss: 0.2806
 994/1500 [==================>...........] - ETA: 6:34 - loss: 1.7016 - regression_loss: 1.4208 - classification_loss: 0.2808
 995/1500 [==================>...........] - ETA: 6:33 - loss: 1.7012 - regression_loss: 1.4205 - classification_loss: 0.2807
 996/1500 [==================>...........] - ETA: 6:32 - loss: 1.7012 - regression_loss: 1.4204 - classification_loss: 0.2808
 997/1500 [==================>...........] - ETA: 6:31 - loss: 1.7009 - regression_loss: 1.4201 - classification_loss: 0.2807
 998/1500 [==================>...........] - ETA: 6:30 - loss: 1.7011 - regression_loss: 1.4204 - classification_loss: 0.2807
 999/1500 [==================>...........] - ETA: 6:30 - loss: 1.7019 - regression_loss: 1.4212 - classification_loss: 0.2807
1000/1500 [===================>..........] - ETA: 6:29 - loss: 1.7012 - regression_loss: 1.4206 - classification_loss: 0.2806
1001/1500 [===================>..........] - ETA: 6:29 - loss: 1.7016 - regression_loss: 1.4210 - classification_loss: 0.2806
1002/1500 [===================>..........] - ETA: 6:28 - loss: 1.7008 - regression_loss: 1.4203 - classification_loss: 0.2805
1003/1500 [===================>..........] - ETA: 6:27 - loss: 1.7015 - regression_loss: 1.4209 - classification_loss: 0.2806
1004/1500 [===================>..........] - ETA: 6:27 - loss: 1.7012 - regression_loss: 1.4206 - classification_loss: 0.2806
1005/1500 [===================>..........] - ETA: 6:26 - loss: 1.7007 - regression_loss: 1.4202 - classification_loss: 0.2805
1006/1500 [===================>..........] - ETA: 6:25 - loss: 1.7009 - regression_loss: 1.4203 - classification_loss: 0.2806
1007/1500 [===================>..........] - ETA: 6:24 - loss: 1.7012 - regression_loss: 1.4207 - classification_loss: 0.2805
1008/1500 [===================>..........] - ETA: 6:24 - loss: 1.7007 - regression_loss: 1.4202 - classification_loss: 0.2805
1009/1500 [===================>..........] - ETA: 6:23 - loss: 1.7008 - regression_loss: 1.4201 - classification_loss: 0.2807
1010/1500 [===================>..........] - ETA: 6:22 - loss: 1.7007 - regression_loss: 1.4200 - classification_loss: 0.2806
1011/1500 [===================>..........] - ETA: 6:21 - loss: 1.7000 - regression_loss: 1.4195 - classification_loss: 0.2805
1012/1500 [===================>..........] - ETA: 6:20 - loss: 1.7008 - regression_loss: 1.4201 - classification_loss: 0.2807
1013/1500 [===================>..........] - ETA: 6:19 - loss: 1.7010 - regression_loss: 1.4204 - classification_loss: 0.2807
1014/1500 [===================>..........] - ETA: 6:18 - loss: 1.7022 - regression_loss: 1.4212 - classification_loss: 0.2810
1015/1500 [===================>..........] - ETA: 6:17 - loss: 1.7024 - regression_loss: 1.4214 - classification_loss: 0.2810
1016/1500 [===================>..........] - ETA: 6:16 - loss: 1.7030 - regression_loss: 1.4219 - classification_loss: 0.2810
1017/1500 [===================>..........] - ETA: 6:16 - loss: 1.7026 - regression_loss: 1.4217 - classification_loss: 0.2809
1018/1500 [===================>..........] - ETA: 6:15 - loss: 1.7021 - regression_loss: 1.4214 - classification_loss: 0.2807
1019/1500 [===================>..........] - ETA: 6:14 - loss: 1.7020 - regression_loss: 1.4213 - classification_loss: 0.2807
1020/1500 [===================>..........] - ETA: 6:14 - loss: 1.7012 - regression_loss: 1.4207 - classification_loss: 0.2805
1021/1500 [===================>..........] - ETA: 6:13 - loss: 1.7005 - regression_loss: 1.4201 - classification_loss: 0.2804
1022/1500 [===================>..........] - ETA: 6:12 - loss: 1.7011 - regression_loss: 1.4206 - classification_loss: 0.2805
1023/1500 [===================>..........] - ETA: 6:11 - loss: 1.7007 - regression_loss: 1.4203 - classification_loss: 0.2804
1024/1500 [===================>..........] - ETA: 6:10 - loss: 1.7011 - regression_loss: 1.4206 - classification_loss: 0.2804
1025/1500 [===================>..........] - ETA: 6:09 - loss: 1.7020 - regression_loss: 1.4213 - classification_loss: 0.2807
1026/1500 [===================>..........] - ETA: 6:08 - loss: 1.7020 - regression_loss: 1.4214 - classification_loss: 0.2806
1027/1500 [===================>..........] - ETA: 6:08 - loss: 1.7016 - regression_loss: 1.4211 - classification_loss: 0.2805
1028/1500 [===================>..........] - ETA: 6:07 - loss: 1.7016 - regression_loss: 1.4211 - classification_loss: 0.2804
1029/1500 [===================>..........] - ETA: 6:06 - loss: 1.7013 - regression_loss: 1.4209 - classification_loss: 0.2804
1030/1500 [===================>..........] - ETA: 6:05 - loss: 1.7013 - regression_loss: 1.4209 - classification_loss: 0.2804
1031/1500 [===================>..........] - ETA: 6:04 - loss: 1.7009 - regression_loss: 1.4206 - classification_loss: 0.2803
1032/1500 [===================>..........] - ETA: 6:03 - loss: 1.7010 - regression_loss: 1.4208 - classification_loss: 0.2802
1033/1500 [===================>..........] - ETA: 6:03 - loss: 1.7021 - regression_loss: 1.4216 - classification_loss: 0.2805
1034/1500 [===================>..........] - ETA: 6:02 - loss: 1.7018 - regression_loss: 1.4214 - classification_loss: 0.2804
1035/1500 [===================>..........] - ETA: 6:02 - loss: 1.7023 - regression_loss: 1.4219 - classification_loss: 0.2804
1036/1500 [===================>..........] - ETA: 6:01 - loss: 1.7019 - regression_loss: 1.4216 - classification_loss: 0.2803
1037/1500 [===================>..........] - ETA: 6:00 - loss: 1.7021 - regression_loss: 1.4217 - classification_loss: 0.2804
1038/1500 [===================>..........] - ETA: 5:59 - loss: 1.7020 - regression_loss: 1.4217 - classification_loss: 0.2803
1039/1500 [===================>..........] - ETA: 5:58 - loss: 1.7021 - regression_loss: 1.4218 - classification_loss: 0.2803
1040/1500 [===================>..........] - ETA: 5:57 - loss: 1.7029 - regression_loss: 1.4226 - classification_loss: 0.2803
1041/1500 [===================>..........] - ETA: 5:57 - loss: 1.7031 - regression_loss: 1.4229 - classification_loss: 0.2802
1042/1500 [===================>..........] - ETA: 5:56 - loss: 1.7042 - regression_loss: 1.4237 - classification_loss: 0.2805
1043/1500 [===================>..........] - ETA: 5:55 - loss: 1.7032 - regression_loss: 1.4228 - classification_loss: 0.2803
1044/1500 [===================>..........] - ETA: 5:55 - loss: 1.7029 - regression_loss: 1.4226 - classification_loss: 0.2803
1045/1500 [===================>..........] - ETA: 5:54 - loss: 1.7026 - regression_loss: 1.4225 - classification_loss: 0.2802
1046/1500 [===================>..........] - ETA: 5:53 - loss: 1.7023 - regression_loss: 1.4223 - classification_loss: 0.2800
1047/1500 [===================>..........] - ETA: 5:52 - loss: 1.7026 - regression_loss: 1.4224 - classification_loss: 0.2801
1048/1500 [===================>..........] - ETA: 5:52 - loss: 1.7029 - regression_loss: 1.4228 - classification_loss: 0.2801
1049/1500 [===================>..........] - ETA: 5:51 - loss: 1.7024 - regression_loss: 1.4224 - classification_loss: 0.2800
1050/1500 [====================>.........] - ETA: 5:50 - loss: 1.7027 - regression_loss: 1.4226 - classification_loss: 0.2801
1051/1500 [====================>.........] - ETA: 5:49 - loss: 1.7028 - regression_loss: 1.4225 - classification_loss: 0.2803
1052/1500 [====================>.........] - ETA: 5:48 - loss: 1.7022 - regression_loss: 1.4221 - classification_loss: 0.2802
1053/1500 [====================>.........] - ETA: 5:47 - loss: 1.7016 - regression_loss: 1.4216 - classification_loss: 0.2800
1054/1500 [====================>.........] - ETA: 5:47 - loss: 1.7009 - regression_loss: 1.4210 - classification_loss: 0.2799
1055/1500 [====================>.........] - ETA: 5:46 - loss: 1.7009 - regression_loss: 1.4210 - classification_loss: 0.2799
1056/1500 [====================>.........] - ETA: 5:45 - loss: 1.7005 - regression_loss: 1.4207 - classification_loss: 0.2798
1057/1500 [====================>.........] - ETA: 5:45 - loss: 1.7000 - regression_loss: 1.4203 - classification_loss: 0.2797
1058/1500 [====================>.........] - ETA: 5:44 - loss: 1.7005 - regression_loss: 1.4207 - classification_loss: 0.2798
1059/1500 [====================>.........] - ETA: 5:43 - loss: 1.7010 - regression_loss: 1.4211 - classification_loss: 0.2799
1060/1500 [====================>.........] - ETA: 5:42 - loss: 1.7014 - regression_loss: 1.4215 - classification_loss: 0.2799
1061/1500 [====================>.........] - ETA: 5:41 - loss: 1.7008 - regression_loss: 1.4211 - classification_loss: 0.2797
1062/1500 [====================>.........] - ETA: 5:40 - loss: 1.7005 - regression_loss: 1.4207 - classification_loss: 0.2798
1063/1500 [====================>.........] - ETA: 5:39 - loss: 1.7012 - regression_loss: 1.4214 - classification_loss: 0.2799
1064/1500 [====================>.........] - ETA: 5:39 - loss: 1.7010 - regression_loss: 1.4212 - classification_loss: 0.2798
1065/1500 [====================>.........] - ETA: 5:38 - loss: 1.7014 - regression_loss: 1.4216 - classification_loss: 0.2798
1066/1500 [====================>.........] - ETA: 5:37 - loss: 1.7019 - regression_loss: 1.4220 - classification_loss: 0.2799
1067/1500 [====================>.........] - ETA: 5:36 - loss: 1.7018 - regression_loss: 1.4220 - classification_loss: 0.2799
1068/1500 [====================>.........] - ETA: 5:36 - loss: 1.7023 - regression_loss: 1.4224 - classification_loss: 0.2799
1069/1500 [====================>.........] - ETA: 5:35 - loss: 1.7019 - regression_loss: 1.4221 - classification_loss: 0.2799
1070/1500 [====================>.........] - ETA: 5:34 - loss: 1.7012 - regression_loss: 1.4214 - classification_loss: 0.2797
1071/1500 [====================>.........] - ETA: 5:33 - loss: 1.7018 - regression_loss: 1.4220 - classification_loss: 0.2798
1072/1500 [====================>.........] - ETA: 5:32 - loss: 1.7019 - regression_loss: 1.4222 - classification_loss: 0.2797
1073/1500 [====================>.........] - ETA: 5:32 - loss: 1.7012 - regression_loss: 1.4216 - classification_loss: 0.2796
1074/1500 [====================>.........] - ETA: 5:31 - loss: 1.7018 - regression_loss: 1.4221 - classification_loss: 0.2797
1075/1500 [====================>.........] - ETA: 5:31 - loss: 1.7017 - regression_loss: 1.4219 - classification_loss: 0.2798
1076/1500 [====================>.........] - ETA: 5:30 - loss: 1.7021 - regression_loss: 1.4222 - classification_loss: 0.2799
1077/1500 [====================>.........] - ETA: 5:29 - loss: 1.7030 - regression_loss: 1.4228 - classification_loss: 0.2802
1078/1500 [====================>.........] - ETA: 5:28 - loss: 1.7042 - regression_loss: 1.4240 - classification_loss: 0.2801
1079/1500 [====================>.........] - ETA: 5:27 - loss: 1.7037 - regression_loss: 1.4237 - classification_loss: 0.2800
1080/1500 [====================>.........] - ETA: 5:27 - loss: 1.7029 - regression_loss: 1.4231 - classification_loss: 0.2799
1081/1500 [====================>.........] - ETA: 5:27 - loss: 1.7038 - regression_loss: 1.4237 - classification_loss: 0.2801
1082/1500 [====================>.........] - ETA: 5:26 - loss: 1.7037 - regression_loss: 1.4237 - classification_loss: 0.2801
1083/1500 [====================>.........] - ETA: 5:25 - loss: 1.7035 - regression_loss: 1.4236 - classification_loss: 0.2800
1084/1500 [====================>.........] - ETA: 5:24 - loss: 1.7034 - regression_loss: 1.4235 - classification_loss: 0.2799
1085/1500 [====================>.........] - ETA: 5:23 - loss: 1.7028 - regression_loss: 1.4230 - classification_loss: 0.2798
1086/1500 [====================>.........] - ETA: 5:22 - loss: 1.7023 - regression_loss: 1.4225 - classification_loss: 0.2798
1087/1500 [====================>.........] - ETA: 5:21 - loss: 1.7019 - regression_loss: 1.4223 - classification_loss: 0.2796
1088/1500 [====================>.........] - ETA: 5:21 - loss: 1.7027 - regression_loss: 1.4229 - classification_loss: 0.2798
1089/1500 [====================>.........] - ETA: 5:20 - loss: 1.7031 - regression_loss: 1.4231 - classification_loss: 0.2800
1090/1500 [====================>.........] - ETA: 5:19 - loss: 1.7029 - regression_loss: 1.4230 - classification_loss: 0.2799
1091/1500 [====================>.........] - ETA: 5:18 - loss: 1.7024 - regression_loss: 1.4227 - classification_loss: 0.2797
1092/1500 [====================>.........] - ETA: 5:17 - loss: 1.7026 - regression_loss: 1.4227 - classification_loss: 0.2798
1093/1500 [====================>.........] - ETA: 5:16 - loss: 1.7027 - regression_loss: 1.4225 - classification_loss: 0.2801
1094/1500 [====================>.........] - ETA: 5:15 - loss: 1.7031 - regression_loss: 1.4229 - classification_loss: 0.2802
1095/1500 [====================>.........] - ETA: 5:15 - loss: 1.7034 - regression_loss: 1.4231 - classification_loss: 0.2803
1096/1500 [====================>.........] - ETA: 5:14 - loss: 1.7027 - regression_loss: 1.4226 - classification_loss: 0.2801
1097/1500 [====================>.........] - ETA: 5:13 - loss: 1.7030 - regression_loss: 1.4228 - classification_loss: 0.2802
1098/1500 [====================>.........] - ETA: 5:13 - loss: 1.7031 - regression_loss: 1.4229 - classification_loss: 0.2802
1099/1500 [====================>.........] - ETA: 5:12 - loss: 1.7025 - regression_loss: 1.4224 - classification_loss: 0.2800
1100/1500 [=====================>........] - ETA: 5:11 - loss: 1.7018 - regression_loss: 1.4218 - classification_loss: 0.2800
1101/1500 [=====================>........] - ETA: 5:10 - loss: 1.7023 - regression_loss: 1.4221 - classification_loss: 0.2801
1102/1500 [=====================>........] - ETA: 5:09 - loss: 1.7014 - regression_loss: 1.4214 - classification_loss: 0.2799
1103/1500 [=====================>........] - ETA: 5:08 - loss: 1.7006 - regression_loss: 1.4208 - classification_loss: 0.2798
1104/1500 [=====================>........] - ETA: 5:08 - loss: 1.6996 - regression_loss: 1.4200 - classification_loss: 0.2796
1105/1500 [=====================>........] - ETA: 5:07 - loss: 1.6993 - regression_loss: 1.4197 - classification_loss: 0.2796
1106/1500 [=====================>........] - ETA: 5:07 - loss: 1.6991 - regression_loss: 1.4195 - classification_loss: 0.2796
1107/1500 [=====================>........] - ETA: 5:06 - loss: 1.6983 - regression_loss: 1.4188 - classification_loss: 0.2795
1108/1500 [=====================>........] - ETA: 5:05 - loss: 1.6982 - regression_loss: 1.4188 - classification_loss: 0.2794
1109/1500 [=====================>........] - ETA: 5:04 - loss: 1.6988 - regression_loss: 1.4194 - classification_loss: 0.2795
1110/1500 [=====================>........] - ETA: 5:03 - loss: 1.6987 - regression_loss: 1.4193 - classification_loss: 0.2794
1111/1500 [=====================>........] - ETA: 5:03 - loss: 1.6984 - regression_loss: 1.4191 - classification_loss: 0.2793
1112/1500 [=====================>........] - ETA: 5:02 - loss: 1.6980 - regression_loss: 1.4188 - classification_loss: 0.2792
1113/1500 [=====================>........] - ETA: 5:01 - loss: 1.6975 - regression_loss: 1.4184 - classification_loss: 0.2791
1114/1500 [=====================>........] - ETA: 5:00 - loss: 1.6968 - regression_loss: 1.4179 - classification_loss: 0.2789
1115/1500 [=====================>........] - ETA: 4:59 - loss: 1.6959 - regression_loss: 1.4171 - classification_loss: 0.2788
1116/1500 [=====================>........] - ETA: 4:59 - loss: 1.6958 - regression_loss: 1.4171 - classification_loss: 0.2787
1117/1500 [=====================>........] - ETA: 4:58 - loss: 1.6953 - regression_loss: 1.4164 - classification_loss: 0.2789
1118/1500 [=====================>........] - ETA: 4:57 - loss: 1.6949 - regression_loss: 1.4161 - classification_loss: 0.2788
1119/1500 [=====================>........] - ETA: 4:56 - loss: 1.6953 - regression_loss: 1.4165 - classification_loss: 0.2788
1120/1500 [=====================>........] - ETA: 4:55 - loss: 1.6952 - regression_loss: 1.4163 - classification_loss: 0.2788
1121/1500 [=====================>........] - ETA: 4:54 - loss: 1.6951 - regression_loss: 1.4164 - classification_loss: 0.2787
1122/1500 [=====================>........] - ETA: 4:53 - loss: 1.6963 - regression_loss: 1.4172 - classification_loss: 0.2791
1123/1500 [=====================>........] - ETA: 4:52 - loss: 1.6956 - regression_loss: 1.4166 - classification_loss: 0.2789
1124/1500 [=====================>........] - ETA: 4:52 - loss: 1.6965 - regression_loss: 1.4173 - classification_loss: 0.2792
1125/1500 [=====================>........] - ETA: 4:51 - loss: 1.6963 - regression_loss: 1.4172 - classification_loss: 0.2791
1126/1500 [=====================>........] - ETA: 4:50 - loss: 1.6962 - regression_loss: 1.4172 - classification_loss: 0.2790
1127/1500 [=====================>........] - ETA: 4:49 - loss: 1.6957 - regression_loss: 1.4168 - classification_loss: 0.2789
1128/1500 [=====================>........] - ETA: 4:49 - loss: 1.6949 - regression_loss: 1.4162 - classification_loss: 0.2788
1129/1500 [=====================>........] - ETA: 4:48 - loss: 1.6958 - regression_loss: 1.4169 - classification_loss: 0.2790
1130/1500 [=====================>........] - ETA: 4:47 - loss: 1.6962 - regression_loss: 1.4173 - classification_loss: 0.2790
1131/1500 [=====================>........] - ETA: 4:46 - loss: 1.6967 - regression_loss: 1.4178 - classification_loss: 0.2789
1132/1500 [=====================>........] - ETA: 4:45 - loss: 1.6965 - regression_loss: 1.4177 - classification_loss: 0.2789
1133/1500 [=====================>........] - ETA: 4:44 - loss: 1.6961 - regression_loss: 1.4173 - classification_loss: 0.2788
1134/1500 [=====================>........] - ETA: 4:44 - loss: 1.6968 - regression_loss: 1.4180 - classification_loss: 0.2788
1135/1500 [=====================>........] - ETA: 4:43 - loss: 1.6967 - regression_loss: 1.4179 - classification_loss: 0.2788
1136/1500 [=====================>........] - ETA: 4:42 - loss: 1.6973 - regression_loss: 1.4185 - classification_loss: 0.2788
1137/1500 [=====================>........] - ETA: 4:41 - loss: 1.6975 - regression_loss: 1.4186 - classification_loss: 0.2789
1138/1500 [=====================>........] - ETA: 4:40 - loss: 1.6977 - regression_loss: 1.4182 - classification_loss: 0.2795
1139/1500 [=====================>........] - ETA: 4:39 - loss: 1.6977 - regression_loss: 1.4184 - classification_loss: 0.2794
1140/1500 [=====================>........] - ETA: 4:39 - loss: 1.6982 - regression_loss: 1.4187 - classification_loss: 0.2795
1141/1500 [=====================>........] - ETA: 4:38 - loss: 1.6979 - regression_loss: 1.4185 - classification_loss: 0.2794
1142/1500 [=====================>........] - ETA: 4:37 - loss: 1.6976 - regression_loss: 1.4183 - classification_loss: 0.2793
1143/1500 [=====================>........] - ETA: 4:36 - loss: 1.6973 - regression_loss: 1.4180 - classification_loss: 0.2792
1144/1500 [=====================>........] - ETA: 4:36 - loss: 1.6969 - regression_loss: 1.4177 - classification_loss: 0.2792
1145/1500 [=====================>........] - ETA: 4:35 - loss: 1.6965 - regression_loss: 1.4174 - classification_loss: 0.2791
1146/1500 [=====================>........] - ETA: 4:34 - loss: 1.6958 - regression_loss: 1.4168 - classification_loss: 0.2789
1147/1500 [=====================>........] - ETA: 4:33 - loss: 1.6953 - regression_loss: 1.4165 - classification_loss: 0.2788
1148/1500 [=====================>........] - ETA: 4:32 - loss: 1.6958 - regression_loss: 1.4170 - classification_loss: 0.2788
1149/1500 [=====================>........] - ETA: 4:32 - loss: 1.6960 - regression_loss: 1.4171 - classification_loss: 0.2789
1150/1500 [======================>.......] - ETA: 4:31 - loss: 1.6960 - regression_loss: 1.4172 - classification_loss: 0.2788
1151/1500 [======================>.......] - ETA: 4:31 - loss: 1.6960 - regression_loss: 1.4173 - classification_loss: 0.2787
1152/1500 [======================>.......] - ETA: 4:30 - loss: 1.6954 - regression_loss: 1.4168 - classification_loss: 0.2787
1153/1500 [======================>.......] - ETA: 4:29 - loss: 1.6963 - regression_loss: 1.4172 - classification_loss: 0.2791
1154/1500 [======================>.......] - ETA: 4:28 - loss: 1.6958 - regression_loss: 1.4168 - classification_loss: 0.2790
1155/1500 [======================>.......] - ETA: 4:28 - loss: 1.6964 - regression_loss: 1.4173 - classification_loss: 0.2791
1156/1500 [======================>.......] - ETA: 4:27 - loss: 1.6970 - regression_loss: 1.4179 - classification_loss: 0.2791
1157/1500 [======================>.......] - ETA: 4:26 - loss: 1.6968 - regression_loss: 1.4177 - classification_loss: 0.2790
1158/1500 [======================>.......] - ETA: 4:25 - loss: 1.6970 - regression_loss: 1.4179 - classification_loss: 0.2790
1159/1500 [======================>.......] - ETA: 4:25 - loss: 1.6963 - regression_loss: 1.4174 - classification_loss: 0.2789
1160/1500 [======================>.......] - ETA: 4:24 - loss: 1.6966 - regression_loss: 1.4176 - classification_loss: 0.2789
1161/1500 [======================>.......] - ETA: 4:23 - loss: 1.6968 - regression_loss: 1.4178 - classification_loss: 0.2790
1162/1500 [======================>.......] - ETA: 4:22 - loss: 1.6962 - regression_loss: 1.4173 - classification_loss: 0.2789
1163/1500 [======================>.......] - ETA: 4:21 - loss: 1.6955 - regression_loss: 1.4167 - classification_loss: 0.2788
1164/1500 [======================>.......] - ETA: 4:21 - loss: 1.6948 - regression_loss: 1.4161 - classification_loss: 0.2786
1165/1500 [======================>.......] - ETA: 4:20 - loss: 1.6940 - regression_loss: 1.4155 - classification_loss: 0.2785
1166/1500 [======================>.......] - ETA: 4:19 - loss: 1.6937 - regression_loss: 1.4152 - classification_loss: 0.2784
1167/1500 [======================>.......] - ETA: 4:18 - loss: 1.6932 - regression_loss: 1.4148 - classification_loss: 0.2783
1168/1500 [======================>.......] - ETA: 4:17 - loss: 1.6929 - regression_loss: 1.4146 - classification_loss: 0.2783
1169/1500 [======================>.......] - ETA: 4:16 - loss: 1.6928 - regression_loss: 1.4143 - classification_loss: 0.2785
1170/1500 [======================>.......] - ETA: 4:16 - loss: 1.6926 - regression_loss: 1.4142 - classification_loss: 0.2784
1171/1500 [======================>.......] - ETA: 4:15 - loss: 1.6923 - regression_loss: 1.4139 - classification_loss: 0.2784
1172/1500 [======================>.......] - ETA: 4:14 - loss: 1.6916 - regression_loss: 1.4134 - classification_loss: 0.2782
1173/1500 [======================>.......] - ETA: 4:13 - loss: 1.6913 - regression_loss: 1.4132 - classification_loss: 0.2781
1174/1500 [======================>.......] - ETA: 4:12 - loss: 1.6911 - regression_loss: 1.4130 - classification_loss: 0.2781
1175/1500 [======================>.......] - ETA: 4:11 - loss: 1.6907 - regression_loss: 1.4128 - classification_loss: 0.2779
1176/1500 [======================>.......] - ETA: 4:11 - loss: 1.6903 - regression_loss: 1.4124 - classification_loss: 0.2779
1177/1500 [======================>.......] - ETA: 4:10 - loss: 1.6904 - regression_loss: 1.4125 - classification_loss: 0.2779
1178/1500 [======================>.......] - ETA: 4:09 - loss: 1.6905 - regression_loss: 1.4125 - classification_loss: 0.2780
1179/1500 [======================>.......] - ETA: 4:08 - loss: 1.6899 - regression_loss: 1.4120 - classification_loss: 0.2779
1180/1500 [======================>.......] - ETA: 4:07 - loss: 1.6891 - regression_loss: 1.4114 - classification_loss: 0.2777
1181/1500 [======================>.......] - ETA: 4:06 - loss: 1.6890 - regression_loss: 1.4114 - classification_loss: 0.2776
1182/1500 [======================>.......] - ETA: 4:05 - loss: 1.6888 - regression_loss: 1.4112 - classification_loss: 0.2776
1183/1500 [======================>.......] - ETA: 4:05 - loss: 1.6877 - regression_loss: 1.4103 - classification_loss: 0.2774
1184/1500 [======================>.......] - ETA: 4:04 - loss: 1.6887 - regression_loss: 1.4111 - classification_loss: 0.2776
1185/1500 [======================>.......] - ETA: 4:03 - loss: 1.6897 - regression_loss: 1.4118 - classification_loss: 0.2779
1186/1500 [======================>.......] - ETA: 4:02 - loss: 1.6892 - regression_loss: 1.4114 - classification_loss: 0.2778
1187/1500 [======================>.......] - ETA: 4:01 - loss: 1.6883 - regression_loss: 1.4107 - classification_loss: 0.2776
1188/1500 [======================>.......] - ETA: 4:00 - loss: 1.6889 - regression_loss: 1.4111 - classification_loss: 0.2777
1189/1500 [======================>.......] - ETA: 3:59 - loss: 1.6884 - regression_loss: 1.4108 - classification_loss: 0.2776
1190/1500 [======================>.......] - ETA: 3:59 - loss: 1.6885 - regression_loss: 1.4108 - classification_loss: 0.2777
1191/1500 [======================>.......] - ETA: 3:58 - loss: 1.6881 - regression_loss: 1.4105 - classification_loss: 0.2776
1192/1500 [======================>.......] - ETA: 3:57 - loss: 1.6873 - regression_loss: 1.4099 - classification_loss: 0.2774
1193/1500 [======================>.......] - ETA: 3:56 - loss: 1.6872 - regression_loss: 1.4099 - classification_loss: 0.2773
1194/1500 [======================>.......] - ETA: 3:55 - loss: 1.6865 - regression_loss: 1.4093 - classification_loss: 0.2772
1195/1500 [======================>.......] - ETA: 3:55 - loss: 1.6871 - regression_loss: 1.4096 - classification_loss: 0.2775
1196/1500 [======================>.......] - ETA: 3:54 - loss: 1.6871 - regression_loss: 1.4095 - classification_loss: 0.2775
1197/1500 [======================>.......] - ETA: 3:53 - loss: 1.6862 - regression_loss: 1.4088 - classification_loss: 0.2774
1198/1500 [======================>.......] - ETA: 3:52 - loss: 1.6870 - regression_loss: 1.4090 - classification_loss: 0.2781
1199/1500 [======================>.......] - ETA: 3:51 - loss: 1.6868 - regression_loss: 1.4088 - classification_loss: 0.2780
1200/1500 [=======================>......] - ETA: 3:51 - loss: 1.6870 - regression_loss: 1.4090 - classification_loss: 0.2780
1201/1500 [=======================>......] - ETA: 3:50 - loss: 1.6871 - regression_loss: 1.4090 - classification_loss: 0.2780
1202/1500 [=======================>......] - ETA: 3:49 - loss: 1.6867 - regression_loss: 1.4087 - classification_loss: 0.2780
1203/1500 [=======================>......] - ETA: 3:48 - loss: 1.6869 - regression_loss: 1.4088 - classification_loss: 0.2781
1204/1500 [=======================>......] - ETA: 3:47 - loss: 1.6869 - regression_loss: 1.4090 - classification_loss: 0.2780
1205/1500 [=======================>......] - ETA: 3:47 - loss: 1.6865 - regression_loss: 1.4086 - classification_loss: 0.2779
1206/1500 [=======================>......] - ETA: 3:46 - loss: 1.6862 - regression_loss: 1.4084 - classification_loss: 0.2778
1207/1500 [=======================>......] - ETA: 3:45 - loss: 1.6862 - regression_loss: 1.4084 - classification_loss: 0.2778
1208/1500 [=======================>......] - ETA: 3:44 - loss: 1.6863 - regression_loss: 1.4084 - classification_loss: 0.2779
1209/1500 [=======================>......] - ETA: 3:44 - loss: 1.6866 - regression_loss: 1.4086 - classification_loss: 0.2780
1210/1500 [=======================>......] - ETA: 3:43 - loss: 1.6867 - regression_loss: 1.4087 - classification_loss: 0.2780
1211/1500 [=======================>......] - ETA: 3:42 - loss: 1.6861 - regression_loss: 1.4082 - classification_loss: 0.2779
1212/1500 [=======================>......] - ETA: 3:41 - loss: 1.6859 - regression_loss: 1.4081 - classification_loss: 0.2778
1213/1500 [=======================>......] - ETA: 3:40 - loss: 1.6868 - regression_loss: 1.4088 - classification_loss: 0.2780
1214/1500 [=======================>......] - ETA: 3:40 - loss: 1.6863 - regression_loss: 1.4084 - classification_loss: 0.2779
1215/1500 [=======================>......] - ETA: 3:39 - loss: 1.6864 - regression_loss: 1.4084 - classification_loss: 0.2780
1216/1500 [=======================>......] - ETA: 3:38 - loss: 1.6863 - regression_loss: 1.4084 - classification_loss: 0.2780
1217/1500 [=======================>......] - ETA: 3:37 - loss: 1.6874 - regression_loss: 1.4091 - classification_loss: 0.2783
1218/1500 [=======================>......] - ETA: 3:36 - loss: 1.6872 - regression_loss: 1.4090 - classification_loss: 0.2782
1219/1500 [=======================>......] - ETA: 3:35 - loss: 1.6864 - regression_loss: 1.4084 - classification_loss: 0.2780
1220/1500 [=======================>......] - ETA: 3:35 - loss: 1.6863 - regression_loss: 1.4083 - classification_loss: 0.2780
1221/1500 [=======================>......] - ETA: 3:34 - loss: 1.6860 - regression_loss: 1.4081 - classification_loss: 0.2779
1222/1500 [=======================>......] - ETA: 3:33 - loss: 1.6855 - regression_loss: 1.4077 - classification_loss: 0.2778
1223/1500 [=======================>......] - ETA: 3:33 - loss: 1.6853 - regression_loss: 1.4075 - classification_loss: 0.2778
1224/1500 [=======================>......] - ETA: 3:32 - loss: 1.6859 - regression_loss: 1.4079 - classification_loss: 0.2780
1225/1500 [=======================>......] - ETA: 3:31 - loss: 1.6855 - regression_loss: 1.4077 - classification_loss: 0.2778
1226/1500 [=======================>......] - ETA: 3:30 - loss: 1.6850 - regression_loss: 1.4073 - classification_loss: 0.2777
1227/1500 [=======================>......] - ETA: 3:29 - loss: 1.6855 - regression_loss: 1.4078 - classification_loss: 0.2778
1228/1500 [=======================>......] - ETA: 3:28 - loss: 1.6851 - regression_loss: 1.4074 - classification_loss: 0.2777
1229/1500 [=======================>......] - ETA: 3:28 - loss: 1.6846 - regression_loss: 1.4071 - classification_loss: 0.2776
1230/1500 [=======================>......] - ETA: 3:27 - loss: 1.6841 - regression_loss: 1.4066 - classification_loss: 0.2775
1231/1500 [=======================>......] - ETA: 3:26 - loss: 1.6853 - regression_loss: 1.4074 - classification_loss: 0.2779
1232/1500 [=======================>......] - ETA: 3:26 - loss: 1.6847 - regression_loss: 1.4070 - classification_loss: 0.2778
1233/1500 [=======================>......] - ETA: 3:25 - loss: 1.6851 - regression_loss: 1.4073 - classification_loss: 0.2779
1234/1500 [=======================>......] - ETA: 3:24 - loss: 1.6866 - regression_loss: 1.4085 - classification_loss: 0.2781
1235/1500 [=======================>......] - ETA: 3:23 - loss: 1.6875 - regression_loss: 1.4093 - classification_loss: 0.2782
1236/1500 [=======================>......] - ETA: 3:22 - loss: 1.6874 - regression_loss: 1.4092 - classification_loss: 0.2781
1237/1500 [=======================>......] - ETA: 3:22 - loss: 1.6880 - regression_loss: 1.4097 - classification_loss: 0.2782
1238/1500 [=======================>......] - ETA: 3:21 - loss: 1.6885 - regression_loss: 1.4102 - classification_loss: 0.2783
1239/1500 [=======================>......] - ETA: 3:20 - loss: 1.6881 - regression_loss: 1.4098 - classification_loss: 0.2782
1240/1500 [=======================>......] - ETA: 3:19 - loss: 1.6880 - regression_loss: 1.4098 - classification_loss: 0.2782
1241/1500 [=======================>......] - ETA: 3:18 - loss: 1.6882 - regression_loss: 1.4100 - classification_loss: 0.2782
1242/1500 [=======================>......] - ETA: 3:18 - loss: 1.6879 - regression_loss: 1.4098 - classification_loss: 0.2781
1243/1500 [=======================>......] - ETA: 3:17 - loss: 1.6874 - regression_loss: 1.4095 - classification_loss: 0.2780
1244/1500 [=======================>......] - ETA: 3:16 - loss: 1.6866 - regression_loss: 1.4088 - classification_loss: 0.2778
1245/1500 [=======================>......] - ETA: 3:15 - loss: 1.6866 - regression_loss: 1.4088 - classification_loss: 0.2777
1246/1500 [=======================>......] - ETA: 3:15 - loss: 1.6865 - regression_loss: 1.4087 - classification_loss: 0.2778
1247/1500 [=======================>......] - ETA: 3:14 - loss: 1.6866 - regression_loss: 1.4085 - classification_loss: 0.2781
1248/1500 [=======================>......] - ETA: 3:13 - loss: 1.6863 - regression_loss: 1.4083 - classification_loss: 0.2780
1249/1500 [=======================>......] - ETA: 3:13 - loss: 1.6862 - regression_loss: 1.4081 - classification_loss: 0.2781
1250/1500 [========================>.....] - ETA: 3:12 - loss: 1.6866 - regression_loss: 1.4085 - classification_loss: 0.2781
1251/1500 [========================>.....] - ETA: 3:11 - loss: 1.6864 - regression_loss: 1.4084 - classification_loss: 0.2780
1252/1500 [========================>.....] - ETA: 3:11 - loss: 1.6866 - regression_loss: 1.4086 - classification_loss: 0.2780
1253/1500 [========================>.....] - ETA: 3:10 - loss: 1.6866 - regression_loss: 1.4085 - classification_loss: 0.2780
1254/1500 [========================>.....] - ETA: 3:09 - loss: 1.6868 - regression_loss: 1.4087 - classification_loss: 0.2781
1255/1500 [========================>.....] - ETA: 3:08 - loss: 1.6865 - regression_loss: 1.4086 - classification_loss: 0.2780
1256/1500 [========================>.....] - ETA: 3:07 - loss: 1.6857 - regression_loss: 1.4079 - classification_loss: 0.2778
1257/1500 [========================>.....] - ETA: 3:07 - loss: 1.6865 - regression_loss: 1.4085 - classification_loss: 0.2780
1258/1500 [========================>.....] - ETA: 3:06 - loss: 1.6863 - regression_loss: 1.4084 - classification_loss: 0.2779
1259/1500 [========================>.....] - ETA: 3:05 - loss: 1.6855 - regression_loss: 1.4077 - classification_loss: 0.2778
1260/1500 [========================>.....] - ETA: 3:04 - loss: 1.6861 - regression_loss: 1.4082 - classification_loss: 0.2779
1261/1500 [========================>.....] - ETA: 3:03 - loss: 1.6865 - regression_loss: 1.4085 - classification_loss: 0.2780
1262/1500 [========================>.....] - ETA: 3:03 - loss: 1.6874 - regression_loss: 1.4093 - classification_loss: 0.2781
1263/1500 [========================>.....] - ETA: 3:02 - loss: 1.6870 - regression_loss: 1.4090 - classification_loss: 0.2781
1264/1500 [========================>.....] - ETA: 3:01 - loss: 1.6876 - regression_loss: 1.4095 - classification_loss: 0.2781
1265/1500 [========================>.....] - ETA: 3:00 - loss: 1.6879 - regression_loss: 1.4098 - classification_loss: 0.2781
1266/1500 [========================>.....] - ETA: 2:59 - loss: 1.6883 - regression_loss: 1.4098 - classification_loss: 0.2785
1267/1500 [========================>.....] - ETA: 2:59 - loss: 1.6881 - regression_loss: 1.4097 - classification_loss: 0.2783
1268/1500 [========================>.....] - ETA: 2:58 - loss: 1.6884 - regression_loss: 1.4100 - classification_loss: 0.2784
1269/1500 [========================>.....] - ETA: 2:57 - loss: 1.6885 - regression_loss: 1.4101 - classification_loss: 0.2784
1270/1500 [========================>.....] - ETA: 2:56 - loss: 1.6880 - regression_loss: 1.4098 - classification_loss: 0.2783
1271/1500 [========================>.....] - ETA: 2:56 - loss: 1.6883 - regression_loss: 1.4100 - classification_loss: 0.2783
1272/1500 [========================>.....] - ETA: 2:55 - loss: 1.6877 - regression_loss: 1.4095 - classification_loss: 0.2781
1273/1500 [========================>.....] - ETA: 2:54 - loss: 1.6871 - regression_loss: 1.4091 - classification_loss: 0.2780
1274/1500 [========================>.....] - ETA: 2:53 - loss: 1.6877 - regression_loss: 1.4097 - classification_loss: 0.2780
1275/1500 [========================>.....] - ETA: 2:53 - loss: 1.6876 - regression_loss: 1.4096 - classification_loss: 0.2780
1276/1500 [========================>.....] - ETA: 2:52 - loss: 1.6873 - regression_loss: 1.4093 - classification_loss: 0.2780
1277/1500 [========================>.....] - ETA: 2:51 - loss: 1.6875 - regression_loss: 1.4095 - classification_loss: 0.2780
1278/1500 [========================>.....] - ETA: 2:51 - loss: 1.6872 - regression_loss: 1.4093 - classification_loss: 0.2779
1279/1500 [========================>.....] - ETA: 2:50 - loss: 1.6868 - regression_loss: 1.4090 - classification_loss: 0.2778
1280/1500 [========================>.....] - ETA: 2:49 - loss: 1.6865 - regression_loss: 1.4086 - classification_loss: 0.2778
1281/1500 [========================>.....] - ETA: 2:48 - loss: 1.6860 - regression_loss: 1.4083 - classification_loss: 0.2777
1282/1500 [========================>.....] - ETA: 2:47 - loss: 1.6854 - regression_loss: 1.4078 - classification_loss: 0.2776
1283/1500 [========================>.....] - ETA: 2:47 - loss: 1.6850 - regression_loss: 1.4075 - classification_loss: 0.2775
1284/1500 [========================>.....] - ETA: 2:46 - loss: 1.6853 - regression_loss: 1.4079 - classification_loss: 0.2775
1285/1500 [========================>.....] - ETA: 2:45 - loss: 1.6862 - regression_loss: 1.4085 - classification_loss: 0.2776
1286/1500 [========================>.....] - ETA: 2:44 - loss: 1.6867 - regression_loss: 1.4090 - classification_loss: 0.2776
1287/1500 [========================>.....] - ETA: 2:44 - loss: 1.6873 - regression_loss: 1.4095 - classification_loss: 0.2778
1288/1500 [========================>.....] - ETA: 2:43 - loss: 1.6870 - regression_loss: 1.4093 - classification_loss: 0.2777
1289/1500 [========================>.....] - ETA: 2:42 - loss: 1.6881 - regression_loss: 1.4093 - classification_loss: 0.2788
1290/1500 [========================>.....] - ETA: 2:41 - loss: 1.6886 - regression_loss: 1.4098 - classification_loss: 0.2788
1291/1500 [========================>.....] - ETA: 2:40 - loss: 1.6883 - regression_loss: 1.4095 - classification_loss: 0.2788
1292/1500 [========================>.....] - ETA: 2:39 - loss: 1.6881 - regression_loss: 1.4094 - classification_loss: 0.2787
1293/1500 [========================>.....] - ETA: 2:39 - loss: 1.6889 - regression_loss: 1.4099 - classification_loss: 0.2790
1294/1500 [========================>.....] - ETA: 2:38 - loss: 1.6890 - regression_loss: 1.4100 - classification_loss: 0.2790
1295/1500 [========================>.....] - ETA: 2:37 - loss: 1.6899 - regression_loss: 1.4107 - classification_loss: 0.2792
1296/1500 [========================>.....] - ETA: 2:36 - loss: 1.6894 - regression_loss: 1.4103 - classification_loss: 0.2791
1297/1500 [========================>.....] - ETA: 2:35 - loss: 1.6896 - regression_loss: 1.4105 - classification_loss: 0.2792
1298/1500 [========================>.....] - ETA: 2:35 - loss: 1.6894 - regression_loss: 1.4103 - classification_loss: 0.2790
1299/1500 [========================>.....] - ETA: 2:34 - loss: 1.6891 - regression_loss: 1.4101 - classification_loss: 0.2790
1300/1500 [=========================>....] - ETA: 2:33 - loss: 1.6890 - regression_loss: 1.4101 - classification_loss: 0.2789
1301/1500 [=========================>....] - ETA: 2:32 - loss: 1.6897 - regression_loss: 1.4107 - classification_loss: 0.2790
1302/1500 [=========================>....] - ETA: 2:31 - loss: 1.6900 - regression_loss: 1.4109 - classification_loss: 0.2791
1303/1500 [=========================>....] - ETA: 2:31 - loss: 1.6899 - regression_loss: 1.4108 - classification_loss: 0.2792
1304/1500 [=========================>....] - ETA: 2:30 - loss: 1.6895 - regression_loss: 1.4104 - classification_loss: 0.2790
1305/1500 [=========================>....] - ETA: 2:29 - loss: 1.6897 - regression_loss: 1.4106 - classification_loss: 0.2790
1306/1500 [=========================>....] - ETA: 2:28 - loss: 1.6892 - regression_loss: 1.4102 - classification_loss: 0.2790
1307/1500 [=========================>....] - ETA: 2:27 - loss: 1.6894 - regression_loss: 1.4105 - classification_loss: 0.2790
1308/1500 [=========================>....] - ETA: 2:27 - loss: 1.6897 - regression_loss: 1.4107 - classification_loss: 0.2791
1309/1500 [=========================>....] - ETA: 2:26 - loss: 1.6903 - regression_loss: 1.4112 - classification_loss: 0.2791
1310/1500 [=========================>....] - ETA: 2:25 - loss: 1.6902 - regression_loss: 1.4110 - classification_loss: 0.2792
1311/1500 [=========================>....] - ETA: 2:24 - loss: 1.6898 - regression_loss: 1.4108 - classification_loss: 0.2790
1312/1500 [=========================>....] - ETA: 2:23 - loss: 1.6896 - regression_loss: 1.4106 - classification_loss: 0.2790
1313/1500 [=========================>....] - ETA: 2:23 - loss: 1.6900 - regression_loss: 1.4109 - classification_loss: 0.2791
1314/1500 [=========================>....] - ETA: 2:22 - loss: 1.6895 - regression_loss: 1.4105 - classification_loss: 0.2790
1315/1500 [=========================>....] - ETA: 2:22 - loss: 1.6897 - regression_loss: 1.4108 - classification_loss: 0.2789
1316/1500 [=========================>....] - ETA: 2:21 - loss: 1.6892 - regression_loss: 1.4104 - classification_loss: 0.2788
1317/1500 [=========================>....] - ETA: 2:20 - loss: 1.6888 - regression_loss: 1.4100 - classification_loss: 0.2788
1318/1500 [=========================>....] - ETA: 2:19 - loss: 1.6882 - regression_loss: 1.4096 - classification_loss: 0.2786
1319/1500 [=========================>....] - ETA: 2:18 - loss: 1.6882 - regression_loss: 1.4096 - classification_loss: 0.2785
1320/1500 [=========================>....] - ETA: 2:18 - loss: 1.6878 - regression_loss: 1.4093 - classification_loss: 0.2785
1321/1500 [=========================>....] - ETA: 2:17 - loss: 1.6881 - regression_loss: 1.4096 - classification_loss: 0.2784
1322/1500 [=========================>....] - ETA: 2:16 - loss: 1.6879 - regression_loss: 1.4095 - classification_loss: 0.2784
1323/1500 [=========================>....] - ETA: 2:15 - loss: 1.6878 - regression_loss: 1.4095 - classification_loss: 0.2783
1324/1500 [=========================>....] - ETA: 2:15 - loss: 1.6886 - regression_loss: 1.4101 - classification_loss: 0.2785
1325/1500 [=========================>....] - ETA: 2:14 - loss: 1.6881 - regression_loss: 1.4098 - classification_loss: 0.2783
1326/1500 [=========================>....] - ETA: 2:13 - loss: 1.6887 - regression_loss: 1.4103 - classification_loss: 0.2784
1327/1500 [=========================>....] - ETA: 2:12 - loss: 1.6880 - regression_loss: 1.4097 - classification_loss: 0.2783
1328/1500 [=========================>....] - ETA: 2:11 - loss: 1.6881 - regression_loss: 1.4098 - classification_loss: 0.2783
1329/1500 [=========================>....] - ETA: 2:11 - loss: 1.6881 - regression_loss: 1.4099 - classification_loss: 0.2782
1330/1500 [=========================>....] - ETA: 2:10 - loss: 1.6875 - regression_loss: 1.4094 - classification_loss: 0.2781
1331/1500 [=========================>....] - ETA: 2:09 - loss: 1.6869 - regression_loss: 1.4089 - classification_loss: 0.2779
1332/1500 [=========================>....] - ETA: 2:08 - loss: 1.6870 - regression_loss: 1.4092 - classification_loss: 0.2778
1333/1500 [=========================>....] - ETA: 2:08 - loss: 1.6880 - regression_loss: 1.4099 - classification_loss: 0.2780
1334/1500 [=========================>....] - ETA: 2:07 - loss: 1.6881 - regression_loss: 1.4101 - classification_loss: 0.2781
1335/1500 [=========================>....] - ETA: 2:06 - loss: 1.6888 - regression_loss: 1.4107 - classification_loss: 0.2781
1336/1500 [=========================>....] - ETA: 2:05 - loss: 1.6892 - regression_loss: 1.4111 - classification_loss: 0.2781
1337/1500 [=========================>....] - ETA: 2:05 - loss: 1.6890 - regression_loss: 1.4109 - classification_loss: 0.2780
1338/1500 [=========================>....] - ETA: 2:04 - loss: 1.6890 - regression_loss: 1.4109 - classification_loss: 0.2781
1339/1500 [=========================>....] - ETA: 2:03 - loss: 1.6895 - regression_loss: 1.4113 - classification_loss: 0.2782
1340/1500 [=========================>....] - ETA: 2:02 - loss: 1.6896 - regression_loss: 1.4115 - classification_loss: 0.2781
1341/1500 [=========================>....] - ETA: 2:02 - loss: 1.6890 - regression_loss: 1.4110 - classification_loss: 0.2780
1342/1500 [=========================>....] - ETA: 2:01 - loss: 1.6890 - regression_loss: 1.4110 - classification_loss: 0.2781
1343/1500 [=========================>....] - ETA: 2:00 - loss: 1.6885 - regression_loss: 1.4105 - classification_loss: 0.2780
1344/1500 [=========================>....] - ETA: 1:59 - loss: 1.6881 - regression_loss: 1.4102 - classification_loss: 0.2779
1345/1500 [=========================>....] - ETA: 1:58 - loss: 1.6883 - regression_loss: 1.4103 - classification_loss: 0.2780
1346/1500 [=========================>....] - ETA: 1:58 - loss: 1.6882 - regression_loss: 1.4103 - classification_loss: 0.2779
1347/1500 [=========================>....] - ETA: 1:57 - loss: 1.6878 - regression_loss: 1.4099 - classification_loss: 0.2779
1348/1500 [=========================>....] - ETA: 1:56 - loss: 1.6885 - regression_loss: 1.4105 - classification_loss: 0.2780
1349/1500 [=========================>....] - ETA: 1:55 - loss: 1.6881 - regression_loss: 1.4102 - classification_loss: 0.2779
1350/1500 [==========================>...] - ETA: 1:55 - loss: 1.6876 - regression_loss: 1.4098 - classification_loss: 0.2778
1351/1500 [==========================>...] - ETA: 1:54 - loss: 1.6871 - regression_loss: 1.4093 - classification_loss: 0.2777
1352/1500 [==========================>...] - ETA: 1:53 - loss: 1.6863 - regression_loss: 1.4087 - classification_loss: 0.2776
1353/1500 [==========================>...] - ETA: 1:52 - loss: 1.6864 - regression_loss: 1.4089 - classification_loss: 0.2776
1354/1500 [==========================>...] - ETA: 1:52 - loss: 1.6862 - regression_loss: 1.4087 - classification_loss: 0.2776
1355/1500 [==========================>...] - ETA: 1:51 - loss: 1.6865 - regression_loss: 1.4089 - classification_loss: 0.2776
1356/1500 [==========================>...] - ETA: 1:50 - loss: 1.6867 - regression_loss: 1.4090 - classification_loss: 0.2777
1357/1500 [==========================>...] - ETA: 1:49 - loss: 1.6869 - regression_loss: 1.4092 - classification_loss: 0.2777
1358/1500 [==========================>...] - ETA: 1:49 - loss: 1.6865 - regression_loss: 1.4089 - classification_loss: 0.2776
1359/1500 [==========================>...] - ETA: 1:48 - loss: 1.6871 - regression_loss: 1.4094 - classification_loss: 0.2777
1360/1500 [==========================>...] - ETA: 1:47 - loss: 1.6867 - regression_loss: 1.4091 - classification_loss: 0.2776
1361/1500 [==========================>...] - ETA: 1:46 - loss: 1.6862 - regression_loss: 1.4087 - classification_loss: 0.2775
1362/1500 [==========================>...] - ETA: 1:45 - loss: 1.6861 - regression_loss: 1.4086 - classification_loss: 0.2774
1363/1500 [==========================>...] - ETA: 1:45 - loss: 1.6855 - regression_loss: 1.4082 - classification_loss: 0.2773
1364/1500 [==========================>...] - ETA: 1:44 - loss: 1.6852 - regression_loss: 1.4080 - classification_loss: 0.2773
1365/1500 [==========================>...] - ETA: 1:43 - loss: 1.6864 - regression_loss: 1.4089 - classification_loss: 0.2775
1366/1500 [==========================>...] - ETA: 1:42 - loss: 1.6861 - regression_loss: 1.4088 - classification_loss: 0.2774
1367/1500 [==========================>...] - ETA: 1:41 - loss: 1.6868 - regression_loss: 1.4094 - classification_loss: 0.2775
1368/1500 [==========================>...] - ETA: 1:41 - loss: 1.6868 - regression_loss: 1.4095 - classification_loss: 0.2774
1369/1500 [==========================>...] - ETA: 1:40 - loss: 1.6868 - regression_loss: 1.4094 - classification_loss: 0.2774
1370/1500 [==========================>...] - ETA: 1:39 - loss: 1.6867 - regression_loss: 1.4094 - classification_loss: 0.2773
1371/1500 [==========================>...] - ETA: 1:38 - loss: 1.6863 - regression_loss: 1.4091 - classification_loss: 0.2772
1372/1500 [==========================>...] - ETA: 1:37 - loss: 1.6859 - regression_loss: 1.4088 - classification_loss: 0.2771
1373/1500 [==========================>...] - ETA: 1:37 - loss: 1.6856 - regression_loss: 1.4085 - classification_loss: 0.2771
1374/1500 [==========================>...] - ETA: 1:36 - loss: 1.6862 - regression_loss: 1.4091 - classification_loss: 0.2771
1375/1500 [==========================>...] - ETA: 1:35 - loss: 1.6864 - regression_loss: 1.4093 - classification_loss: 0.2771
1376/1500 [==========================>...] - ETA: 1:34 - loss: 1.6856 - regression_loss: 1.4087 - classification_loss: 0.2769
1377/1500 [==========================>...] - ETA: 1:34 - loss: 1.6852 - regression_loss: 1.4083 - classification_loss: 0.2768
1378/1500 [==========================>...] - ETA: 1:33 - loss: 1.6848 - regression_loss: 1.4081 - classification_loss: 0.2767
1379/1500 [==========================>...] - ETA: 1:32 - loss: 1.6840 - regression_loss: 1.4075 - classification_loss: 0.2765
1380/1500 [==========================>...] - ETA: 1:31 - loss: 1.6841 - regression_loss: 1.4072 - classification_loss: 0.2769
1381/1500 [==========================>...] - ETA: 1:31 - loss: 1.6847 - regression_loss: 1.4077 - classification_loss: 0.2770
1382/1500 [==========================>...] - ETA: 1:30 - loss: 1.6846 - regression_loss: 1.4076 - classification_loss: 0.2770
1383/1500 [==========================>...] - ETA: 1:29 - loss: 1.6847 - regression_loss: 1.4077 - classification_loss: 0.2770
1384/1500 [==========================>...] - ETA: 1:28 - loss: 1.6855 - regression_loss: 1.4083 - classification_loss: 0.2772
1385/1500 [==========================>...] - ETA: 1:28 - loss: 1.6853 - regression_loss: 1.4082 - classification_loss: 0.2771
1386/1500 [==========================>...] - ETA: 1:27 - loss: 1.6848 - regression_loss: 1.4078 - classification_loss: 0.2770
1387/1500 [==========================>...] - ETA: 1:26 - loss: 1.6851 - regression_loss: 1.4081 - classification_loss: 0.2770
1388/1500 [==========================>...] - ETA: 1:25 - loss: 1.6845 - regression_loss: 1.4076 - classification_loss: 0.2769
1389/1500 [==========================>...] - ETA: 1:25 - loss: 1.6845 - regression_loss: 1.4077 - classification_loss: 0.2768
1390/1500 [==========================>...] - ETA: 1:24 - loss: 1.6840 - regression_loss: 1.4073 - classification_loss: 0.2767
1391/1500 [==========================>...] - ETA: 1:23 - loss: 1.6839 - regression_loss: 1.4072 - classification_loss: 0.2767
1392/1500 [==========================>...] - ETA: 1:22 - loss: 1.6845 - regression_loss: 1.4071 - classification_loss: 0.2773
1393/1500 [==========================>...] - ETA: 1:22 - loss: 1.6841 - regression_loss: 1.4068 - classification_loss: 0.2773
1394/1500 [==========================>...] - ETA: 1:21 - loss: 1.6842 - regression_loss: 1.4070 - classification_loss: 0.2772
1395/1500 [==========================>...] - ETA: 1:20 - loss: 1.6848 - regression_loss: 1.4076 - classification_loss: 0.2773
1396/1500 [==========================>...] - ETA: 1:19 - loss: 1.6848 - regression_loss: 1.4076 - classification_loss: 0.2772
1397/1500 [==========================>...] - ETA: 1:19 - loss: 1.6848 - regression_loss: 1.4075 - classification_loss: 0.2773
1398/1500 [==========================>...] - ETA: 1:18 - loss: 1.6844 - regression_loss: 1.4071 - classification_loss: 0.2773
1399/1500 [==========================>...] - ETA: 1:17 - loss: 1.6850 - regression_loss: 1.4076 - classification_loss: 0.2774
1400/1500 [===========================>..] - ETA: 1:16 - loss: 1.6850 - regression_loss: 1.4076 - classification_loss: 0.2774
1401/1500 [===========================>..] - ETA: 1:16 - loss: 1.6848 - regression_loss: 1.4075 - classification_loss: 0.2773
1402/1500 [===========================>..] - ETA: 1:15 - loss: 1.6851 - regression_loss: 1.4078 - classification_loss: 0.2773
1403/1500 [===========================>..] - ETA: 1:14 - loss: 1.6852 - regression_loss: 1.4078 - classification_loss: 0.2774
1404/1500 [===========================>..] - ETA: 1:13 - loss: 1.6851 - regression_loss: 1.4077 - classification_loss: 0.2774
1405/1500 [===========================>..] - ETA: 1:13 - loss: 1.6849 - regression_loss: 1.4076 - classification_loss: 0.2774
1406/1500 [===========================>..] - ETA: 1:12 - loss: 1.6857 - regression_loss: 1.4080 - classification_loss: 0.2776
1407/1500 [===========================>..] - ETA: 1:11 - loss: 1.6858 - regression_loss: 1.4082 - classification_loss: 0.2776
1408/1500 [===========================>..] - ETA: 1:10 - loss: 1.6853 - regression_loss: 1.4078 - classification_loss: 0.2775
1409/1500 [===========================>..] - ETA: 1:09 - loss: 1.6852 - regression_loss: 1.4077 - classification_loss: 0.2775
1410/1500 [===========================>..] - ETA: 1:09 - loss: 1.6857 - regression_loss: 1.4080 - classification_loss: 0.2777
1411/1500 [===========================>..] - ETA: 1:08 - loss: 1.6853 - regression_loss: 1.4078 - classification_loss: 0.2775
1412/1500 [===========================>..] - ETA: 1:07 - loss: 1.6851 - regression_loss: 1.4077 - classification_loss: 0.2775
1413/1500 [===========================>..] - ETA: 1:06 - loss: 1.6862 - regression_loss: 1.4087 - classification_loss: 0.2776
1414/1500 [===========================>..] - ETA: 1:06 - loss: 1.6860 - regression_loss: 1.4085 - classification_loss: 0.2775
1415/1500 [===========================>..] - ETA: 1:05 - loss: 1.6862 - regression_loss: 1.4087 - classification_loss: 0.2776
1416/1500 [===========================>..] - ETA: 1:04 - loss: 1.6863 - regression_loss: 1.4088 - classification_loss: 0.2775
1417/1500 [===========================>..] - ETA: 1:03 - loss: 1.6855 - regression_loss: 1.4081 - classification_loss: 0.2774
1418/1500 [===========================>..] - ETA: 1:03 - loss: 1.6852 - regression_loss: 1.4079 - classification_loss: 0.2774
1419/1500 [===========================>..] - ETA: 1:02 - loss: 1.6848 - regression_loss: 1.4076 - classification_loss: 0.2773
1420/1500 [===========================>..] - ETA: 1:01 - loss: 1.6847 - regression_loss: 1.4074 - classification_loss: 0.2772
1421/1500 [===========================>..] - ETA: 1:00 - loss: 1.6843 - regression_loss: 1.4071 - classification_loss: 0.2773
1422/1500 [===========================>..] - ETA: 59s - loss: 1.6838 - regression_loss: 1.4067 - classification_loss: 0.2771 
1423/1500 [===========================>..] - ETA: 59s - loss: 1.6839 - regression_loss: 1.4068 - classification_loss: 0.2771
1424/1500 [===========================>..] - ETA: 58s - loss: 1.6834 - regression_loss: 1.4064 - classification_loss: 0.2771
1425/1500 [===========================>..] - ETA: 57s - loss: 1.6840 - regression_loss: 1.4067 - classification_loss: 0.2772
1426/1500 [===========================>..] - ETA: 56s - loss: 1.6840 - regression_loss: 1.4068 - classification_loss: 0.2772
1427/1500 [===========================>..] - ETA: 56s - loss: 1.6835 - regression_loss: 1.4063 - classification_loss: 0.2771
1428/1500 [===========================>..] - ETA: 55s - loss: 1.6834 - regression_loss: 1.4063 - classification_loss: 0.2771
1429/1500 [===========================>..] - ETA: 54s - loss: 1.6846 - regression_loss: 1.4072 - classification_loss: 0.2773
1430/1500 [===========================>..] - ETA: 53s - loss: 1.6844 - regression_loss: 1.4070 - classification_loss: 0.2774
1431/1500 [===========================>..] - ETA: 53s - loss: 1.6846 - regression_loss: 1.4072 - classification_loss: 0.2774
1432/1500 [===========================>..] - ETA: 52s - loss: 1.6845 - regression_loss: 1.4072 - classification_loss: 0.2773
1433/1500 [===========================>..] - ETA: 51s - loss: 1.6837 - regression_loss: 1.4065 - classification_loss: 0.2772
1434/1500 [===========================>..] - ETA: 50s - loss: 1.6837 - regression_loss: 1.4066 - classification_loss: 0.2771
1435/1500 [===========================>..] - ETA: 49s - loss: 1.6837 - regression_loss: 1.4066 - classification_loss: 0.2771
1436/1500 [===========================>..] - ETA: 49s - loss: 1.6838 - regression_loss: 1.4067 - classification_loss: 0.2771
1437/1500 [===========================>..] - ETA: 48s - loss: 1.6835 - regression_loss: 1.4065 - classification_loss: 0.2770
1438/1500 [===========================>..] - ETA: 47s - loss: 1.6830 - regression_loss: 1.4061 - classification_loss: 0.2769
1439/1500 [===========================>..] - ETA: 46s - loss: 1.6824 - regression_loss: 1.4056 - classification_loss: 0.2768
1440/1500 [===========================>..] - ETA: 46s - loss: 1.6821 - regression_loss: 1.4054 - classification_loss: 0.2768
1441/1500 [===========================>..] - ETA: 45s - loss: 1.6816 - regression_loss: 1.4049 - classification_loss: 0.2766
1442/1500 [===========================>..] - ETA: 44s - loss: 1.6811 - regression_loss: 1.4046 - classification_loss: 0.2765
1443/1500 [===========================>..] - ETA: 43s - loss: 1.6815 - regression_loss: 1.4050 - classification_loss: 0.2765
1444/1500 [===========================>..] - ETA: 42s - loss: 1.6812 - regression_loss: 1.4047 - classification_loss: 0.2766
1445/1500 [===========================>..] - ETA: 42s - loss: 1.6810 - regression_loss: 1.4044 - classification_loss: 0.2765
1446/1500 [===========================>..] - ETA: 41s - loss: 1.6811 - regression_loss: 1.4046 - classification_loss: 0.2765
1447/1500 [===========================>..] - ETA: 40s - loss: 1.6808 - regression_loss: 1.4045 - classification_loss: 0.2764
1448/1500 [===========================>..] - ETA: 39s - loss: 1.6811 - regression_loss: 1.4048 - classification_loss: 0.2763
1449/1500 [===========================>..] - ETA: 39s - loss: 1.6816 - regression_loss: 1.4052 - classification_loss: 0.2764
1450/1500 [============================>.] - ETA: 38s - loss: 1.6815 - regression_loss: 1.4052 - classification_loss: 0.2763
1451/1500 [============================>.] - ETA: 37s - loss: 1.6811 - regression_loss: 1.4049 - classification_loss: 0.2762
1452/1500 [============================>.] - ETA: 36s - loss: 1.6811 - regression_loss: 1.4050 - classification_loss: 0.2761
1453/1500 [============================>.] - ETA: 36s - loss: 1.6803 - regression_loss: 1.4044 - classification_loss: 0.2760
1454/1500 [============================>.] - ETA: 35s - loss: 1.6801 - regression_loss: 1.4041 - classification_loss: 0.2760
1455/1500 [============================>.] - ETA: 34s - loss: 1.6796 - regression_loss: 1.4037 - classification_loss: 0.2758
1456/1500 [============================>.] - ETA: 33s - loss: 1.6789 - regression_loss: 1.4032 - classification_loss: 0.2757
1457/1500 [============================>.] - ETA: 32s - loss: 1.6786 - regression_loss: 1.4029 - classification_loss: 0.2756
1458/1500 [============================>.] - ETA: 32s - loss: 1.6788 - regression_loss: 1.4031 - classification_loss: 0.2757
1459/1500 [============================>.] - ETA: 31s - loss: 1.6787 - regression_loss: 1.4031 - classification_loss: 0.2756
1460/1500 [============================>.] - ETA: 30s - loss: 1.6787 - regression_loss: 1.4030 - classification_loss: 0.2757
1461/1500 [============================>.] - ETA: 29s - loss: 1.6782 - regression_loss: 1.4026 - classification_loss: 0.2756
1462/1500 [============================>.] - ETA: 29s - loss: 1.6778 - regression_loss: 1.4023 - classification_loss: 0.2755
1463/1500 [============================>.] - ETA: 28s - loss: 1.6774 - regression_loss: 1.4020 - classification_loss: 0.2754
1464/1500 [============================>.] - ETA: 27s - loss: 1.6780 - regression_loss: 1.4024 - classification_loss: 0.2755
1465/1500 [============================>.] - ETA: 26s - loss: 1.6772 - regression_loss: 1.4018 - classification_loss: 0.2754
1466/1500 [============================>.] - ETA: 26s - loss: 1.6770 - regression_loss: 1.4017 - classification_loss: 0.2753
1467/1500 [============================>.] - ETA: 25s - loss: 1.6768 - regression_loss: 1.4015 - classification_loss: 0.2753
1468/1500 [============================>.] - ETA: 24s - loss: 1.6764 - regression_loss: 1.4012 - classification_loss: 0.2752
1469/1500 [============================>.] - ETA: 23s - loss: 1.6766 - regression_loss: 1.4014 - classification_loss: 0.2752
1470/1500 [============================>.] - ETA: 22s - loss: 1.6761 - regression_loss: 1.4010 - classification_loss: 0.2751
1471/1500 [============================>.] - ETA: 22s - loss: 1.6761 - regression_loss: 1.4011 - classification_loss: 0.2751
1472/1500 [============================>.] - ETA: 21s - loss: 1.6759 - regression_loss: 1.4009 - classification_loss: 0.2751
1473/1500 [============================>.] - ETA: 20s - loss: 1.6761 - regression_loss: 1.4010 - classification_loss: 0.2751
1474/1500 [============================>.] - ETA: 19s - loss: 1.6763 - regression_loss: 1.4012 - classification_loss: 0.2751
1475/1500 [============================>.] - ETA: 19s - loss: 1.6760 - regression_loss: 1.4009 - classification_loss: 0.2751
1476/1500 [============================>.] - ETA: 18s - loss: 1.6754 - regression_loss: 1.4005 - classification_loss: 0.2749
1477/1500 [============================>.] - ETA: 17s - loss: 1.6759 - regression_loss: 1.4010 - classification_loss: 0.2750
1478/1500 [============================>.] - ETA: 16s - loss: 1.6758 - regression_loss: 1.4009 - classification_loss: 0.2749
1479/1500 [============================>.] - ETA: 16s - loss: 1.6756 - regression_loss: 1.4007 - classification_loss: 0.2749
1480/1500 [============================>.] - ETA: 15s - loss: 1.6749 - regression_loss: 1.4001 - classification_loss: 0.2748
1481/1500 [============================>.] - ETA: 14s - loss: 1.6742 - regression_loss: 1.3994 - classification_loss: 0.2747
1482/1500 [============================>.] - ETA: 13s - loss: 1.6741 - regression_loss: 1.3995 - classification_loss: 0.2747
1483/1500 [============================>.] - ETA: 13s - loss: 1.6738 - regression_loss: 1.3992 - classification_loss: 0.2746
1484/1500 [============================>.] - ETA: 12s - loss: 1.6742 - regression_loss: 1.3996 - classification_loss: 0.2746
1485/1500 [============================>.] - ETA: 11s - loss: 1.6749 - regression_loss: 1.4001 - classification_loss: 0.2748
1486/1500 [============================>.] - ETA: 10s - loss: 1.6744 - regression_loss: 1.3997 - classification_loss: 0.2747
1487/1500 [============================>.] - ETA: 9s - loss: 1.6739 - regression_loss: 1.3993 - classification_loss: 0.2746 
1488/1500 [============================>.] - ETA: 9s - loss: 1.6736 - regression_loss: 1.3991 - classification_loss: 0.2745
1489/1500 [============================>.] - ETA: 8s - loss: 1.6731 - regression_loss: 1.3987 - classification_loss: 0.2744
1490/1500 [============================>.] - ETA: 7s - loss: 1.6727 - regression_loss: 1.3984 - classification_loss: 0.2743
1491/1500 [============================>.] - ETA: 6s - loss: 1.6720 - regression_loss: 1.3978 - classification_loss: 0.2742
1492/1500 [============================>.] - ETA: 6s - loss: 1.6712 - regression_loss: 1.3972 - classification_loss: 0.2741
1493/1500 [============================>.] - ETA: 5s - loss: 1.6707 - regression_loss: 1.3967 - classification_loss: 0.2740
1494/1500 [============================>.] - ETA: 4s - loss: 1.6706 - regression_loss: 1.3967 - classification_loss: 0.2739
1495/1500 [============================>.] - ETA: 3s - loss: 1.6708 - regression_loss: 1.3968 - classification_loss: 0.2739
1496/1500 [============================>.] - ETA: 3s - loss: 1.6707 - regression_loss: 1.3968 - classification_loss: 0.2739
1497/1500 [============================>.] - ETA: 2s - loss: 1.6708 - regression_loss: 1.3969 - classification_loss: 0.2739
1498/1500 [============================>.] - ETA: 1s - loss: 1.6706 - regression_loss: 1.3967 - classification_loss: 0.2738
1499/1500 [============================>.] - ETA: 0s - loss: 1.6705 - regression_loss: 1.3966 - classification_loss: 0.2739
1500/1500 [==============================] - 1148s 765ms/step - loss: 1.6699 - regression_loss: 1.3961 - classification_loss: 0.2737

Epoch 00004: saving model to ./snapshots/resnet50_csv_04.h5
Epoch 5/10

   1/1500 [..............................] - ETA: 10:27 - loss: 1.8509 - regression_loss: 1.4946 - classification_loss: 0.3563
   2/1500 [..............................] - ETA: 10:10 - loss: 1.6947 - regression_loss: 1.4305 - classification_loss: 0.2642
   3/1500 [..............................] - ETA: 18:08 - loss: 1.9695 - regression_loss: 1.5341 - classification_loss: 0.4354
   4/1500 [..............................] - ETA: 16:25 - loss: 1.8654 - regression_loss: 1.5021 - classification_loss: 0.3632
   5/1500 [..............................] - ETA: 17:01 - loss: 1.6882 - regression_loss: 1.3503 - classification_loss: 0.3380
   6/1500 [..............................] - ETA: 22:06 - loss: 1.6467 - regression_loss: 1.3379 - classification_loss: 0.3087
   7/1500 [..............................] - ETA: 22:11 - loss: 1.7497 - regression_loss: 1.4231 - classification_loss: 0.3266
   8/1500 [..............................] - ETA: 20:25 - loss: 1.6963 - regression_loss: 1.3870 - classification_loss: 0.3094
   9/1500 [..............................] - ETA: 19:43 - loss: 1.7207 - regression_loss: 1.4099 - classification_loss: 0.3108
  10/1500 [..............................] - ETA: 19:26 - loss: 1.7144 - regression_loss: 1.4061 - classification_loss: 0.3082
  11/1500 [..............................] - ETA: 18:23 - loss: 1.7525 - regression_loss: 1.4568 - classification_loss: 0.2957
  12/1500 [..............................] - ETA: 19:17 - loss: 1.6680 - regression_loss: 1.3919 - classification_loss: 0.2760
  13/1500 [..............................] - ETA: 18:25 - loss: 1.6307 - regression_loss: 1.3658 - classification_loss: 0.2648
  14/1500 [..............................] - ETA: 20:18 - loss: 1.6025 - regression_loss: 1.3440 - classification_loss: 0.2585
  15/1500 [..............................] - ETA: 20:01 - loss: 1.6410 - regression_loss: 1.3851 - classification_loss: 0.2559
  16/1500 [..............................] - ETA: 19:51 - loss: 1.5718 - regression_loss: 1.3286 - classification_loss: 0.2433
  17/1500 [..............................] - ETA: 20:33 - loss: 1.5650 - regression_loss: 1.3241 - classification_loss: 0.2409
  18/1500 [..............................] - ETA: 20:04 - loss: 1.5300 - regression_loss: 1.2892 - classification_loss: 0.2408
  19/1500 [..............................] - ETA: 19:36 - loss: 1.5695 - regression_loss: 1.3255 - classification_loss: 0.2440
  20/1500 [..............................] - ETA: 19:09 - loss: 1.5772 - regression_loss: 1.3375 - classification_loss: 0.2397
  21/1500 [..............................] - ETA: 19:48 - loss: 1.5699 - regression_loss: 1.3335 - classification_loss: 0.2364
  22/1500 [..............................] - ETA: 19:48 - loss: 1.5409 - regression_loss: 1.3115 - classification_loss: 0.2294
  23/1500 [..............................] - ETA: 20:21 - loss: 1.5722 - regression_loss: 1.3357 - classification_loss: 0.2366
  24/1500 [..............................] - ETA: 20:19 - loss: 1.5562 - regression_loss: 1.3241 - classification_loss: 0.2320
  25/1500 [..............................] - ETA: 19:59 - loss: 1.5730 - regression_loss: 1.3204 - classification_loss: 0.2526
  26/1500 [..............................] - ETA: 19:36 - loss: 1.5716 - regression_loss: 1.3198 - classification_loss: 0.2519
  27/1500 [..............................] - ETA: 19:47 - loss: 1.6097 - regression_loss: 1.3464 - classification_loss: 0.2633
  28/1500 [..............................] - ETA: 19:26 - loss: 1.6073 - regression_loss: 1.3457 - classification_loss: 0.2616
  29/1500 [..............................] - ETA: 19:43 - loss: 1.5914 - regression_loss: 1.3337 - classification_loss: 0.2577
  30/1500 [..............................] - ETA: 19:21 - loss: 1.5968 - regression_loss: 1.3424 - classification_loss: 0.2544
  31/1500 [..............................] - ETA: 19:35 - loss: 1.6197 - regression_loss: 1.3616 - classification_loss: 0.2581
  32/1500 [..............................] - ETA: 19:12 - loss: 1.6865 - regression_loss: 1.4122 - classification_loss: 0.2743
  33/1500 [..............................] - ETA: 18:54 - loss: 1.6769 - regression_loss: 1.4036 - classification_loss: 0.2733
  34/1500 [..............................] - ETA: 18:53 - loss: 1.6759 - regression_loss: 1.4022 - classification_loss: 0.2737
  35/1500 [..............................] - ETA: 18:38 - loss: 1.6737 - regression_loss: 1.4009 - classification_loss: 0.2728
  36/1500 [..............................] - ETA: 18:30 - loss: 1.6770 - regression_loss: 1.4059 - classification_loss: 0.2711
  37/1500 [..............................] - ETA: 18:54 - loss: 1.6860 - regression_loss: 1.4132 - classification_loss: 0.2727
  38/1500 [..............................] - ETA: 18:39 - loss: 1.6792 - regression_loss: 1.4081 - classification_loss: 0.2711
  39/1500 [..............................] - ETA: 19:01 - loss: 1.6706 - regression_loss: 1.3999 - classification_loss: 0.2707
  40/1500 [..............................] - ETA: 18:47 - loss: 1.6537 - regression_loss: 1.3878 - classification_loss: 0.2659
  41/1500 [..............................] - ETA: 18:34 - loss: 1.6341 - regression_loss: 1.3711 - classification_loss: 0.2630
  42/1500 [..............................] - ETA: 18:22 - loss: 1.6184 - regression_loss: 1.3575 - classification_loss: 0.2608
  43/1500 [..............................] - ETA: 18:30 - loss: 1.6414 - regression_loss: 1.3760 - classification_loss: 0.2654
  44/1500 [..............................] - ETA: 18:21 - loss: 1.6484 - regression_loss: 1.3855 - classification_loss: 0.2629
  45/1500 [..............................] - ETA: 18:22 - loss: 1.6393 - regression_loss: 1.3760 - classification_loss: 0.2633
  46/1500 [..............................] - ETA: 18:32 - loss: 1.6457 - regression_loss: 1.3859 - classification_loss: 0.2598
  47/1500 [..............................] - ETA: 18:51 - loss: 1.6511 - regression_loss: 1.3895 - classification_loss: 0.2616
  48/1500 [..............................] - ETA: 18:39 - loss: 1.6695 - regression_loss: 1.4047 - classification_loss: 0.2649
  49/1500 [..............................] - ETA: 18:41 - loss: 1.6685 - regression_loss: 1.4035 - classification_loss: 0.2650
  50/1500 [>.............................] - ETA: 18:35 - loss: 1.6790 - regression_loss: 1.4158 - classification_loss: 0.2632
  51/1500 [>.............................] - ETA: 18:31 - loss: 1.6599 - regression_loss: 1.3999 - classification_loss: 0.2600
  52/1500 [>.............................] - ETA: 18:25 - loss: 1.6434 - regression_loss: 1.3865 - classification_loss: 0.2569
  53/1500 [>.............................] - ETA: 18:16 - loss: 1.6562 - regression_loss: 1.3985 - classification_loss: 0.2577
  54/1500 [>.............................] - ETA: 18:06 - loss: 1.6487 - regression_loss: 1.3931 - classification_loss: 0.2556
  55/1500 [>.............................] - ETA: 18:02 - loss: 1.6640 - regression_loss: 1.4017 - classification_loss: 0.2623
  56/1500 [>.............................] - ETA: 17:59 - loss: 1.6713 - regression_loss: 1.4064 - classification_loss: 0.2650
  57/1500 [>.............................] - ETA: 17:54 - loss: 1.6548 - regression_loss: 1.3937 - classification_loss: 0.2611
  58/1500 [>.............................] - ETA: 18:13 - loss: 1.6503 - regression_loss: 1.3908 - classification_loss: 0.2595
  59/1500 [>.............................] - ETA: 18:04 - loss: 1.6386 - regression_loss: 1.3822 - classification_loss: 0.2565
  60/1500 [>.............................] - ETA: 17:55 - loss: 1.6565 - regression_loss: 1.3979 - classification_loss: 0.2587
  61/1500 [>.............................] - ETA: 17:59 - loss: 1.6533 - regression_loss: 1.3964 - classification_loss: 0.2569
  62/1500 [>.............................] - ETA: 17:49 - loss: 1.6457 - regression_loss: 1.3893 - classification_loss: 0.2564
  63/1500 [>.............................] - ETA: 18:00 - loss: 1.6598 - regression_loss: 1.4017 - classification_loss: 0.2581
  64/1500 [>.............................] - ETA: 17:52 - loss: 1.6523 - regression_loss: 1.3928 - classification_loss: 0.2595
  65/1500 [>.............................] - ETA: 18:07 - loss: 1.6539 - regression_loss: 1.3928 - classification_loss: 0.2612
  66/1500 [>.............................] - ETA: 17:58 - loss: 1.6452 - regression_loss: 1.3859 - classification_loss: 0.2592
  67/1500 [>.............................] - ETA: 17:51 - loss: 1.6519 - regression_loss: 1.3923 - classification_loss: 0.2596
  68/1500 [>.............................] - ETA: 17:58 - loss: 1.6508 - regression_loss: 1.3907 - classification_loss: 0.2601
  69/1500 [>.............................] - ETA: 17:54 - loss: 1.6397 - regression_loss: 1.3821 - classification_loss: 0.2576
  70/1500 [>.............................] - ETA: 17:46 - loss: 1.6326 - regression_loss: 1.3772 - classification_loss: 0.2554
  71/1500 [>.............................] - ETA: 17:51 - loss: 1.6284 - regression_loss: 1.3736 - classification_loss: 0.2548
  72/1500 [>.............................] - ETA: 17:58 - loss: 1.6250 - regression_loss: 1.3717 - classification_loss: 0.2534
  73/1500 [>.............................] - ETA: 18:00 - loss: 1.6409 - regression_loss: 1.3833 - classification_loss: 0.2576
  74/1500 [>.............................] - ETA: 18:01 - loss: 1.6314 - regression_loss: 1.3750 - classification_loss: 0.2564
  75/1500 [>.............................] - ETA: 18:04 - loss: 1.6251 - regression_loss: 1.3705 - classification_loss: 0.2547
  76/1500 [>.............................] - ETA: 17:56 - loss: 1.6437 - regression_loss: 1.3843 - classification_loss: 0.2595
  77/1500 [>.............................] - ETA: 17:52 - loss: 1.6599 - regression_loss: 1.3928 - classification_loss: 0.2671
  78/1500 [>.............................] - ETA: 17:44 - loss: 1.6535 - regression_loss: 1.3888 - classification_loss: 0.2647
  79/1500 [>.............................] - ETA: 17:42 - loss: 1.6418 - regression_loss: 1.3796 - classification_loss: 0.2622
  80/1500 [>.............................] - ETA: 17:35 - loss: 1.6350 - regression_loss: 1.3743 - classification_loss: 0.2607
  81/1500 [>.............................] - ETA: 17:37 - loss: 1.6397 - regression_loss: 1.3782 - classification_loss: 0.2616
  82/1500 [>.............................] - ETA: 17:30 - loss: 1.6289 - regression_loss: 1.3696 - classification_loss: 0.2592
  83/1500 [>.............................] - ETA: 17:25 - loss: 1.6273 - regression_loss: 1.3692 - classification_loss: 0.2581
  84/1500 [>.............................] - ETA: 17:20 - loss: 1.6145 - regression_loss: 1.3585 - classification_loss: 0.2560
  85/1500 [>.............................] - ETA: 17:17 - loss: 1.6087 - regression_loss: 1.3545 - classification_loss: 0.2542
  86/1500 [>.............................] - ETA: 17:13 - loss: 1.6202 - regression_loss: 1.3652 - classification_loss: 0.2550
  87/1500 [>.............................] - ETA: 17:13 - loss: 1.6258 - regression_loss: 1.3695 - classification_loss: 0.2564
  88/1500 [>.............................] - ETA: 17:07 - loss: 1.6259 - regression_loss: 1.3702 - classification_loss: 0.2558
  89/1500 [>.............................] - ETA: 17:02 - loss: 1.6410 - regression_loss: 1.3819 - classification_loss: 0.2591
  90/1500 [>.............................] - ETA: 16:56 - loss: 1.6438 - regression_loss: 1.3849 - classification_loss: 0.2589
  91/1500 [>.............................] - ETA: 16:51 - loss: 1.6379 - regression_loss: 1.3807 - classification_loss: 0.2572
  92/1500 [>.............................] - ETA: 17:09 - loss: 1.6453 - regression_loss: 1.3878 - classification_loss: 0.2575
  93/1500 [>.............................] - ETA: 17:06 - loss: 1.6401 - regression_loss: 1.3831 - classification_loss: 0.2570
  94/1500 [>.............................] - ETA: 17:01 - loss: 1.6465 - regression_loss: 1.3880 - classification_loss: 0.2585
  95/1500 [>.............................] - ETA: 16:55 - loss: 1.6531 - regression_loss: 1.3926 - classification_loss: 0.2606
  96/1500 [>.............................] - ETA: 16:50 - loss: 1.6461 - regression_loss: 1.3872 - classification_loss: 0.2590
  97/1500 [>.............................] - ETA: 17:04 - loss: 1.6451 - regression_loss: 1.3869 - classification_loss: 0.2582
  98/1500 [>.............................] - ETA: 17:07 - loss: 1.6383 - regression_loss: 1.3810 - classification_loss: 0.2573
  99/1500 [>.............................] - ETA: 17:11 - loss: 1.6311 - regression_loss: 1.3752 - classification_loss: 0.2559
 100/1500 [=>............................] - ETA: 17:09 - loss: 1.6300 - regression_loss: 1.3754 - classification_loss: 0.2547
 101/1500 [=>............................] - ETA: 17:31 - loss: 1.6533 - regression_loss: 1.3803 - classification_loss: 0.2730
 102/1500 [=>............................] - ETA: 17:40 - loss: 1.6442 - regression_loss: 1.3730 - classification_loss: 0.2712
 103/1500 [=>............................] - ETA: 17:37 - loss: 1.6414 - regression_loss: 1.3715 - classification_loss: 0.2699
 104/1500 [=>............................] - ETA: 17:32 - loss: 1.6398 - regression_loss: 1.3692 - classification_loss: 0.2705
 105/1500 [=>............................] - ETA: 17:33 - loss: 1.6454 - regression_loss: 1.3727 - classification_loss: 0.2728
 106/1500 [=>............................] - ETA: 17:51 - loss: 1.6481 - regression_loss: 1.3743 - classification_loss: 0.2737
 107/1500 [=>............................] - ETA: 17:46 - loss: 1.6483 - regression_loss: 1.3746 - classification_loss: 0.2737
 108/1500 [=>............................] - ETA: 17:47 - loss: 1.6495 - regression_loss: 1.3767 - classification_loss: 0.2728
 109/1500 [=>............................] - ETA: 17:42 - loss: 1.6518 - regression_loss: 1.3791 - classification_loss: 0.2727
 110/1500 [=>............................] - ETA: 17:37 - loss: 1.6635 - regression_loss: 1.3884 - classification_loss: 0.2751
 111/1500 [=>............................] - ETA: 17:41 - loss: 1.6628 - regression_loss: 1.3882 - classification_loss: 0.2746
 112/1500 [=>............................] - ETA: 17:49 - loss: 1.6680 - regression_loss: 1.3919 - classification_loss: 0.2761
 113/1500 [=>............................] - ETA: 17:53 - loss: 1.6601 - regression_loss: 1.3857 - classification_loss: 0.2744
 114/1500 [=>............................] - ETA: 17:48 - loss: 1.6562 - regression_loss: 1.3821 - classification_loss: 0.2741
 115/1500 [=>............................] - ETA: 17:44 - loss: 1.6632 - regression_loss: 1.3889 - classification_loss: 0.2742
 116/1500 [=>............................] - ETA: 17:47 - loss: 1.6695 - regression_loss: 1.3931 - classification_loss: 0.2763
 117/1500 [=>............................] - ETA: 17:43 - loss: 1.6628 - regression_loss: 1.3878 - classification_loss: 0.2751
 118/1500 [=>............................] - ETA: 17:38 - loss: 1.6650 - regression_loss: 1.3892 - classification_loss: 0.2758
 119/1500 [=>............................] - ETA: 17:33 - loss: 1.6640 - regression_loss: 1.3896 - classification_loss: 0.2744
 120/1500 [=>............................] - ETA: 17:28 - loss: 1.6586 - regression_loss: 1.3858 - classification_loss: 0.2728
 121/1500 [=>............................] - ETA: 17:23 - loss: 1.6541 - regression_loss: 1.3813 - classification_loss: 0.2727
 122/1500 [=>............................] - ETA: 17:24 - loss: 1.6597 - regression_loss: 1.3845 - classification_loss: 0.2752
 123/1500 [=>............................] - ETA: 17:32 - loss: 1.6541 - regression_loss: 1.3802 - classification_loss: 0.2739
 124/1500 [=>............................] - ETA: 17:33 - loss: 1.6577 - regression_loss: 1.3826 - classification_loss: 0.2751
 125/1500 [=>............................] - ETA: 17:28 - loss: 1.6585 - regression_loss: 1.3844 - classification_loss: 0.2741
 126/1500 [=>............................] - ETA: 17:31 - loss: 1.6605 - regression_loss: 1.3864 - classification_loss: 0.2741
 127/1500 [=>............................] - ETA: 17:26 - loss: 1.6541 - regression_loss: 1.3813 - classification_loss: 0.2728
 128/1500 [=>............................] - ETA: 17:22 - loss: 1.6615 - regression_loss: 1.3865 - classification_loss: 0.2750
 129/1500 [=>............................] - ETA: 17:25 - loss: 1.6575 - regression_loss: 1.3838 - classification_loss: 0.2737
 130/1500 [=>............................] - ETA: 17:31 - loss: 1.6620 - regression_loss: 1.3876 - classification_loss: 0.2744
 131/1500 [=>............................] - ETA: 17:27 - loss: 1.6636 - regression_loss: 1.3895 - classification_loss: 0.2741
 132/1500 [=>............................] - ETA: 17:22 - loss: 1.6603 - regression_loss: 1.3871 - classification_loss: 0.2732
 133/1500 [=>............................] - ETA: 17:18 - loss: 1.6560 - regression_loss: 1.3840 - classification_loss: 0.2721
 134/1500 [=>............................] - ETA: 17:18 - loss: 1.6560 - regression_loss: 1.3847 - classification_loss: 0.2713
 135/1500 [=>............................] - ETA: 17:15 - loss: 1.6639 - regression_loss: 1.3892 - classification_loss: 0.2747
 136/1500 [=>............................] - ETA: 17:20 - loss: 1.6705 - regression_loss: 1.3930 - classification_loss: 0.2775
 137/1500 [=>............................] - ETA: 17:18 - loss: 1.6777 - regression_loss: 1.3999 - classification_loss: 0.2777
 138/1500 [=>............................] - ETA: 17:24 - loss: 1.6808 - regression_loss: 1.4016 - classification_loss: 0.2792
 139/1500 [=>............................] - ETA: 17:24 - loss: 1.6824 - regression_loss: 1.4034 - classification_loss: 0.2790
 140/1500 [=>............................] - ETA: 17:21 - loss: 1.6825 - regression_loss: 1.4041 - classification_loss: 0.2784
 141/1500 [=>............................] - ETA: 17:16 - loss: 1.6835 - regression_loss: 1.4045 - classification_loss: 0.2789
 142/1500 [=>............................] - ETA: 17:19 - loss: 1.6879 - regression_loss: 1.4077 - classification_loss: 0.2803
 143/1500 [=>............................] - ETA: 17:19 - loss: 1.6799 - regression_loss: 1.4012 - classification_loss: 0.2787
 144/1500 [=>............................] - ETA: 17:17 - loss: 1.6777 - regression_loss: 1.4001 - classification_loss: 0.2777
 145/1500 [=>............................] - ETA: 17:22 - loss: 1.6720 - regression_loss: 1.3955 - classification_loss: 0.2765
 146/1500 [=>............................] - ETA: 17:27 - loss: 1.6794 - regression_loss: 1.4008 - classification_loss: 0.2786
 147/1500 [=>............................] - ETA: 17:30 - loss: 1.6728 - regression_loss: 1.3956 - classification_loss: 0.2772
 148/1500 [=>............................] - ETA: 17:29 - loss: 1.6695 - regression_loss: 1.3933 - classification_loss: 0.2762
 149/1500 [=>............................] - ETA: 17:25 - loss: 1.6679 - regression_loss: 1.3925 - classification_loss: 0.2754
 150/1500 [==>...........................] - ETA: 17:24 - loss: 1.6638 - regression_loss: 1.3893 - classification_loss: 0.2745
 151/1500 [==>...........................] - ETA: 17:20 - loss: 1.6585 - regression_loss: 1.3854 - classification_loss: 0.2731
 152/1500 [==>...........................] - ETA: 17:27 - loss: 1.6610 - regression_loss: 1.3872 - classification_loss: 0.2738
 153/1500 [==>...........................] - ETA: 17:34 - loss: 1.6600 - regression_loss: 1.3870 - classification_loss: 0.2730
 154/1500 [==>...........................] - ETA: 17:32 - loss: 1.6574 - regression_loss: 1.3854 - classification_loss: 0.2720
 155/1500 [==>...........................] - ETA: 17:36 - loss: 1.6615 - regression_loss: 1.3890 - classification_loss: 0.2725
 156/1500 [==>...........................] - ETA: 17:32 - loss: 1.6619 - regression_loss: 1.3900 - classification_loss: 0.2719
 157/1500 [==>...........................] - ETA: 17:27 - loss: 1.6591 - regression_loss: 1.3876 - classification_loss: 0.2715
 158/1500 [==>...........................] - ETA: 17:23 - loss: 1.6561 - regression_loss: 1.3848 - classification_loss: 0.2713
 159/1500 [==>...........................] - ETA: 17:19 - loss: 1.6540 - regression_loss: 1.3819 - classification_loss: 0.2721
 160/1500 [==>...........................] - ETA: 17:15 - loss: 1.6549 - regression_loss: 1.3828 - classification_loss: 0.2721
 161/1500 [==>...........................] - ETA: 17:11 - loss: 1.6496 - regression_loss: 1.3786 - classification_loss: 0.2709
 162/1500 [==>...........................] - ETA: 17:08 - loss: 1.6510 - regression_loss: 1.3795 - classification_loss: 0.2715
 163/1500 [==>...........................] - ETA: 17:05 - loss: 1.6543 - regression_loss: 1.3820 - classification_loss: 0.2723
 164/1500 [==>...........................] - ETA: 17:01 - loss: 1.6613 - regression_loss: 1.3876 - classification_loss: 0.2736
 165/1500 [==>...........................] - ETA: 17:00 - loss: 1.6593 - regression_loss: 1.3859 - classification_loss: 0.2734
 166/1500 [==>...........................] - ETA: 16:56 - loss: 1.6571 - regression_loss: 1.3847 - classification_loss: 0.2724
 167/1500 [==>...........................] - ETA: 16:55 - loss: 1.6552 - regression_loss: 1.3835 - classification_loss: 0.2717
 168/1500 [==>...........................] - ETA: 16:56 - loss: 1.6545 - regression_loss: 1.3831 - classification_loss: 0.2714
 169/1500 [==>...........................] - ETA: 16:57 - loss: 1.6587 - regression_loss: 1.3871 - classification_loss: 0.2715
 170/1500 [==>...........................] - ETA: 17:01 - loss: 1.6567 - regression_loss: 1.3854 - classification_loss: 0.2713
 171/1500 [==>...........................] - ETA: 16:58 - loss: 1.6583 - regression_loss: 1.3863 - classification_loss: 0.2720
 172/1500 [==>...........................] - ETA: 16:59 - loss: 1.6568 - regression_loss: 1.3855 - classification_loss: 0.2713
 173/1500 [==>...........................] - ETA: 16:58 - loss: 1.6576 - regression_loss: 1.3859 - classification_loss: 0.2717
 174/1500 [==>...........................] - ETA: 16:56 - loss: 1.6540 - regression_loss: 1.3827 - classification_loss: 0.2712
 175/1500 [==>...........................] - ETA: 16:53 - loss: 1.6623 - regression_loss: 1.3891 - classification_loss: 0.2732
 176/1500 [==>...........................] - ETA: 16:59 - loss: 1.6605 - regression_loss: 1.3872 - classification_loss: 0.2733
 177/1500 [==>...........................] - ETA: 16:56 - loss: 1.6592 - regression_loss: 1.3865 - classification_loss: 0.2726
 178/1500 [==>...........................] - ETA: 16:53 - loss: 1.6550 - regression_loss: 1.3833 - classification_loss: 0.2716
 179/1500 [==>...........................] - ETA: 16:57 - loss: 1.6523 - regression_loss: 1.3813 - classification_loss: 0.2709
 180/1500 [==>...........................] - ETA: 17:11 - loss: 1.6518 - regression_loss: 1.3808 - classification_loss: 0.2711
 181/1500 [==>...........................] - ETA: 17:12 - loss: 1.6594 - regression_loss: 1.3858 - classification_loss: 0.2736
 182/1500 [==>...........................] - ETA: 17:08 - loss: 1.6620 - regression_loss: 1.3874 - classification_loss: 0.2746
 183/1500 [==>...........................] - ETA: 17:04 - loss: 1.6601 - regression_loss: 1.3857 - classification_loss: 0.2744
 184/1500 [==>...........................] - ETA: 17:06 - loss: 1.6582 - regression_loss: 1.3835 - classification_loss: 0.2747
 185/1500 [==>...........................] - ETA: 17:02 - loss: 1.6529 - regression_loss: 1.3791 - classification_loss: 0.2738
 186/1500 [==>...........................] - ETA: 16:59 - loss: 1.6512 - regression_loss: 1.3781 - classification_loss: 0.2730
 187/1500 [==>...........................] - ETA: 16:58 - loss: 1.6502 - regression_loss: 1.3779 - classification_loss: 0.2723
 188/1500 [==>...........................] - ETA: 16:54 - loss: 1.6452 - regression_loss: 1.3741 - classification_loss: 0.2711
 189/1500 [==>...........................] - ETA: 16:55 - loss: 1.6472 - regression_loss: 1.3768 - classification_loss: 0.2705
 190/1500 [==>...........................] - ETA: 16:51 - loss: 1.6467 - regression_loss: 1.3764 - classification_loss: 0.2702
 191/1500 [==>...........................] - ETA: 16:55 - loss: 1.6483 - regression_loss: 1.3785 - classification_loss: 0.2698
 192/1500 [==>...........................] - ETA: 16:52 - loss: 1.6451 - regression_loss: 1.3761 - classification_loss: 0.2690
 193/1500 [==>...........................] - ETA: 16:48 - loss: 1.6440 - regression_loss: 1.3753 - classification_loss: 0.2688
 194/1500 [==>...........................] - ETA: 16:48 - loss: 1.6426 - regression_loss: 1.3742 - classification_loss: 0.2684
 195/1500 [==>...........................] - ETA: 16:50 - loss: 1.6462 - regression_loss: 1.3767 - classification_loss: 0.2695
 196/1500 [==>...........................] - ETA: 16:54 - loss: 1.6477 - regression_loss: 1.3742 - classification_loss: 0.2735
 197/1500 [==>...........................] - ETA: 16:51 - loss: 1.6484 - regression_loss: 1.3747 - classification_loss: 0.2737
 198/1500 [==>...........................] - ETA: 16:48 - loss: 1.6464 - regression_loss: 1.3736 - classification_loss: 0.2728
 199/1500 [==>...........................] - ETA: 17:04 - loss: 1.6474 - regression_loss: 1.3740 - classification_loss: 0.2734
 200/1500 [===>..........................] - ETA: 17:04 - loss: 1.6527 - regression_loss: 1.3787 - classification_loss: 0.2740
 201/1500 [===>..........................] - ETA: 17:11 - loss: 1.6494 - regression_loss: 1.3762 - classification_loss: 0.2732
 202/1500 [===>..........................] - ETA: 17:09 - loss: 1.6530 - regression_loss: 1.3790 - classification_loss: 0.2741
 203/1500 [===>..........................] - ETA: 17:13 - loss: 1.6503 - regression_loss: 1.3766 - classification_loss: 0.2737
 204/1500 [===>..........................] - ETA: 17:15 - loss: 1.6470 - regression_loss: 1.3741 - classification_loss: 0.2730
 205/1500 [===>..........................] - ETA: 17:13 - loss: 1.6430 - regression_loss: 1.3709 - classification_loss: 0.2722
 206/1500 [===>..........................] - ETA: 17:12 - loss: 1.6399 - regression_loss: 1.3680 - classification_loss: 0.2720
 207/1500 [===>..........................] - ETA: 17:12 - loss: 1.6393 - regression_loss: 1.3676 - classification_loss: 0.2717
 208/1500 [===>..........................] - ETA: 17:09 - loss: 1.6374 - regression_loss: 1.3656 - classification_loss: 0.2719
 209/1500 [===>..........................] - ETA: 17:06 - loss: 1.6384 - regression_loss: 1.3666 - classification_loss: 0.2718
 210/1500 [===>..........................] - ETA: 17:03 - loss: 1.6401 - regression_loss: 1.3681 - classification_loss: 0.2720
 211/1500 [===>..........................] - ETA: 17:00 - loss: 1.6407 - regression_loss: 1.3689 - classification_loss: 0.2718
 212/1500 [===>..........................] - ETA: 17:01 - loss: 1.6413 - regression_loss: 1.3697 - classification_loss: 0.2715
 213/1500 [===>..........................] - ETA: 17:02 - loss: 1.6397 - regression_loss: 1.3685 - classification_loss: 0.2712
 214/1500 [===>..........................] - ETA: 17:01 - loss: 1.6450 - regression_loss: 1.3740 - classification_loss: 0.2710
 215/1500 [===>..........................] - ETA: 16:58 - loss: 1.6439 - regression_loss: 1.3733 - classification_loss: 0.2706
 216/1500 [===>..........................] - ETA: 16:55 - loss: 1.6418 - regression_loss: 1.3716 - classification_loss: 0.2702
 217/1500 [===>..........................] - ETA: 16:52 - loss: 1.6391 - regression_loss: 1.3695 - classification_loss: 0.2696
 218/1500 [===>..........................] - ETA: 16:49 - loss: 1.6438 - regression_loss: 1.3729 - classification_loss: 0.2709
 219/1500 [===>..........................] - ETA: 16:47 - loss: 1.6407 - regression_loss: 1.3703 - classification_loss: 0.2704
 220/1500 [===>..........................] - ETA: 16:46 - loss: 1.6458 - regression_loss: 1.3741 - classification_loss: 0.2717
 221/1500 [===>..........................] - ETA: 16:51 - loss: 1.6443 - regression_loss: 1.3721 - classification_loss: 0.2722
 222/1500 [===>..........................] - ETA: 16:54 - loss: 1.6455 - regression_loss: 1.3736 - classification_loss: 0.2718
 223/1500 [===>..........................] - ETA: 16:52 - loss: 1.6472 - regression_loss: 1.3757 - classification_loss: 0.2715
 224/1500 [===>..........................] - ETA: 16:49 - loss: 1.6479 - regression_loss: 1.3759 - classification_loss: 0.2720
 225/1500 [===>..........................] - ETA: 16:53 - loss: 1.6549 - regression_loss: 1.3796 - classification_loss: 0.2753
 226/1500 [===>..........................] - ETA: 16:50 - loss: 1.6534 - regression_loss: 1.3785 - classification_loss: 0.2750
 227/1500 [===>..........................] - ETA: 16:47 - loss: 1.6531 - regression_loss: 1.3787 - classification_loss: 0.2744
 228/1500 [===>..........................] - ETA: 16:44 - loss: 1.6526 - regression_loss: 1.3780 - classification_loss: 0.2747
 229/1500 [===>..........................] - ETA: 16:41 - loss: 1.6559 - regression_loss: 1.3808 - classification_loss: 0.2751
 230/1500 [===>..........................] - ETA: 16:41 - loss: 1.6521 - regression_loss: 1.3777 - classification_loss: 0.2744
 231/1500 [===>..........................] - ETA: 16:38 - loss: 1.6557 - regression_loss: 1.3802 - classification_loss: 0.2754
 232/1500 [===>..........................] - ETA: 16:36 - loss: 1.6548 - regression_loss: 1.3799 - classification_loss: 0.2748
 233/1500 [===>..........................] - ETA: 16:34 - loss: 1.6595 - regression_loss: 1.3831 - classification_loss: 0.2765
 234/1500 [===>..........................] - ETA: 16:32 - loss: 1.6642 - regression_loss: 1.3869 - classification_loss: 0.2773
 235/1500 [===>..........................] - ETA: 16:46 - loss: 1.6637 - regression_loss: 1.3866 - classification_loss: 0.2770
 236/1500 [===>..........................] - ETA: 16:43 - loss: 1.6636 - regression_loss: 1.3867 - classification_loss: 0.2769
 237/1500 [===>..........................] - ETA: 16:40 - loss: 1.6610 - regression_loss: 1.3847 - classification_loss: 0.2762
 238/1500 [===>..........................] - ETA: 16:38 - loss: 1.6583 - regression_loss: 1.3826 - classification_loss: 0.2757
 239/1500 [===>..........................] - ETA: 16:35 - loss: 1.6563 - regression_loss: 1.3811 - classification_loss: 0.2752
 240/1500 [===>..........................] - ETA: 16:32 - loss: 1.6569 - regression_loss: 1.3819 - classification_loss: 0.2751
 241/1500 [===>..........................] - ETA: 16:30 - loss: 1.6560 - regression_loss: 1.3811 - classification_loss: 0.2749
 242/1500 [===>..........................] - ETA: 16:27 - loss: 1.6551 - regression_loss: 1.3805 - classification_loss: 0.2746
 243/1500 [===>..........................] - ETA: 16:25 - loss: 1.6520 - regression_loss: 1.3781 - classification_loss: 0.2739
 244/1500 [===>..........................] - ETA: 16:23 - loss: 1.6483 - regression_loss: 1.3752 - classification_loss: 0.2731
 245/1500 [===>..........................] - ETA: 16:20 - loss: 1.6495 - regression_loss: 1.3766 - classification_loss: 0.2729
 246/1500 [===>..........................] - ETA: 16:20 - loss: 1.6535 - regression_loss: 1.3801 - classification_loss: 0.2733
 247/1500 [===>..........................] - ETA: 16:18 - loss: 1.6516 - regression_loss: 1.3788 - classification_loss: 0.2728
 248/1500 [===>..........................] - ETA: 16:18 - loss: 1.6483 - regression_loss: 1.3759 - classification_loss: 0.2723
 249/1500 [===>..........................] - ETA: 16:15 - loss: 1.6502 - regression_loss: 1.3783 - classification_loss: 0.2720
 250/1500 [====>.........................] - ETA: 16:13 - loss: 1.6491 - regression_loss: 1.3776 - classification_loss: 0.2715
 251/1500 [====>.........................] - ETA: 16:10 - loss: 1.6456 - regression_loss: 1.3746 - classification_loss: 0.2709
 252/1500 [====>.........................] - ETA: 16:10 - loss: 1.6434 - regression_loss: 1.3727 - classification_loss: 0.2707
 253/1500 [====>.........................] - ETA: 16:07 - loss: 1.6402 - regression_loss: 1.3702 - classification_loss: 0.2700
 254/1500 [====>.........................] - ETA: 16:07 - loss: 1.6390 - regression_loss: 1.3691 - classification_loss: 0.2698
 255/1500 [====>.........................] - ETA: 16:11 - loss: 1.6384 - regression_loss: 1.3691 - classification_loss: 0.2693
 256/1500 [====>.........................] - ETA: 16:08 - loss: 1.6399 - regression_loss: 1.3708 - classification_loss: 0.2691
 257/1500 [====>.........................] - ETA: 16:05 - loss: 1.6415 - regression_loss: 1.3722 - classification_loss: 0.2693
 258/1500 [====>.........................] - ETA: 16:05 - loss: 1.6395 - regression_loss: 1.3703 - classification_loss: 0.2692
 259/1500 [====>.........................] - ETA: 16:02 - loss: 1.6410 - regression_loss: 1.3716 - classification_loss: 0.2694
 260/1500 [====>.........................] - ETA: 16:00 - loss: 1.6425 - regression_loss: 1.3732 - classification_loss: 0.2693
 261/1500 [====>.........................] - ETA: 15:58 - loss: 1.6474 - regression_loss: 1.3769 - classification_loss: 0.2705
 262/1500 [====>.........................] - ETA: 15:56 - loss: 1.6462 - regression_loss: 1.3763 - classification_loss: 0.2699
 263/1500 [====>.........................] - ETA: 15:54 - loss: 1.6452 - regression_loss: 1.3758 - classification_loss: 0.2694
 264/1500 [====>.........................] - ETA: 15:53 - loss: 1.6423 - regression_loss: 1.3735 - classification_loss: 0.2688
 265/1500 [====>.........................] - ETA: 15:50 - loss: 1.6412 - regression_loss: 1.3726 - classification_loss: 0.2686
 266/1500 [====>.........................] - ETA: 15:49 - loss: 1.6398 - regression_loss: 1.3715 - classification_loss: 0.2682
 267/1500 [====>.........................] - ETA: 15:52 - loss: 1.6390 - regression_loss: 1.3698 - classification_loss: 0.2692
 268/1500 [====>.........................] - ETA: 15:54 - loss: 1.6395 - regression_loss: 1.3705 - classification_loss: 0.2691
 269/1500 [====>.........................] - ETA: 15:55 - loss: 1.6382 - regression_loss: 1.3691 - classification_loss: 0.2691
 270/1500 [====>.........................] - ETA: 15:54 - loss: 1.6384 - regression_loss: 1.3692 - classification_loss: 0.2691
 271/1500 [====>.........................] - ETA: 15:51 - loss: 1.6385 - regression_loss: 1.3693 - classification_loss: 0.2692
 272/1500 [====>.........................] - ETA: 15:52 - loss: 1.6357 - regression_loss: 1.3671 - classification_loss: 0.2686
 273/1500 [====>.........................] - ETA: 15:52 - loss: 1.6369 - regression_loss: 1.3683 - classification_loss: 0.2685
 274/1500 [====>.........................] - ETA: 15:49 - loss: 1.6357 - regression_loss: 1.3677 - classification_loss: 0.2680
 275/1500 [====>.........................] - ETA: 15:59 - loss: 1.6406 - regression_loss: 1.3719 - classification_loss: 0.2687
 276/1500 [====>.........................] - ETA: 15:59 - loss: 1.6383 - regression_loss: 1.3703 - classification_loss: 0.2680
 277/1500 [====>.........................] - ETA: 16:00 - loss: 1.6353 - regression_loss: 1.3678 - classification_loss: 0.2674
 278/1500 [====>.........................] - ETA: 16:02 - loss: 1.6358 - regression_loss: 1.3679 - classification_loss: 0.2680
 279/1500 [====>.........................] - ETA: 16:00 - loss: 1.6392 - regression_loss: 1.3710 - classification_loss: 0.2682
 280/1500 [====>.........................] - ETA: 16:02 - loss: 1.6437 - regression_loss: 1.3750 - classification_loss: 0.2687
 281/1500 [====>.........................] - ETA: 16:02 - loss: 1.6464 - regression_loss: 1.3771 - classification_loss: 0.2693
 282/1500 [====>.........................] - ETA: 16:13 - loss: 1.6486 - regression_loss: 1.3790 - classification_loss: 0.2696
 283/1500 [====>.........................] - ETA: 16:12 - loss: 1.6512 - regression_loss: 1.3810 - classification_loss: 0.2702
 284/1500 [====>.........................] - ETA: 16:12 - loss: 1.6525 - regression_loss: 1.3826 - classification_loss: 0.2699
 285/1500 [====>.........................] - ETA: 16:09 - loss: 1.6542 - regression_loss: 1.3844 - classification_loss: 0.2698
 286/1500 [====>.........................] - ETA: 16:09 - loss: 1.6562 - regression_loss: 1.3864 - classification_loss: 0.2698
 287/1500 [====>.........................] - ETA: 16:14 - loss: 1.6538 - regression_loss: 1.3845 - classification_loss: 0.2693
 288/1500 [====>.........................] - ETA: 16:14 - loss: 1.6559 - regression_loss: 1.3861 - classification_loss: 0.2699
 289/1500 [====>.........................] - ETA: 16:12 - loss: 1.6541 - regression_loss: 1.3846 - classification_loss: 0.2695
 290/1500 [====>.........................] - ETA: 16:09 - loss: 1.6591 - regression_loss: 1.3887 - classification_loss: 0.2704
 291/1500 [====>.........................] - ETA: 16:11 - loss: 1.6625 - regression_loss: 1.3915 - classification_loss: 0.2711
 292/1500 [====>.........................] - ETA: 16:11 - loss: 1.6594 - regression_loss: 1.3889 - classification_loss: 0.2704
 293/1500 [====>.........................] - ETA: 16:08 - loss: 1.6610 - regression_loss: 1.3903 - classification_loss: 0.2707
 294/1500 [====>.........................] - ETA: 16:10 - loss: 1.6614 - regression_loss: 1.3907 - classification_loss: 0.2707
 295/1500 [====>.........................] - ETA: 16:07 - loss: 1.6597 - regression_loss: 1.3895 - classification_loss: 0.2702
 296/1500 [====>.........................] - ETA: 16:07 - loss: 1.6562 - regression_loss: 1.3866 - classification_loss: 0.2696
 297/1500 [====>.........................] - ETA: 16:04 - loss: 1.6549 - regression_loss: 1.3859 - classification_loss: 0.2691
 298/1500 [====>.........................] - ETA: 16:04 - loss: 1.6534 - regression_loss: 1.3847 - classification_loss: 0.2687
 299/1500 [====>.........................] - ETA: 16:03 - loss: 1.6507 - regression_loss: 1.3826 - classification_loss: 0.2681
 300/1500 [=====>........................] - ETA: 16:01 - loss: 1.6496 - regression_loss: 1.3818 - classification_loss: 0.2678
 301/1500 [=====>........................] - ETA: 15:58 - loss: 1.6478 - regression_loss: 1.3804 - classification_loss: 0.2674
 302/1500 [=====>........................] - ETA: 15:59 - loss: 1.6531 - regression_loss: 1.3797 - classification_loss: 0.2734
 303/1500 [=====>........................] - ETA: 16:00 - loss: 1.6607 - regression_loss: 1.3800 - classification_loss: 0.2807
 304/1500 [=====>........................] - ETA: 15:59 - loss: 1.6601 - regression_loss: 1.3798 - classification_loss: 0.2803
 305/1500 [=====>........................] - ETA: 15:59 - loss: 1.6569 - regression_loss: 1.3774 - classification_loss: 0.2796
 306/1500 [=====>........................] - ETA: 15:58 - loss: 1.6554 - regression_loss: 1.3759 - classification_loss: 0.2795
 307/1500 [=====>........................] - ETA: 15:55 - loss: 1.6540 - regression_loss: 1.3745 - classification_loss: 0.2795
 308/1500 [=====>........................] - ETA: 15:54 - loss: 1.6523 - regression_loss: 1.3733 - classification_loss: 0.2790
 309/1500 [=====>........................] - ETA: 15:52 - loss: 1.6531 - regression_loss: 1.3739 - classification_loss: 0.2792
 310/1500 [=====>........................] - ETA: 15:50 - loss: 1.6509 - regression_loss: 1.3723 - classification_loss: 0.2786
 311/1500 [=====>........................] - ETA: 15:49 - loss: 1.6498 - regression_loss: 1.3716 - classification_loss: 0.2783
 312/1500 [=====>........................] - ETA: 15:49 - loss: 1.6462 - regression_loss: 1.3685 - classification_loss: 0.2777
 313/1500 [=====>........................] - ETA: 15:49 - loss: 1.6434 - regression_loss: 1.3661 - classification_loss: 0.2773
 314/1500 [=====>........................] - ETA: 15:48 - loss: 1.6448 - regression_loss: 1.3669 - classification_loss: 0.2778
 315/1500 [=====>........................] - ETA: 15:49 - loss: 1.6474 - regression_loss: 1.3690 - classification_loss: 0.2785
 316/1500 [=====>........................] - ETA: 15:49 - loss: 1.6508 - regression_loss: 1.3715 - classification_loss: 0.2793
 317/1500 [=====>........................] - ETA: 15:47 - loss: 1.6504 - regression_loss: 1.3715 - classification_loss: 0.2789
 318/1500 [=====>........................] - ETA: 15:44 - loss: 1.6505 - regression_loss: 1.3721 - classification_loss: 0.2784
 319/1500 [=====>........................] - ETA: 15:42 - loss: 1.6503 - regression_loss: 1.3722 - classification_loss: 0.2782
 320/1500 [=====>........................] - ETA: 15:40 - loss: 1.6523 - regression_loss: 1.3740 - classification_loss: 0.2784
 321/1500 [=====>........................] - ETA: 15:38 - loss: 1.6506 - regression_loss: 1.3728 - classification_loss: 0.2778
 322/1500 [=====>........................] - ETA: 15:36 - loss: 1.6492 - regression_loss: 1.3717 - classification_loss: 0.2776
 323/1500 [=====>........................] - ETA: 15:35 - loss: 1.6478 - regression_loss: 1.3706 - classification_loss: 0.2772
 324/1500 [=====>........................] - ETA: 15:35 - loss: 1.6469 - regression_loss: 1.3701 - classification_loss: 0.2769
 325/1500 [=====>........................] - ETA: 15:35 - loss: 1.6465 - regression_loss: 1.3696 - classification_loss: 0.2769
 326/1500 [=====>........................] - ETA: 15:37 - loss: 1.6457 - regression_loss: 1.3688 - classification_loss: 0.2769
 327/1500 [=====>........................] - ETA: 15:37 - loss: 1.6445 - regression_loss: 1.3682 - classification_loss: 0.2764
 328/1500 [=====>........................] - ETA: 15:35 - loss: 1.6424 - regression_loss: 1.3665 - classification_loss: 0.2759
 329/1500 [=====>........................] - ETA: 15:36 - loss: 1.6432 - regression_loss: 1.3664 - classification_loss: 0.2768
 330/1500 [=====>........................] - ETA: 15:35 - loss: 1.6416 - regression_loss: 1.3652 - classification_loss: 0.2764
 331/1500 [=====>........................] - ETA: 15:32 - loss: 1.6398 - regression_loss: 1.3639 - classification_loss: 0.2759
 332/1500 [=====>........................] - ETA: 15:35 - loss: 1.6402 - regression_loss: 1.3646 - classification_loss: 0.2756
 333/1500 [=====>........................] - ETA: 15:33 - loss: 1.6369 - regression_loss: 1.3618 - classification_loss: 0.2750
 334/1500 [=====>........................] - ETA: 15:33 - loss: 1.6344 - regression_loss: 1.3599 - classification_loss: 0.2746
 335/1500 [=====>........................] - ETA: 15:31 - loss: 1.6334 - regression_loss: 1.3588 - classification_loss: 0.2746
 336/1500 [=====>........................] - ETA: 15:29 - loss: 1.6317 - regression_loss: 1.3576 - classification_loss: 0.2741
 337/1500 [=====>........................] - ETA: 15:27 - loss: 1.6299 - regression_loss: 1.3563 - classification_loss: 0.2736
 338/1500 [=====>........................] - ETA: 15:28 - loss: 1.6311 - regression_loss: 1.3576 - classification_loss: 0.2735
 339/1500 [=====>........................] - ETA: 15:27 - loss: 1.6350 - regression_loss: 1.3606 - classification_loss: 0.2744
 340/1500 [=====>........................] - ETA: 15:33 - loss: 1.6354 - regression_loss: 1.3601 - classification_loss: 0.2753
 341/1500 [=====>........................] - ETA: 15:31 - loss: 1.6346 - regression_loss: 1.3596 - classification_loss: 0.2750
 342/1500 [=====>........................] - ETA: 15:29 - loss: 1.6344 - regression_loss: 1.3597 - classification_loss: 0.2747
 343/1500 [=====>........................] - ETA: 15:27 - loss: 1.6348 - regression_loss: 1.3603 - classification_loss: 0.2745
 344/1500 [=====>........................] - ETA: 15:25 - loss: 1.6321 - regression_loss: 1.3582 - classification_loss: 0.2739
 345/1500 [=====>........................] - ETA: 15:23 - loss: 1.6331 - regression_loss: 1.3593 - classification_loss: 0.2738
 346/1500 [=====>........................] - ETA: 15:21 - loss: 1.6320 - regression_loss: 1.3587 - classification_loss: 0.2733
 347/1500 [=====>........................] - ETA: 15:18 - loss: 1.6335 - regression_loss: 1.3603 - classification_loss: 0.2732
 348/1500 [=====>........................] - ETA: 15:16 - loss: 1.6330 - regression_loss: 1.3600 - classification_loss: 0.2730
 349/1500 [=====>........................] - ETA: 15:14 - loss: 1.6336 - regression_loss: 1.3602 - classification_loss: 0.2734
 350/1500 [======>.......................] - ETA: 15:14 - loss: 1.6324 - regression_loss: 1.3590 - classification_loss: 0.2735
 351/1500 [======>.......................] - ETA: 15:14 - loss: 1.6311 - regression_loss: 1.3580 - classification_loss: 0.2731
 352/1500 [======>.......................] - ETA: 15:13 - loss: 1.6297 - regression_loss: 1.3570 - classification_loss: 0.2726
 353/1500 [======>.......................] - ETA: 15:12 - loss: 1.6278 - regression_loss: 1.3555 - classification_loss: 0.2722
 354/1500 [======>.......................] - ETA: 15:12 - loss: 1.6262 - regression_loss: 1.3542 - classification_loss: 0.2720
 355/1500 [======>.......................] - ETA: 15:11 - loss: 1.6289 - regression_loss: 1.3565 - classification_loss: 0.2724
 356/1500 [======>.......................] - ETA: 15:13 - loss: 1.6274 - regression_loss: 1.3553 - classification_loss: 0.2720
 357/1500 [======>.......................] - ETA: 15:15 - loss: 1.6280 - regression_loss: 1.3557 - classification_loss: 0.2724
 358/1500 [======>.......................] - ETA: 15:14 - loss: 1.6259 - regression_loss: 1.3540 - classification_loss: 0.2718
 359/1500 [======>.......................] - ETA: 15:12 - loss: 1.6282 - regression_loss: 1.3561 - classification_loss: 0.2721
 360/1500 [======>.......................] - ETA: 15:11 - loss: 1.6294 - regression_loss: 1.3575 - classification_loss: 0.2720
 361/1500 [======>.......................] - ETA: 15:14 - loss: 1.6270 - regression_loss: 1.3554 - classification_loss: 0.2715
 362/1500 [======>.......................] - ETA: 15:12 - loss: 1.6261 - regression_loss: 1.3547 - classification_loss: 0.2714
 363/1500 [======>.......................] - ETA: 15:11 - loss: 1.6246 - regression_loss: 1.3532 - classification_loss: 0.2714
 364/1500 [======>.......................] - ETA: 15:12 - loss: 1.6259 - regression_loss: 1.3543 - classification_loss: 0.2717
 365/1500 [======>.......................] - ETA: 15:10 - loss: 1.6238 - regression_loss: 1.3526 - classification_loss: 0.2712
 366/1500 [======>.......................] - ETA: 15:08 - loss: 1.6234 - regression_loss: 1.3524 - classification_loss: 0.2709
 367/1500 [======>.......................] - ETA: 15:09 - loss: 1.6271 - regression_loss: 1.3559 - classification_loss: 0.2712
 368/1500 [======>.......................] - ETA: 15:12 - loss: 1.6260 - regression_loss: 1.3553 - classification_loss: 0.2707
 369/1500 [======>.......................] - ETA: 15:16 - loss: 1.6287 - regression_loss: 1.3575 - classification_loss: 0.2712
 370/1500 [======>.......................] - ETA: 15:17 - loss: 1.6310 - regression_loss: 1.3586 - classification_loss: 0.2724
 371/1500 [======>.......................] - ETA: 15:17 - loss: 1.6302 - regression_loss: 1.3581 - classification_loss: 0.2721
 372/1500 [======>.......................] - ETA: 15:16 - loss: 1.6303 - regression_loss: 1.3577 - classification_loss: 0.2726
 373/1500 [======>.......................] - ETA: 15:16 - loss: 1.6287 - regression_loss: 1.3565 - classification_loss: 0.2722
 374/1500 [======>.......................] - ETA: 15:14 - loss: 1.6295 - regression_loss: 1.3575 - classification_loss: 0.2720
 375/1500 [======>.......................] - ETA: 15:12 - loss: 1.6310 - regression_loss: 1.3588 - classification_loss: 0.2722
 376/1500 [======>.......................] - ETA: 15:10 - loss: 1.6308 - regression_loss: 1.3587 - classification_loss: 0.2720
 377/1500 [======>.......................] - ETA: 15:08 - loss: 1.6303 - regression_loss: 1.3586 - classification_loss: 0.2717
 378/1500 [======>.......................] - ETA: 15:06 - loss: 1.6306 - regression_loss: 1.3592 - classification_loss: 0.2714
 379/1500 [======>.......................] - ETA: 15:09 - loss: 1.6340 - regression_loss: 1.3617 - classification_loss: 0.2723
 380/1500 [======>.......................] - ETA: 15:07 - loss: 1.6341 - regression_loss: 1.3621 - classification_loss: 0.2720
 381/1500 [======>.......................] - ETA: 15:08 - loss: 1.6338 - regression_loss: 1.3617 - classification_loss: 0.2721
 382/1500 [======>.......................] - ETA: 15:06 - loss: 1.6329 - regression_loss: 1.3610 - classification_loss: 0.2719
 383/1500 [======>.......................] - ETA: 15:04 - loss: 1.6342 - regression_loss: 1.3624 - classification_loss: 0.2718
 384/1500 [======>.......................] - ETA: 15:02 - loss: 1.6338 - regression_loss: 1.3622 - classification_loss: 0.2716
 385/1500 [======>.......................] - ETA: 15:00 - loss: 1.6351 - regression_loss: 1.3634 - classification_loss: 0.2717
 386/1500 [======>.......................] - ETA: 14:58 - loss: 1.6376 - regression_loss: 1.3654 - classification_loss: 0.2722
 387/1500 [======>.......................] - ETA: 14:59 - loss: 1.6363 - regression_loss: 1.3645 - classification_loss: 0.2718
 388/1500 [======>.......................] - ETA: 14:58 - loss: 1.6382 - regression_loss: 1.3661 - classification_loss: 0.2721
 389/1500 [======>.......................] - ETA: 14:56 - loss: 1.6370 - regression_loss: 1.3651 - classification_loss: 0.2718
 390/1500 [======>.......................] - ETA: 14:54 - loss: 1.6369 - regression_loss: 1.3652 - classification_loss: 0.2717
 391/1500 [======>.......................] - ETA: 14:57 - loss: 1.6355 - regression_loss: 1.3640 - classification_loss: 0.2714
 392/1500 [======>.......................] - ETA: 14:56 - loss: 1.6372 - regression_loss: 1.3655 - classification_loss: 0.2717
 393/1500 [======>.......................] - ETA: 14:54 - loss: 1.6356 - regression_loss: 1.3643 - classification_loss: 0.2713
 394/1500 [======>.......................] - ETA: 14:53 - loss: 1.6364 - regression_loss: 1.3651 - classification_loss: 0.2713
 395/1500 [======>.......................] - ETA: 14:52 - loss: 1.6351 - regression_loss: 1.3639 - classification_loss: 0.2712
 396/1500 [======>.......................] - ETA: 14:53 - loss: 1.6343 - regression_loss: 1.3635 - classification_loss: 0.2708
 397/1500 [======>.......................] - ETA: 14:52 - loss: 1.6365 - regression_loss: 1.3646 - classification_loss: 0.2719
 398/1500 [======>.......................] - ETA: 14:51 - loss: 1.6391 - regression_loss: 1.3666 - classification_loss: 0.2725
 399/1500 [======>.......................] - ETA: 14:51 - loss: 1.6397 - regression_loss: 1.3670 - classification_loss: 0.2726
 400/1500 [=======>......................] - ETA: 14:49 - loss: 1.6391 - regression_loss: 1.3666 - classification_loss: 0.2725
 401/1500 [=======>......................] - ETA: 14:48 - loss: 1.6412 - regression_loss: 1.3687 - classification_loss: 0.2725
 402/1500 [=======>......................] - ETA: 14:48 - loss: 1.6416 - regression_loss: 1.3691 - classification_loss: 0.2725
 403/1500 [=======>......................] - ETA: 14:46 - loss: 1.6414 - regression_loss: 1.3688 - classification_loss: 0.2726
 404/1500 [=======>......................] - ETA: 14:47 - loss: 1.6399 - regression_loss: 1.3673 - classification_loss: 0.2726
 405/1500 [=======>......................] - ETA: 14:45 - loss: 1.6388 - regression_loss: 1.3664 - classification_loss: 0.2724
 406/1500 [=======>......................] - ETA: 14:44 - loss: 1.6375 - regression_loss: 1.3655 - classification_loss: 0.2720
 407/1500 [=======>......................] - ETA: 14:42 - loss: 1.6361 - regression_loss: 1.3644 - classification_loss: 0.2717
 408/1500 [=======>......................] - ETA: 14:44 - loss: 1.6347 - regression_loss: 1.3634 - classification_loss: 0.2714
 409/1500 [=======>......................] - ETA: 14:43 - loss: 1.6332 - regression_loss: 1.3620 - classification_loss: 0.2712
 410/1500 [=======>......................] - ETA: 14:42 - loss: 1.6316 - regression_loss: 1.3606 - classification_loss: 0.2710
 411/1500 [=======>......................] - ETA: 14:42 - loss: 1.6308 - regression_loss: 1.3598 - classification_loss: 0.2709
 412/1500 [=======>......................] - ETA: 14:40 - loss: 1.6297 - regression_loss: 1.3590 - classification_loss: 0.2706
 413/1500 [=======>......................] - ETA: 14:39 - loss: 1.6320 - regression_loss: 1.3611 - classification_loss: 0.2710
 414/1500 [=======>......................] - ETA: 14:39 - loss: 1.6297 - regression_loss: 1.3591 - classification_loss: 0.2706
 415/1500 [=======>......................] - ETA: 14:39 - loss: 1.6315 - regression_loss: 1.3603 - classification_loss: 0.2712
 416/1500 [=======>......................] - ETA: 14:38 - loss: 1.6308 - regression_loss: 1.3599 - classification_loss: 0.2708
 417/1500 [=======>......................] - ETA: 14:38 - loss: 1.6317 - regression_loss: 1.3605 - classification_loss: 0.2711
 418/1500 [=======>......................] - ETA: 14:36 - loss: 1.6303 - regression_loss: 1.3597 - classification_loss: 0.2706
 419/1500 [=======>......................] - ETA: 14:35 - loss: 1.6293 - regression_loss: 1.3590 - classification_loss: 0.2703
 420/1500 [=======>......................] - ETA: 14:34 - loss: 1.6296 - regression_loss: 1.3594 - classification_loss: 0.2702
 421/1500 [=======>......................] - ETA: 14:37 - loss: 1.6305 - regression_loss: 1.3603 - classification_loss: 0.2702
 422/1500 [=======>......................] - ETA: 14:35 - loss: 1.6296 - regression_loss: 1.3598 - classification_loss: 0.2697
 423/1500 [=======>......................] - ETA: 14:33 - loss: 1.6287 - regression_loss: 1.3590 - classification_loss: 0.2697
 424/1500 [=======>......................] - ETA: 14:34 - loss: 1.6279 - regression_loss: 1.3585 - classification_loss: 0.2694
 425/1500 [=======>......................] - ETA: 14:33 - loss: 1.6272 - regression_loss: 1.3581 - classification_loss: 0.2691
 426/1500 [=======>......................] - ETA: 14:31 - loss: 1.6267 - regression_loss: 1.3580 - classification_loss: 0.2687
 427/1500 [=======>......................] - ETA: 14:29 - loss: 1.6288 - regression_loss: 1.3598 - classification_loss: 0.2690
 428/1500 [=======>......................] - ETA: 14:28 - loss: 1.6285 - regression_loss: 1.3598 - classification_loss: 0.2687
 429/1500 [=======>......................] - ETA: 14:27 - loss: 1.6294 - regression_loss: 1.3606 - classification_loss: 0.2688
 430/1500 [=======>......................] - ETA: 14:25 - loss: 1.6288 - regression_loss: 1.3600 - classification_loss: 0.2688
 431/1500 [=======>......................] - ETA: 14:24 - loss: 1.6274 - regression_loss: 1.3589 - classification_loss: 0.2685
 432/1500 [=======>......................] - ETA: 14:22 - loss: 1.6274 - regression_loss: 1.3589 - classification_loss: 0.2685
 433/1500 [=======>......................] - ETA: 14:20 - loss: 1.6273 - regression_loss: 1.3590 - classification_loss: 0.2683
 434/1500 [=======>......................] - ETA: 14:20 - loss: 1.6286 - regression_loss: 1.3602 - classification_loss: 0.2683
 435/1500 [=======>......................] - ETA: 14:19 - loss: 1.6282 - regression_loss: 1.3598 - classification_loss: 0.2684
 436/1500 [=======>......................] - ETA: 14:23 - loss: 1.6270 - regression_loss: 1.3589 - classification_loss: 0.2682
 437/1500 [=======>......................] - ETA: 14:24 - loss: 1.6306 - regression_loss: 1.3620 - classification_loss: 0.2686
 438/1500 [=======>......................] - ETA: 14:25 - loss: 1.6298 - regression_loss: 1.3615 - classification_loss: 0.2683
 439/1500 [=======>......................] - ETA: 14:24 - loss: 1.6289 - regression_loss: 1.3609 - classification_loss: 0.2679
 440/1500 [=======>......................] - ETA: 14:22 - loss: 1.6319 - regression_loss: 1.3636 - classification_loss: 0.2684
 441/1500 [=======>......................] - ETA: 14:22 - loss: 1.6309 - regression_loss: 1.3629 - classification_loss: 0.2681
 442/1500 [=======>......................] - ETA: 14:21 - loss: 1.6300 - regression_loss: 1.3621 - classification_loss: 0.2679
 443/1500 [=======>......................] - ETA: 14:20 - loss: 1.6294 - regression_loss: 1.3618 - classification_loss: 0.2676
 444/1500 [=======>......................] - ETA: 14:19 - loss: 1.6308 - regression_loss: 1.3631 - classification_loss: 0.2677
 445/1500 [=======>......................] - ETA: 14:17 - loss: 1.6310 - regression_loss: 1.3632 - classification_loss: 0.2677
 446/1500 [=======>......................] - ETA: 14:16 - loss: 1.6303 - regression_loss: 1.3627 - classification_loss: 0.2676
 447/1500 [=======>......................] - ETA: 14:14 - loss: 1.6288 - regression_loss: 1.3616 - classification_loss: 0.2672
 448/1500 [=======>......................] - ETA: 14:13 - loss: 1.6294 - regression_loss: 1.3624 - classification_loss: 0.2670
 449/1500 [=======>......................] - ETA: 14:15 - loss: 1.6294 - regression_loss: 1.3627 - classification_loss: 0.2667
 450/1500 [========>.....................] - ETA: 14:14 - loss: 1.6278 - regression_loss: 1.3615 - classification_loss: 0.2663
 451/1500 [========>.....................] - ETA: 14:13 - loss: 1.6270 - regression_loss: 1.3608 - classification_loss: 0.2662
 452/1500 [========>.....................] - ETA: 14:11 - loss: 1.6275 - regression_loss: 1.3608 - classification_loss: 0.2667
 453/1500 [========>.....................] - ETA: 14:13 - loss: 1.6292 - regression_loss: 1.3619 - classification_loss: 0.2674
 454/1500 [========>.....................] - ETA: 14:12 - loss: 1.6281 - regression_loss: 1.3611 - classification_loss: 0.2670
 455/1500 [========>.....................] - ETA: 14:10 - loss: 1.6278 - regression_loss: 1.3607 - classification_loss: 0.2672
 456/1500 [========>.....................] - ETA: 14:12 - loss: 1.6270 - regression_loss: 1.3601 - classification_loss: 0.2669
 457/1500 [========>.....................] - ETA: 14:10 - loss: 1.6290 - regression_loss: 1.3619 - classification_loss: 0.2671
 458/1500 [========>.....................] - ETA: 14:09 - loss: 1.6318 - regression_loss: 1.3646 - classification_loss: 0.2672
 459/1500 [========>.....................] - ETA: 14:08 - loss: 1.6323 - regression_loss: 1.3652 - classification_loss: 0.2671
 460/1500 [========>.....................] - ETA: 14:06 - loss: 1.6313 - regression_loss: 1.3644 - classification_loss: 0.2668
 461/1500 [========>.....................] - ETA: 14:04 - loss: 1.6321 - regression_loss: 1.3655 - classification_loss: 0.2667
 462/1500 [========>.....................] - ETA: 14:05 - loss: 1.6320 - regression_loss: 1.3655 - classification_loss: 0.2665
 463/1500 [========>.....................] - ETA: 14:03 - loss: 1.6332 - regression_loss: 1.3668 - classification_loss: 0.2664
 464/1500 [========>.....................] - ETA: 14:03 - loss: 1.6320 - regression_loss: 1.3660 - classification_loss: 0.2660
 465/1500 [========>.....................] - ETA: 14:01 - loss: 1.6304 - regression_loss: 1.3648 - classification_loss: 0.2656
 466/1500 [========>.....................] - ETA: 14:00 - loss: 1.6310 - regression_loss: 1.3654 - classification_loss: 0.2657
 467/1500 [========>.....................] - ETA: 14:00 - loss: 1.6302 - regression_loss: 1.3649 - classification_loss: 0.2654
 468/1500 [========>.....................] - ETA: 14:01 - loss: 1.6293 - regression_loss: 1.3642 - classification_loss: 0.2650
 469/1500 [========>.....................] - ETA: 13:59 - loss: 1.6277 - regression_loss: 1.3631 - classification_loss: 0.2646
 470/1500 [========>.....................] - ETA: 13:59 - loss: 1.6298 - regression_loss: 1.3647 - classification_loss: 0.2651
 471/1500 [========>.....................] - ETA: 13:59 - loss: 1.6278 - regression_loss: 1.3618 - classification_loss: 0.2661
 472/1500 [========>.....................] - ETA: 13:57 - loss: 1.6268 - regression_loss: 1.3612 - classification_loss: 0.2656
 473/1500 [========>.....................] - ETA: 13:56 - loss: 1.6252 - regression_loss: 1.3598 - classification_loss: 0.2654
 474/1500 [========>.....................] - ETA: 13:54 - loss: 1.6249 - regression_loss: 1.3596 - classification_loss: 0.2652
 475/1500 [========>.....................] - ETA: 13:54 - loss: 1.6244 - regression_loss: 1.3594 - classification_loss: 0.2650
 476/1500 [========>.....................] - ETA: 13:55 - loss: 1.6245 - regression_loss: 1.3597 - classification_loss: 0.2648
 477/1500 [========>.....................] - ETA: 13:54 - loss: 1.6250 - regression_loss: 1.3603 - classification_loss: 0.2647
 478/1500 [========>.....................] - ETA: 13:53 - loss: 1.6243 - regression_loss: 1.3598 - classification_loss: 0.2645
 479/1500 [========>.....................] - ETA: 13:56 - loss: 1.6259 - regression_loss: 1.3611 - classification_loss: 0.2648
 480/1500 [========>.....................] - ETA: 13:55 - loss: 1.6262 - regression_loss: 1.3613 - classification_loss: 0.2648
 481/1500 [========>.....................] - ETA: 13:54 - loss: 1.6255 - regression_loss: 1.3608 - classification_loss: 0.2647
 482/1500 [========>.....................] - ETA: 13:53 - loss: 1.6261 - regression_loss: 1.3613 - classification_loss: 0.2648
 483/1500 [========>.....................] - ETA: 13:51 - loss: 1.6258 - regression_loss: 1.3613 - classification_loss: 0.2645
 484/1500 [========>.....................] - ETA: 13:50 - loss: 1.6249 - regression_loss: 1.3606 - classification_loss: 0.2643
 485/1500 [========>.....................] - ETA: 13:48 - loss: 1.6243 - regression_loss: 1.3601 - classification_loss: 0.2641
 486/1500 [========>.....................] - ETA: 13:48 - loss: 1.6240 - regression_loss: 1.3599 - classification_loss: 0.2641
 487/1500 [========>.....................] - ETA: 13:49 - loss: 1.6258 - regression_loss: 1.3611 - classification_loss: 0.2647
 488/1500 [========>.....................] - ETA: 13:48 - loss: 1.6251 - regression_loss: 1.3605 - classification_loss: 0.2646
 489/1500 [========>.....................] - ETA: 13:48 - loss: 1.6264 - regression_loss: 1.3614 - classification_loss: 0.2650
 490/1500 [========>.....................] - ETA: 13:46 - loss: 1.6254 - regression_loss: 1.3606 - classification_loss: 0.2648
 491/1500 [========>.....................] - ETA: 13:45 - loss: 1.6239 - regression_loss: 1.3594 - classification_loss: 0.2645
 492/1500 [========>.....................] - ETA: 13:43 - loss: 1.6246 - regression_loss: 1.3599 - classification_loss: 0.2648
 493/1500 [========>.....................] - ETA: 13:43 - loss: 1.6249 - regression_loss: 1.3596 - classification_loss: 0.2652
 494/1500 [========>.....................] - ETA: 13:41 - loss: 1.6245 - regression_loss: 1.3591 - classification_loss: 0.2654
 495/1500 [========>.....................] - ETA: 13:39 - loss: 1.6254 - regression_loss: 1.3593 - classification_loss: 0.2661
 496/1500 [========>.....................] - ETA: 13:38 - loss: 1.6249 - regression_loss: 1.3590 - classification_loss: 0.2659
 497/1500 [========>.....................] - ETA: 13:36 - loss: 1.6243 - regression_loss: 1.3586 - classification_loss: 0.2657
 498/1500 [========>.....................] - ETA: 13:36 - loss: 1.6234 - regression_loss: 1.3578 - classification_loss: 0.2656
 499/1500 [========>.....................] - ETA: 13:34 - loss: 1.6216 - regression_loss: 1.3562 - classification_loss: 0.2654
 500/1500 [=========>....................] - ETA: 13:33 - loss: 1.6223 - regression_loss: 1.3569 - classification_loss: 0.2654
 501/1500 [=========>....................] - ETA: 13:31 - loss: 1.6222 - regression_loss: 1.3570 - classification_loss: 0.2652
 502/1500 [=========>....................] - ETA: 13:31 - loss: 1.6206 - regression_loss: 1.3558 - classification_loss: 0.2648
 503/1500 [=========>....................] - ETA: 13:30 - loss: 1.6204 - regression_loss: 1.3557 - classification_loss: 0.2647
 504/1500 [=========>....................] - ETA: 13:28 - loss: 1.6189 - regression_loss: 1.3546 - classification_loss: 0.2643
 505/1500 [=========>....................] - ETA: 13:28 - loss: 1.6199 - regression_loss: 1.3556 - classification_loss: 0.2644
 506/1500 [=========>....................] - ETA: 13:26 - loss: 1.6184 - regression_loss: 1.3544 - classification_loss: 0.2641
 507/1500 [=========>....................] - ETA: 13:26 - loss: 1.6178 - regression_loss: 1.3540 - classification_loss: 0.2638
 508/1500 [=========>....................] - ETA: 13:24 - loss: 1.6163 - regression_loss: 1.3528 - classification_loss: 0.2635
 509/1500 [=========>....................] - ETA: 13:25 - loss: 1.6177 - regression_loss: 1.3540 - classification_loss: 0.2636
 510/1500 [=========>....................] - ETA: 13:25 - loss: 1.6203 - regression_loss: 1.3560 - classification_loss: 0.2643
 511/1500 [=========>....................] - ETA: 13:24 - loss: 1.6226 - regression_loss: 1.3581 - classification_loss: 0.2646
 512/1500 [=========>....................] - ETA: 13:23 - loss: 1.6226 - regression_loss: 1.3579 - classification_loss: 0.2647
 513/1500 [=========>....................] - ETA: 13:21 - loss: 1.6237 - regression_loss: 1.3588 - classification_loss: 0.2649
 514/1500 [=========>....................] - ETA: 13:20 - loss: 1.6213 - regression_loss: 1.3567 - classification_loss: 0.2646
 515/1500 [=========>....................] - ETA: 13:21 - loss: 1.6225 - regression_loss: 1.3577 - classification_loss: 0.2649
 516/1500 [=========>....................] - ETA: 13:21 - loss: 1.6235 - regression_loss: 1.3585 - classification_loss: 0.2650
 517/1500 [=========>....................] - ETA: 13:20 - loss: 1.6227 - regression_loss: 1.3574 - classification_loss: 0.2653
 518/1500 [=========>....................] - ETA: 13:19 - loss: 1.6212 - regression_loss: 1.3563 - classification_loss: 0.2649
 519/1500 [=========>....................] - ETA: 13:18 - loss: 1.6197 - regression_loss: 1.3551 - classification_loss: 0.2646
 520/1500 [=========>....................] - ETA: 13:17 - loss: 1.6208 - regression_loss: 1.3561 - classification_loss: 0.2646
 521/1500 [=========>....................] - ETA: 13:16 - loss: 1.6222 - regression_loss: 1.3574 - classification_loss: 0.2647
 522/1500 [=========>....................] - ETA: 13:14 - loss: 1.6224 - regression_loss: 1.3575 - classification_loss: 0.2649
 523/1500 [=========>....................] - ETA: 13:13 - loss: 1.6233 - regression_loss: 1.3583 - classification_loss: 0.2650
 524/1500 [=========>....................] - ETA: 13:17 - loss: 1.6235 - regression_loss: 1.3586 - classification_loss: 0.2649
 525/1500 [=========>....................] - ETA: 13:17 - loss: 1.6222 - regression_loss: 1.3576 - classification_loss: 0.2645
 526/1500 [=========>....................] - ETA: 13:15 - loss: 1.6223 - regression_loss: 1.3577 - classification_loss: 0.2646
 527/1500 [=========>....................] - ETA: 13:14 - loss: 1.6210 - regression_loss: 1.3568 - classification_loss: 0.2642
 528/1500 [=========>....................] - ETA: 13:14 - loss: 1.6228 - regression_loss: 1.3583 - classification_loss: 0.2645
 529/1500 [=========>....................] - ETA: 13:12 - loss: 1.6229 - regression_loss: 1.3584 - classification_loss: 0.2645
 530/1500 [=========>....................] - ETA: 13:11 - loss: 1.6224 - regression_loss: 1.3581 - classification_loss: 0.2643
 531/1500 [=========>....................] - ETA: 13:09 - loss: 1.6228 - regression_loss: 1.3583 - classification_loss: 0.2645
 532/1500 [=========>....................] - ETA: 13:09 - loss: 1.6228 - regression_loss: 1.3584 - classification_loss: 0.2643
 533/1500 [=========>....................] - ETA: 13:09 - loss: 1.6229 - regression_loss: 1.3586 - classification_loss: 0.2643
 534/1500 [=========>....................] - ETA: 13:07 - loss: 1.6211 - regression_loss: 1.3570 - classification_loss: 0.2640
 535/1500 [=========>....................] - ETA: 13:06 - loss: 1.6214 - regression_loss: 1.3574 - classification_loss: 0.2640
 536/1500 [=========>....................] - ETA: 13:04 - loss: 1.6208 - regression_loss: 1.3567 - classification_loss: 0.2641
 537/1500 [=========>....................] - ETA: 13:03 - loss: 1.6191 - regression_loss: 1.3554 - classification_loss: 0.2637
 538/1500 [=========>....................] - ETA: 13:02 - loss: 1.6182 - regression_loss: 1.3548 - classification_loss: 0.2634
 539/1500 [=========>....................] - ETA: 13:01 - loss: 1.6195 - regression_loss: 1.3557 - classification_loss: 0.2637
 540/1500 [=========>....................] - ETA: 13:02 - loss: 1.6182 - regression_loss: 1.3548 - classification_loss: 0.2634
 541/1500 [=========>....................] - ETA: 13:01 - loss: 1.6182 - regression_loss: 1.3548 - classification_loss: 0.2635
 542/1500 [=========>....................] - ETA: 13:00 - loss: 1.6170 - regression_loss: 1.3537 - classification_loss: 0.2632
 543/1500 [=========>....................] - ETA: 12:58 - loss: 1.6154 - regression_loss: 1.3524 - classification_loss: 0.2629
 544/1500 [=========>....................] - ETA: 12:57 - loss: 1.6146 - regression_loss: 1.3520 - classification_loss: 0.2626
 545/1500 [=========>....................] - ETA: 12:55 - loss: 1.6143 - regression_loss: 1.3518 - classification_loss: 0.2625
 546/1500 [=========>....................] - ETA: 12:54 - loss: 1.6147 - regression_loss: 1.3524 - classification_loss: 0.2623
 547/1500 [=========>....................] - ETA: 12:52 - loss: 1.6154 - regression_loss: 1.3532 - classification_loss: 0.2622
 548/1500 [=========>....................] - ETA: 12:52 - loss: 1.6150 - regression_loss: 1.3529 - classification_loss: 0.2621
 549/1500 [=========>....................] - ETA: 12:50 - loss: 1.6165 - regression_loss: 1.3544 - classification_loss: 0.2621
 550/1500 [==========>...................] - ETA: 12:52 - loss: 1.6169 - regression_loss: 1.3546 - classification_loss: 0.2623
 551/1500 [==========>...................] - ETA: 12:50 - loss: 1.6167 - regression_loss: 1.3546 - classification_loss: 0.2621
 552/1500 [==========>...................] - ETA: 12:50 - loss: 1.6183 - regression_loss: 1.3559 - classification_loss: 0.2625
 553/1500 [==========>...................] - ETA: 12:49 - loss: 1.6167 - regression_loss: 1.3546 - classification_loss: 0.2622
 554/1500 [==========>...................] - ETA: 12:48 - loss: 1.6169 - regression_loss: 1.3548 - classification_loss: 0.2620
 555/1500 [==========>...................] - ETA: 12:46 - loss: 1.6167 - regression_loss: 1.3547 - classification_loss: 0.2620
 556/1500 [==========>...................] - ETA: 12:45 - loss: 1.6172 - regression_loss: 1.3552 - classification_loss: 0.2620
 557/1500 [==========>...................] - ETA: 12:43 - loss: 1.6168 - regression_loss: 1.3548 - classification_loss: 0.2620
 558/1500 [==========>...................] - ETA: 12:42 - loss: 1.6169 - regression_loss: 1.3551 - classification_loss: 0.2619
 559/1500 [==========>...................] - ETA: 12:42 - loss: 1.6172 - regression_loss: 1.3552 - classification_loss: 0.2620
 560/1500 [==========>...................] - ETA: 12:41 - loss: 1.6164 - regression_loss: 1.3545 - classification_loss: 0.2618
 561/1500 [==========>...................] - ETA: 12:40 - loss: 1.6165 - regression_loss: 1.3549 - classification_loss: 0.2616
 562/1500 [==========>...................] - ETA: 12:39 - loss: 1.6151 - regression_loss: 1.3538 - classification_loss: 0.2613
 563/1500 [==========>...................] - ETA: 12:37 - loss: 1.6145 - regression_loss: 1.3535 - classification_loss: 0.2610
 564/1500 [==========>...................] - ETA: 12:36 - loss: 1.6147 - regression_loss: 1.3537 - classification_loss: 0.2611
 565/1500 [==========>...................] - ETA: 12:35 - loss: 1.6156 - regression_loss: 1.3544 - classification_loss: 0.2612
 566/1500 [==========>...................] - ETA: 12:34 - loss: 1.6159 - regression_loss: 1.3548 - classification_loss: 0.2611
 567/1500 [==========>...................] - ETA: 12:32 - loss: 1.6163 - regression_loss: 1.3552 - classification_loss: 0.2612
 568/1500 [==========>...................] - ETA: 12:33 - loss: 1.6148 - regression_loss: 1.3539 - classification_loss: 0.2609
 569/1500 [==========>...................] - ETA: 12:32 - loss: 1.6146 - regression_loss: 1.3537 - classification_loss: 0.2609
 570/1500 [==========>...................] - ETA: 12:30 - loss: 1.6134 - regression_loss: 1.3528 - classification_loss: 0.2606
 571/1500 [==========>...................] - ETA: 12:29 - loss: 1.6140 - regression_loss: 1.3534 - classification_loss: 0.2607
 572/1500 [==========>...................] - ETA: 12:27 - loss: 1.6130 - regression_loss: 1.3525 - classification_loss: 0.2606
 573/1500 [==========>...................] - ETA: 12:27 - loss: 1.6123 - regression_loss: 1.3520 - classification_loss: 0.2603
 574/1500 [==========>...................] - ETA: 12:29 - loss: 1.6134 - regression_loss: 1.3529 - classification_loss: 0.2605
 575/1500 [==========>...................] - ETA: 12:27 - loss: 1.6130 - regression_loss: 1.3526 - classification_loss: 0.2604
 576/1500 [==========>...................] - ETA: 12:26 - loss: 1.6129 - regression_loss: 1.3526 - classification_loss: 0.2603
 577/1500 [==========>...................] - ETA: 12:26 - loss: 1.6119 - regression_loss: 1.3518 - classification_loss: 0.2601
 578/1500 [==========>...................] - ETA: 12:25 - loss: 1.6119 - regression_loss: 1.3518 - classification_loss: 0.2601
 579/1500 [==========>...................] - ETA: 12:23 - loss: 1.6118 - regression_loss: 1.3518 - classification_loss: 0.2600
 580/1500 [==========>...................] - ETA: 12:22 - loss: 1.6110 - regression_loss: 1.3513 - classification_loss: 0.2597
 581/1500 [==========>...................] - ETA: 12:21 - loss: 1.6120 - regression_loss: 1.3522 - classification_loss: 0.2597
 582/1500 [==========>...................] - ETA: 12:20 - loss: 1.6116 - regression_loss: 1.3521 - classification_loss: 0.2595
 583/1500 [==========>...................] - ETA: 12:18 - loss: 1.6113 - regression_loss: 1.3518 - classification_loss: 0.2595
 584/1500 [==========>...................] - ETA: 12:17 - loss: 1.6109 - regression_loss: 1.3516 - classification_loss: 0.2594
 585/1500 [==========>...................] - ETA: 12:15 - loss: 1.6096 - regression_loss: 1.3506 - classification_loss: 0.2591
 586/1500 [==========>...................] - ETA: 12:14 - loss: 1.6101 - regression_loss: 1.3502 - classification_loss: 0.2599
 587/1500 [==========>...................] - ETA: 12:12 - loss: 1.6113 - regression_loss: 1.3513 - classification_loss: 0.2600
 588/1500 [==========>...................] - ETA: 12:13 - loss: 1.6108 - regression_loss: 1.3510 - classification_loss: 0.2598
 589/1500 [==========>...................] - ETA: 12:12 - loss: 1.6106 - regression_loss: 1.3510 - classification_loss: 0.2596
 590/1500 [==========>...................] - ETA: 12:10 - loss: 1.6128 - regression_loss: 1.3525 - classification_loss: 0.2603
 591/1500 [==========>...................] - ETA: 12:09 - loss: 1.6124 - regression_loss: 1.3523 - classification_loss: 0.2601
 592/1500 [==========>...................] - ETA: 12:11 - loss: 1.6116 - regression_loss: 1.3517 - classification_loss: 0.2599
 593/1500 [==========>...................] - ETA: 12:12 - loss: 1.6128 - regression_loss: 1.3526 - classification_loss: 0.2602
 594/1500 [==========>...................] - ETA: 12:12 - loss: 1.6141 - regression_loss: 1.3534 - classification_loss: 0.2608
 595/1500 [==========>...................] - ETA: 12:11 - loss: 1.6140 - regression_loss: 1.3535 - classification_loss: 0.2605
 596/1500 [==========>...................] - ETA: 12:10 - loss: 1.6159 - regression_loss: 1.3550 - classification_loss: 0.2609
 597/1500 [==========>...................] - ETA: 12:10 - loss: 1.6147 - regression_loss: 1.3540 - classification_loss: 0.2607
 598/1500 [==========>...................] - ETA: 12:10 - loss: 1.6151 - regression_loss: 1.3542 - classification_loss: 0.2609
 599/1500 [==========>...................] - ETA: 12:09 - loss: 1.6151 - regression_loss: 1.3541 - classification_loss: 0.2611
 600/1500 [===========>..................] - ETA: 12:07 - loss: 1.6153 - regression_loss: 1.3542 - classification_loss: 0.2611
 601/1500 [===========>..................] - ETA: 12:06 - loss: 1.6143 - regression_loss: 1.3534 - classification_loss: 0.2609
 602/1500 [===========>..................] - ETA: 12:04 - loss: 1.6138 - regression_loss: 1.3530 - classification_loss: 0.2608
 603/1500 [===========>..................] - ETA: 12:03 - loss: 1.6134 - regression_loss: 1.3526 - classification_loss: 0.2607
 604/1500 [===========>..................] - ETA: 12:02 - loss: 1.6139 - regression_loss: 1.3530 - classification_loss: 0.2609
 605/1500 [===========>..................] - ETA: 12:01 - loss: 1.6135 - regression_loss: 1.3526 - classification_loss: 0.2608
 606/1500 [===========>..................] - ETA: 12:01 - loss: 1.6126 - regression_loss: 1.3520 - classification_loss: 0.2606
 607/1500 [===========>..................] - ETA: 11:59 - loss: 1.6116 - regression_loss: 1.3510 - classification_loss: 0.2606
 608/1500 [===========>..................] - ETA: 11:58 - loss: 1.6106 - regression_loss: 1.3503 - classification_loss: 0.2603
 609/1500 [===========>..................] - ETA: 11:57 - loss: 1.6103 - regression_loss: 1.3502 - classification_loss: 0.2602
 610/1500 [===========>..................] - ETA: 11:58 - loss: 1.6099 - regression_loss: 1.3499 - classification_loss: 0.2600
 611/1500 [===========>..................] - ETA: 11:56 - loss: 1.6100 - regression_loss: 1.3500 - classification_loss: 0.2600
 612/1500 [===========>..................] - ETA: 11:55 - loss: 1.6112 - regression_loss: 1.3510 - classification_loss: 0.2601
 613/1500 [===========>..................] - ETA: 11:54 - loss: 1.6096 - regression_loss: 1.3498 - classification_loss: 0.2599
 614/1500 [===========>..................] - ETA: 11:54 - loss: 1.6100 - regression_loss: 1.3502 - classification_loss: 0.2598
 615/1500 [===========>..................] - ETA: 11:53 - loss: 1.6083 - regression_loss: 1.3488 - classification_loss: 0.2595
 616/1500 [===========>..................] - ETA: 11:52 - loss: 1.6085 - regression_loss: 1.3490 - classification_loss: 0.2595
 617/1500 [===========>..................] - ETA: 11:51 - loss: 1.6075 - regression_loss: 1.3480 - classification_loss: 0.2595
 618/1500 [===========>..................] - ETA: 11:50 - loss: 1.6097 - regression_loss: 1.3496 - classification_loss: 0.2601
 619/1500 [===========>..................] - ETA: 11:51 - loss: 1.6085 - regression_loss: 1.3486 - classification_loss: 0.2599
 620/1500 [===========>..................] - ETA: 11:50 - loss: 1.6106 - regression_loss: 1.3502 - classification_loss: 0.2604
 621/1500 [===========>..................] - ETA: 11:49 - loss: 1.6120 - regression_loss: 1.3513 - classification_loss: 0.2607
 622/1500 [===========>..................] - ETA: 11:48 - loss: 1.6112 - regression_loss: 1.3507 - classification_loss: 0.2605
 623/1500 [===========>..................] - ETA: 11:47 - loss: 1.6122 - regression_loss: 1.3515 - classification_loss: 0.2607
 624/1500 [===========>..................] - ETA: 11:46 - loss: 1.6124 - regression_loss: 1.3516 - classification_loss: 0.2608
 625/1500 [===========>..................] - ETA: 11:46 - loss: 1.6131 - regression_loss: 1.3517 - classification_loss: 0.2614
 626/1500 [===========>..................] - ETA: 11:44 - loss: 1.6134 - regression_loss: 1.3515 - classification_loss: 0.2619
 627/1500 [===========>..................] - ETA: 11:43 - loss: 1.6143 - regression_loss: 1.3519 - classification_loss: 0.2624
 628/1500 [===========>..................] - ETA: 11:42 - loss: 1.6126 - regression_loss: 1.3505 - classification_loss: 0.2622
 629/1500 [===========>..................] - ETA: 11:41 - loss: 1.6122 - regression_loss: 1.3502 - classification_loss: 0.2620
 630/1500 [===========>..................] - ETA: 11:41 - loss: 1.6129 - regression_loss: 1.3508 - classification_loss: 0.2621
 631/1500 [===========>..................] - ETA: 11:41 - loss: 1.6124 - regression_loss: 1.3504 - classification_loss: 0.2621
 632/1500 [===========>..................] - ETA: 11:41 - loss: 1.6119 - regression_loss: 1.3498 - classification_loss: 0.2620
 633/1500 [===========>..................] - ETA: 11:41 - loss: 1.6122 - regression_loss: 1.3503 - classification_loss: 0.2619
 634/1500 [===========>..................] - ETA: 11:40 - loss: 1.6121 - regression_loss: 1.3503 - classification_loss: 0.2618
 635/1500 [===========>..................] - ETA: 11:39 - loss: 1.6126 - regression_loss: 1.3510 - classification_loss: 0.2616
 636/1500 [===========>..................] - ETA: 11:37 - loss: 1.6130 - regression_loss: 1.3515 - classification_loss: 0.2615
 637/1500 [===========>..................] - ETA: 11:36 - loss: 1.6133 - regression_loss: 1.3519 - classification_loss: 0.2614
 638/1500 [===========>..................] - ETA: 11:34 - loss: 1.6137 - regression_loss: 1.3523 - classification_loss: 0.2614
 639/1500 [===========>..................] - ETA: 11:34 - loss: 1.6155 - regression_loss: 1.3537 - classification_loss: 0.2617
 640/1500 [===========>..................] - ETA: 11:33 - loss: 1.6168 - regression_loss: 1.3544 - classification_loss: 0.2623
 641/1500 [===========>..................] - ETA: 11:32 - loss: 1.6196 - regression_loss: 1.3571 - classification_loss: 0.2625
 642/1500 [===========>..................] - ETA: 11:30 - loss: 1.6187 - regression_loss: 1.3564 - classification_loss: 0.2623
 643/1500 [===========>..................] - ETA: 11:30 - loss: 1.6197 - regression_loss: 1.3569 - classification_loss: 0.2628
 644/1500 [===========>..................] - ETA: 11:30 - loss: 1.6215 - regression_loss: 1.3585 - classification_loss: 0.2630
 645/1500 [===========>..................] - ETA: 11:29 - loss: 1.6206 - regression_loss: 1.3579 - classification_loss: 0.2627
 646/1500 [===========>..................] - ETA: 11:29 - loss: 1.6202 - regression_loss: 1.3576 - classification_loss: 0.2626
 647/1500 [===========>..................] - ETA: 11:28 - loss: 1.6199 - regression_loss: 1.3574 - classification_loss: 0.2625
 648/1500 [===========>..................] - ETA: 11:28 - loss: 1.6211 - regression_loss: 1.3583 - classification_loss: 0.2628
 649/1500 [===========>..................] - ETA: 11:27 - loss: 1.6220 - regression_loss: 1.3592 - classification_loss: 0.2628
 650/1500 [============>.................] - ETA: 11:26 - loss: 1.6208 - regression_loss: 1.3582 - classification_loss: 0.2626
 651/1500 [============>.................] - ETA: 11:25 - loss: 1.6213 - regression_loss: 1.3587 - classification_loss: 0.2626
 652/1500 [============>.................] - ETA: 11:24 - loss: 1.6216 - regression_loss: 1.3589 - classification_loss: 0.2627
 653/1500 [============>.................] - ETA: 11:23 - loss: 1.6210 - regression_loss: 1.3586 - classification_loss: 0.2624
 654/1500 [============>.................] - ETA: 11:21 - loss: 1.6211 - regression_loss: 1.3586 - classification_loss: 0.2625
 655/1500 [============>.................] - ETA: 11:20 - loss: 1.6209 - regression_loss: 1.3584 - classification_loss: 0.2625
 656/1500 [============>.................] - ETA: 11:22 - loss: 1.6199 - regression_loss: 1.3577 - classification_loss: 0.2622
 657/1500 [============>.................] - ETA: 11:21 - loss: 1.6200 - regression_loss: 1.3578 - classification_loss: 0.2622
 658/1500 [============>.................] - ETA: 11:21 - loss: 1.6199 - regression_loss: 1.3574 - classification_loss: 0.2625
 659/1500 [============>.................] - ETA: 11:20 - loss: 1.6193 - regression_loss: 1.3569 - classification_loss: 0.2623
 660/1500 [============>.................] - ETA: 11:19 - loss: 1.6194 - regression_loss: 1.3568 - classification_loss: 0.2626
 661/1500 [============>.................] - ETA: 11:18 - loss: 1.6183 - regression_loss: 1.3560 - classification_loss: 0.2624
 662/1500 [============>.................] - ETA: 11:17 - loss: 1.6183 - regression_loss: 1.3560 - classification_loss: 0.2623
 663/1500 [============>.................] - ETA: 11:17 - loss: 1.6190 - regression_loss: 1.3565 - classification_loss: 0.2624
 664/1500 [============>.................] - ETA: 11:16 - loss: 1.6193 - regression_loss: 1.3569 - classification_loss: 0.2624
 665/1500 [============>.................] - ETA: 11:15 - loss: 1.6180 - regression_loss: 1.3559 - classification_loss: 0.2621
 666/1500 [============>.................] - ETA: 11:14 - loss: 1.6171 - regression_loss: 1.3552 - classification_loss: 0.2619
 667/1500 [============>.................] - ETA: 11:13 - loss: 1.6181 - regression_loss: 1.3561 - classification_loss: 0.2620
 668/1500 [============>.................] - ETA: 11:11 - loss: 1.6169 - regression_loss: 1.3552 - classification_loss: 0.2618
 669/1500 [============>.................] - ETA: 11:10 - loss: 1.6186 - regression_loss: 1.3566 - classification_loss: 0.2620
 670/1500 [============>.................] - ETA: 11:10 - loss: 1.6170 - regression_loss: 1.3553 - classification_loss: 0.2617
 671/1500 [============>.................] - ETA: 11:10 - loss: 1.6169 - regression_loss: 1.3551 - classification_loss: 0.2617
 672/1500 [============>.................] - ETA: 11:09 - loss: 1.6155 - regression_loss: 1.3540 - classification_loss: 0.2615
 673/1500 [============>.................] - ETA: 11:09 - loss: 1.6169 - regression_loss: 1.3552 - classification_loss: 0.2618
 674/1500 [============>.................] - ETA: 11:08 - loss: 1.6161 - regression_loss: 1.3546 - classification_loss: 0.2616
 675/1500 [============>.................] - ETA: 11:06 - loss: 1.6168 - regression_loss: 1.3551 - classification_loss: 0.2616
 676/1500 [============>.................] - ETA: 11:06 - loss: 1.6156 - regression_loss: 1.3542 - classification_loss: 0.2613
 677/1500 [============>.................] - ETA: 11:05 - loss: 1.6151 - regression_loss: 1.3539 - classification_loss: 0.2612
 678/1500 [============>.................] - ETA: 11:04 - loss: 1.6142 - regression_loss: 1.3532 - classification_loss: 0.2611
 679/1500 [============>.................] - ETA: 11:03 - loss: 1.6150 - regression_loss: 1.3537 - classification_loss: 0.2613
 680/1500 [============>.................] - ETA: 11:01 - loss: 1.6135 - regression_loss: 1.3525 - classification_loss: 0.2610
 681/1500 [============>.................] - ETA: 11:01 - loss: 1.6144 - regression_loss: 1.3533 - classification_loss: 0.2612
 682/1500 [============>.................] - ETA: 11:00 - loss: 1.6133 - regression_loss: 1.3524 - classification_loss: 0.2609
 683/1500 [============>.................] - ETA: 10:59 - loss: 1.6153 - regression_loss: 1.3538 - classification_loss: 0.2614
 684/1500 [============>.................] - ETA: 10:58 - loss: 1.6159 - regression_loss: 1.3542 - classification_loss: 0.2617
 685/1500 [============>.................] - ETA: 10:56 - loss: 1.6147 - regression_loss: 1.3533 - classification_loss: 0.2614
 686/1500 [============>.................] - ETA: 10:55 - loss: 1.6142 - regression_loss: 1.3529 - classification_loss: 0.2613
 687/1500 [============>.................] - ETA: 10:55 - loss: 1.6128 - regression_loss: 1.3518 - classification_loss: 0.2610
 688/1500 [============>.................] - ETA: 10:55 - loss: 1.6137 - regression_loss: 1.3525 - classification_loss: 0.2612
 689/1500 [============>.................] - ETA: 10:54 - loss: 1.6146 - regression_loss: 1.3532 - classification_loss: 0.2614
 690/1500 [============>.................] - ETA: 10:52 - loss: 1.6145 - regression_loss: 1.3532 - classification_loss: 0.2614
 691/1500 [============>.................] - ETA: 10:52 - loss: 1.6144 - regression_loss: 1.3531 - classification_loss: 0.2612
 692/1500 [============>.................] - ETA: 10:51 - loss: 1.6148 - regression_loss: 1.3535 - classification_loss: 0.2613
 693/1500 [============>.................] - ETA: 10:50 - loss: 1.6139 - regression_loss: 1.3529 - classification_loss: 0.2611
 694/1500 [============>.................] - ETA: 10:49 - loss: 1.6139 - regression_loss: 1.3530 - classification_loss: 0.2610
 695/1500 [============>.................] - ETA: 10:48 - loss: 1.6154 - regression_loss: 1.3542 - classification_loss: 0.2611
 696/1500 [============>.................] - ETA: 10:47 - loss: 1.6152 - regression_loss: 1.3537 - classification_loss: 0.2615
 697/1500 [============>.................] - ETA: 10:46 - loss: 1.6150 - regression_loss: 1.3534 - classification_loss: 0.2616
 698/1500 [============>.................] - ETA: 10:46 - loss: 1.6157 - regression_loss: 1.3538 - classification_loss: 0.2619
 699/1500 [============>.................] - ETA: 10:44 - loss: 1.6160 - regression_loss: 1.3543 - classification_loss: 0.2618
 700/1500 [=============>................] - ETA: 10:43 - loss: 1.6158 - regression_loss: 1.3542 - classification_loss: 0.2616
 701/1500 [=============>................] - ETA: 10:42 - loss: 1.6167 - regression_loss: 1.3551 - classification_loss: 0.2616
 702/1500 [=============>................] - ETA: 10:41 - loss: 1.6173 - regression_loss: 1.3556 - classification_loss: 0.2617
 703/1500 [=============>................] - ETA: 10:40 - loss: 1.6169 - regression_loss: 1.3554 - classification_loss: 0.2615
 704/1500 [=============>................] - ETA: 10:39 - loss: 1.6176 - regression_loss: 1.3561 - classification_loss: 0.2615
 705/1500 [=============>................] - ETA: 10:38 - loss: 1.6170 - regression_loss: 1.3557 - classification_loss: 0.2613
 706/1500 [=============>................] - ETA: 10:38 - loss: 1.6166 - regression_loss: 1.3556 - classification_loss: 0.2611
 707/1500 [=============>................] - ETA: 10:37 - loss: 1.6176 - regression_loss: 1.3566 - classification_loss: 0.2610
 708/1500 [=============>................] - ETA: 10:36 - loss: 1.6169 - regression_loss: 1.3562 - classification_loss: 0.2607
 709/1500 [=============>................] - ETA: 10:35 - loss: 1.6171 - regression_loss: 1.3564 - classification_loss: 0.2607
 710/1500 [=============>................] - ETA: 10:34 - loss: 1.6171 - regression_loss: 1.3564 - classification_loss: 0.2607
 711/1500 [=============>................] - ETA: 10:33 - loss: 1.6177 - regression_loss: 1.3570 - classification_loss: 0.2606
 712/1500 [=============>................] - ETA: 10:32 - loss: 1.6181 - regression_loss: 1.3574 - classification_loss: 0.2607
 713/1500 [=============>................] - ETA: 10:31 - loss: 1.6187 - regression_loss: 1.3579 - classification_loss: 0.2609
 714/1500 [=============>................] - ETA: 10:30 - loss: 1.6193 - regression_loss: 1.3583 - classification_loss: 0.2610
 715/1500 [=============>................] - ETA: 10:29 - loss: 1.6213 - regression_loss: 1.3603 - classification_loss: 0.2609
 716/1500 [=============>................] - ETA: 10:29 - loss: 1.6218 - regression_loss: 1.3609 - classification_loss: 0.2610
 717/1500 [=============>................] - ETA: 10:29 - loss: 1.6223 - regression_loss: 1.3614 - classification_loss: 0.2609
 718/1500 [=============>................] - ETA: 10:28 - loss: 1.6221 - regression_loss: 1.3613 - classification_loss: 0.2608
 719/1500 [=============>................] - ETA: 10:27 - loss: 1.6217 - regression_loss: 1.3611 - classification_loss: 0.2606
 720/1500 [=============>................] - ETA: 10:25 - loss: 1.6223 - regression_loss: 1.3617 - classification_loss: 0.2606
 721/1500 [=============>................] - ETA: 10:25 - loss: 1.6223 - regression_loss: 1.3619 - classification_loss: 0.2604
 722/1500 [=============>................] - ETA: 10:24 - loss: 1.6238 - regression_loss: 1.3632 - classification_loss: 0.2606
 723/1500 [=============>................] - ETA: 10:24 - loss: 1.6241 - regression_loss: 1.3635 - classification_loss: 0.2606
 724/1500 [=============>................] - ETA: 10:22 - loss: 1.6237 - regression_loss: 1.3633 - classification_loss: 0.2604
 725/1500 [=============>................] - ETA: 10:22 - loss: 1.6229 - regression_loss: 1.3627 - classification_loss: 0.2602
 726/1500 [=============>................] - ETA: 10:20 - loss: 1.6219 - regression_loss: 1.3619 - classification_loss: 0.2599
 727/1500 [=============>................] - ETA: 10:20 - loss: 1.6215 - regression_loss: 1.3617 - classification_loss: 0.2599
 728/1500 [=============>................] - ETA: 10:21 - loss: 1.6218 - regression_loss: 1.3618 - classification_loss: 0.2600
 729/1500 [=============>................] - ETA: 10:20 - loss: 1.6212 - regression_loss: 1.3613 - classification_loss: 0.2599
 730/1500 [=============>................] - ETA: 10:19 - loss: 1.6201 - regression_loss: 1.3604 - classification_loss: 0.2597
 731/1500 [=============>................] - ETA: 10:19 - loss: 1.6209 - regression_loss: 1.3608 - classification_loss: 0.2601
 732/1500 [=============>................] - ETA: 10:17 - loss: 1.6207 - regression_loss: 1.3608 - classification_loss: 0.2599
 733/1500 [=============>................] - ETA: 10:17 - loss: 1.6226 - regression_loss: 1.3621 - classification_loss: 0.2605
 734/1500 [=============>................] - ETA: 10:16 - loss: 1.6240 - regression_loss: 1.3628 - classification_loss: 0.2613
 735/1500 [=============>................] - ETA: 10:15 - loss: 1.6230 - regression_loss: 1.3619 - classification_loss: 0.2611
 736/1500 [=============>................] - ETA: 10:14 - loss: 1.6235 - regression_loss: 1.3624 - classification_loss: 0.2611
 737/1500 [=============>................] - ETA: 10:14 - loss: 1.6233 - regression_loss: 1.3622 - classification_loss: 0.2611
 738/1500 [=============>................] - ETA: 10:12 - loss: 1.6222 - regression_loss: 1.3612 - classification_loss: 0.2609
 739/1500 [=============>................] - ETA: 10:12 - loss: 1.6217 - regression_loss: 1.3607 - classification_loss: 0.2610
 740/1500 [=============>................] - ETA: 10:12 - loss: 1.6202 - regression_loss: 1.3595 - classification_loss: 0.2607
 741/1500 [=============>................] - ETA: 10:11 - loss: 1.6191 - regression_loss: 1.3586 - classification_loss: 0.2605
 742/1500 [=============>................] - ETA: 10:10 - loss: 1.6188 - regression_loss: 1.3584 - classification_loss: 0.2604
 743/1500 [=============>................] - ETA: 10:09 - loss: 1.6200 - regression_loss: 1.3593 - classification_loss: 0.2607
 744/1500 [=============>................] - ETA: 10:08 - loss: 1.6197 - regression_loss: 1.3590 - classification_loss: 0.2607
 745/1500 [=============>................] - ETA: 10:07 - loss: 1.6189 - regression_loss: 1.3584 - classification_loss: 0.2605
 746/1500 [=============>................] - ETA: 10:07 - loss: 1.6191 - regression_loss: 1.3586 - classification_loss: 0.2605
 747/1500 [=============>................] - ETA: 10:07 - loss: 1.6203 - regression_loss: 1.3595 - classification_loss: 0.2608
 748/1500 [=============>................] - ETA: 10:07 - loss: 1.6215 - regression_loss: 1.3603 - classification_loss: 0.2612
 749/1500 [=============>................] - ETA: 10:06 - loss: 1.6212 - regression_loss: 1.3600 - classification_loss: 0.2612
 750/1500 [==============>...............] - ETA: 10:06 - loss: 1.6206 - regression_loss: 1.3595 - classification_loss: 0.2611
 751/1500 [==============>...............] - ETA: 10:05 - loss: 1.6211 - regression_loss: 1.3600 - classification_loss: 0.2611
 752/1500 [==============>...............] - ETA: 10:04 - loss: 1.6218 - regression_loss: 1.3607 - classification_loss: 0.2611
 753/1500 [==============>...............] - ETA: 10:03 - loss: 1.6214 - regression_loss: 1.3604 - classification_loss: 0.2610
 754/1500 [==============>...............] - ETA: 10:02 - loss: 1.6221 - regression_loss: 1.3607 - classification_loss: 0.2614
 755/1500 [==============>...............] - ETA: 10:01 - loss: 1.6215 - regression_loss: 1.3601 - classification_loss: 0.2614
 756/1500 [==============>...............] - ETA: 10:00 - loss: 1.6218 - regression_loss: 1.3604 - classification_loss: 0.2614
 757/1500 [==============>...............] - ETA: 9:59 - loss: 1.6210 - regression_loss: 1.3597 - classification_loss: 0.2613 
 758/1500 [==============>...............] - ETA: 9:58 - loss: 1.6217 - regression_loss: 1.3606 - classification_loss: 0.2611
 759/1500 [==============>...............] - ETA: 9:58 - loss: 1.6222 - regression_loss: 1.3611 - classification_loss: 0.2612
 760/1500 [==============>...............] - ETA: 9:57 - loss: 1.6219 - regression_loss: 1.3609 - classification_loss: 0.2610
 761/1500 [==============>...............] - ETA: 9:56 - loss: 1.6210 - regression_loss: 1.3602 - classification_loss: 0.2608
 762/1500 [==============>...............] - ETA: 9:55 - loss: 1.6219 - regression_loss: 1.3608 - classification_loss: 0.2610
 763/1500 [==============>...............] - ETA: 9:54 - loss: 1.6228 - regression_loss: 1.3618 - classification_loss: 0.2610
 764/1500 [==============>...............] - ETA: 9:52 - loss: 1.6233 - regression_loss: 1.3623 - classification_loss: 0.2610
 765/1500 [==============>...............] - ETA: 9:51 - loss: 1.6240 - regression_loss: 1.3629 - classification_loss: 0.2611
 766/1500 [==============>...............] - ETA: 9:50 - loss: 1.6235 - regression_loss: 1.3624 - classification_loss: 0.2611
 767/1500 [==============>...............] - ETA: 9:49 - loss: 1.6237 - regression_loss: 1.3625 - classification_loss: 0.2611
 768/1500 [==============>...............] - ETA: 9:49 - loss: 1.6240 - regression_loss: 1.3629 - classification_loss: 0.2611
 769/1500 [==============>...............] - ETA: 9:49 - loss: 1.6250 - regression_loss: 1.3638 - classification_loss: 0.2612
 770/1500 [==============>...............] - ETA: 9:49 - loss: 1.6245 - regression_loss: 1.3635 - classification_loss: 0.2611
 771/1500 [==============>...............] - ETA: 9:49 - loss: 1.6245 - regression_loss: 1.3636 - classification_loss: 0.2609
 772/1500 [==============>...............] - ETA: 9:48 - loss: 1.6253 - regression_loss: 1.3644 - classification_loss: 0.2609
 773/1500 [==============>...............] - ETA: 9:46 - loss: 1.6247 - regression_loss: 1.3640 - classification_loss: 0.2607
 774/1500 [==============>...............] - ETA: 9:46 - loss: 1.6237 - regression_loss: 1.3631 - classification_loss: 0.2606
 775/1500 [==============>...............] - ETA: 9:45 - loss: 1.6230 - regression_loss: 1.3626 - classification_loss: 0.2604
 776/1500 [==============>...............] - ETA: 9:45 - loss: 1.6226 - regression_loss: 1.3624 - classification_loss: 0.2602
 777/1500 [==============>...............] - ETA: 9:44 - loss: 1.6239 - regression_loss: 1.3634 - classification_loss: 0.2605
 778/1500 [==============>...............] - ETA: 9:43 - loss: 1.6234 - regression_loss: 1.3630 - classification_loss: 0.2604
 779/1500 [==============>...............] - ETA: 9:42 - loss: 1.6229 - regression_loss: 1.3624 - classification_loss: 0.2605
 780/1500 [==============>...............] - ETA: 9:41 - loss: 1.6215 - regression_loss: 1.3613 - classification_loss: 0.2602
 781/1500 [==============>...............] - ETA: 9:41 - loss: 1.6206 - regression_loss: 1.3606 - classification_loss: 0.2600
 782/1500 [==============>...............] - ETA: 9:41 - loss: 1.6199 - regression_loss: 1.3601 - classification_loss: 0.2598
 783/1500 [==============>...............] - ETA: 9:39 - loss: 1.6195 - regression_loss: 1.3598 - classification_loss: 0.2597
 784/1500 [==============>...............] - ETA: 9:38 - loss: 1.6194 - regression_loss: 1.3598 - classification_loss: 0.2596
 785/1500 [==============>...............] - ETA: 9:37 - loss: 1.6199 - regression_loss: 1.3601 - classification_loss: 0.2597
 786/1500 [==============>...............] - ETA: 9:37 - loss: 1.6195 - regression_loss: 1.3599 - classification_loss: 0.2596
 787/1500 [==============>...............] - ETA: 9:36 - loss: 1.6201 - regression_loss: 1.3606 - classification_loss: 0.2595
 788/1500 [==============>...............] - ETA: 9:35 - loss: 1.6194 - regression_loss: 1.3600 - classification_loss: 0.2593
 789/1500 [==============>...............] - ETA: 9:34 - loss: 1.6204 - regression_loss: 1.3608 - classification_loss: 0.2595
 790/1500 [==============>...............] - ETA: 9:34 - loss: 1.6202 - regression_loss: 1.3607 - classification_loss: 0.2595
 791/1500 [==============>...............] - ETA: 9:33 - loss: 1.6199 - regression_loss: 1.3605 - classification_loss: 0.2594
 792/1500 [==============>...............] - ETA: 9:33 - loss: 1.6191 - regression_loss: 1.3598 - classification_loss: 0.2592
 793/1500 [==============>...............] - ETA: 9:32 - loss: 1.6187 - regression_loss: 1.3592 - classification_loss: 0.2595
 794/1500 [==============>...............] - ETA: 9:31 - loss: 1.6182 - regression_loss: 1.3589 - classification_loss: 0.2593
 795/1500 [==============>...............] - ETA: 9:29 - loss: 1.6185 - regression_loss: 1.3591 - classification_loss: 0.2594
 796/1500 [==============>...............] - ETA: 9:28 - loss: 1.6183 - regression_loss: 1.3590 - classification_loss: 0.2593
 797/1500 [==============>...............] - ETA: 9:27 - loss: 1.6181 - regression_loss: 1.3589 - classification_loss: 0.2592
 798/1500 [==============>...............] - ETA: 9:26 - loss: 1.6176 - regression_loss: 1.3586 - classification_loss: 0.2590
 799/1500 [==============>...............] - ETA: 9:25 - loss: 1.6170 - regression_loss: 1.3582 - classification_loss: 0.2589
 800/1500 [===============>..............] - ETA: 9:24 - loss: 1.6160 - regression_loss: 1.3573 - classification_loss: 0.2587
 801/1500 [===============>..............] - ETA: 9:23 - loss: 1.6153 - regression_loss: 1.3568 - classification_loss: 0.2585
 802/1500 [===============>..............] - ETA: 9:22 - loss: 1.6148 - regression_loss: 1.3565 - classification_loss: 0.2583
 803/1500 [===============>..............] - ETA: 9:21 - loss: 1.6141 - regression_loss: 1.3559 - classification_loss: 0.2582
 804/1500 [===============>..............] - ETA: 9:20 - loss: 1.6133 - regression_loss: 1.3553 - classification_loss: 0.2580
 805/1500 [===============>..............] - ETA: 9:19 - loss: 1.6137 - regression_loss: 1.3557 - classification_loss: 0.2579
 806/1500 [===============>..............] - ETA: 9:18 - loss: 1.6135 - regression_loss: 1.3557 - classification_loss: 0.2578
 807/1500 [===============>..............] - ETA: 9:18 - loss: 1.6146 - regression_loss: 1.3561 - classification_loss: 0.2585
 808/1500 [===============>..............] - ETA: 9:17 - loss: 1.6144 - regression_loss: 1.3558 - classification_loss: 0.2586
 809/1500 [===============>..............] - ETA: 9:16 - loss: 1.6141 - regression_loss: 1.3556 - classification_loss: 0.2585
 810/1500 [===============>..............] - ETA: 9:16 - loss: 1.6133 - regression_loss: 1.3550 - classification_loss: 0.2583
 811/1500 [===============>..............] - ETA: 9:17 - loss: 1.6134 - regression_loss: 1.3550 - classification_loss: 0.2584
 812/1500 [===============>..............] - ETA: 9:15 - loss: 1.6127 - regression_loss: 1.3544 - classification_loss: 0.2583
 813/1500 [===============>..............] - ETA: 9:14 - loss: 1.6132 - regression_loss: 1.3547 - classification_loss: 0.2585
 814/1500 [===============>..............] - ETA: 9:13 - loss: 1.6142 - regression_loss: 1.3557 - classification_loss: 0.2584
 815/1500 [===============>..............] - ETA: 9:12 - loss: 1.6149 - regression_loss: 1.3561 - classification_loss: 0.2588
 816/1500 [===============>..............] - ETA: 9:12 - loss: 1.6155 - regression_loss: 1.3566 - classification_loss: 0.2589
 817/1500 [===============>..............] - ETA: 9:10 - loss: 1.6159 - regression_loss: 1.3568 - classification_loss: 0.2591
 818/1500 [===============>..............] - ETA: 9:09 - loss: 1.6149 - regression_loss: 1.3559 - classification_loss: 0.2590
 819/1500 [===============>..............] - ETA: 9:11 - loss: 1.6137 - regression_loss: 1.3549 - classification_loss: 0.2588
 820/1500 [===============>..............] - ETA: 9:09 - loss: 1.6151 - regression_loss: 1.3558 - classification_loss: 0.2593
 821/1500 [===============>..............] - ETA: 9:08 - loss: 1.6137 - regression_loss: 1.3546 - classification_loss: 0.2591
 822/1500 [===============>..............] - ETA: 9:07 - loss: 1.6131 - regression_loss: 1.3540 - classification_loss: 0.2590
 823/1500 [===============>..............] - ETA: 9:07 - loss: 1.6137 - regression_loss: 1.3547 - classification_loss: 0.2590
 824/1500 [===============>..............] - ETA: 9:06 - loss: 1.6130 - regression_loss: 1.3541 - classification_loss: 0.2589
 825/1500 [===============>..............] - ETA: 9:05 - loss: 1.6120 - regression_loss: 1.3534 - classification_loss: 0.2586
 826/1500 [===============>..............] - ETA: 9:04 - loss: 1.6130 - regression_loss: 1.3541 - classification_loss: 0.2589
 827/1500 [===============>..............] - ETA: 9:03 - loss: 1.6121 - regression_loss: 1.3532 - classification_loss: 0.2589
 828/1500 [===============>..............] - ETA: 9:02 - loss: 1.6112 - regression_loss: 1.3525 - classification_loss: 0.2587
 829/1500 [===============>..............] - ETA: 9:02 - loss: 1.6109 - regression_loss: 1.3524 - classification_loss: 0.2586
 830/1500 [===============>..............] - ETA: 9:01 - loss: 1.6109 - regression_loss: 1.3524 - classification_loss: 0.2585
 831/1500 [===============>..............] - ETA: 9:00 - loss: 1.6102 - regression_loss: 1.3518 - classification_loss: 0.2584
 832/1500 [===============>..............] - ETA: 8:59 - loss: 1.6092 - regression_loss: 1.3511 - classification_loss: 0.2582
 833/1500 [===============>..............] - ETA: 8:58 - loss: 1.6101 - regression_loss: 1.3519 - classification_loss: 0.2582
 834/1500 [===============>..............] - ETA: 8:57 - loss: 1.6088 - regression_loss: 1.3508 - classification_loss: 0.2579
 835/1500 [===============>..............] - ETA: 8:57 - loss: 1.6081 - regression_loss: 1.3503 - classification_loss: 0.2578
 836/1500 [===============>..............] - ETA: 8:56 - loss: 1.6083 - regression_loss: 1.3505 - classification_loss: 0.2578
 837/1500 [===============>..............] - ETA: 8:56 - loss: 1.6072 - regression_loss: 1.3496 - classification_loss: 0.2577
 838/1500 [===============>..............] - ETA: 8:55 - loss: 1.6064 - regression_loss: 1.3489 - classification_loss: 0.2575
 839/1500 [===============>..............] - ETA: 8:54 - loss: 1.6057 - regression_loss: 1.3484 - classification_loss: 0.2573
 840/1500 [===============>..............] - ETA: 8:54 - loss: 1.6050 - regression_loss: 1.3476 - classification_loss: 0.2574
 841/1500 [===============>..............] - ETA: 8:53 - loss: 1.6057 - regression_loss: 1.3482 - classification_loss: 0.2575
 842/1500 [===============>..............] - ETA: 8:52 - loss: 1.6047 - regression_loss: 1.3473 - classification_loss: 0.2573
 843/1500 [===============>..............] - ETA: 8:51 - loss: 1.6042 - regression_loss: 1.3468 - classification_loss: 0.2573
 844/1500 [===============>..............] - ETA: 8:50 - loss: 1.6040 - regression_loss: 1.3467 - classification_loss: 0.2573
 845/1500 [===============>..............] - ETA: 8:49 - loss: 1.6046 - regression_loss: 1.3472 - classification_loss: 0.2574
 846/1500 [===============>..............] - ETA: 8:49 - loss: 1.6055 - regression_loss: 1.3480 - classification_loss: 0.2575
 847/1500 [===============>..............] - ETA: 8:48 - loss: 1.6065 - regression_loss: 1.3487 - classification_loss: 0.2578
 848/1500 [===============>..............] - ETA: 8:47 - loss: 1.6063 - regression_loss: 1.3486 - classification_loss: 0.2577
 849/1500 [===============>..............] - ETA: 8:47 - loss: 1.6055 - regression_loss: 1.3479 - classification_loss: 0.2576
 850/1500 [================>.............] - ETA: 8:46 - loss: 1.6054 - regression_loss: 1.3479 - classification_loss: 0.2575
 851/1500 [================>.............] - ETA: 8:45 - loss: 1.6057 - regression_loss: 1.3481 - classification_loss: 0.2575
 852/1500 [================>.............] - ETA: 8:44 - loss: 1.6050 - regression_loss: 1.3475 - classification_loss: 0.2575
 853/1500 [================>.............] - ETA: 8:43 - loss: 1.6048 - regression_loss: 1.3474 - classification_loss: 0.2575
 854/1500 [================>.............] - ETA: 8:43 - loss: 1.6042 - regression_loss: 1.3469 - classification_loss: 0.2573
 855/1500 [================>.............] - ETA: 8:42 - loss: 1.6044 - regression_loss: 1.3469 - classification_loss: 0.2575
 856/1500 [================>.............] - ETA: 8:41 - loss: 1.6037 - regression_loss: 1.3463 - classification_loss: 0.2574
 857/1500 [================>.............] - ETA: 8:40 - loss: 1.6035 - regression_loss: 1.3462 - classification_loss: 0.2573
 858/1500 [================>.............] - ETA: 8:39 - loss: 1.6038 - regression_loss: 1.3466 - classification_loss: 0.2572
 859/1500 [================>.............] - ETA: 8:38 - loss: 1.6041 - regression_loss: 1.3469 - classification_loss: 0.2572
 860/1500 [================>.............] - ETA: 8:37 - loss: 1.6039 - regression_loss: 1.3469 - classification_loss: 0.2571
 861/1500 [================>.............] - ETA: 8:36 - loss: 1.6056 - regression_loss: 1.3484 - classification_loss: 0.2572
 862/1500 [================>.............] - ETA: 8:35 - loss: 1.6052 - regression_loss: 1.3481 - classification_loss: 0.2570
 863/1500 [================>.............] - ETA: 8:35 - loss: 1.6042 - regression_loss: 1.3474 - classification_loss: 0.2568
 864/1500 [================>.............] - ETA: 8:34 - loss: 1.6030 - regression_loss: 1.3463 - classification_loss: 0.2566
 865/1500 [================>.............] - ETA: 8:33 - loss: 1.6042 - regression_loss: 1.3474 - classification_loss: 0.2567
 866/1500 [================>.............] - ETA: 8:32 - loss: 1.6032 - regression_loss: 1.3466 - classification_loss: 0.2566
 867/1500 [================>.............] - ETA: 8:31 - loss: 1.6029 - regression_loss: 1.3464 - classification_loss: 0.2565
 868/1500 [================>.............] - ETA: 8:30 - loss: 1.6024 - regression_loss: 1.3460 - classification_loss: 0.2564
 869/1500 [================>.............] - ETA: 8:30 - loss: 1.6030 - regression_loss: 1.3465 - classification_loss: 0.2565
 870/1500 [================>.............] - ETA: 8:29 - loss: 1.6031 - regression_loss: 1.3467 - classification_loss: 0.2564
 871/1500 [================>.............] - ETA: 8:28 - loss: 1.6033 - regression_loss: 1.3469 - classification_loss: 0.2564
 872/1500 [================>.............] - ETA: 8:27 - loss: 1.6023 - regression_loss: 1.3462 - classification_loss: 0.2561
 873/1500 [================>.............] - ETA: 8:26 - loss: 1.6025 - regression_loss: 1.3464 - classification_loss: 0.2561
 874/1500 [================>.............] - ETA: 8:26 - loss: 1.6015 - regression_loss: 1.3456 - classification_loss: 0.2559
 875/1500 [================>.............] - ETA: 8:24 - loss: 1.6013 - regression_loss: 1.3454 - classification_loss: 0.2559
 876/1500 [================>.............] - ETA: 8:24 - loss: 1.6005 - regression_loss: 1.3447 - classification_loss: 0.2558
 877/1500 [================>.............] - ETA: 8:23 - loss: 1.5995 - regression_loss: 1.3439 - classification_loss: 0.2556
 878/1500 [================>.............] - ETA: 8:22 - loss: 1.6004 - regression_loss: 1.3448 - classification_loss: 0.2556
 879/1500 [================>.............] - ETA: 8:22 - loss: 1.6016 - regression_loss: 1.3461 - classification_loss: 0.2556
 880/1500 [================>.............] - ETA: 8:21 - loss: 1.6013 - regression_loss: 1.3459 - classification_loss: 0.2554
 881/1500 [================>.............] - ETA: 8:20 - loss: 1.6002 - regression_loss: 1.3450 - classification_loss: 0.2552
 882/1500 [================>.............] - ETA: 8:19 - loss: 1.6007 - regression_loss: 1.3454 - classification_loss: 0.2553
 883/1500 [================>.............] - ETA: 8:19 - loss: 1.5999 - regression_loss: 1.3448 - classification_loss: 0.2552
 884/1500 [================>.............] - ETA: 8:18 - loss: 1.6014 - regression_loss: 1.3456 - classification_loss: 0.2559
 885/1500 [================>.............] - ETA: 8:18 - loss: 1.6026 - regression_loss: 1.3462 - classification_loss: 0.2564
 886/1500 [================>.............] - ETA: 8:18 - loss: 1.6034 - regression_loss: 1.3469 - classification_loss: 0.2565
 887/1500 [================>.............] - ETA: 8:17 - loss: 1.6033 - regression_loss: 1.3468 - classification_loss: 0.2564
 888/1500 [================>.............] - ETA: 8:16 - loss: 1.6041 - regression_loss: 1.3475 - classification_loss: 0.2566
 889/1500 [================>.............] - ETA: 8:15 - loss: 1.6048 - regression_loss: 1.3483 - classification_loss: 0.2566
 890/1500 [================>.............] - ETA: 8:14 - loss: 1.6046 - regression_loss: 1.3480 - classification_loss: 0.2566
 891/1500 [================>.............] - ETA: 8:13 - loss: 1.6042 - regression_loss: 1.3477 - classification_loss: 0.2566
 892/1500 [================>.............] - ETA: 8:12 - loss: 1.6040 - regression_loss: 1.3475 - classification_loss: 0.2565
 893/1500 [================>.............] - ETA: 8:11 - loss: 1.6034 - regression_loss: 1.3469 - classification_loss: 0.2565
 894/1500 [================>.............] - ETA: 8:10 - loss: 1.6022 - regression_loss: 1.3459 - classification_loss: 0.2563
 895/1500 [================>.............] - ETA: 8:10 - loss: 1.6023 - regression_loss: 1.3459 - classification_loss: 0.2563
 896/1500 [================>.............] - ETA: 8:09 - loss: 1.6017 - regression_loss: 1.3454 - classification_loss: 0.2562
 897/1500 [================>.............] - ETA: 8:08 - loss: 1.6004 - regression_loss: 1.3444 - classification_loss: 0.2560
 898/1500 [================>.............] - ETA: 8:06 - loss: 1.6010 - regression_loss: 1.3450 - classification_loss: 0.2560
 899/1500 [================>.............] - ETA: 8:05 - loss: 1.6014 - regression_loss: 1.3455 - classification_loss: 0.2559
 900/1500 [=================>............] - ETA: 8:04 - loss: 1.6015 - regression_loss: 1.3457 - classification_loss: 0.2558
 901/1500 [=================>............] - ETA: 8:03 - loss: 1.6013 - regression_loss: 1.3455 - classification_loss: 0.2558
 902/1500 [=================>............] - ETA: 8:03 - loss: 1.6011 - regression_loss: 1.3454 - classification_loss: 0.2557
 903/1500 [=================>............] - ETA: 8:02 - loss: 1.6005 - regression_loss: 1.3450 - classification_loss: 0.2555
 904/1500 [=================>............] - ETA: 8:02 - loss: 1.6009 - regression_loss: 1.3453 - classification_loss: 0.2556
 905/1500 [=================>............] - ETA: 8:01 - loss: 1.6023 - regression_loss: 1.3464 - classification_loss: 0.2558
 906/1500 [=================>............] - ETA: 8:00 - loss: 1.6010 - regression_loss: 1.3454 - classification_loss: 0.2557
 907/1500 [=================>............] - ETA: 7:59 - loss: 1.6002 - regression_loss: 1.3447 - classification_loss: 0.2556
 908/1500 [=================>............] - ETA: 7:58 - loss: 1.6001 - regression_loss: 1.3446 - classification_loss: 0.2555
 909/1500 [=================>............] - ETA: 7:57 - loss: 1.5993 - regression_loss: 1.3439 - classification_loss: 0.2554
 910/1500 [=================>............] - ETA: 7:56 - loss: 1.5981 - regression_loss: 1.3428 - classification_loss: 0.2553
 911/1500 [=================>............] - ETA: 7:55 - loss: 1.5990 - regression_loss: 1.3436 - classification_loss: 0.2554
 912/1500 [=================>............] - ETA: 7:54 - loss: 1.5979 - regression_loss: 1.3427 - classification_loss: 0.2552
 913/1500 [=================>............] - ETA: 7:53 - loss: 1.5980 - regression_loss: 1.3429 - classification_loss: 0.2551
 914/1500 [=================>............] - ETA: 7:52 - loss: 1.5978 - regression_loss: 1.3426 - classification_loss: 0.2552
 915/1500 [=================>............] - ETA: 7:52 - loss: 1.5969 - regression_loss: 1.3419 - classification_loss: 0.2550
 916/1500 [=================>............] - ETA: 7:51 - loss: 1.5961 - regression_loss: 1.3413 - classification_loss: 0.2548
 917/1500 [=================>............] - ETA: 7:50 - loss: 1.5961 - regression_loss: 1.3413 - classification_loss: 0.2548
 918/1500 [=================>............] - ETA: 7:49 - loss: 1.5972 - regression_loss: 1.3423 - classification_loss: 0.2549
 919/1500 [=================>............] - ETA: 7:48 - loss: 1.5963 - regression_loss: 1.3416 - classification_loss: 0.2548
 920/1500 [=================>............] - ETA: 7:47 - loss: 1.5965 - regression_loss: 1.3416 - classification_loss: 0.2549
 921/1500 [=================>............] - ETA: 7:46 - loss: 1.5960 - regression_loss: 1.3412 - classification_loss: 0.2549
 922/1500 [=================>............] - ETA: 7:45 - loss: 1.5957 - regression_loss: 1.3410 - classification_loss: 0.2548
 923/1500 [=================>............] - ETA: 7:45 - loss: 1.5948 - regression_loss: 1.3402 - classification_loss: 0.2546
 924/1500 [=================>............] - ETA: 7:44 - loss: 1.5946 - regression_loss: 1.3401 - classification_loss: 0.2545
 925/1500 [=================>............] - ETA: 7:43 - loss: 1.5938 - regression_loss: 1.3394 - classification_loss: 0.2544
 926/1500 [=================>............] - ETA: 7:43 - loss: 1.5933 - regression_loss: 1.3390 - classification_loss: 0.2543
 927/1500 [=================>............] - ETA: 7:42 - loss: 1.5925 - regression_loss: 1.3383 - classification_loss: 0.2542
 928/1500 [=================>............] - ETA: 7:41 - loss: 1.5924 - regression_loss: 1.3383 - classification_loss: 0.2541
 929/1500 [=================>............] - ETA: 7:40 - loss: 1.5936 - regression_loss: 1.3391 - classification_loss: 0.2544
 930/1500 [=================>............] - ETA: 7:39 - loss: 1.5946 - regression_loss: 1.3399 - classification_loss: 0.2547
 931/1500 [=================>............] - ETA: 7:39 - loss: 1.5955 - regression_loss: 1.3406 - classification_loss: 0.2548
 932/1500 [=================>............] - ETA: 7:38 - loss: 1.5953 - regression_loss: 1.3406 - classification_loss: 0.2547
 933/1500 [=================>............] - ETA: 7:38 - loss: 1.5971 - regression_loss: 1.3417 - classification_loss: 0.2554
 934/1500 [=================>............] - ETA: 7:37 - loss: 1.5965 - regression_loss: 1.3413 - classification_loss: 0.2552
 935/1500 [=================>............] - ETA: 7:36 - loss: 1.5964 - regression_loss: 1.3414 - classification_loss: 0.2550
 936/1500 [=================>............] - ETA: 7:34 - loss: 1.5973 - regression_loss: 1.3422 - classification_loss: 0.2551
 937/1500 [=================>............] - ETA: 7:35 - loss: 1.5981 - regression_loss: 1.3429 - classification_loss: 0.2552
 938/1500 [=================>............] - ETA: 7:35 - loss: 1.5975 - regression_loss: 1.3425 - classification_loss: 0.2550
 939/1500 [=================>............] - ETA: 7:34 - loss: 1.5977 - regression_loss: 1.3426 - classification_loss: 0.2551
 940/1500 [=================>............] - ETA: 7:33 - loss: 1.5979 - regression_loss: 1.3426 - classification_loss: 0.2553
 941/1500 [=================>............] - ETA: 7:32 - loss: 1.5974 - regression_loss: 1.3423 - classification_loss: 0.2552
 942/1500 [=================>............] - ETA: 7:31 - loss: 1.5975 - regression_loss: 1.3424 - classification_loss: 0.2551
 943/1500 [=================>............] - ETA: 7:31 - loss: 1.5971 - regression_loss: 1.3420 - classification_loss: 0.2551
 944/1500 [=================>............] - ETA: 7:30 - loss: 1.5971 - regression_loss: 1.3421 - classification_loss: 0.2551
 945/1500 [=================>............] - ETA: 7:29 - loss: 1.5972 - regression_loss: 1.3422 - classification_loss: 0.2550
 946/1500 [=================>............] - ETA: 7:28 - loss: 1.5970 - regression_loss: 1.3419 - classification_loss: 0.2550
 947/1500 [=================>............] - ETA: 7:27 - loss: 1.5961 - regression_loss: 1.3412 - classification_loss: 0.2549
 948/1500 [=================>............] - ETA: 7:26 - loss: 1.5962 - regression_loss: 1.3412 - classification_loss: 0.2549
 949/1500 [=================>............] - ETA: 7:25 - loss: 1.5969 - regression_loss: 1.3418 - classification_loss: 0.2551
 950/1500 [==================>...........] - ETA: 7:24 - loss: 1.5970 - regression_loss: 1.3420 - classification_loss: 0.2550
 951/1500 [==================>...........] - ETA: 7:23 - loss: 1.5972 - regression_loss: 1.3422 - classification_loss: 0.2551
 952/1500 [==================>...........] - ETA: 7:22 - loss: 1.5976 - regression_loss: 1.3424 - classification_loss: 0.2552
 953/1500 [==================>...........] - ETA: 7:22 - loss: 1.5975 - regression_loss: 1.3423 - classification_loss: 0.2552
 954/1500 [==================>...........] - ETA: 7:21 - loss: 1.5988 - regression_loss: 1.3435 - classification_loss: 0.2554
 955/1500 [==================>...........] - ETA: 7:20 - loss: 1.5982 - regression_loss: 1.3430 - classification_loss: 0.2552
 956/1500 [==================>...........] - ETA: 7:19 - loss: 1.5984 - regression_loss: 1.3432 - classification_loss: 0.2552
 957/1500 [==================>...........] - ETA: 7:18 - loss: 1.5990 - regression_loss: 1.3437 - classification_loss: 0.2553
 958/1500 [==================>...........] - ETA: 7:17 - loss: 1.5981 - regression_loss: 1.3429 - classification_loss: 0.2551
 959/1500 [==================>...........] - ETA: 7:16 - loss: 1.5975 - regression_loss: 1.3425 - classification_loss: 0.2550
 960/1500 [==================>...........] - ETA: 7:15 - loss: 1.5971 - regression_loss: 1.3423 - classification_loss: 0.2549
 961/1500 [==================>...........] - ETA: 7:15 - loss: 1.5977 - regression_loss: 1.3423 - classification_loss: 0.2554
 962/1500 [==================>...........] - ETA: 7:14 - loss: 1.5972 - regression_loss: 1.3419 - classification_loss: 0.2552
 963/1500 [==================>...........] - ETA: 7:14 - loss: 1.5965 - regression_loss: 1.3415 - classification_loss: 0.2550
 964/1500 [==================>...........] - ETA: 7:13 - loss: 1.5954 - regression_loss: 1.3405 - classification_loss: 0.2549
 965/1500 [==================>...........] - ETA: 7:12 - loss: 1.5951 - regression_loss: 1.3404 - classification_loss: 0.2548
 966/1500 [==================>...........] - ETA: 7:11 - loss: 1.5950 - regression_loss: 1.3404 - classification_loss: 0.2546
 967/1500 [==================>...........] - ETA: 7:10 - loss: 1.5947 - regression_loss: 1.3401 - classification_loss: 0.2545
 968/1500 [==================>...........] - ETA: 7:09 - loss: 1.5950 - regression_loss: 1.3405 - classification_loss: 0.2545
 969/1500 [==================>...........] - ETA: 7:09 - loss: 1.5956 - regression_loss: 1.3410 - classification_loss: 0.2546
 970/1500 [==================>...........] - ETA: 7:08 - loss: 1.5949 - regression_loss: 1.3404 - classification_loss: 0.2545
 971/1500 [==================>...........] - ETA: 7:07 - loss: 1.5946 - regression_loss: 1.3402 - classification_loss: 0.2544
 972/1500 [==================>...........] - ETA: 7:06 - loss: 1.5947 - regression_loss: 1.3402 - classification_loss: 0.2544
 973/1500 [==================>...........] - ETA: 7:05 - loss: 1.5946 - regression_loss: 1.3403 - classification_loss: 0.2543
 974/1500 [==================>...........] - ETA: 7:04 - loss: 1.5939 - regression_loss: 1.3397 - classification_loss: 0.2542
 975/1500 [==================>...........] - ETA: 7:03 - loss: 1.5932 - regression_loss: 1.3391 - classification_loss: 0.2541
 976/1500 [==================>...........] - ETA: 7:02 - loss: 1.5925 - regression_loss: 1.3386 - classification_loss: 0.2539
 977/1500 [==================>...........] - ETA: 7:01 - loss: 1.5926 - regression_loss: 1.3388 - classification_loss: 0.2538
 978/1500 [==================>...........] - ETA: 7:00 - loss: 1.5928 - regression_loss: 1.3389 - classification_loss: 0.2539
 979/1500 [==================>...........] - ETA: 6:59 - loss: 1.5935 - regression_loss: 1.3394 - classification_loss: 0.2541
 980/1500 [==================>...........] - ETA: 6:58 - loss: 1.5948 - regression_loss: 1.3403 - classification_loss: 0.2545
 981/1500 [==================>...........] - ETA: 6:57 - loss: 1.5946 - regression_loss: 1.3402 - classification_loss: 0.2544
 982/1500 [==================>...........] - ETA: 6:56 - loss: 1.5953 - regression_loss: 1.3407 - classification_loss: 0.2546
 983/1500 [==================>...........] - ETA: 6:55 - loss: 1.5956 - regression_loss: 1.3409 - classification_loss: 0.2547
 984/1500 [==================>...........] - ETA: 6:55 - loss: 1.5961 - regression_loss: 1.3413 - classification_loss: 0.2548
 985/1500 [==================>...........] - ETA: 6:54 - loss: 1.5953 - regression_loss: 1.3407 - classification_loss: 0.2546
 986/1500 [==================>...........] - ETA: 6:53 - loss: 1.5947 - regression_loss: 1.3401 - classification_loss: 0.2546
 987/1500 [==================>...........] - ETA: 6:52 - loss: 1.5951 - regression_loss: 1.3404 - classification_loss: 0.2547
 988/1500 [==================>...........] - ETA: 6:51 - loss: 1.5943 - regression_loss: 1.3396 - classification_loss: 0.2547
 989/1500 [==================>...........] - ETA: 6:50 - loss: 1.5934 - regression_loss: 1.3389 - classification_loss: 0.2545
 990/1500 [==================>...........] - ETA: 6:49 - loss: 1.5938 - regression_loss: 1.3392 - classification_loss: 0.2546
 991/1500 [==================>...........] - ETA: 6:48 - loss: 1.5933 - regression_loss: 1.3389 - classification_loss: 0.2544
 992/1500 [==================>...........] - ETA: 6:47 - loss: 1.5933 - regression_loss: 1.3389 - classification_loss: 0.2544
 993/1500 [==================>...........] - ETA: 6:47 - loss: 1.5925 - regression_loss: 1.3382 - classification_loss: 0.2543
 994/1500 [==================>...........] - ETA: 6:46 - loss: 1.5921 - regression_loss: 1.3379 - classification_loss: 0.2541
 995/1500 [==================>...........] - ETA: 6:45 - loss: 1.5932 - regression_loss: 1.3388 - classification_loss: 0.2544
 996/1500 [==================>...........] - ETA: 6:44 - loss: 1.5935 - regression_loss: 1.3391 - classification_loss: 0.2544
 997/1500 [==================>...........] - ETA: 6:43 - loss: 1.5930 - regression_loss: 1.3386 - classification_loss: 0.2544
 998/1500 [==================>...........] - ETA: 6:42 - loss: 1.5919 - regression_loss: 1.3377 - classification_loss: 0.2542
 999/1500 [==================>...........] - ETA: 6:41 - loss: 1.5908 - regression_loss: 1.3368 - classification_loss: 0.2540
1000/1500 [===================>..........] - ETA: 6:40 - loss: 1.5909 - regression_loss: 1.3369 - classification_loss: 0.2540
1001/1500 [===================>..........] - ETA: 6:39 - loss: 1.5903 - regression_loss: 1.3365 - classification_loss: 0.2538
1002/1500 [===================>..........] - ETA: 6:38 - loss: 1.5901 - regression_loss: 1.3362 - classification_loss: 0.2538
1003/1500 [===================>..........] - ETA: 6:37 - loss: 1.5892 - regression_loss: 1.3355 - classification_loss: 0.2537
1004/1500 [===================>..........] - ETA: 6:36 - loss: 1.5892 - regression_loss: 1.3354 - classification_loss: 0.2538
1005/1500 [===================>..........] - ETA: 6:36 - loss: 1.5898 - regression_loss: 1.3360 - classification_loss: 0.2538
1006/1500 [===================>..........] - ETA: 6:35 - loss: 1.5892 - regression_loss: 1.3354 - classification_loss: 0.2538
1007/1500 [===================>..........] - ETA: 6:34 - loss: 1.5887 - regression_loss: 1.3349 - classification_loss: 0.2538
1008/1500 [===================>..........] - ETA: 6:34 - loss: 1.5891 - regression_loss: 1.3352 - classification_loss: 0.2540
1009/1500 [===================>..........] - ETA: 6:33 - loss: 1.5887 - regression_loss: 1.3348 - classification_loss: 0.2538
1010/1500 [===================>..........] - ETA: 6:32 - loss: 1.5878 - regression_loss: 1.3341 - classification_loss: 0.2537
1011/1500 [===================>..........] - ETA: 6:31 - loss: 1.5868 - regression_loss: 1.3333 - classification_loss: 0.2535
1012/1500 [===================>..........] - ETA: 6:30 - loss: 1.5867 - regression_loss: 1.3333 - classification_loss: 0.2534
1013/1500 [===================>..........] - ETA: 6:29 - loss: 1.5861 - regression_loss: 1.3329 - classification_loss: 0.2533
1014/1500 [===================>..........] - ETA: 6:28 - loss: 1.5856 - regression_loss: 1.3324 - classification_loss: 0.2532
1015/1500 [===================>..........] - ETA: 6:27 - loss: 1.5858 - regression_loss: 1.3325 - classification_loss: 0.2533
1016/1500 [===================>..........] - ETA: 6:26 - loss: 1.5851 - regression_loss: 1.3318 - classification_loss: 0.2533
1017/1500 [===================>..........] - ETA: 6:26 - loss: 1.5848 - regression_loss: 1.3315 - classification_loss: 0.2533
1018/1500 [===================>..........] - ETA: 6:25 - loss: 1.5854 - regression_loss: 1.3317 - classification_loss: 0.2537
1019/1500 [===================>..........] - ETA: 6:24 - loss: 1.5852 - regression_loss: 1.3314 - classification_loss: 0.2537
1020/1500 [===================>..........] - ETA: 6:23 - loss: 1.5850 - regression_loss: 1.3313 - classification_loss: 0.2537
1021/1500 [===================>..........] - ETA: 6:22 - loss: 1.5843 - regression_loss: 1.3307 - classification_loss: 0.2536
1022/1500 [===================>..........] - ETA: 6:21 - loss: 1.5836 - regression_loss: 1.3302 - classification_loss: 0.2535
1023/1500 [===================>..........] - ETA: 6:21 - loss: 1.5831 - regression_loss: 1.3297 - classification_loss: 0.2534
1024/1500 [===================>..........] - ETA: 6:21 - loss: 1.5824 - regression_loss: 1.3291 - classification_loss: 0.2533
1025/1500 [===================>..........] - ETA: 6:20 - loss: 1.5832 - regression_loss: 1.3298 - classification_loss: 0.2534
1026/1500 [===================>..........] - ETA: 6:19 - loss: 1.5827 - regression_loss: 1.3294 - classification_loss: 0.2533
1027/1500 [===================>..........] - ETA: 6:19 - loss: 1.5826 - regression_loss: 1.3294 - classification_loss: 0.2533
1028/1500 [===================>..........] - ETA: 6:18 - loss: 1.5836 - regression_loss: 1.3301 - classification_loss: 0.2534
1029/1500 [===================>..........] - ETA: 6:17 - loss: 1.5840 - regression_loss: 1.3305 - classification_loss: 0.2534
1030/1500 [===================>..........] - ETA: 6:16 - loss: 1.5838 - regression_loss: 1.3302 - classification_loss: 0.2537
1031/1500 [===================>..........] - ETA: 6:15 - loss: 1.5844 - regression_loss: 1.3307 - classification_loss: 0.2537
1032/1500 [===================>..........] - ETA: 6:14 - loss: 1.5842 - regression_loss: 1.3305 - classification_loss: 0.2536
1033/1500 [===================>..........] - ETA: 6:13 - loss: 1.5844 - regression_loss: 1.3307 - classification_loss: 0.2537
1034/1500 [===================>..........] - ETA: 6:13 - loss: 1.5838 - regression_loss: 1.3303 - classification_loss: 0.2535
1035/1500 [===================>..........] - ETA: 6:12 - loss: 1.5838 - regression_loss: 1.3304 - classification_loss: 0.2534
1036/1500 [===================>..........] - ETA: 6:12 - loss: 1.5844 - regression_loss: 1.3311 - classification_loss: 0.2534
1037/1500 [===================>..........] - ETA: 6:11 - loss: 1.5848 - regression_loss: 1.3314 - classification_loss: 0.2534
1038/1500 [===================>..........] - ETA: 6:10 - loss: 1.5848 - regression_loss: 1.3314 - classification_loss: 0.2534
1039/1500 [===================>..........] - ETA: 6:09 - loss: 1.5858 - regression_loss: 1.3322 - classification_loss: 0.2537
1040/1500 [===================>..........] - ETA: 6:09 - loss: 1.5874 - regression_loss: 1.3337 - classification_loss: 0.2538
1041/1500 [===================>..........] - ETA: 6:08 - loss: 1.5881 - regression_loss: 1.3341 - classification_loss: 0.2540
1042/1500 [===================>..........] - ETA: 6:08 - loss: 1.5886 - regression_loss: 1.3345 - classification_loss: 0.2541
1043/1500 [===================>..........] - ETA: 6:07 - loss: 1.5886 - regression_loss: 1.3345 - classification_loss: 0.2541
1044/1500 [===================>..........] - ETA: 6:06 - loss: 1.5888 - regression_loss: 1.3347 - classification_loss: 0.2541
1045/1500 [===================>..........] - ETA: 6:05 - loss: 1.5893 - regression_loss: 1.3351 - classification_loss: 0.2542
1046/1500 [===================>..........] - ETA: 6:05 - loss: 1.5900 - regression_loss: 1.3357 - classification_loss: 0.2543
1047/1500 [===================>..........] - ETA: 6:04 - loss: 1.5893 - regression_loss: 1.3352 - classification_loss: 0.2542
1048/1500 [===================>..........] - ETA: 6:03 - loss: 1.5894 - regression_loss: 1.3353 - classification_loss: 0.2542
1049/1500 [===================>..........] - ETA: 6:02 - loss: 1.5900 - regression_loss: 1.3359 - classification_loss: 0.2541
1050/1500 [====================>.........] - ETA: 6:01 - loss: 1.5896 - regression_loss: 1.3357 - classification_loss: 0.2540
1051/1500 [====================>.........] - ETA: 6:00 - loss: 1.5890 - regression_loss: 1.3351 - classification_loss: 0.2539
1052/1500 [====================>.........] - ETA: 5:59 - loss: 1.5890 - regression_loss: 1.3350 - classification_loss: 0.2540
1053/1500 [====================>.........] - ETA: 5:58 - loss: 1.5889 - regression_loss: 1.3351 - classification_loss: 0.2539
1054/1500 [====================>.........] - ETA: 5:57 - loss: 1.5881 - regression_loss: 1.3344 - classification_loss: 0.2537
1055/1500 [====================>.........] - ETA: 5:57 - loss: 1.5876 - regression_loss: 1.3339 - classification_loss: 0.2537
1056/1500 [====================>.........] - ETA: 5:56 - loss: 1.5881 - regression_loss: 1.3344 - classification_loss: 0.2538
1057/1500 [====================>.........] - ETA: 5:55 - loss: 1.5878 - regression_loss: 1.3341 - classification_loss: 0.2537
1058/1500 [====================>.........] - ETA: 5:54 - loss: 1.5871 - regression_loss: 1.3335 - classification_loss: 0.2536
1059/1500 [====================>.........] - ETA: 5:53 - loss: 1.5873 - regression_loss: 1.3336 - classification_loss: 0.2537
1060/1500 [====================>.........] - ETA: 5:52 - loss: 1.5881 - regression_loss: 1.3343 - classification_loss: 0.2539
1061/1500 [====================>.........] - ETA: 5:52 - loss: 1.5884 - regression_loss: 1.3345 - classification_loss: 0.2539
1062/1500 [====================>.........] - ETA: 5:51 - loss: 1.5890 - regression_loss: 1.3350 - classification_loss: 0.2540
1063/1500 [====================>.........] - ETA: 5:50 - loss: 1.5890 - regression_loss: 1.3350 - classification_loss: 0.2541
1064/1500 [====================>.........] - ETA: 5:49 - loss: 1.5888 - regression_loss: 1.3348 - classification_loss: 0.2539
1065/1500 [====================>.........] - ETA: 5:49 - loss: 1.5895 - regression_loss: 1.3353 - classification_loss: 0.2542
1066/1500 [====================>.........] - ETA: 5:48 - loss: 1.5887 - regression_loss: 1.3347 - classification_loss: 0.2540
1067/1500 [====================>.........] - ETA: 5:47 - loss: 1.5896 - regression_loss: 1.3354 - classification_loss: 0.2542
1068/1500 [====================>.........] - ETA: 5:46 - loss: 1.5907 - regression_loss: 1.3363 - classification_loss: 0.2545
1069/1500 [====================>.........] - ETA: 5:45 - loss: 1.5904 - regression_loss: 1.3360 - classification_loss: 0.2544
1070/1500 [====================>.........] - ETA: 5:44 - loss: 1.5899 - regression_loss: 1.3356 - classification_loss: 0.2543
1071/1500 [====================>.........] - ETA: 5:43 - loss: 1.5896 - regression_loss: 1.3354 - classification_loss: 0.2542
1072/1500 [====================>.........] - ETA: 5:42 - loss: 1.5904 - regression_loss: 1.3361 - classification_loss: 0.2543
1073/1500 [====================>.........] - ETA: 5:41 - loss: 1.5894 - regression_loss: 1.3353 - classification_loss: 0.2542
1074/1500 [====================>.........] - ETA: 5:40 - loss: 1.5900 - regression_loss: 1.3357 - classification_loss: 0.2543
1075/1500 [====================>.........] - ETA: 5:39 - loss: 1.5902 - regression_loss: 1.3360 - classification_loss: 0.2543
1076/1500 [====================>.........] - ETA: 5:39 - loss: 1.5907 - regression_loss: 1.3364 - classification_loss: 0.2543
1077/1500 [====================>.........] - ETA: 5:38 - loss: 1.5900 - regression_loss: 1.3358 - classification_loss: 0.2542
1078/1500 [====================>.........] - ETA: 5:37 - loss: 1.5895 - regression_loss: 1.3354 - classification_loss: 0.2542
1079/1500 [====================>.........] - ETA: 5:36 - loss: 1.5908 - regression_loss: 1.3363 - classification_loss: 0.2544
1080/1500 [====================>.........] - ETA: 5:35 - loss: 1.5899 - regression_loss: 1.3356 - classification_loss: 0.2543
1081/1500 [====================>.........] - ETA: 5:34 - loss: 1.5899 - regression_loss: 1.3356 - classification_loss: 0.2542
1082/1500 [====================>.........] - ETA: 5:33 - loss: 1.5896 - regression_loss: 1.3353 - classification_loss: 0.2543
1083/1500 [====================>.........] - ETA: 5:33 - loss: 1.5895 - regression_loss: 1.3353 - classification_loss: 0.2543
1084/1500 [====================>.........] - ETA: 5:32 - loss: 1.5895 - regression_loss: 1.3352 - classification_loss: 0.2542
1085/1500 [====================>.........] - ETA: 5:31 - loss: 1.5901 - regression_loss: 1.3357 - classification_loss: 0.2544
1086/1500 [====================>.........] - ETA: 5:30 - loss: 1.5900 - regression_loss: 1.3356 - classification_loss: 0.2544
1087/1500 [====================>.........] - ETA: 5:30 - loss: 1.5893 - regression_loss: 1.3352 - classification_loss: 0.2542
1088/1500 [====================>.........] - ETA: 5:29 - loss: 1.5893 - regression_loss: 1.3350 - classification_loss: 0.2543
1089/1500 [====================>.........] - ETA: 5:28 - loss: 1.5891 - regression_loss: 1.3348 - classification_loss: 0.2542
1090/1500 [====================>.........] - ETA: 5:27 - loss: 1.5901 - regression_loss: 1.3357 - classification_loss: 0.2544
1091/1500 [====================>.........] - ETA: 5:26 - loss: 1.5893 - regression_loss: 1.3351 - classification_loss: 0.2542
1092/1500 [====================>.........] - ETA: 5:25 - loss: 1.5891 - regression_loss: 1.3348 - classification_loss: 0.2543
1093/1500 [====================>.........] - ETA: 5:25 - loss: 1.5886 - regression_loss: 1.3345 - classification_loss: 0.2541
1094/1500 [====================>.........] - ETA: 5:24 - loss: 1.5895 - regression_loss: 1.3352 - classification_loss: 0.2543
1095/1500 [====================>.........] - ETA: 5:23 - loss: 1.5898 - regression_loss: 1.3354 - classification_loss: 0.2544
1096/1500 [====================>.........] - ETA: 5:22 - loss: 1.5901 - regression_loss: 1.3357 - classification_loss: 0.2544
1097/1500 [====================>.........] - ETA: 5:22 - loss: 1.5892 - regression_loss: 1.3349 - classification_loss: 0.2542
1098/1500 [====================>.........] - ETA: 5:21 - loss: 1.5885 - regression_loss: 1.3344 - classification_loss: 0.2541
1099/1500 [====================>.........] - ETA: 5:20 - loss: 1.5876 - regression_loss: 1.3336 - classification_loss: 0.2540
1100/1500 [=====================>........] - ETA: 5:19 - loss: 1.5870 - regression_loss: 1.3332 - classification_loss: 0.2539
1101/1500 [=====================>........] - ETA: 5:18 - loss: 1.5867 - regression_loss: 1.3329 - classification_loss: 0.2539
1102/1500 [=====================>........] - ETA: 5:17 - loss: 1.5873 - regression_loss: 1.3331 - classification_loss: 0.2542
1103/1500 [=====================>........] - ETA: 5:16 - loss: 1.5865 - regression_loss: 1.3324 - classification_loss: 0.2541
1104/1500 [=====================>........] - ETA: 5:15 - loss: 1.5861 - regression_loss: 1.3321 - classification_loss: 0.2540
1105/1500 [=====================>........] - ETA: 5:15 - loss: 1.5868 - regression_loss: 1.3325 - classification_loss: 0.2542
1106/1500 [=====================>........] - ETA: 5:14 - loss: 1.5863 - regression_loss: 1.3322 - classification_loss: 0.2541
1107/1500 [=====================>........] - ETA: 5:13 - loss: 1.5860 - regression_loss: 1.3320 - classification_loss: 0.2540
1108/1500 [=====================>........] - ETA: 5:12 - loss: 1.5853 - regression_loss: 1.3314 - classification_loss: 0.2539
1109/1500 [=====================>........] - ETA: 5:11 - loss: 1.5862 - regression_loss: 1.3322 - classification_loss: 0.2540
1110/1500 [=====================>........] - ETA: 5:10 - loss: 1.5871 - regression_loss: 1.3328 - classification_loss: 0.2543
1111/1500 [=====================>........] - ETA: 5:09 - loss: 1.5867 - regression_loss: 1.3324 - classification_loss: 0.2543
1112/1500 [=====================>........] - ETA: 5:08 - loss: 1.5858 - regression_loss: 1.3317 - classification_loss: 0.2541
1113/1500 [=====================>........] - ETA: 5:07 - loss: 1.5856 - regression_loss: 1.3316 - classification_loss: 0.2540
1114/1500 [=====================>........] - ETA: 5:07 - loss: 1.5849 - regression_loss: 1.3310 - classification_loss: 0.2539
1115/1500 [=====================>........] - ETA: 5:06 - loss: 1.5858 - regression_loss: 1.3317 - classification_loss: 0.2541
1116/1500 [=====================>........] - ETA: 5:05 - loss: 1.5854 - regression_loss: 1.3314 - classification_loss: 0.2540
1117/1500 [=====================>........] - ETA: 5:04 - loss: 1.5854 - regression_loss: 1.3315 - classification_loss: 0.2539
1118/1500 [=====================>........] - ETA: 5:04 - loss: 1.5850 - regression_loss: 1.3312 - classification_loss: 0.2538
1119/1500 [=====================>........] - ETA: 5:03 - loss: 1.5840 - regression_loss: 1.3303 - classification_loss: 0.2536
1120/1500 [=====================>........] - ETA: 5:02 - loss: 1.5836 - regression_loss: 1.3301 - classification_loss: 0.2535
1121/1500 [=====================>........] - ETA: 5:01 - loss: 1.5838 - regression_loss: 1.3303 - classification_loss: 0.2535
1122/1500 [=====================>........] - ETA: 5:00 - loss: 1.5840 - regression_loss: 1.3305 - classification_loss: 0.2535
1123/1500 [=====================>........] - ETA: 4:59 - loss: 1.5837 - regression_loss: 1.3303 - classification_loss: 0.2535
1124/1500 [=====================>........] - ETA: 4:59 - loss: 1.5838 - regression_loss: 1.3304 - classification_loss: 0.2534
1125/1500 [=====================>........] - ETA: 4:58 - loss: 1.5846 - regression_loss: 1.3310 - classification_loss: 0.2536
1126/1500 [=====================>........] - ETA: 4:57 - loss: 1.5839 - regression_loss: 1.3305 - classification_loss: 0.2534
1127/1500 [=====================>........] - ETA: 4:56 - loss: 1.5838 - regression_loss: 1.3304 - classification_loss: 0.2534
1128/1500 [=====================>........] - ETA: 4:55 - loss: 1.5845 - regression_loss: 1.3309 - classification_loss: 0.2536
1129/1500 [=====================>........] - ETA: 4:54 - loss: 1.5842 - regression_loss: 1.3307 - classification_loss: 0.2535
1130/1500 [=====================>........] - ETA: 4:54 - loss: 1.5837 - regression_loss: 1.3303 - classification_loss: 0.2533
1131/1500 [=====================>........] - ETA: 4:53 - loss: 1.5842 - regression_loss: 1.3309 - classification_loss: 0.2533
1132/1500 [=====================>........] - ETA: 4:52 - loss: 1.5834 - regression_loss: 1.3303 - classification_loss: 0.2532
1133/1500 [=====================>........] - ETA: 4:51 - loss: 1.5837 - regression_loss: 1.3305 - classification_loss: 0.2531
1134/1500 [=====================>........] - ETA: 4:51 - loss: 1.5830 - regression_loss: 1.3300 - classification_loss: 0.2530
1135/1500 [=====================>........] - ETA: 4:50 - loss: 1.5829 - regression_loss: 1.3299 - classification_loss: 0.2530
1136/1500 [=====================>........] - ETA: 4:49 - loss: 1.5833 - regression_loss: 1.3302 - classification_loss: 0.2530
1137/1500 [=====================>........] - ETA: 4:48 - loss: 1.5841 - regression_loss: 1.3310 - classification_loss: 0.2531
1138/1500 [=====================>........] - ETA: 4:47 - loss: 1.5838 - regression_loss: 1.3308 - classification_loss: 0.2530
1139/1500 [=====================>........] - ETA: 4:47 - loss: 1.5831 - regression_loss: 1.3302 - classification_loss: 0.2529
1140/1500 [=====================>........] - ETA: 4:46 - loss: 1.5824 - regression_loss: 1.3297 - classification_loss: 0.2528
1141/1500 [=====================>........] - ETA: 4:45 - loss: 1.5830 - regression_loss: 1.3303 - classification_loss: 0.2528
1142/1500 [=====================>........] - ETA: 4:44 - loss: 1.5832 - regression_loss: 1.3303 - classification_loss: 0.2529
1143/1500 [=====================>........] - ETA: 4:43 - loss: 1.5826 - regression_loss: 1.3299 - classification_loss: 0.2527
1144/1500 [=====================>........] - ETA: 4:42 - loss: 1.5829 - regression_loss: 1.3302 - classification_loss: 0.2527
1145/1500 [=====================>........] - ETA: 4:42 - loss: 1.5827 - regression_loss: 1.3300 - classification_loss: 0.2527
1146/1500 [=====================>........] - ETA: 4:41 - loss: 1.5818 - regression_loss: 1.3293 - classification_loss: 0.2526
1147/1500 [=====================>........] - ETA: 4:40 - loss: 1.5816 - regression_loss: 1.3291 - classification_loss: 0.2525
1148/1500 [=====================>........] - ETA: 4:39 - loss: 1.5814 - regression_loss: 1.3290 - classification_loss: 0.2524
1149/1500 [=====================>........] - ETA: 4:38 - loss: 1.5814 - regression_loss: 1.3290 - classification_loss: 0.2524
1150/1500 [======================>.......] - ETA: 4:38 - loss: 1.5812 - regression_loss: 1.3288 - classification_loss: 0.2524
1151/1500 [======================>.......] - ETA: 4:37 - loss: 1.5812 - regression_loss: 1.3286 - classification_loss: 0.2525
1152/1500 [======================>.......] - ETA: 4:36 - loss: 1.5804 - regression_loss: 1.3281 - classification_loss: 0.2524
1153/1500 [======================>.......] - ETA: 4:35 - loss: 1.5800 - regression_loss: 1.3277 - classification_loss: 0.2522
1154/1500 [======================>.......] - ETA: 4:35 - loss: 1.5798 - regression_loss: 1.3275 - classification_loss: 0.2523
1155/1500 [======================>.......] - ETA: 4:34 - loss: 1.5805 - regression_loss: 1.3281 - classification_loss: 0.2525
1156/1500 [======================>.......] - ETA: 4:33 - loss: 1.5803 - regression_loss: 1.3280 - classification_loss: 0.2524
1157/1500 [======================>.......] - ETA: 4:32 - loss: 1.5804 - regression_loss: 1.3281 - classification_loss: 0.2523
1158/1500 [======================>.......] - ETA: 4:32 - loss: 1.5811 - regression_loss: 1.3286 - classification_loss: 0.2525
1159/1500 [======================>.......] - ETA: 4:31 - loss: 1.5810 - regression_loss: 1.3285 - classification_loss: 0.2525
1160/1500 [======================>.......] - ETA: 4:30 - loss: 1.5805 - regression_loss: 1.3281 - classification_loss: 0.2523
1161/1500 [======================>.......] - ETA: 4:29 - loss: 1.5798 - regression_loss: 1.3276 - classification_loss: 0.2522
1162/1500 [======================>.......] - ETA: 4:29 - loss: 1.5804 - regression_loss: 1.3280 - classification_loss: 0.2524
1163/1500 [======================>.......] - ETA: 4:28 - loss: 1.5796 - regression_loss: 1.3274 - classification_loss: 0.2523
1164/1500 [======================>.......] - ETA: 4:27 - loss: 1.5806 - regression_loss: 1.3282 - classification_loss: 0.2524
1165/1500 [======================>.......] - ETA: 4:27 - loss: 1.5808 - regression_loss: 1.3284 - classification_loss: 0.2524
1166/1500 [======================>.......] - ETA: 4:26 - loss: 1.5812 - regression_loss: 1.3287 - classification_loss: 0.2525
1167/1500 [======================>.......] - ETA: 4:25 - loss: 1.5811 - regression_loss: 1.3287 - classification_loss: 0.2524
1168/1500 [======================>.......] - ETA: 4:24 - loss: 1.5813 - regression_loss: 1.3288 - classification_loss: 0.2525
1169/1500 [======================>.......] - ETA: 4:24 - loss: 1.5807 - regression_loss: 1.3284 - classification_loss: 0.2523
1170/1500 [======================>.......] - ETA: 4:23 - loss: 1.5809 - regression_loss: 1.3286 - classification_loss: 0.2524
1171/1500 [======================>.......] - ETA: 4:22 - loss: 1.5812 - regression_loss: 1.3288 - classification_loss: 0.2524
1172/1500 [======================>.......] - ETA: 4:22 - loss: 1.5807 - regression_loss: 1.3284 - classification_loss: 0.2523
1173/1500 [======================>.......] - ETA: 4:21 - loss: 1.5808 - regression_loss: 1.3284 - classification_loss: 0.2523
1174/1500 [======================>.......] - ETA: 4:20 - loss: 1.5806 - regression_loss: 1.3283 - classification_loss: 0.2523
1175/1500 [======================>.......] - ETA: 4:19 - loss: 1.5800 - regression_loss: 1.3278 - classification_loss: 0.2522
1176/1500 [======================>.......] - ETA: 4:18 - loss: 1.5824 - regression_loss: 1.3281 - classification_loss: 0.2543
1177/1500 [======================>.......] - ETA: 4:18 - loss: 1.5819 - regression_loss: 1.3276 - classification_loss: 0.2542
1178/1500 [======================>.......] - ETA: 4:17 - loss: 1.5818 - regression_loss: 1.3276 - classification_loss: 0.2542
1179/1500 [======================>.......] - ETA: 4:16 - loss: 1.5830 - regression_loss: 1.3280 - classification_loss: 0.2550
1180/1500 [======================>.......] - ETA: 4:15 - loss: 1.5831 - regression_loss: 1.3281 - classification_loss: 0.2550
1181/1500 [======================>.......] - ETA: 4:14 - loss: 1.5831 - regression_loss: 1.3282 - classification_loss: 0.2550
1182/1500 [======================>.......] - ETA: 4:13 - loss: 1.5830 - regression_loss: 1.3281 - classification_loss: 0.2549
1183/1500 [======================>.......] - ETA: 4:12 - loss: 1.5826 - regression_loss: 1.3278 - classification_loss: 0.2548
1184/1500 [======================>.......] - ETA: 4:12 - loss: 1.5819 - regression_loss: 1.3272 - classification_loss: 0.2547
1185/1500 [======================>.......] - ETA: 4:11 - loss: 1.5816 - regression_loss: 1.3270 - classification_loss: 0.2546
1186/1500 [======================>.......] - ETA: 4:10 - loss: 1.5810 - regression_loss: 1.3266 - classification_loss: 0.2545
1187/1500 [======================>.......] - ETA: 4:10 - loss: 1.5810 - regression_loss: 1.3264 - classification_loss: 0.2545
1188/1500 [======================>.......] - ETA: 4:09 - loss: 1.5817 - regression_loss: 1.3270 - classification_loss: 0.2547
1189/1500 [======================>.......] - ETA: 4:08 - loss: 1.5809 - regression_loss: 1.3264 - classification_loss: 0.2545
1190/1500 [======================>.......] - ETA: 4:07 - loss: 1.5803 - regression_loss: 1.3259 - classification_loss: 0.2544
1191/1500 [======================>.......] - ETA: 4:06 - loss: 1.5801 - regression_loss: 1.3258 - classification_loss: 0.2544
1192/1500 [======================>.......] - ETA: 4:06 - loss: 1.5807 - regression_loss: 1.3263 - classification_loss: 0.2545
1193/1500 [======================>.......] - ETA: 4:05 - loss: 1.5807 - regression_loss: 1.3263 - classification_loss: 0.2544
1194/1500 [======================>.......] - ETA: 4:04 - loss: 1.5812 - regression_loss: 1.3267 - classification_loss: 0.2545
1195/1500 [======================>.......] - ETA: 4:03 - loss: 1.5804 - regression_loss: 1.3260 - classification_loss: 0.2544
1196/1500 [======================>.......] - ETA: 4:02 - loss: 1.5795 - regression_loss: 1.3252 - classification_loss: 0.2543
1197/1500 [======================>.......] - ETA: 4:02 - loss: 1.5791 - regression_loss: 1.3249 - classification_loss: 0.2542
1198/1500 [======================>.......] - ETA: 4:01 - loss: 1.5790 - regression_loss: 1.3247 - classification_loss: 0.2543
1199/1500 [======================>.......] - ETA: 4:01 - loss: 1.5785 - regression_loss: 1.3243 - classification_loss: 0.2542
1200/1500 [=======================>......] - ETA: 4:00 - loss: 1.5784 - regression_loss: 1.3243 - classification_loss: 0.2541
1201/1500 [=======================>......] - ETA: 3:59 - loss: 1.5788 - regression_loss: 1.3247 - classification_loss: 0.2540
1202/1500 [=======================>......] - ETA: 3:58 - loss: 1.5794 - regression_loss: 1.3254 - classification_loss: 0.2540
1203/1500 [=======================>......] - ETA: 3:57 - loss: 1.5796 - regression_loss: 1.3256 - classification_loss: 0.2541
1204/1500 [=======================>......] - ETA: 3:57 - loss: 1.5789 - regression_loss: 1.3250 - classification_loss: 0.2539
1205/1500 [=======================>......] - ETA: 3:56 - loss: 1.5792 - regression_loss: 1.3254 - classification_loss: 0.2538
1206/1500 [=======================>......] - ETA: 3:55 - loss: 1.5787 - regression_loss: 1.3251 - classification_loss: 0.2537
1207/1500 [=======================>......] - ETA: 3:54 - loss: 1.5791 - regression_loss: 1.3255 - classification_loss: 0.2537
1208/1500 [=======================>......] - ETA: 3:54 - loss: 1.5800 - regression_loss: 1.3262 - classification_loss: 0.2538
1209/1500 [=======================>......] - ETA: 3:53 - loss: 1.5794 - regression_loss: 1.3257 - classification_loss: 0.2537
1210/1500 [=======================>......] - ETA: 3:52 - loss: 1.5789 - regression_loss: 1.3252 - classification_loss: 0.2537
1211/1500 [=======================>......] - ETA: 3:51 - loss: 1.5791 - regression_loss: 1.3254 - classification_loss: 0.2537
1212/1500 [=======================>......] - ETA: 3:50 - loss: 1.5782 - regression_loss: 1.3247 - classification_loss: 0.2535
1213/1500 [=======================>......] - ETA: 3:49 - loss: 1.5778 - regression_loss: 1.3243 - classification_loss: 0.2535
1214/1500 [=======================>......] - ETA: 3:48 - loss: 1.5782 - regression_loss: 1.3247 - classification_loss: 0.2535
1215/1500 [=======================>......] - ETA: 3:48 - loss: 1.5788 - regression_loss: 1.3252 - classification_loss: 0.2536
1216/1500 [=======================>......] - ETA: 3:47 - loss: 1.5789 - regression_loss: 1.3252 - classification_loss: 0.2537
1217/1500 [=======================>......] - ETA: 3:46 - loss: 1.5789 - regression_loss: 1.3251 - classification_loss: 0.2538
1218/1500 [=======================>......] - ETA: 3:45 - loss: 1.5791 - regression_loss: 1.3254 - classification_loss: 0.2537
1219/1500 [=======================>......] - ETA: 3:44 - loss: 1.5793 - regression_loss: 1.3256 - classification_loss: 0.2537
1220/1500 [=======================>......] - ETA: 3:43 - loss: 1.5791 - regression_loss: 1.3255 - classification_loss: 0.2535
1221/1500 [=======================>......] - ETA: 3:43 - loss: 1.5792 - regression_loss: 1.3257 - classification_loss: 0.2535
1222/1500 [=======================>......] - ETA: 3:42 - loss: 1.5786 - regression_loss: 1.3252 - classification_loss: 0.2534
1223/1500 [=======================>......] - ETA: 3:42 - loss: 1.5783 - regression_loss: 1.3250 - classification_loss: 0.2533
1224/1500 [=======================>......] - ETA: 3:41 - loss: 1.5782 - regression_loss: 1.3249 - classification_loss: 0.2533
1225/1500 [=======================>......] - ETA: 3:40 - loss: 1.5784 - regression_loss: 1.3251 - classification_loss: 0.2534
1226/1500 [=======================>......] - ETA: 3:39 - loss: 1.5778 - regression_loss: 1.3246 - classification_loss: 0.2532
1227/1500 [=======================>......] - ETA: 3:38 - loss: 1.5770 - regression_loss: 1.3239 - classification_loss: 0.2531
1228/1500 [=======================>......] - ETA: 3:37 - loss: 1.5774 - regression_loss: 1.3242 - classification_loss: 0.2532
1229/1500 [=======================>......] - ETA: 3:37 - loss: 1.5769 - regression_loss: 1.3238 - classification_loss: 0.2531
1230/1500 [=======================>......] - ETA: 3:36 - loss: 1.5765 - regression_loss: 1.3234 - classification_loss: 0.2530
1231/1500 [=======================>......] - ETA: 3:35 - loss: 1.5765 - regression_loss: 1.3234 - classification_loss: 0.2531
1232/1500 [=======================>......] - ETA: 3:34 - loss: 1.5760 - regression_loss: 1.3230 - classification_loss: 0.2530
1233/1500 [=======================>......] - ETA: 3:33 - loss: 1.5759 - regression_loss: 1.3230 - classification_loss: 0.2530
1234/1500 [=======================>......] - ETA: 3:32 - loss: 1.5763 - regression_loss: 1.3233 - classification_loss: 0.2530
1235/1500 [=======================>......] - ETA: 3:32 - loss: 1.5765 - regression_loss: 1.3236 - classification_loss: 0.2529
1236/1500 [=======================>......] - ETA: 3:31 - loss: 1.5767 - regression_loss: 1.3237 - classification_loss: 0.2530
1237/1500 [=======================>......] - ETA: 3:30 - loss: 1.5774 - regression_loss: 1.3243 - classification_loss: 0.2531
1238/1500 [=======================>......] - ETA: 3:29 - loss: 1.5769 - regression_loss: 1.3240 - classification_loss: 0.2529
1239/1500 [=======================>......] - ETA: 3:28 - loss: 1.5775 - regression_loss: 1.3246 - classification_loss: 0.2529
1240/1500 [=======================>......] - ETA: 3:27 - loss: 1.5768 - regression_loss: 1.3240 - classification_loss: 0.2528
1241/1500 [=======================>......] - ETA: 3:27 - loss: 1.5777 - regression_loss: 1.3248 - classification_loss: 0.2530
1242/1500 [=======================>......] - ETA: 3:26 - loss: 1.5777 - regression_loss: 1.3248 - classification_loss: 0.2529
1243/1500 [=======================>......] - ETA: 3:25 - loss: 1.5774 - regression_loss: 1.3246 - classification_loss: 0.2528
1244/1500 [=======================>......] - ETA: 3:24 - loss: 1.5771 - regression_loss: 1.3243 - classification_loss: 0.2528
1245/1500 [=======================>......] - ETA: 3:23 - loss: 1.5798 - regression_loss: 1.3245 - classification_loss: 0.2553
1246/1500 [=======================>......] - ETA: 3:23 - loss: 1.5791 - regression_loss: 1.3239 - classification_loss: 0.2552
1247/1500 [=======================>......] - ETA: 3:22 - loss: 1.5800 - regression_loss: 1.3246 - classification_loss: 0.2554
1248/1500 [=======================>......] - ETA: 3:21 - loss: 1.5806 - regression_loss: 1.3249 - classification_loss: 0.2556
1249/1500 [=======================>......] - ETA: 3:20 - loss: 1.5802 - regression_loss: 1.3247 - classification_loss: 0.2555
1250/1500 [========================>.....] - ETA: 3:20 - loss: 1.5803 - regression_loss: 1.3248 - classification_loss: 0.2555
1251/1500 [========================>.....] - ETA: 3:19 - loss: 1.5808 - regression_loss: 1.3252 - classification_loss: 0.2556
1252/1500 [========================>.....] - ETA: 3:18 - loss: 1.5812 - regression_loss: 1.3256 - classification_loss: 0.2556
1253/1500 [========================>.....] - ETA: 3:17 - loss: 1.5808 - regression_loss: 1.3254 - classification_loss: 0.2555
1254/1500 [========================>.....] - ETA: 3:16 - loss: 1.5815 - regression_loss: 1.3259 - classification_loss: 0.2556
1255/1500 [========================>.....] - ETA: 3:16 - loss: 1.5811 - regression_loss: 1.3256 - classification_loss: 0.2555
1256/1500 [========================>.....] - ETA: 3:15 - loss: 1.5807 - regression_loss: 1.3254 - classification_loss: 0.2553
1257/1500 [========================>.....] - ETA: 3:14 - loss: 1.5801 - regression_loss: 1.3249 - classification_loss: 0.2552
1258/1500 [========================>.....] - ETA: 3:13 - loss: 1.5806 - regression_loss: 1.3252 - classification_loss: 0.2554
1259/1500 [========================>.....] - ETA: 3:12 - loss: 1.5806 - regression_loss: 1.3253 - classification_loss: 0.2553
1260/1500 [========================>.....] - ETA: 3:11 - loss: 1.5800 - regression_loss: 1.3248 - classification_loss: 0.2551
1261/1500 [========================>.....] - ETA: 3:10 - loss: 1.5793 - regression_loss: 1.3243 - classification_loss: 0.2550
1262/1500 [========================>.....] - ETA: 3:10 - loss: 1.5791 - regression_loss: 1.3241 - classification_loss: 0.2550
1263/1500 [========================>.....] - ETA: 3:09 - loss: 1.5791 - regression_loss: 1.3241 - classification_loss: 0.2550
1264/1500 [========================>.....] - ETA: 3:08 - loss: 1.5788 - regression_loss: 1.3239 - classification_loss: 0.2549
1265/1500 [========================>.....] - ETA: 3:07 - loss: 1.5786 - regression_loss: 1.3238 - classification_loss: 0.2548
1266/1500 [========================>.....] - ETA: 3:07 - loss: 1.5780 - regression_loss: 1.3233 - classification_loss: 0.2547
1267/1500 [========================>.....] - ETA: 3:06 - loss: 1.5774 - regression_loss: 1.3228 - classification_loss: 0.2546
1268/1500 [========================>.....] - ETA: 3:05 - loss: 1.5773 - regression_loss: 1.3227 - classification_loss: 0.2546
1269/1500 [========================>.....] - ETA: 3:04 - loss: 1.5779 - regression_loss: 1.3233 - classification_loss: 0.2546
1270/1500 [========================>.....] - ETA: 3:03 - loss: 1.5774 - regression_loss: 1.3229 - classification_loss: 0.2545
1271/1500 [========================>.....] - ETA: 3:02 - loss: 1.5767 - regression_loss: 1.3223 - classification_loss: 0.2544
1272/1500 [========================>.....] - ETA: 3:02 - loss: 1.5769 - regression_loss: 1.3225 - classification_loss: 0.2545
1273/1500 [========================>.....] - ETA: 3:01 - loss: 1.5773 - regression_loss: 1.3227 - classification_loss: 0.2545
1274/1500 [========================>.....] - ETA: 3:00 - loss: 1.5777 - regression_loss: 1.3232 - classification_loss: 0.2546
1275/1500 [========================>.....] - ETA: 2:59 - loss: 1.5777 - regression_loss: 1.3231 - classification_loss: 0.2546
1276/1500 [========================>.....] - ETA: 2:58 - loss: 1.5775 - regression_loss: 1.3229 - classification_loss: 0.2546
1277/1500 [========================>.....] - ETA: 2:57 - loss: 1.5773 - regression_loss: 1.3228 - classification_loss: 0.2545
1278/1500 [========================>.....] - ETA: 2:57 - loss: 1.5773 - regression_loss: 1.3226 - classification_loss: 0.2546
1279/1500 [========================>.....] - ETA: 2:56 - loss: 1.5763 - regression_loss: 1.3219 - classification_loss: 0.2545
1280/1500 [========================>.....] - ETA: 2:55 - loss: 1.5762 - regression_loss: 1.3217 - classification_loss: 0.2545
1281/1500 [========================>.....] - ETA: 2:54 - loss: 1.5772 - regression_loss: 1.3225 - classification_loss: 0.2547
1282/1500 [========================>.....] - ETA: 2:53 - loss: 1.5772 - regression_loss: 1.3224 - classification_loss: 0.2548
1283/1500 [========================>.....] - ETA: 2:53 - loss: 1.5776 - regression_loss: 1.3228 - classification_loss: 0.2549
1284/1500 [========================>.....] - ETA: 2:52 - loss: 1.5778 - regression_loss: 1.3229 - classification_loss: 0.2549
1285/1500 [========================>.....] - ETA: 2:51 - loss: 1.5771 - regression_loss: 1.3223 - classification_loss: 0.2547
1286/1500 [========================>.....] - ETA: 2:50 - loss: 1.5772 - regression_loss: 1.3224 - classification_loss: 0.2548
1287/1500 [========================>.....] - ETA: 2:50 - loss: 1.5773 - regression_loss: 1.3225 - classification_loss: 0.2548
1288/1500 [========================>.....] - ETA: 2:49 - loss: 1.5769 - regression_loss: 1.3222 - classification_loss: 0.2547
1289/1500 [========================>.....] - ETA: 2:48 - loss: 1.5777 - regression_loss: 1.3227 - classification_loss: 0.2549
1290/1500 [========================>.....] - ETA: 2:47 - loss: 1.5784 - regression_loss: 1.3234 - classification_loss: 0.2550
1291/1500 [========================>.....] - ETA: 2:46 - loss: 1.5784 - regression_loss: 1.3234 - classification_loss: 0.2549
1292/1500 [========================>.....] - ETA: 2:45 - loss: 1.5779 - regression_loss: 1.3230 - classification_loss: 0.2548
1293/1500 [========================>.....] - ETA: 2:45 - loss: 1.5773 - regression_loss: 1.3226 - classification_loss: 0.2547
1294/1500 [========================>.....] - ETA: 2:44 - loss: 1.5780 - regression_loss: 1.3232 - classification_loss: 0.2549
1295/1500 [========================>.....] - ETA: 2:43 - loss: 1.5774 - regression_loss: 1.3227 - classification_loss: 0.2547
1296/1500 [========================>.....] - ETA: 2:42 - loss: 1.5768 - regression_loss: 1.3221 - classification_loss: 0.2547
1297/1500 [========================>.....] - ETA: 2:42 - loss: 1.5761 - regression_loss: 1.3216 - classification_loss: 0.2545
1298/1500 [========================>.....] - ETA: 2:41 - loss: 1.5762 - regression_loss: 1.3217 - classification_loss: 0.2545
1299/1500 [========================>.....] - ETA: 2:40 - loss: 1.5764 - regression_loss: 1.3218 - classification_loss: 0.2545
1300/1500 [=========================>....] - ETA: 2:39 - loss: 1.5759 - regression_loss: 1.3215 - classification_loss: 0.2544
1301/1500 [=========================>....] - ETA: 2:38 - loss: 1.5765 - regression_loss: 1.3221 - classification_loss: 0.2545
1302/1500 [=========================>....] - ETA: 2:38 - loss: 1.5772 - regression_loss: 1.3225 - classification_loss: 0.2547
1303/1500 [=========================>....] - ETA: 2:37 - loss: 1.5769 - regression_loss: 1.3223 - classification_loss: 0.2546
1304/1500 [=========================>....] - ETA: 2:36 - loss: 1.5766 - regression_loss: 1.3218 - classification_loss: 0.2548
1305/1500 [=========================>....] - ETA: 2:35 - loss: 1.5764 - regression_loss: 1.3214 - classification_loss: 0.2549
1306/1500 [=========================>....] - ETA: 2:35 - loss: 1.5766 - regression_loss: 1.3217 - classification_loss: 0.2549
1307/1500 [=========================>....] - ETA: 2:34 - loss: 1.5766 - regression_loss: 1.3216 - classification_loss: 0.2550
1308/1500 [=========================>....] - ETA: 2:33 - loss: 1.5776 - regression_loss: 1.3223 - classification_loss: 0.2553
1309/1500 [=========================>....] - ETA: 2:32 - loss: 1.5770 - regression_loss: 1.3219 - classification_loss: 0.2551
1310/1500 [=========================>....] - ETA: 2:31 - loss: 1.5764 - regression_loss: 1.3214 - classification_loss: 0.2550
1311/1500 [=========================>....] - ETA: 2:31 - loss: 1.5761 - regression_loss: 1.3212 - classification_loss: 0.2549
1312/1500 [=========================>....] - ETA: 2:30 - loss: 1.5766 - regression_loss: 1.3217 - classification_loss: 0.2549
1313/1500 [=========================>....] - ETA: 2:29 - loss: 1.5761 - regression_loss: 1.3214 - classification_loss: 0.2548
1314/1500 [=========================>....] - ETA: 2:28 - loss: 1.5757 - regression_loss: 1.3210 - classification_loss: 0.2547
1315/1500 [=========================>....] - ETA: 2:28 - loss: 1.5755 - regression_loss: 1.3208 - classification_loss: 0.2547
1316/1500 [=========================>....] - ETA: 2:27 - loss: 1.5751 - regression_loss: 1.3204 - classification_loss: 0.2547
1317/1500 [=========================>....] - ETA: 2:26 - loss: 1.5747 - regression_loss: 1.3201 - classification_loss: 0.2546
1318/1500 [=========================>....] - ETA: 2:25 - loss: 1.5746 - regression_loss: 1.3200 - classification_loss: 0.2545
1319/1500 [=========================>....] - ETA: 2:24 - loss: 1.5750 - regression_loss: 1.3204 - classification_loss: 0.2546
1320/1500 [=========================>....] - ETA: 2:23 - loss: 1.5745 - regression_loss: 1.3200 - classification_loss: 0.2546
1321/1500 [=========================>....] - ETA: 2:22 - loss: 1.5739 - regression_loss: 1.3195 - classification_loss: 0.2544
1322/1500 [=========================>....] - ETA: 2:22 - loss: 1.5733 - regression_loss: 1.3189 - classification_loss: 0.2543
1323/1500 [=========================>....] - ETA: 2:21 - loss: 1.5732 - regression_loss: 1.3190 - classification_loss: 0.2543
1324/1500 [=========================>....] - ETA: 2:20 - loss: 1.5733 - regression_loss: 1.3191 - classification_loss: 0.2542
1325/1500 [=========================>....] - ETA: 2:19 - loss: 1.5730 - regression_loss: 1.3189 - classification_loss: 0.2541
1326/1500 [=========================>....] - ETA: 2:18 - loss: 1.5730 - regression_loss: 1.3189 - classification_loss: 0.2541
1327/1500 [=========================>....] - ETA: 2:18 - loss: 1.5729 - regression_loss: 1.3188 - classification_loss: 0.2541
1328/1500 [=========================>....] - ETA: 2:17 - loss: 1.5725 - regression_loss: 1.3185 - classification_loss: 0.2541
1329/1500 [=========================>....] - ETA: 2:16 - loss: 1.5724 - regression_loss: 1.3184 - classification_loss: 0.2541
1330/1500 [=========================>....] - ETA: 2:15 - loss: 1.5719 - regression_loss: 1.3179 - classification_loss: 0.2539
1331/1500 [=========================>....] - ETA: 2:15 - loss: 1.5714 - regression_loss: 1.3176 - classification_loss: 0.2538
1332/1500 [=========================>....] - ETA: 2:14 - loss: 1.5710 - regression_loss: 1.3172 - classification_loss: 0.2537
1333/1500 [=========================>....] - ETA: 2:13 - loss: 1.5716 - regression_loss: 1.3178 - classification_loss: 0.2538
1334/1500 [=========================>....] - ETA: 2:12 - loss: 1.5710 - regression_loss: 1.3173 - classification_loss: 0.2537
1335/1500 [=========================>....] - ETA: 2:11 - loss: 1.5718 - regression_loss: 1.3179 - classification_loss: 0.2539
1336/1500 [=========================>....] - ETA: 2:11 - loss: 1.5720 - regression_loss: 1.3181 - classification_loss: 0.2538
1337/1500 [=========================>....] - ETA: 2:10 - loss: 1.5722 - regression_loss: 1.3184 - classification_loss: 0.2538
1338/1500 [=========================>....] - ETA: 2:09 - loss: 1.5719 - regression_loss: 1.3181 - classification_loss: 0.2538
1339/1500 [=========================>....] - ETA: 2:08 - loss: 1.5714 - regression_loss: 1.3178 - classification_loss: 0.2536
1340/1500 [=========================>....] - ETA: 2:07 - loss: 1.5710 - regression_loss: 1.3175 - classification_loss: 0.2535
1341/1500 [=========================>....] - ETA: 2:06 - loss: 1.5709 - regression_loss: 1.3174 - classification_loss: 0.2534
1342/1500 [=========================>....] - ETA: 2:06 - loss: 1.5707 - regression_loss: 1.3173 - classification_loss: 0.2534
1343/1500 [=========================>....] - ETA: 2:05 - loss: 1.5703 - regression_loss: 1.3170 - classification_loss: 0.2532
1344/1500 [=========================>....] - ETA: 2:04 - loss: 1.5700 - regression_loss: 1.3169 - classification_loss: 0.2532
1345/1500 [=========================>....] - ETA: 2:03 - loss: 1.5701 - regression_loss: 1.3166 - classification_loss: 0.2535
1346/1500 [=========================>....] - ETA: 2:02 - loss: 1.5701 - regression_loss: 1.3166 - classification_loss: 0.2535
1347/1500 [=========================>....] - ETA: 2:02 - loss: 1.5702 - regression_loss: 1.3167 - classification_loss: 0.2535
1348/1500 [=========================>....] - ETA: 2:01 - loss: 1.5699 - regression_loss: 1.3164 - classification_loss: 0.2535
1349/1500 [=========================>....] - ETA: 2:00 - loss: 1.5692 - regression_loss: 1.3159 - classification_loss: 0.2533
1350/1500 [==========================>...] - ETA: 1:59 - loss: 1.5689 - regression_loss: 1.3157 - classification_loss: 0.2532
1351/1500 [==========================>...] - ETA: 1:58 - loss: 1.5697 - regression_loss: 1.3164 - classification_loss: 0.2533
1352/1500 [==========================>...] - ETA: 1:58 - loss: 1.5698 - regression_loss: 1.3166 - classification_loss: 0.2532
1353/1500 [==========================>...] - ETA: 1:57 - loss: 1.5703 - regression_loss: 1.3169 - classification_loss: 0.2533
1354/1500 [==========================>...] - ETA: 1:56 - loss: 1.5702 - regression_loss: 1.3169 - classification_loss: 0.2533
1355/1500 [==========================>...] - ETA: 1:55 - loss: 1.5699 - regression_loss: 1.3167 - classification_loss: 0.2532
1356/1500 [==========================>...] - ETA: 1:54 - loss: 1.5696 - regression_loss: 1.3164 - classification_loss: 0.2532
1357/1500 [==========================>...] - ETA: 1:53 - loss: 1.5699 - regression_loss: 1.3166 - classification_loss: 0.2532
1358/1500 [==========================>...] - ETA: 1:53 - loss: 1.5701 - regression_loss: 1.3169 - classification_loss: 0.2532
1359/1500 [==========================>...] - ETA: 1:52 - loss: 1.5694 - regression_loss: 1.3163 - classification_loss: 0.2531
1360/1500 [==========================>...] - ETA: 1:51 - loss: 1.5693 - regression_loss: 1.3163 - classification_loss: 0.2530
1361/1500 [==========================>...] - ETA: 1:50 - loss: 1.5691 - regression_loss: 1.3161 - classification_loss: 0.2531
1362/1500 [==========================>...] - ETA: 1:49 - loss: 1.5695 - regression_loss: 1.3165 - classification_loss: 0.2530
1363/1500 [==========================>...] - ETA: 1:49 - loss: 1.5692 - regression_loss: 1.3163 - classification_loss: 0.2529
1364/1500 [==========================>...] - ETA: 1:48 - loss: 1.5684 - regression_loss: 1.3156 - classification_loss: 0.2528
1365/1500 [==========================>...] - ETA: 1:47 - loss: 1.5681 - regression_loss: 1.3154 - classification_loss: 0.2527
1366/1500 [==========================>...] - ETA: 1:46 - loss: 1.5682 - regression_loss: 1.3155 - classification_loss: 0.2527
1367/1500 [==========================>...] - ETA: 1:46 - loss: 1.5679 - regression_loss: 1.3153 - classification_loss: 0.2526
1368/1500 [==========================>...] - ETA: 1:45 - loss: 1.5690 - regression_loss: 1.3160 - classification_loss: 0.2530
1369/1500 [==========================>...] - ETA: 1:44 - loss: 1.5684 - regression_loss: 1.3156 - classification_loss: 0.2528
1370/1500 [==========================>...] - ETA: 1:43 - loss: 1.5682 - regression_loss: 1.3153 - classification_loss: 0.2529
1371/1500 [==========================>...] - ETA: 1:43 - loss: 1.5678 - regression_loss: 1.3150 - classification_loss: 0.2528
1372/1500 [==========================>...] - ETA: 1:42 - loss: 1.5674 - regression_loss: 1.3147 - classification_loss: 0.2527
1373/1500 [==========================>...] - ETA: 1:41 - loss: 1.5669 - regression_loss: 1.3143 - classification_loss: 0.2526
1374/1500 [==========================>...] - ETA: 1:40 - loss: 1.5669 - regression_loss: 1.3144 - classification_loss: 0.2525
1375/1500 [==========================>...] - ETA: 1:39 - loss: 1.5666 - regression_loss: 1.3142 - classification_loss: 0.2524
1376/1500 [==========================>...] - ETA: 1:39 - loss: 1.5662 - regression_loss: 1.3139 - classification_loss: 0.2523
1377/1500 [==========================>...] - ETA: 1:38 - loss: 1.5658 - regression_loss: 1.3136 - classification_loss: 0.2522
1378/1500 [==========================>...] - ETA: 1:37 - loss: 1.5656 - regression_loss: 1.3135 - classification_loss: 0.2521
1379/1500 [==========================>...] - ETA: 1:36 - loss: 1.5659 - regression_loss: 1.3134 - classification_loss: 0.2525
1380/1500 [==========================>...] - ETA: 1:36 - loss: 1.5656 - regression_loss: 1.3132 - classification_loss: 0.2524
1381/1500 [==========================>...] - ETA: 1:35 - loss: 1.5649 - regression_loss: 1.3127 - classification_loss: 0.2523
1382/1500 [==========================>...] - ETA: 1:34 - loss: 1.5649 - regression_loss: 1.3127 - classification_loss: 0.2522
1383/1500 [==========================>...] - ETA: 1:33 - loss: 1.5647 - regression_loss: 1.3126 - classification_loss: 0.2521
1384/1500 [==========================>...] - ETA: 1:32 - loss: 1.5643 - regression_loss: 1.3122 - classification_loss: 0.2521
1385/1500 [==========================>...] - ETA: 1:32 - loss: 1.5637 - regression_loss: 1.3118 - classification_loss: 0.2519
1386/1500 [==========================>...] - ETA: 1:31 - loss: 1.5632 - regression_loss: 1.3113 - classification_loss: 0.2519
1387/1500 [==========================>...] - ETA: 1:30 - loss: 1.5627 - regression_loss: 1.3109 - classification_loss: 0.2518
1388/1500 [==========================>...] - ETA: 1:29 - loss: 1.5632 - regression_loss: 1.3113 - classification_loss: 0.2518
1389/1500 [==========================>...] - ETA: 1:28 - loss: 1.5633 - regression_loss: 1.3115 - classification_loss: 0.2518
1390/1500 [==========================>...] - ETA: 1:27 - loss: 1.5645 - regression_loss: 1.3125 - classification_loss: 0.2519
1391/1500 [==========================>...] - ETA: 1:27 - loss: 1.5641 - regression_loss: 1.3123 - classification_loss: 0.2518
1392/1500 [==========================>...] - ETA: 1:26 - loss: 1.5643 - regression_loss: 1.3125 - classification_loss: 0.2518
1393/1500 [==========================>...] - ETA: 1:25 - loss: 1.5647 - regression_loss: 1.3128 - classification_loss: 0.2519
1394/1500 [==========================>...] - ETA: 1:24 - loss: 1.5641 - regression_loss: 1.3123 - classification_loss: 0.2518
1395/1500 [==========================>...] - ETA: 1:23 - loss: 1.5642 - regression_loss: 1.3121 - classification_loss: 0.2521
1396/1500 [==========================>...] - ETA: 1:23 - loss: 1.5642 - regression_loss: 1.3121 - classification_loss: 0.2521
1397/1500 [==========================>...] - ETA: 1:22 - loss: 1.5638 - regression_loss: 1.3118 - classification_loss: 0.2520
1398/1500 [==========================>...] - ETA: 1:21 - loss: 1.5634 - regression_loss: 1.3115 - classification_loss: 0.2519
1399/1500 [==========================>...] - ETA: 1:20 - loss: 1.5632 - regression_loss: 1.3113 - classification_loss: 0.2519
1400/1500 [===========================>..] - ETA: 1:19 - loss: 1.5628 - regression_loss: 1.3110 - classification_loss: 0.2518
1401/1500 [===========================>..] - ETA: 1:19 - loss: 1.5622 - regression_loss: 1.3105 - classification_loss: 0.2516
1402/1500 [===========================>..] - ETA: 1:18 - loss: 1.5630 - regression_loss: 1.3112 - classification_loss: 0.2518
1403/1500 [===========================>..] - ETA: 1:17 - loss: 1.5632 - regression_loss: 1.3114 - classification_loss: 0.2518
1404/1500 [===========================>..] - ETA: 1:16 - loss: 1.5629 - regression_loss: 1.3112 - classification_loss: 0.2517
1405/1500 [===========================>..] - ETA: 1:15 - loss: 1.5622 - regression_loss: 1.3107 - classification_loss: 0.2516
1406/1500 [===========================>..] - ETA: 1:15 - loss: 1.5617 - regression_loss: 1.3102 - classification_loss: 0.2515
1407/1500 [===========================>..] - ETA: 1:14 - loss: 1.5613 - regression_loss: 1.3098 - classification_loss: 0.2515
1408/1500 [===========================>..] - ETA: 1:13 - loss: 1.5615 - regression_loss: 1.3100 - classification_loss: 0.2515
1409/1500 [===========================>..] - ETA: 1:12 - loss: 1.5613 - regression_loss: 1.3099 - classification_loss: 0.2514
1410/1500 [===========================>..] - ETA: 1:11 - loss: 1.5612 - regression_loss: 1.3098 - classification_loss: 0.2514
1411/1500 [===========================>..] - ETA: 1:11 - loss: 1.5608 - regression_loss: 1.3095 - classification_loss: 0.2513
1412/1500 [===========================>..] - ETA: 1:10 - loss: 1.5606 - regression_loss: 1.3094 - classification_loss: 0.2512
1413/1500 [===========================>..] - ETA: 1:09 - loss: 1.5602 - regression_loss: 1.3091 - classification_loss: 0.2511
1414/1500 [===========================>..] - ETA: 1:08 - loss: 1.5601 - regression_loss: 1.3090 - classification_loss: 0.2511
1415/1500 [===========================>..] - ETA: 1:07 - loss: 1.5599 - regression_loss: 1.3089 - classification_loss: 0.2511
1416/1500 [===========================>..] - ETA: 1:07 - loss: 1.5604 - regression_loss: 1.3092 - classification_loss: 0.2512
1417/1500 [===========================>..] - ETA: 1:06 - loss: 1.5601 - regression_loss: 1.3089 - classification_loss: 0.2512
1418/1500 [===========================>..] - ETA: 1:05 - loss: 1.5599 - regression_loss: 1.3088 - classification_loss: 0.2511
1419/1500 [===========================>..] - ETA: 1:04 - loss: 1.5594 - regression_loss: 1.3084 - classification_loss: 0.2509
1420/1500 [===========================>..] - ETA: 1:03 - loss: 1.5599 - regression_loss: 1.3086 - classification_loss: 0.2512
1421/1500 [===========================>..] - ETA: 1:03 - loss: 1.5592 - regression_loss: 1.3081 - classification_loss: 0.2511
1422/1500 [===========================>..] - ETA: 1:02 - loss: 1.5588 - regression_loss: 1.3078 - classification_loss: 0.2510
1423/1500 [===========================>..] - ETA: 1:01 - loss: 1.5584 - regression_loss: 1.3074 - classification_loss: 0.2510
1424/1500 [===========================>..] - ETA: 1:00 - loss: 1.5578 - regression_loss: 1.3069 - classification_loss: 0.2509
1425/1500 [===========================>..] - ETA: 59s - loss: 1.5574 - regression_loss: 1.3065 - classification_loss: 0.2509 
1426/1500 [===========================>..] - ETA: 59s - loss: 1.5570 - regression_loss: 1.3063 - classification_loss: 0.2507
1427/1500 [===========================>..] - ETA: 58s - loss: 1.5570 - regression_loss: 1.3063 - classification_loss: 0.2507
1428/1500 [===========================>..] - ETA: 57s - loss: 1.5567 - regression_loss: 1.3062 - classification_loss: 0.2506
1429/1500 [===========================>..] - ETA: 56s - loss: 1.5560 - regression_loss: 1.3056 - classification_loss: 0.2504
1430/1500 [===========================>..] - ETA: 55s - loss: 1.5555 - regression_loss: 1.3052 - classification_loss: 0.2503
1431/1500 [===========================>..] - ETA: 55s - loss: 1.5558 - regression_loss: 1.3055 - classification_loss: 0.2503
1432/1500 [===========================>..] - ETA: 54s - loss: 1.5556 - regression_loss: 1.3053 - classification_loss: 0.2503
1433/1500 [===========================>..] - ETA: 53s - loss: 1.5558 - regression_loss: 1.3055 - classification_loss: 0.2502
1434/1500 [===========================>..] - ETA: 52s - loss: 1.5555 - regression_loss: 1.3053 - classification_loss: 0.2502
1435/1500 [===========================>..] - ETA: 51s - loss: 1.5552 - regression_loss: 1.3051 - classification_loss: 0.2501
1436/1500 [===========================>..] - ETA: 51s - loss: 1.5547 - regression_loss: 1.3047 - classification_loss: 0.2500
1437/1500 [===========================>..] - ETA: 50s - loss: 1.5549 - regression_loss: 1.3050 - classification_loss: 0.2500
1438/1500 [===========================>..] - ETA: 49s - loss: 1.5546 - regression_loss: 1.3048 - classification_loss: 0.2499
1439/1500 [===========================>..] - ETA: 48s - loss: 1.5546 - regression_loss: 1.3048 - classification_loss: 0.2498
1440/1500 [===========================>..] - ETA: 47s - loss: 1.5542 - regression_loss: 1.3045 - classification_loss: 0.2497
1441/1500 [===========================>..] - ETA: 47s - loss: 1.5540 - regression_loss: 1.3042 - classification_loss: 0.2498
1442/1500 [===========================>..] - ETA: 46s - loss: 1.5537 - regression_loss: 1.3039 - classification_loss: 0.2498
1443/1500 [===========================>..] - ETA: 45s - loss: 1.5533 - regression_loss: 1.3037 - classification_loss: 0.2497
1444/1500 [===========================>..] - ETA: 44s - loss: 1.5529 - regression_loss: 1.3033 - classification_loss: 0.2496
1445/1500 [===========================>..] - ETA: 43s - loss: 1.5536 - regression_loss: 1.3038 - classification_loss: 0.2498
1446/1500 [===========================>..] - ETA: 43s - loss: 1.5534 - regression_loss: 1.3037 - classification_loss: 0.2497
1447/1500 [===========================>..] - ETA: 42s - loss: 1.5543 - regression_loss: 1.3044 - classification_loss: 0.2498
1448/1500 [===========================>..] - ETA: 41s - loss: 1.5537 - regression_loss: 1.3039 - classification_loss: 0.2497
1449/1500 [===========================>..] - ETA: 40s - loss: 1.5534 - regression_loss: 1.3038 - classification_loss: 0.2497
1450/1500 [============================>.] - ETA: 39s - loss: 1.5532 - regression_loss: 1.3035 - classification_loss: 0.2496
1451/1500 [============================>.] - ETA: 39s - loss: 1.5525 - regression_loss: 1.3030 - classification_loss: 0.2495
1452/1500 [============================>.] - ETA: 38s - loss: 1.5528 - regression_loss: 1.3032 - classification_loss: 0.2495
1453/1500 [============================>.] - ETA: 37s - loss: 1.5528 - regression_loss: 1.3033 - classification_loss: 0.2495
1454/1500 [============================>.] - ETA: 36s - loss: 1.5534 - regression_loss: 1.3038 - classification_loss: 0.2496
1455/1500 [============================>.] - ETA: 35s - loss: 1.5539 - regression_loss: 1.3041 - classification_loss: 0.2497
1456/1500 [============================>.] - ETA: 35s - loss: 1.5540 - regression_loss: 1.3043 - classification_loss: 0.2497
1457/1500 [============================>.] - ETA: 34s - loss: 1.5537 - regression_loss: 1.3040 - classification_loss: 0.2497
1458/1500 [============================>.] - ETA: 33s - loss: 1.5536 - regression_loss: 1.3040 - classification_loss: 0.2496
1459/1500 [============================>.] - ETA: 32s - loss: 1.5531 - regression_loss: 1.3036 - classification_loss: 0.2495
1460/1500 [============================>.] - ETA: 31s - loss: 1.5534 - regression_loss: 1.3038 - classification_loss: 0.2496
1461/1500 [============================>.] - ETA: 31s - loss: 1.5530 - regression_loss: 1.3035 - classification_loss: 0.2495
1462/1500 [============================>.] - ETA: 30s - loss: 1.5523 - regression_loss: 1.3029 - classification_loss: 0.2494
1463/1500 [============================>.] - ETA: 29s - loss: 1.5525 - regression_loss: 1.3031 - classification_loss: 0.2495
1464/1500 [============================>.] - ETA: 28s - loss: 1.5529 - regression_loss: 1.3034 - classification_loss: 0.2495
1465/1500 [============================>.] - ETA: 27s - loss: 1.5530 - regression_loss: 1.3036 - classification_loss: 0.2495
1466/1500 [============================>.] - ETA: 27s - loss: 1.5534 - regression_loss: 1.3039 - classification_loss: 0.2494
1467/1500 [============================>.] - ETA: 26s - loss: 1.5535 - regression_loss: 1.3040 - classification_loss: 0.2495
1468/1500 [============================>.] - ETA: 25s - loss: 1.5532 - regression_loss: 1.3038 - classification_loss: 0.2494
1469/1500 [============================>.] - ETA: 24s - loss: 1.5531 - regression_loss: 1.3036 - classification_loss: 0.2494
1470/1500 [============================>.] - ETA: 23s - loss: 1.5536 - regression_loss: 1.3041 - classification_loss: 0.2495
1471/1500 [============================>.] - ETA: 23s - loss: 1.5530 - regression_loss: 1.3036 - classification_loss: 0.2494
1472/1500 [============================>.] - ETA: 22s - loss: 1.5527 - regression_loss: 1.3034 - classification_loss: 0.2493
1473/1500 [============================>.] - ETA: 21s - loss: 1.5529 - regression_loss: 1.3036 - classification_loss: 0.2492
1474/1500 [============================>.] - ETA: 20s - loss: 1.5525 - regression_loss: 1.3034 - classification_loss: 0.2491
1475/1500 [============================>.] - ETA: 19s - loss: 1.5518 - regression_loss: 1.3028 - classification_loss: 0.2490
1476/1500 [============================>.] - ETA: 19s - loss: 1.5520 - regression_loss: 1.3030 - classification_loss: 0.2490
1477/1500 [============================>.] - ETA: 18s - loss: 1.5522 - regression_loss: 1.3032 - classification_loss: 0.2490
1478/1500 [============================>.] - ETA: 17s - loss: 1.5524 - regression_loss: 1.3034 - classification_loss: 0.2490
1479/1500 [============================>.] - ETA: 16s - loss: 1.5521 - regression_loss: 1.3031 - classification_loss: 0.2490
1480/1500 [============================>.] - ETA: 15s - loss: 1.5527 - regression_loss: 1.3037 - classification_loss: 0.2490
1481/1500 [============================>.] - ETA: 15s - loss: 1.5528 - regression_loss: 1.3038 - classification_loss: 0.2490
1482/1500 [============================>.] - ETA: 14s - loss: 1.5535 - regression_loss: 1.3043 - classification_loss: 0.2492
1483/1500 [============================>.] - ETA: 13s - loss: 1.5538 - regression_loss: 1.3046 - classification_loss: 0.2492
1484/1500 [============================>.] - ETA: 12s - loss: 1.5535 - regression_loss: 1.3044 - classification_loss: 0.2491
1485/1500 [============================>.] - ETA: 11s - loss: 1.5533 - regression_loss: 1.3042 - classification_loss: 0.2491
1486/1500 [============================>.] - ETA: 11s - loss: 1.5530 - regression_loss: 1.3040 - classification_loss: 0.2489
1487/1500 [============================>.] - ETA: 10s - loss: 1.5530 - regression_loss: 1.3041 - classification_loss: 0.2489
1488/1500 [============================>.] - ETA: 9s - loss: 1.5530 - regression_loss: 1.3041 - classification_loss: 0.2489 
1489/1500 [============================>.] - ETA: 8s - loss: 1.5526 - regression_loss: 1.3038 - classification_loss: 0.2488
1490/1500 [============================>.] - ETA: 7s - loss: 1.5522 - regression_loss: 1.3035 - classification_loss: 0.2487
1491/1500 [============================>.] - ETA: 7s - loss: 1.5515 - regression_loss: 1.3029 - classification_loss: 0.2486
1492/1500 [============================>.] - ETA: 6s - loss: 1.5523 - regression_loss: 1.3035 - classification_loss: 0.2488
1493/1500 [============================>.] - ETA: 5s - loss: 1.5524 - regression_loss: 1.3036 - classification_loss: 0.2488
1494/1500 [============================>.] - ETA: 4s - loss: 1.5523 - regression_loss: 1.3036 - classification_loss: 0.2487
1495/1500 [============================>.] - ETA: 3s - loss: 1.5525 - regression_loss: 1.3037 - classification_loss: 0.2488
1496/1500 [============================>.] - ETA: 3s - loss: 1.5530 - regression_loss: 1.3039 - classification_loss: 0.2490
1497/1500 [============================>.] - ETA: 2s - loss: 1.5532 - regression_loss: 1.3042 - classification_loss: 0.2490
1498/1500 [============================>.] - ETA: 1s - loss: 1.5533 - regression_loss: 1.3042 - classification_loss: 0.2491
1499/1500 [============================>.] - ETA: 0s - loss: 1.5530 - regression_loss: 1.3040 - classification_loss: 0.2490
1500/1500 [==============================] - 1195s 796ms/step - loss: 1.5534 - regression_loss: 1.3037 - classification_loss: 0.2496

Epoch 00005: saving model to ./snapshots/resnet50_csv_05.h5
Epoch 6/10

   1/1500 [..............................] - ETA: 8:16 - loss: 1.3144 - regression_loss: 1.1547 - classification_loss: 0.1597
   2/1500 [..............................] - ETA: 9:23 - loss: 1.7923 - regression_loss: 1.5505 - classification_loss: 0.2418
   3/1500 [..............................] - ETA: 9:42 - loss: 1.5089 - regression_loss: 1.3224 - classification_loss: 0.1865
   4/1500 [..............................] - ETA: 10:01 - loss: 1.3777 - regression_loss: 1.2115 - classification_loss: 0.1662
   5/1500 [..............................] - ETA: 9:59 - loss: 1.1933 - regression_loss: 1.0508 - classification_loss: 0.1425 
   6/1500 [..............................] - ETA: 12:01 - loss: 1.1059 - regression_loss: 0.9623 - classification_loss: 0.1437
   7/1500 [..............................] - ETA: 13:21 - loss: 1.1187 - regression_loss: 0.9665 - classification_loss: 0.1522
   8/1500 [..............................] - ETA: 12:55 - loss: 1.0362 - regression_loss: 0.8971 - classification_loss: 0.1391
   9/1500 [..............................] - ETA: 16:35 - loss: 1.0416 - regression_loss: 0.8886 - classification_loss: 0.1530
  10/1500 [..............................] - ETA: 15:57 - loss: 1.0336 - regression_loss: 0.8875 - classification_loss: 0.1461
  11/1500 [..............................] - ETA: 15:26 - loss: 1.0640 - regression_loss: 0.9107 - classification_loss: 0.1533
  12/1500 [..............................] - ETA: 17:24 - loss: 1.0477 - regression_loss: 0.9008 - classification_loss: 0.1469
  13/1500 [..............................] - ETA: 16:56 - loss: 1.0892 - regression_loss: 0.9416 - classification_loss: 0.1476
  14/1500 [..............................] - ETA: 16:33 - loss: 1.0486 - regression_loss: 0.9067 - classification_loss: 0.1419
  15/1500 [..............................] - ETA: 18:03 - loss: 1.0869 - regression_loss: 0.9407 - classification_loss: 0.1461
  16/1500 [..............................] - ETA: 18:14 - loss: 1.1039 - regression_loss: 0.9558 - classification_loss: 0.1481
  17/1500 [..............................] - ETA: 18:37 - loss: 1.1053 - regression_loss: 0.9592 - classification_loss: 0.1461
  18/1500 [..............................] - ETA: 18:09 - loss: 1.1190 - regression_loss: 0.9676 - classification_loss: 0.1514
  19/1500 [..............................] - ETA: 18:20 - loss: 1.1178 - regression_loss: 0.9680 - classification_loss: 0.1498
  20/1500 [..............................] - ETA: 18:54 - loss: 1.1235 - regression_loss: 0.9744 - classification_loss: 0.1491
  21/1500 [..............................] - ETA: 19:05 - loss: 1.1769 - regression_loss: 1.0214 - classification_loss: 0.1555
  22/1500 [..............................] - ETA: 18:40 - loss: 1.1671 - regression_loss: 1.0145 - classification_loss: 0.1526
  23/1500 [..............................] - ETA: 18:18 - loss: 1.1806 - regression_loss: 1.0283 - classification_loss: 0.1523
  24/1500 [..............................] - ETA: 18:26 - loss: 1.2505 - regression_loss: 1.0793 - classification_loss: 0.1711
  25/1500 [..............................] - ETA: 18:55 - loss: 1.2481 - regression_loss: 1.0782 - classification_loss: 0.1700
  26/1500 [..............................] - ETA: 18:35 - loss: 1.2313 - regression_loss: 1.0638 - classification_loss: 0.1674
  27/1500 [..............................] - ETA: 18:37 - loss: 1.2899 - regression_loss: 1.1039 - classification_loss: 0.1860
  28/1500 [..............................] - ETA: 20:06 - loss: 1.3303 - regression_loss: 1.1342 - classification_loss: 0.1961
  29/1500 [..............................] - ETA: 19:42 - loss: 1.3034 - regression_loss: 1.1098 - classification_loss: 0.1936
  30/1500 [..............................] - ETA: 19:22 - loss: 1.2841 - regression_loss: 1.0953 - classification_loss: 0.1888
  31/1500 [..............................] - ETA: 19:03 - loss: 1.2606 - regression_loss: 1.0752 - classification_loss: 0.1854
  32/1500 [..............................] - ETA: 19:10 - loss: 1.2723 - regression_loss: 1.0842 - classification_loss: 0.1882
  33/1500 [..............................] - ETA: 19:30 - loss: 1.2760 - regression_loss: 1.0866 - classification_loss: 0.1894
  34/1500 [..............................] - ETA: 20:56 - loss: 1.2865 - regression_loss: 1.0903 - classification_loss: 0.1962
  35/1500 [..............................] - ETA: 20:37 - loss: 1.2922 - regression_loss: 1.0963 - classification_loss: 0.1960
  36/1500 [..............................] - ETA: 20:20 - loss: 1.2886 - regression_loss: 1.0943 - classification_loss: 0.1943
  37/1500 [..............................] - ETA: 20:02 - loss: 1.3168 - regression_loss: 1.1179 - classification_loss: 0.1989
  38/1500 [..............................] - ETA: 20:04 - loss: 1.3093 - regression_loss: 1.1108 - classification_loss: 0.1985
  39/1500 [..............................] - ETA: 20:10 - loss: 1.3207 - regression_loss: 1.1202 - classification_loss: 0.2005
  40/1500 [..............................] - ETA: 20:01 - loss: 1.3230 - regression_loss: 1.1203 - classification_loss: 0.2027
  41/1500 [..............................] - ETA: 19:56 - loss: 1.3244 - regression_loss: 1.1229 - classification_loss: 0.2015
  42/1500 [..............................] - ETA: 19:40 - loss: 1.3130 - regression_loss: 1.1130 - classification_loss: 0.2000
  43/1500 [..............................] - ETA: 19:40 - loss: 1.3478 - regression_loss: 1.1385 - classification_loss: 0.2092
  44/1500 [..............................] - ETA: 19:27 - loss: 1.3396 - regression_loss: 1.1303 - classification_loss: 0.2092
  45/1500 [..............................] - ETA: 19:13 - loss: 1.3503 - regression_loss: 1.1391 - classification_loss: 0.2112
  46/1500 [..............................] - ETA: 19:00 - loss: 1.3406 - regression_loss: 1.1310 - classification_loss: 0.2095
  47/1500 [..............................] - ETA: 18:48 - loss: 1.3466 - regression_loss: 1.1374 - classification_loss: 0.2092
  48/1500 [..............................] - ETA: 19:12 - loss: 1.3501 - regression_loss: 1.1410 - classification_loss: 0.2091
  49/1500 [..............................] - ETA: 19:05 - loss: 1.3374 - regression_loss: 1.1308 - classification_loss: 0.2067
  50/1500 [>.............................] - ETA: 19:03 - loss: 1.3533 - regression_loss: 1.1427 - classification_loss: 0.2105
  51/1500 [>.............................] - ETA: 19:00 - loss: 1.3526 - regression_loss: 1.1425 - classification_loss: 0.2101
  52/1500 [>.............................] - ETA: 18:51 - loss: 1.3843 - regression_loss: 1.1679 - classification_loss: 0.2165
  53/1500 [>.............................] - ETA: 19:10 - loss: 1.4042 - regression_loss: 1.1822 - classification_loss: 0.2220
  54/1500 [>.............................] - ETA: 19:00 - loss: 1.3872 - regression_loss: 1.1681 - classification_loss: 0.2191
  55/1500 [>.............................] - ETA: 19:03 - loss: 1.3911 - regression_loss: 1.1722 - classification_loss: 0.2190
  56/1500 [>.............................] - ETA: 19:01 - loss: 1.3840 - regression_loss: 1.1650 - classification_loss: 0.2190
  57/1500 [>.............................] - ETA: 18:49 - loss: 1.3839 - regression_loss: 1.1653 - classification_loss: 0.2186
  58/1500 [>.............................] - ETA: 18:40 - loss: 1.3847 - regression_loss: 1.1678 - classification_loss: 0.2169
  59/1500 [>.............................] - ETA: 18:48 - loss: 1.3939 - regression_loss: 1.1744 - classification_loss: 0.2195
  60/1500 [>.............................] - ETA: 18:38 - loss: 1.3782 - regression_loss: 1.1611 - classification_loss: 0.2171
  61/1500 [>.............................] - ETA: 18:46 - loss: 1.3972 - regression_loss: 1.1749 - classification_loss: 0.2222
  62/1500 [>.............................] - ETA: 19:48 - loss: 1.3976 - regression_loss: 1.1754 - classification_loss: 0.2222
  63/1500 [>.............................] - ETA: 19:46 - loss: 1.3871 - regression_loss: 1.1672 - classification_loss: 0.2199
  64/1500 [>.............................] - ETA: 19:45 - loss: 1.3845 - regression_loss: 1.1654 - classification_loss: 0.2191
  65/1500 [>.............................] - ETA: 19:36 - loss: 1.3838 - regression_loss: 1.1604 - classification_loss: 0.2234
  66/1500 [>.............................] - ETA: 19:31 - loss: 1.3865 - regression_loss: 1.1589 - classification_loss: 0.2276
  67/1500 [>.............................] - ETA: 19:21 - loss: 1.3769 - regression_loss: 1.1512 - classification_loss: 0.2257
  68/1500 [>.............................] - ETA: 19:26 - loss: 1.3703 - regression_loss: 1.1443 - classification_loss: 0.2260
  69/1500 [>.............................] - ETA: 19:19 - loss: 1.3754 - regression_loss: 1.1494 - classification_loss: 0.2261
  70/1500 [>.............................] - ETA: 19:31 - loss: 1.3725 - regression_loss: 1.1481 - classification_loss: 0.2244
  71/1500 [>.............................] - ETA: 19:32 - loss: 1.3692 - regression_loss: 1.1468 - classification_loss: 0.2224
  72/1500 [>.............................] - ETA: 20:25 - loss: 1.3761 - regression_loss: 1.1543 - classification_loss: 0.2218
  73/1500 [>.............................] - ETA: 20:16 - loss: 1.3752 - regression_loss: 1.1537 - classification_loss: 0.2215
  74/1500 [>.............................] - ETA: 20:09 - loss: 1.3739 - regression_loss: 1.1530 - classification_loss: 0.2210
  75/1500 [>.............................] - ETA: 19:59 - loss: 1.3878 - regression_loss: 1.1637 - classification_loss: 0.2241
  76/1500 [>.............................] - ETA: 19:55 - loss: 1.3792 - regression_loss: 1.1576 - classification_loss: 0.2217
  77/1500 [>.............................] - ETA: 20:05 - loss: 1.3709 - regression_loss: 1.1512 - classification_loss: 0.2196
  78/1500 [>.............................] - ETA: 20:09 - loss: 1.3823 - regression_loss: 1.1598 - classification_loss: 0.2224
  79/1500 [>.............................] - ETA: 20:01 - loss: 1.3744 - regression_loss: 1.1537 - classification_loss: 0.2208
  80/1500 [>.............................] - ETA: 19:52 - loss: 1.3666 - regression_loss: 1.1478 - classification_loss: 0.2187
  81/1500 [>.............................] - ETA: 19:44 - loss: 1.3785 - regression_loss: 1.1583 - classification_loss: 0.2201
  82/1500 [>.............................] - ETA: 19:51 - loss: 1.3925 - regression_loss: 1.1709 - classification_loss: 0.2216
  83/1500 [>.............................] - ETA: 19:44 - loss: 1.3948 - regression_loss: 1.1739 - classification_loss: 0.2209
  84/1500 [>.............................] - ETA: 19:38 - loss: 1.3904 - regression_loss: 1.1703 - classification_loss: 0.2201
  85/1500 [>.............................] - ETA: 19:44 - loss: 1.3921 - regression_loss: 1.1723 - classification_loss: 0.2198
  86/1500 [>.............................] - ETA: 19:39 - loss: 1.4085 - regression_loss: 1.1847 - classification_loss: 0.2238
  87/1500 [>.............................] - ETA: 19:33 - loss: 1.4116 - regression_loss: 1.1886 - classification_loss: 0.2229
  88/1500 [>.............................] - ETA: 19:26 - loss: 1.4133 - regression_loss: 1.1908 - classification_loss: 0.2225
  89/1500 [>.............................] - ETA: 19:20 - loss: 1.4110 - regression_loss: 1.1893 - classification_loss: 0.2218
  90/1500 [>.............................] - ETA: 19:13 - loss: 1.4040 - regression_loss: 1.1822 - classification_loss: 0.2218
  91/1500 [>.............................] - ETA: 19:06 - loss: 1.4122 - regression_loss: 1.1893 - classification_loss: 0.2229
  92/1500 [>.............................] - ETA: 19:04 - loss: 1.4055 - regression_loss: 1.1841 - classification_loss: 0.2214
  93/1500 [>.............................] - ETA: 18:58 - loss: 1.4092 - regression_loss: 1.1880 - classification_loss: 0.2212
  94/1500 [>.............................] - ETA: 18:54 - loss: 1.4081 - regression_loss: 1.1880 - classification_loss: 0.2200
  95/1500 [>.............................] - ETA: 18:58 - loss: 1.4083 - regression_loss: 1.1882 - classification_loss: 0.2201
  96/1500 [>.............................] - ETA: 19:08 - loss: 1.4131 - regression_loss: 1.1937 - classification_loss: 0.2194
  97/1500 [>.............................] - ETA: 19:02 - loss: 1.4197 - regression_loss: 1.1992 - classification_loss: 0.2205
  98/1500 [>.............................] - ETA: 18:55 - loss: 1.4279 - regression_loss: 1.2058 - classification_loss: 0.2222
  99/1500 [>.............................] - ETA: 18:48 - loss: 1.4281 - regression_loss: 1.2076 - classification_loss: 0.2205
 100/1500 [=>............................] - ETA: 18:42 - loss: 1.4300 - regression_loss: 1.2093 - classification_loss: 0.2207
 101/1500 [=>............................] - ETA: 18:36 - loss: 1.4284 - regression_loss: 1.2072 - classification_loss: 0.2212
 102/1500 [=>............................] - ETA: 18:29 - loss: 1.4311 - regression_loss: 1.2090 - classification_loss: 0.2221
 103/1500 [=>............................] - ETA: 18:23 - loss: 1.4310 - regression_loss: 1.2080 - classification_loss: 0.2230
 104/1500 [=>............................] - ETA: 18:17 - loss: 1.4308 - regression_loss: 1.2083 - classification_loss: 0.2225
 105/1500 [=>............................] - ETA: 18:15 - loss: 1.4273 - regression_loss: 1.2051 - classification_loss: 0.2223
 106/1500 [=>............................] - ETA: 18:19 - loss: 1.4270 - regression_loss: 1.2041 - classification_loss: 0.2228
 107/1500 [=>............................] - ETA: 18:25 - loss: 1.4444 - regression_loss: 1.2115 - classification_loss: 0.2329
 108/1500 [=>............................] - ETA: 18:19 - loss: 1.4458 - regression_loss: 1.2127 - classification_loss: 0.2331
 109/1500 [=>............................] - ETA: 18:13 - loss: 1.4416 - regression_loss: 1.2098 - classification_loss: 0.2318
 110/1500 [=>............................] - ETA: 18:07 - loss: 1.4400 - regression_loss: 1.2093 - classification_loss: 0.2307
 111/1500 [=>............................] - ETA: 18:17 - loss: 1.4411 - regression_loss: 1.2109 - classification_loss: 0.2302
 112/1500 [=>............................] - ETA: 18:12 - loss: 1.4383 - regression_loss: 1.2093 - classification_loss: 0.2290
 113/1500 [=>............................] - ETA: 18:07 - loss: 1.4337 - regression_loss: 1.2057 - classification_loss: 0.2280
 114/1500 [=>............................] - ETA: 18:05 - loss: 1.4327 - regression_loss: 1.2043 - classification_loss: 0.2284
 115/1500 [=>............................] - ETA: 18:00 - loss: 1.4254 - regression_loss: 1.1986 - classification_loss: 0.2268
 116/1500 [=>............................] - ETA: 18:10 - loss: 1.4218 - regression_loss: 1.1952 - classification_loss: 0.2266
 117/1500 [=>............................] - ETA: 18:04 - loss: 1.4304 - regression_loss: 1.2014 - classification_loss: 0.2290
 118/1500 [=>............................] - ETA: 18:04 - loss: 1.4300 - regression_loss: 1.2019 - classification_loss: 0.2280
 119/1500 [=>............................] - ETA: 18:02 - loss: 1.4280 - regression_loss: 1.2001 - classification_loss: 0.2279
 120/1500 [=>............................] - ETA: 17:57 - loss: 1.4275 - regression_loss: 1.1994 - classification_loss: 0.2282
 121/1500 [=>............................] - ETA: 17:56 - loss: 1.4271 - regression_loss: 1.1994 - classification_loss: 0.2277
 122/1500 [=>............................] - ETA: 17:58 - loss: 1.4306 - regression_loss: 1.2020 - classification_loss: 0.2286
 123/1500 [=>............................] - ETA: 17:53 - loss: 1.4270 - regression_loss: 1.1992 - classification_loss: 0.2278
 124/1500 [=>............................] - ETA: 17:48 - loss: 1.4291 - regression_loss: 1.2010 - classification_loss: 0.2281
 125/1500 [=>............................] - ETA: 17:43 - loss: 1.4282 - regression_loss: 1.2008 - classification_loss: 0.2274
 126/1500 [=>............................] - ETA: 17:41 - loss: 1.4240 - regression_loss: 1.1974 - classification_loss: 0.2266
 127/1500 [=>............................] - ETA: 17:38 - loss: 1.4256 - regression_loss: 1.1978 - classification_loss: 0.2278
 128/1500 [=>............................] - ETA: 17:33 - loss: 1.4319 - regression_loss: 1.2032 - classification_loss: 0.2287
 129/1500 [=>............................] - ETA: 17:37 - loss: 1.4270 - regression_loss: 1.1995 - classification_loss: 0.2275
 130/1500 [=>............................] - ETA: 17:39 - loss: 1.4329 - regression_loss: 1.2043 - classification_loss: 0.2285
 131/1500 [=>............................] - ETA: 17:39 - loss: 1.4388 - regression_loss: 1.2086 - classification_loss: 0.2302
 132/1500 [=>............................] - ETA: 17:41 - loss: 1.4358 - regression_loss: 1.2065 - classification_loss: 0.2294
 133/1500 [=>............................] - ETA: 17:39 - loss: 1.4352 - regression_loss: 1.2064 - classification_loss: 0.2288
 134/1500 [=>............................] - ETA: 17:39 - loss: 1.4382 - regression_loss: 1.2058 - classification_loss: 0.2324
 135/1500 [=>............................] - ETA: 17:41 - loss: 1.4402 - regression_loss: 1.2062 - classification_loss: 0.2340
 136/1500 [=>............................] - ETA: 17:43 - loss: 1.4444 - regression_loss: 1.2077 - classification_loss: 0.2367
 137/1500 [=>............................] - ETA: 17:38 - loss: 1.4455 - regression_loss: 1.2083 - classification_loss: 0.2372
 138/1500 [=>............................] - ETA: 17:41 - loss: 1.4397 - regression_loss: 1.2035 - classification_loss: 0.2362
 139/1500 [=>............................] - ETA: 17:39 - loss: 1.4357 - regression_loss: 1.2006 - classification_loss: 0.2351
 140/1500 [=>............................] - ETA: 17:40 - loss: 1.4340 - regression_loss: 1.1987 - classification_loss: 0.2353
 141/1500 [=>............................] - ETA: 17:36 - loss: 1.4312 - regression_loss: 1.1962 - classification_loss: 0.2349
 142/1500 [=>............................] - ETA: 17:32 - loss: 1.4349 - regression_loss: 1.1990 - classification_loss: 0.2359
 143/1500 [=>............................] - ETA: 17:30 - loss: 1.4358 - regression_loss: 1.2005 - classification_loss: 0.2353
 144/1500 [=>............................] - ETA: 17:33 - loss: 1.4331 - regression_loss: 1.1987 - classification_loss: 0.2344
 145/1500 [=>............................] - ETA: 17:29 - loss: 1.4312 - regression_loss: 1.1972 - classification_loss: 0.2340
 146/1500 [=>............................] - ETA: 17:30 - loss: 1.4385 - regression_loss: 1.2025 - classification_loss: 0.2360
 147/1500 [=>............................] - ETA: 17:44 - loss: 1.4351 - regression_loss: 1.1995 - classification_loss: 0.2355
 148/1500 [=>............................] - ETA: 17:50 - loss: 1.4422 - regression_loss: 1.2053 - classification_loss: 0.2369
 149/1500 [=>............................] - ETA: 17:52 - loss: 1.4443 - regression_loss: 1.2068 - classification_loss: 0.2374
 150/1500 [==>...........................] - ETA: 17:48 - loss: 1.4443 - regression_loss: 1.2077 - classification_loss: 0.2366
 151/1500 [==>...........................] - ETA: 17:44 - loss: 1.4440 - regression_loss: 1.2073 - classification_loss: 0.2366
 152/1500 [==>...........................] - ETA: 17:44 - loss: 1.4417 - regression_loss: 1.2057 - classification_loss: 0.2360
 153/1500 [==>...........................] - ETA: 17:40 - loss: 1.4464 - regression_loss: 1.2096 - classification_loss: 0.2368
 154/1500 [==>...........................] - ETA: 17:57 - loss: 1.4485 - regression_loss: 1.2115 - classification_loss: 0.2370
 155/1500 [==>...........................] - ETA: 17:54 - loss: 1.4459 - regression_loss: 1.2093 - classification_loss: 0.2366
 156/1500 [==>...........................] - ETA: 17:50 - loss: 1.4432 - regression_loss: 1.2073 - classification_loss: 0.2358
 157/1500 [==>...........................] - ETA: 17:46 - loss: 1.4465 - regression_loss: 1.2110 - classification_loss: 0.2355
 158/1500 [==>...........................] - ETA: 17:42 - loss: 1.4422 - regression_loss: 1.2076 - classification_loss: 0.2346
 159/1500 [==>...........................] - ETA: 17:38 - loss: 1.4453 - regression_loss: 1.2072 - classification_loss: 0.2381
 160/1500 [==>...........................] - ETA: 17:35 - loss: 1.4451 - regression_loss: 1.2076 - classification_loss: 0.2375
 161/1500 [==>...........................] - ETA: 17:31 - loss: 1.4467 - regression_loss: 1.2091 - classification_loss: 0.2376
 162/1500 [==>...........................] - ETA: 17:35 - loss: 1.4405 - regression_loss: 1.2040 - classification_loss: 0.2365
 163/1500 [==>...........................] - ETA: 17:41 - loss: 1.4358 - regression_loss: 1.2001 - classification_loss: 0.2357
 164/1500 [==>...........................] - ETA: 17:42 - loss: 1.4376 - regression_loss: 1.2012 - classification_loss: 0.2364
 165/1500 [==>...........................] - ETA: 17:42 - loss: 1.4407 - regression_loss: 1.2044 - classification_loss: 0.2363
 166/1500 [==>...........................] - ETA: 17:41 - loss: 1.4402 - regression_loss: 1.2032 - classification_loss: 0.2370
 167/1500 [==>...........................] - ETA: 17:45 - loss: 1.4368 - regression_loss: 1.2002 - classification_loss: 0.2365
 168/1500 [==>...........................] - ETA: 17:41 - loss: 1.4396 - regression_loss: 1.2027 - classification_loss: 0.2369
 169/1500 [==>...........................] - ETA: 17:38 - loss: 1.4377 - regression_loss: 1.2013 - classification_loss: 0.2364
 170/1500 [==>...........................] - ETA: 17:34 - loss: 1.4345 - regression_loss: 1.1991 - classification_loss: 0.2354
 171/1500 [==>...........................] - ETA: 17:30 - loss: 1.4304 - regression_loss: 1.1958 - classification_loss: 0.2347
 172/1500 [==>...........................] - ETA: 17:27 - loss: 1.4366 - regression_loss: 1.2008 - classification_loss: 0.2358
 173/1500 [==>...........................] - ETA: 17:24 - loss: 1.4413 - regression_loss: 1.2051 - classification_loss: 0.2362
 174/1500 [==>...........................] - ETA: 17:25 - loss: 1.4425 - regression_loss: 1.2065 - classification_loss: 0.2359
 175/1500 [==>...........................] - ETA: 17:22 - loss: 1.4439 - regression_loss: 1.2081 - classification_loss: 0.2358
 176/1500 [==>...........................] - ETA: 17:21 - loss: 1.4506 - regression_loss: 1.2132 - classification_loss: 0.2374
 177/1500 [==>...........................] - ETA: 17:26 - loss: 1.4477 - regression_loss: 1.2107 - classification_loss: 0.2370
 178/1500 [==>...........................] - ETA: 17:33 - loss: 1.4418 - regression_loss: 1.2058 - classification_loss: 0.2359
 179/1500 [==>...........................] - ETA: 17:30 - loss: 1.4440 - regression_loss: 1.2078 - classification_loss: 0.2362
 180/1500 [==>...........................] - ETA: 17:27 - loss: 1.4428 - regression_loss: 1.2075 - classification_loss: 0.2354
 181/1500 [==>...........................] - ETA: 17:26 - loss: 1.4383 - regression_loss: 1.2037 - classification_loss: 0.2346
 182/1500 [==>...........................] - ETA: 17:28 - loss: 1.4363 - regression_loss: 1.2023 - classification_loss: 0.2340
 183/1500 [==>...........................] - ETA: 17:25 - loss: 1.4380 - regression_loss: 1.2039 - classification_loss: 0.2341
 184/1500 [==>...........................] - ETA: 17:21 - loss: 1.4346 - regression_loss: 1.2013 - classification_loss: 0.2333
 185/1500 [==>...........................] - ETA: 17:27 - loss: 1.4333 - regression_loss: 1.2005 - classification_loss: 0.2328
 186/1500 [==>...........................] - ETA: 17:27 - loss: 1.4302 - regression_loss: 1.1971 - classification_loss: 0.2331
 187/1500 [==>...........................] - ETA: 17:24 - loss: 1.4267 - regression_loss: 1.1945 - classification_loss: 0.2322
 188/1500 [==>...........................] - ETA: 17:21 - loss: 1.4282 - regression_loss: 1.1958 - classification_loss: 0.2325
 189/1500 [==>...........................] - ETA: 17:17 - loss: 1.4281 - regression_loss: 1.1962 - classification_loss: 0.2319
 190/1500 [==>...........................] - ETA: 17:13 - loss: 1.4261 - regression_loss: 1.1941 - classification_loss: 0.2319
 191/1500 [==>...........................] - ETA: 17:10 - loss: 1.4253 - regression_loss: 1.1936 - classification_loss: 0.2317
 192/1500 [==>...........................] - ETA: 17:09 - loss: 1.4236 - regression_loss: 1.1927 - classification_loss: 0.2309
 193/1500 [==>...........................] - ETA: 17:09 - loss: 1.4207 - regression_loss: 1.1906 - classification_loss: 0.2301
 194/1500 [==>...........................] - ETA: 17:08 - loss: 1.4206 - regression_loss: 1.1904 - classification_loss: 0.2302
 195/1500 [==>...........................] - ETA: 17:10 - loss: 1.4281 - regression_loss: 1.1958 - classification_loss: 0.2323
 196/1500 [==>...........................] - ETA: 17:13 - loss: 1.4269 - regression_loss: 1.1953 - classification_loss: 0.2316
 197/1500 [==>...........................] - ETA: 17:14 - loss: 1.4292 - regression_loss: 1.1972 - classification_loss: 0.2321
 198/1500 [==>...........................] - ETA: 17:17 - loss: 1.4369 - regression_loss: 1.2025 - classification_loss: 0.2344
 199/1500 [==>...........................] - ETA: 17:14 - loss: 1.4369 - regression_loss: 1.2029 - classification_loss: 0.2340
 200/1500 [===>..........................] - ETA: 17:10 - loss: 1.4383 - regression_loss: 1.2037 - classification_loss: 0.2346
 201/1500 [===>..........................] - ETA: 17:15 - loss: 1.4392 - regression_loss: 1.2048 - classification_loss: 0.2344
 202/1500 [===>..........................] - ETA: 17:12 - loss: 1.4388 - regression_loss: 1.2045 - classification_loss: 0.2343
 203/1500 [===>..........................] - ETA: 17:13 - loss: 1.4413 - regression_loss: 1.2069 - classification_loss: 0.2344
 204/1500 [===>..........................] - ETA: 17:10 - loss: 1.4397 - regression_loss: 1.2057 - classification_loss: 0.2339
 205/1500 [===>..........................] - ETA: 17:06 - loss: 1.4459 - regression_loss: 1.2103 - classification_loss: 0.2356
 206/1500 [===>..........................] - ETA: 17:06 - loss: 1.4432 - regression_loss: 1.2082 - classification_loss: 0.2349
 207/1500 [===>..........................] - ETA: 17:03 - loss: 1.4446 - regression_loss: 1.2079 - classification_loss: 0.2367
 208/1500 [===>..........................] - ETA: 17:00 - loss: 1.4447 - regression_loss: 1.2079 - classification_loss: 0.2368
 209/1500 [===>..........................] - ETA: 16:57 - loss: 1.4433 - regression_loss: 1.2069 - classification_loss: 0.2365
 210/1500 [===>..........................] - ETA: 16:54 - loss: 1.4392 - regression_loss: 1.2033 - classification_loss: 0.2359
 211/1500 [===>..........................] - ETA: 16:51 - loss: 1.4371 - regression_loss: 1.2019 - classification_loss: 0.2353
 212/1500 [===>..........................] - ETA: 16:52 - loss: 1.4424 - regression_loss: 1.2063 - classification_loss: 0.2361
 213/1500 [===>..........................] - ETA: 16:52 - loss: 1.4453 - regression_loss: 1.2086 - classification_loss: 0.2366
 214/1500 [===>..........................] - ETA: 16:53 - loss: 1.4444 - regression_loss: 1.2081 - classification_loss: 0.2362
 215/1500 [===>..........................] - ETA: 16:56 - loss: 1.4406 - regression_loss: 1.2046 - classification_loss: 0.2360
 216/1500 [===>..........................] - ETA: 16:59 - loss: 1.4446 - regression_loss: 1.2079 - classification_loss: 0.2367
 217/1500 [===>..........................] - ETA: 17:06 - loss: 1.4446 - regression_loss: 1.2080 - classification_loss: 0.2366
 218/1500 [===>..........................] - ETA: 17:03 - loss: 1.4417 - regression_loss: 1.2058 - classification_loss: 0.2359
 219/1500 [===>..........................] - ETA: 17:00 - loss: 1.4404 - regression_loss: 1.2042 - classification_loss: 0.2362
 220/1500 [===>..........................] - ETA: 16:57 - loss: 1.4485 - regression_loss: 1.2114 - classification_loss: 0.2371
 221/1500 [===>..........................] - ETA: 16:54 - loss: 1.4460 - regression_loss: 1.2094 - classification_loss: 0.2366
 222/1500 [===>..........................] - ETA: 16:55 - loss: 1.4442 - regression_loss: 1.2078 - classification_loss: 0.2364
 223/1500 [===>..........................] - ETA: 16:53 - loss: 1.4416 - regression_loss: 1.2059 - classification_loss: 0.2357
 224/1500 [===>..........................] - ETA: 16:50 - loss: 1.4394 - regression_loss: 1.2044 - classification_loss: 0.2350
 225/1500 [===>..........................] - ETA: 16:50 - loss: 1.4373 - regression_loss: 1.2028 - classification_loss: 0.2346
 226/1500 [===>..........................] - ETA: 16:49 - loss: 1.4396 - regression_loss: 1.2053 - classification_loss: 0.2343
 227/1500 [===>..........................] - ETA: 16:49 - loss: 1.4361 - regression_loss: 1.2023 - classification_loss: 0.2338
 228/1500 [===>..........................] - ETA: 16:53 - loss: 1.4388 - regression_loss: 1.2033 - classification_loss: 0.2356
 229/1500 [===>..........................] - ETA: 16:50 - loss: 1.4376 - regression_loss: 1.2023 - classification_loss: 0.2353
 230/1500 [===>..........................] - ETA: 16:48 - loss: 1.4385 - regression_loss: 1.2017 - classification_loss: 0.2368
 231/1500 [===>..........................] - ETA: 16:45 - loss: 1.4369 - regression_loss: 1.2005 - classification_loss: 0.2364
 232/1500 [===>..........................] - ETA: 16:42 - loss: 1.4359 - regression_loss: 1.1997 - classification_loss: 0.2363
 233/1500 [===>..........................] - ETA: 16:40 - loss: 1.4337 - regression_loss: 1.1980 - classification_loss: 0.2358
 234/1500 [===>..........................] - ETA: 16:38 - loss: 1.4323 - regression_loss: 1.1967 - classification_loss: 0.2357
 235/1500 [===>..........................] - ETA: 16:36 - loss: 1.4289 - regression_loss: 1.1938 - classification_loss: 0.2351
 236/1500 [===>..........................] - ETA: 16:33 - loss: 1.4280 - regression_loss: 1.1933 - classification_loss: 0.2347
 237/1500 [===>..........................] - ETA: 16:31 - loss: 1.4247 - regression_loss: 1.1901 - classification_loss: 0.2346
 238/1500 [===>..........................] - ETA: 16:28 - loss: 1.4242 - regression_loss: 1.1901 - classification_loss: 0.2341
 239/1500 [===>..........................] - ETA: 16:25 - loss: 1.4255 - regression_loss: 1.1908 - classification_loss: 0.2347
 240/1500 [===>..........................] - ETA: 16:22 - loss: 1.4273 - regression_loss: 1.1918 - classification_loss: 0.2355
 241/1500 [===>..........................] - ETA: 16:19 - loss: 1.4257 - regression_loss: 1.1906 - classification_loss: 0.2351
 242/1500 [===>..........................] - ETA: 16:19 - loss: 1.4273 - regression_loss: 1.1918 - classification_loss: 0.2355
 243/1500 [===>..........................] - ETA: 16:18 - loss: 1.4253 - regression_loss: 1.1901 - classification_loss: 0.2352
 244/1500 [===>..........................] - ETA: 16:15 - loss: 1.4245 - regression_loss: 1.1894 - classification_loss: 0.2351
 245/1500 [===>..........................] - ETA: 16:19 - loss: 1.4276 - regression_loss: 1.1919 - classification_loss: 0.2357
 246/1500 [===>..........................] - ETA: 16:18 - loss: 1.4324 - regression_loss: 1.1963 - classification_loss: 0.2361
 247/1500 [===>..........................] - ETA: 16:16 - loss: 1.4326 - regression_loss: 1.1969 - classification_loss: 0.2357
 248/1500 [===>..........................] - ETA: 16:15 - loss: 1.4331 - regression_loss: 1.1971 - classification_loss: 0.2361
 249/1500 [===>..........................] - ETA: 16:17 - loss: 1.4380 - regression_loss: 1.2007 - classification_loss: 0.2373
 250/1500 [====>.........................] - ETA: 16:19 - loss: 1.4378 - regression_loss: 1.2006 - classification_loss: 0.2372
 251/1500 [====>.........................] - ETA: 16:16 - loss: 1.4343 - regression_loss: 1.1978 - classification_loss: 0.2364
 252/1500 [====>.........................] - ETA: 16:16 - loss: 1.4341 - regression_loss: 1.1976 - classification_loss: 0.2365
 253/1500 [====>.........................] - ETA: 16:20 - loss: 1.4328 - regression_loss: 1.1969 - classification_loss: 0.2360
 254/1500 [====>.........................] - ETA: 16:18 - loss: 1.4333 - regression_loss: 1.1973 - classification_loss: 0.2360
 255/1500 [====>.........................] - ETA: 16:20 - loss: 1.4351 - regression_loss: 1.1986 - classification_loss: 0.2366
 256/1500 [====>.........................] - ETA: 16:19 - loss: 1.4357 - regression_loss: 1.1986 - classification_loss: 0.2371
 257/1500 [====>.........................] - ETA: 16:17 - loss: 1.4343 - regression_loss: 1.1975 - classification_loss: 0.2368
 258/1500 [====>.........................] - ETA: 16:16 - loss: 1.4338 - regression_loss: 1.1972 - classification_loss: 0.2365
 259/1500 [====>.........................] - ETA: 16:14 - loss: 1.4334 - regression_loss: 1.1971 - classification_loss: 0.2364
 260/1500 [====>.........................] - ETA: 16:11 - loss: 1.4333 - regression_loss: 1.1969 - classification_loss: 0.2365
 261/1500 [====>.........................] - ETA: 16:10 - loss: 1.4327 - regression_loss: 1.1967 - classification_loss: 0.2360
 262/1500 [====>.........................] - ETA: 16:09 - loss: 1.4301 - regression_loss: 1.1947 - classification_loss: 0.2355
 263/1500 [====>.........................] - ETA: 16:08 - loss: 1.4271 - regression_loss: 1.1923 - classification_loss: 0.2348
 264/1500 [====>.........................] - ETA: 16:09 - loss: 1.4265 - regression_loss: 1.1921 - classification_loss: 0.2345
 265/1500 [====>.........................] - ETA: 16:09 - loss: 1.4260 - regression_loss: 1.1899 - classification_loss: 0.2361
 266/1500 [====>.........................] - ETA: 16:07 - loss: 1.4235 - regression_loss: 1.1880 - classification_loss: 0.2355
 267/1500 [====>.........................] - ETA: 16:05 - loss: 1.4234 - regression_loss: 1.1885 - classification_loss: 0.2349
 268/1500 [====>.........................] - ETA: 16:02 - loss: 1.4225 - regression_loss: 1.1879 - classification_loss: 0.2346
 269/1500 [====>.........................] - ETA: 16:02 - loss: 1.4190 - regression_loss: 1.1850 - classification_loss: 0.2340
 270/1500 [====>.........................] - ETA: 16:02 - loss: 1.4181 - regression_loss: 1.1846 - classification_loss: 0.2335
 271/1500 [====>.........................] - ETA: 16:03 - loss: 1.4217 - regression_loss: 1.1882 - classification_loss: 0.2335
 272/1500 [====>.........................] - ETA: 16:02 - loss: 1.4201 - regression_loss: 1.1870 - classification_loss: 0.2331
 273/1500 [====>.........................] - ETA: 16:03 - loss: 1.4190 - regression_loss: 1.1861 - classification_loss: 0.2329
 274/1500 [====>.........................] - ETA: 16:01 - loss: 1.4189 - regression_loss: 1.1864 - classification_loss: 0.2324
 275/1500 [====>.........................] - ETA: 15:58 - loss: 1.4163 - regression_loss: 1.1844 - classification_loss: 0.2319
 276/1500 [====>.........................] - ETA: 15:55 - loss: 1.4161 - regression_loss: 1.1843 - classification_loss: 0.2317
 277/1500 [====>.........................] - ETA: 15:53 - loss: 1.4154 - regression_loss: 1.1837 - classification_loss: 0.2317
 278/1500 [====>.........................] - ETA: 15:53 - loss: 1.4208 - regression_loss: 1.1875 - classification_loss: 0.2332
 279/1500 [====>.........................] - ETA: 15:53 - loss: 1.4223 - regression_loss: 1.1887 - classification_loss: 0.2336
 280/1500 [====>.........................] - ETA: 15:53 - loss: 1.4259 - regression_loss: 1.1911 - classification_loss: 0.2348
 281/1500 [====>.........................] - ETA: 15:53 - loss: 1.4261 - regression_loss: 1.1909 - classification_loss: 0.2353
 282/1500 [====>.........................] - ETA: 15:59 - loss: 1.4293 - regression_loss: 1.1932 - classification_loss: 0.2361
 283/1500 [====>.........................] - ETA: 16:00 - loss: 1.4312 - regression_loss: 1.1946 - classification_loss: 0.2367
 284/1500 [====>.........................] - ETA: 16:02 - loss: 1.4314 - regression_loss: 1.1952 - classification_loss: 0.2362
 285/1500 [====>.........................] - ETA: 16:03 - loss: 1.4332 - regression_loss: 1.1966 - classification_loss: 0.2366
 286/1500 [====>.........................] - ETA: 16:03 - loss: 1.4357 - regression_loss: 1.1993 - classification_loss: 0.2364
 287/1500 [====>.........................] - ETA: 16:05 - loss: 1.4381 - regression_loss: 1.2015 - classification_loss: 0.2366
 288/1500 [====>.........................] - ETA: 16:05 - loss: 1.4377 - regression_loss: 1.2001 - classification_loss: 0.2376
 289/1500 [====>.........................] - ETA: 16:07 - loss: 1.4405 - regression_loss: 1.2027 - classification_loss: 0.2378
 290/1500 [====>.........................] - ETA: 16:04 - loss: 1.4406 - regression_loss: 1.2031 - classification_loss: 0.2374
 291/1500 [====>.........................] - ETA: 16:07 - loss: 1.4436 - regression_loss: 1.2055 - classification_loss: 0.2381
 292/1500 [====>.........................] - ETA: 16:06 - loss: 1.4459 - regression_loss: 1.2073 - classification_loss: 0.2387
 293/1500 [====>.........................] - ETA: 16:03 - loss: 1.4448 - regression_loss: 1.2062 - classification_loss: 0.2386
 294/1500 [====>.........................] - ETA: 16:03 - loss: 1.4462 - regression_loss: 1.2071 - classification_loss: 0.2391
 295/1500 [====>.........................] - ETA: 16:03 - loss: 1.4432 - regression_loss: 1.2047 - classification_loss: 0.2385
 296/1500 [====>.........................] - ETA: 16:02 - loss: 1.4434 - regression_loss: 1.2051 - classification_loss: 0.2383
 297/1500 [====>.........................] - ETA: 15:59 - loss: 1.4404 - regression_loss: 1.2027 - classification_loss: 0.2377
 298/1500 [====>.........................] - ETA: 15:59 - loss: 1.4402 - regression_loss: 1.2021 - classification_loss: 0.2380
 299/1500 [====>.........................] - ETA: 15:57 - loss: 1.4403 - regression_loss: 1.2026 - classification_loss: 0.2378
 300/1500 [=====>........................] - ETA: 15:54 - loss: 1.4386 - regression_loss: 1.2011 - classification_loss: 0.2374
 301/1500 [=====>........................] - ETA: 15:56 - loss: 1.4376 - regression_loss: 1.2001 - classification_loss: 0.2375
 302/1500 [=====>........................] - ETA: 15:56 - loss: 1.4406 - regression_loss: 1.2025 - classification_loss: 0.2381
 303/1500 [=====>........................] - ETA: 15:55 - loss: 1.4433 - regression_loss: 1.2048 - classification_loss: 0.2385
 304/1500 [=====>........................] - ETA: 15:57 - loss: 1.4455 - regression_loss: 1.2066 - classification_loss: 0.2389
 305/1500 [=====>........................] - ETA: 15:54 - loss: 1.4434 - regression_loss: 1.2048 - classification_loss: 0.2386
 306/1500 [=====>........................] - ETA: 15:56 - loss: 1.4441 - regression_loss: 1.2053 - classification_loss: 0.2388
 307/1500 [=====>........................] - ETA: 15:54 - loss: 1.4446 - regression_loss: 1.2059 - classification_loss: 0.2387
 308/1500 [=====>........................] - ETA: 15:55 - loss: 1.4443 - regression_loss: 1.2050 - classification_loss: 0.2393
 309/1500 [=====>........................] - ETA: 15:56 - loss: 1.4428 - regression_loss: 1.2038 - classification_loss: 0.2390
 310/1500 [=====>........................] - ETA: 15:54 - loss: 1.4420 - regression_loss: 1.2031 - classification_loss: 0.2389
 311/1500 [=====>........................] - ETA: 15:54 - loss: 1.4398 - regression_loss: 1.2015 - classification_loss: 0.2384
 312/1500 [=====>........................] - ETA: 15:52 - loss: 1.4419 - regression_loss: 1.2028 - classification_loss: 0.2390
 313/1500 [=====>........................] - ETA: 15:53 - loss: 1.4409 - regression_loss: 1.2019 - classification_loss: 0.2390
 314/1500 [=====>........................] - ETA: 15:50 - loss: 1.4414 - regression_loss: 1.2018 - classification_loss: 0.2396
 315/1500 [=====>........................] - ETA: 15:50 - loss: 1.4406 - regression_loss: 1.2010 - classification_loss: 0.2396
 316/1500 [=====>........................] - ETA: 15:48 - loss: 1.4405 - regression_loss: 1.2008 - classification_loss: 0.2397
 317/1500 [=====>........................] - ETA: 15:48 - loss: 1.4435 - regression_loss: 1.2035 - classification_loss: 0.2400
 318/1500 [=====>........................] - ETA: 15:46 - loss: 1.4425 - regression_loss: 1.2030 - classification_loss: 0.2395
 319/1500 [=====>........................] - ETA: 15:43 - loss: 1.4429 - regression_loss: 1.2037 - classification_loss: 0.2393
 320/1500 [=====>........................] - ETA: 15:42 - loss: 1.4448 - regression_loss: 1.2056 - classification_loss: 0.2391
 321/1500 [=====>........................] - ETA: 15:41 - loss: 1.4449 - regression_loss: 1.2058 - classification_loss: 0.2391
 322/1500 [=====>........................] - ETA: 15:38 - loss: 1.4460 - regression_loss: 1.2066 - classification_loss: 0.2394
 323/1500 [=====>........................] - ETA: 15:36 - loss: 1.4456 - regression_loss: 1.2065 - classification_loss: 0.2392
 324/1500 [=====>........................] - ETA: 15:34 - loss: 1.4433 - regression_loss: 1.2044 - classification_loss: 0.2389
 325/1500 [=====>........................] - ETA: 15:33 - loss: 1.4441 - regression_loss: 1.2051 - classification_loss: 0.2390
 326/1500 [=====>........................] - ETA: 15:31 - loss: 1.4470 - regression_loss: 1.2077 - classification_loss: 0.2393
 327/1500 [=====>........................] - ETA: 15:28 - loss: 1.4469 - regression_loss: 1.2081 - classification_loss: 0.2388
 328/1500 [=====>........................] - ETA: 15:26 - loss: 1.4501 - regression_loss: 1.2113 - classification_loss: 0.2389
 329/1500 [=====>........................] - ETA: 15:25 - loss: 1.4504 - regression_loss: 1.2118 - classification_loss: 0.2386
 330/1500 [=====>........................] - ETA: 15:25 - loss: 1.4498 - regression_loss: 1.2115 - classification_loss: 0.2383
 331/1500 [=====>........................] - ETA: 15:23 - loss: 1.4496 - regression_loss: 1.2113 - classification_loss: 0.2383
 332/1500 [=====>........................] - ETA: 15:22 - loss: 1.4478 - regression_loss: 1.2099 - classification_loss: 0.2379
 333/1500 [=====>........................] - ETA: 15:22 - loss: 1.4492 - regression_loss: 1.2112 - classification_loss: 0.2380
 334/1500 [=====>........................] - ETA: 15:19 - loss: 1.4491 - regression_loss: 1.2114 - classification_loss: 0.2377
 335/1500 [=====>........................] - ETA: 15:19 - loss: 1.4517 - regression_loss: 1.2135 - classification_loss: 0.2382
 336/1500 [=====>........................] - ETA: 15:19 - loss: 1.4520 - regression_loss: 1.2139 - classification_loss: 0.2380
 337/1500 [=====>........................] - ETA: 15:17 - loss: 1.4514 - regression_loss: 1.2133 - classification_loss: 0.2381
 338/1500 [=====>........................] - ETA: 15:16 - loss: 1.4512 - regression_loss: 1.2133 - classification_loss: 0.2379
 339/1500 [=====>........................] - ETA: 15:17 - loss: 1.4526 - regression_loss: 1.2147 - classification_loss: 0.2379
 340/1500 [=====>........................] - ETA: 15:16 - loss: 1.4507 - regression_loss: 1.2131 - classification_loss: 0.2376
 341/1500 [=====>........................] - ETA: 15:14 - loss: 1.4481 - regression_loss: 1.2110 - classification_loss: 0.2371
 342/1500 [=====>........................] - ETA: 15:12 - loss: 1.4486 - regression_loss: 1.2118 - classification_loss: 0.2369
 343/1500 [=====>........................] - ETA: 15:12 - loss: 1.4496 - regression_loss: 1.2127 - classification_loss: 0.2369
 344/1500 [=====>........................] - ETA: 15:14 - loss: 1.4480 - regression_loss: 1.2112 - classification_loss: 0.2368
 345/1500 [=====>........................] - ETA: 15:13 - loss: 1.4488 - regression_loss: 1.2118 - classification_loss: 0.2369
 346/1500 [=====>........................] - ETA: 15:11 - loss: 1.4466 - regression_loss: 1.2101 - classification_loss: 0.2365
 347/1500 [=====>........................] - ETA: 15:09 - loss: 1.4474 - regression_loss: 1.2110 - classification_loss: 0.2364
 348/1500 [=====>........................] - ETA: 15:09 - loss: 1.4483 - regression_loss: 1.2121 - classification_loss: 0.2362
 349/1500 [=====>........................] - ETA: 15:07 - loss: 1.4515 - regression_loss: 1.2145 - classification_loss: 0.2370
 350/1500 [======>.......................] - ETA: 15:10 - loss: 1.4513 - regression_loss: 1.2144 - classification_loss: 0.2369
 351/1500 [======>.......................] - ETA: 15:12 - loss: 1.4501 - regression_loss: 1.2136 - classification_loss: 0.2365
 352/1500 [======>.......................] - ETA: 15:10 - loss: 1.4483 - regression_loss: 1.2118 - classification_loss: 0.2366
 353/1500 [======>.......................] - ETA: 15:08 - loss: 1.4460 - regression_loss: 1.2099 - classification_loss: 0.2361
 354/1500 [======>.......................] - ETA: 15:06 - loss: 1.4481 - regression_loss: 1.2110 - classification_loss: 0.2371
 355/1500 [======>.......................] - ETA: 15:04 - loss: 1.4492 - regression_loss: 1.2122 - classification_loss: 0.2370
 356/1500 [======>.......................] - ETA: 15:06 - loss: 1.4468 - regression_loss: 1.2101 - classification_loss: 0.2368
 357/1500 [======>.......................] - ETA: 15:04 - loss: 1.4453 - regression_loss: 1.2089 - classification_loss: 0.2365
 358/1500 [======>.......................] - ETA: 15:02 - loss: 1.4442 - regression_loss: 1.2080 - classification_loss: 0.2361
 359/1500 [======>.......................] - ETA: 15:02 - loss: 1.4460 - regression_loss: 1.2097 - classification_loss: 0.2362
 360/1500 [======>.......................] - ETA: 15:01 - loss: 1.4475 - regression_loss: 1.2113 - classification_loss: 0.2362
 361/1500 [======>.......................] - ETA: 14:59 - loss: 1.4468 - regression_loss: 1.2109 - classification_loss: 0.2359
 362/1500 [======>.......................] - ETA: 14:58 - loss: 1.4498 - regression_loss: 1.2137 - classification_loss: 0.2361
 363/1500 [======>.......................] - ETA: 14:56 - loss: 1.4496 - regression_loss: 1.2133 - classification_loss: 0.2362
 364/1500 [======>.......................] - ETA: 14:54 - loss: 1.4491 - regression_loss: 1.2132 - classification_loss: 0.2359
 365/1500 [======>.......................] - ETA: 14:53 - loss: 1.4501 - regression_loss: 1.2143 - classification_loss: 0.2358
 366/1500 [======>.......................] - ETA: 14:51 - loss: 1.4507 - regression_loss: 1.2149 - classification_loss: 0.2358
 367/1500 [======>.......................] - ETA: 14:49 - loss: 1.4505 - regression_loss: 1.2150 - classification_loss: 0.2355
 368/1500 [======>.......................] - ETA: 14:53 - loss: 1.4517 - regression_loss: 1.2166 - classification_loss: 0.2351
 369/1500 [======>.......................] - ETA: 14:52 - loss: 1.4546 - regression_loss: 1.2192 - classification_loss: 0.2354
 370/1500 [======>.......................] - ETA: 14:51 - loss: 1.4538 - regression_loss: 1.2186 - classification_loss: 0.2353
 371/1500 [======>.......................] - ETA: 14:57 - loss: 1.4515 - regression_loss: 1.2166 - classification_loss: 0.2348
 372/1500 [======>.......................] - ETA: 14:55 - loss: 1.4512 - regression_loss: 1.2166 - classification_loss: 0.2347
 373/1500 [======>.......................] - ETA: 14:54 - loss: 1.4499 - regression_loss: 1.2155 - classification_loss: 0.2345
 374/1500 [======>.......................] - ETA: 14:53 - loss: 1.4509 - regression_loss: 1.2162 - classification_loss: 0.2347
 375/1500 [======>.......................] - ETA: 14:56 - loss: 1.4527 - regression_loss: 1.2176 - classification_loss: 0.2351
 376/1500 [======>.......................] - ETA: 14:59 - loss: 1.4553 - regression_loss: 1.2189 - classification_loss: 0.2363
 377/1500 [======>.......................] - ETA: 14:57 - loss: 1.4532 - regression_loss: 1.2174 - classification_loss: 0.2358
 378/1500 [======>.......................] - ETA: 14:56 - loss: 1.4526 - regression_loss: 1.2170 - classification_loss: 0.2356
 379/1500 [======>.......................] - ETA: 14:55 - loss: 1.4530 - regression_loss: 1.2172 - classification_loss: 0.2358
 380/1500 [======>.......................] - ETA: 14:53 - loss: 1.4556 - regression_loss: 1.2192 - classification_loss: 0.2364
 381/1500 [======>.......................] - ETA: 14:53 - loss: 1.4539 - regression_loss: 1.2178 - classification_loss: 0.2361
 382/1500 [======>.......................] - ETA: 14:51 - loss: 1.4530 - regression_loss: 1.2170 - classification_loss: 0.2360
 383/1500 [======>.......................] - ETA: 14:49 - loss: 1.4526 - regression_loss: 1.2169 - classification_loss: 0.2357
 384/1500 [======>.......................] - ETA: 14:47 - loss: 1.4519 - regression_loss: 1.2161 - classification_loss: 0.2358
 385/1500 [======>.......................] - ETA: 14:45 - loss: 1.4501 - regression_loss: 1.2145 - classification_loss: 0.2355
 386/1500 [======>.......................] - ETA: 14:45 - loss: 1.4484 - regression_loss: 1.2132 - classification_loss: 0.2352
 387/1500 [======>.......................] - ETA: 14:45 - loss: 1.4483 - regression_loss: 1.2133 - classification_loss: 0.2349
 388/1500 [======>.......................] - ETA: 14:44 - loss: 1.4484 - regression_loss: 1.2137 - classification_loss: 0.2347
 389/1500 [======>.......................] - ETA: 14:43 - loss: 1.4461 - regression_loss: 1.2118 - classification_loss: 0.2343
 390/1500 [======>.......................] - ETA: 14:41 - loss: 1.4464 - regression_loss: 1.2118 - classification_loss: 0.2346
 391/1500 [======>.......................] - ETA: 14:39 - loss: 1.4455 - regression_loss: 1.2109 - classification_loss: 0.2346
 392/1500 [======>.......................] - ETA: 14:42 - loss: 1.4467 - regression_loss: 1.2113 - classification_loss: 0.2354
 393/1500 [======>.......................] - ETA: 14:42 - loss: 1.4470 - regression_loss: 1.2119 - classification_loss: 0.2352
 394/1500 [======>.......................] - ETA: 14:40 - loss: 1.4474 - regression_loss: 1.2126 - classification_loss: 0.2348
 395/1500 [======>.......................] - ETA: 14:39 - loss: 1.4480 - regression_loss: 1.2132 - classification_loss: 0.2347
 396/1500 [======>.......................] - ETA: 14:40 - loss: 1.4483 - regression_loss: 1.2130 - classification_loss: 0.2353
 397/1500 [======>.......................] - ETA: 14:38 - loss: 1.4494 - regression_loss: 1.2142 - classification_loss: 0.2352
 398/1500 [======>.......................] - ETA: 14:36 - loss: 1.4485 - regression_loss: 1.2134 - classification_loss: 0.2351
 399/1500 [======>.......................] - ETA: 14:36 - loss: 1.4475 - regression_loss: 1.2125 - classification_loss: 0.2350
 400/1500 [=======>......................] - ETA: 14:35 - loss: 1.4472 - regression_loss: 1.2123 - classification_loss: 0.2349
 401/1500 [=======>......................] - ETA: 14:34 - loss: 1.4467 - regression_loss: 1.2121 - classification_loss: 0.2347
 402/1500 [=======>......................] - ETA: 14:36 - loss: 1.4483 - regression_loss: 1.2132 - classification_loss: 0.2351
 403/1500 [=======>......................] - ETA: 14:37 - loss: 1.4467 - regression_loss: 1.2119 - classification_loss: 0.2348
 404/1500 [=======>......................] - ETA: 14:35 - loss: 1.4505 - regression_loss: 1.2149 - classification_loss: 0.2356
 405/1500 [=======>......................] - ETA: 14:34 - loss: 1.4512 - regression_loss: 1.2159 - classification_loss: 0.2353
 406/1500 [=======>......................] - ETA: 14:32 - loss: 1.4500 - regression_loss: 1.2148 - classification_loss: 0.2351
 407/1500 [=======>......................] - ETA: 14:30 - loss: 1.4510 - regression_loss: 1.2154 - classification_loss: 0.2355
 408/1500 [=======>......................] - ETA: 14:30 - loss: 1.4520 - regression_loss: 1.2158 - classification_loss: 0.2362
 409/1500 [=======>......................] - ETA: 14:30 - loss: 1.4548 - regression_loss: 1.2180 - classification_loss: 0.2368
 410/1500 [=======>......................] - ETA: 14:29 - loss: 1.4558 - regression_loss: 1.2189 - classification_loss: 0.2369
 411/1500 [=======>......................] - ETA: 14:27 - loss: 1.4533 - regression_loss: 1.2169 - classification_loss: 0.2364
 412/1500 [=======>......................] - ETA: 14:25 - loss: 1.4527 - regression_loss: 1.2163 - classification_loss: 0.2365
 413/1500 [=======>......................] - ETA: 14:26 - loss: 1.4527 - regression_loss: 1.2163 - classification_loss: 0.2364
 414/1500 [=======>......................] - ETA: 14:24 - loss: 1.4509 - regression_loss: 1.2149 - classification_loss: 0.2360
 415/1500 [=======>......................] - ETA: 14:23 - loss: 1.4518 - regression_loss: 1.2153 - classification_loss: 0.2365
 416/1500 [=======>......................] - ETA: 14:21 - loss: 1.4517 - regression_loss: 1.2154 - classification_loss: 0.2362
 417/1500 [=======>......................] - ETA: 14:21 - loss: 1.4528 - regression_loss: 1.2166 - classification_loss: 0.2362
 418/1500 [=======>......................] - ETA: 14:20 - loss: 1.4515 - regression_loss: 1.2158 - classification_loss: 0.2357
 419/1500 [=======>......................] - ETA: 14:18 - loss: 1.4507 - regression_loss: 1.2150 - classification_loss: 0.2357
 420/1500 [=======>......................] - ETA: 14:16 - loss: 1.4501 - regression_loss: 1.2146 - classification_loss: 0.2355
 421/1500 [=======>......................] - ETA: 14:15 - loss: 1.4521 - regression_loss: 1.2165 - classification_loss: 0.2356
 422/1500 [=======>......................] - ETA: 14:15 - loss: 1.4532 - regression_loss: 1.2170 - classification_loss: 0.2362
 423/1500 [=======>......................] - ETA: 14:14 - loss: 1.4559 - regression_loss: 1.2194 - classification_loss: 0.2364
 424/1500 [=======>......................] - ETA: 14:14 - loss: 1.4547 - regression_loss: 1.2187 - classification_loss: 0.2360
 425/1500 [=======>......................] - ETA: 14:13 - loss: 1.4527 - regression_loss: 1.2171 - classification_loss: 0.2356
 426/1500 [=======>......................] - ETA: 14:11 - loss: 1.4532 - regression_loss: 1.2174 - classification_loss: 0.2358
 427/1500 [=======>......................] - ETA: 14:10 - loss: 1.4521 - regression_loss: 1.2164 - classification_loss: 0.2357
 428/1500 [=======>......................] - ETA: 14:08 - loss: 1.4510 - regression_loss: 1.2156 - classification_loss: 0.2354
 429/1500 [=======>......................] - ETA: 14:07 - loss: 1.4505 - regression_loss: 1.2155 - classification_loss: 0.2350
 430/1500 [=======>......................] - ETA: 14:08 - loss: 1.4508 - regression_loss: 1.2159 - classification_loss: 0.2349
 431/1500 [=======>......................] - ETA: 14:07 - loss: 1.4497 - regression_loss: 1.2150 - classification_loss: 0.2347
 432/1500 [=======>......................] - ETA: 14:07 - loss: 1.4492 - regression_loss: 1.2148 - classification_loss: 0.2344
 433/1500 [=======>......................] - ETA: 14:05 - loss: 1.4492 - regression_loss: 1.2150 - classification_loss: 0.2342
 434/1500 [=======>......................] - ETA: 14:04 - loss: 1.4495 - regression_loss: 1.2155 - classification_loss: 0.2340
 435/1500 [=======>......................] - ETA: 14:02 - loss: 1.4517 - regression_loss: 1.2171 - classification_loss: 0.2346
 436/1500 [=======>......................] - ETA: 14:01 - loss: 1.4527 - regression_loss: 1.2180 - classification_loss: 0.2347
 437/1500 [=======>......................] - ETA: 13:59 - loss: 1.4523 - regression_loss: 1.2176 - classification_loss: 0.2347
 438/1500 [=======>......................] - ETA: 13:58 - loss: 1.4521 - regression_loss: 1.2173 - classification_loss: 0.2348
 439/1500 [=======>......................] - ETA: 13:59 - loss: 1.4517 - regression_loss: 1.2168 - classification_loss: 0.2349
 440/1500 [=======>......................] - ETA: 13:58 - loss: 1.4527 - regression_loss: 1.2175 - classification_loss: 0.2352
 441/1500 [=======>......................] - ETA: 13:56 - loss: 1.4515 - regression_loss: 1.2166 - classification_loss: 0.2349
 442/1500 [=======>......................] - ETA: 13:55 - loss: 1.4502 - regression_loss: 1.2156 - classification_loss: 0.2346
 443/1500 [=======>......................] - ETA: 13:53 - loss: 1.4486 - regression_loss: 1.2141 - classification_loss: 0.2346
 444/1500 [=======>......................] - ETA: 13:55 - loss: 1.4498 - regression_loss: 1.2146 - classification_loss: 0.2351
 445/1500 [=======>......................] - ETA: 13:55 - loss: 1.4511 - regression_loss: 1.2160 - classification_loss: 0.2351
 446/1500 [=======>......................] - ETA: 13:53 - loss: 1.4512 - regression_loss: 1.2161 - classification_loss: 0.2350
 447/1500 [=======>......................] - ETA: 13:53 - loss: 1.4511 - regression_loss: 1.2162 - classification_loss: 0.2349
 448/1500 [=======>......................] - ETA: 13:54 - loss: 1.4503 - regression_loss: 1.2155 - classification_loss: 0.2347
 449/1500 [=======>......................] - ETA: 13:52 - loss: 1.4511 - regression_loss: 1.2159 - classification_loss: 0.2352
 450/1500 [========>.....................] - ETA: 13:52 - loss: 1.4513 - regression_loss: 1.2162 - classification_loss: 0.2352
 451/1500 [========>.....................] - ETA: 13:51 - loss: 1.4548 - regression_loss: 1.2192 - classification_loss: 0.2356
 452/1500 [========>.....................] - ETA: 13:49 - loss: 1.4568 - regression_loss: 1.2212 - classification_loss: 0.2356
 453/1500 [========>.....................] - ETA: 13:47 - loss: 1.4551 - regression_loss: 1.2198 - classification_loss: 0.2353
 454/1500 [========>.....................] - ETA: 13:47 - loss: 1.4561 - regression_loss: 1.2208 - classification_loss: 0.2353
 455/1500 [========>.....................] - ETA: 13:46 - loss: 1.4553 - regression_loss: 1.2201 - classification_loss: 0.2352
 456/1500 [========>.....................] - ETA: 13:45 - loss: 1.4539 - regression_loss: 1.2189 - classification_loss: 0.2350
 457/1500 [========>.....................] - ETA: 13:45 - loss: 1.4525 - regression_loss: 1.2178 - classification_loss: 0.2348
 458/1500 [========>.....................] - ETA: 13:44 - loss: 1.4523 - regression_loss: 1.2176 - classification_loss: 0.2347
 459/1500 [========>.....................] - ETA: 13:43 - loss: 1.4529 - regression_loss: 1.2179 - classification_loss: 0.2350
 460/1500 [========>.....................] - ETA: 13:43 - loss: 1.4539 - regression_loss: 1.2187 - classification_loss: 0.2352
 461/1500 [========>.....................] - ETA: 13:41 - loss: 1.4529 - regression_loss: 1.2180 - classification_loss: 0.2349
 462/1500 [========>.....................] - ETA: 13:39 - loss: 1.4523 - regression_loss: 1.2175 - classification_loss: 0.2348
 463/1500 [========>.....................] - ETA: 13:38 - loss: 1.4551 - regression_loss: 1.2199 - classification_loss: 0.2352
 464/1500 [========>.....................] - ETA: 13:39 - loss: 1.4544 - regression_loss: 1.2194 - classification_loss: 0.2350
 465/1500 [========>.....................] - ETA: 13:38 - loss: 1.4558 - regression_loss: 1.2205 - classification_loss: 0.2352
 466/1500 [========>.....................] - ETA: 13:36 - loss: 1.4557 - regression_loss: 1.2203 - classification_loss: 0.2354
 467/1500 [========>.....................] - ETA: 13:35 - loss: 1.4571 - regression_loss: 1.2218 - classification_loss: 0.2353
 468/1500 [========>.....................] - ETA: 13:35 - loss: 1.4556 - regression_loss: 1.2207 - classification_loss: 0.2349
 469/1500 [========>.....................] - ETA: 13:34 - loss: 1.4584 - regression_loss: 1.2227 - classification_loss: 0.2357
 470/1500 [========>.....................] - ETA: 13:32 - loss: 1.4586 - regression_loss: 1.2229 - classification_loss: 0.2356
 471/1500 [========>.....................] - ETA: 13:33 - loss: 1.4590 - regression_loss: 1.2234 - classification_loss: 0.2356
 472/1500 [========>.....................] - ETA: 13:32 - loss: 1.4586 - regression_loss: 1.2232 - classification_loss: 0.2354
 473/1500 [========>.....................] - ETA: 13:35 - loss: 1.4599 - regression_loss: 1.2242 - classification_loss: 0.2357
 474/1500 [========>.....................] - ETA: 13:36 - loss: 1.4606 - regression_loss: 1.2247 - classification_loss: 0.2358
 475/1500 [========>.....................] - ETA: 13:36 - loss: 1.4601 - regression_loss: 1.2243 - classification_loss: 0.2358
 476/1500 [========>.....................] - ETA: 13:34 - loss: 1.4589 - regression_loss: 1.2234 - classification_loss: 0.2355
 477/1500 [========>.....................] - ETA: 13:34 - loss: 1.4593 - regression_loss: 1.2238 - classification_loss: 0.2355
 478/1500 [========>.....................] - ETA: 13:34 - loss: 1.4595 - regression_loss: 1.2239 - classification_loss: 0.2356
 479/1500 [========>.....................] - ETA: 13:33 - loss: 1.4591 - regression_loss: 1.2237 - classification_loss: 0.2355
 480/1500 [========>.....................] - ETA: 13:33 - loss: 1.4569 - regression_loss: 1.2219 - classification_loss: 0.2351
 481/1500 [========>.....................] - ETA: 13:33 - loss: 1.4571 - regression_loss: 1.2220 - classification_loss: 0.2351
 482/1500 [========>.....................] - ETA: 13:31 - loss: 1.4571 - regression_loss: 1.2222 - classification_loss: 0.2348
 483/1500 [========>.....................] - ETA: 13:31 - loss: 1.4564 - regression_loss: 1.2219 - classification_loss: 0.2345
 484/1500 [========>.....................] - ETA: 13:32 - loss: 1.4577 - regression_loss: 1.2231 - classification_loss: 0.2346
 485/1500 [========>.....................] - ETA: 13:30 - loss: 1.4573 - regression_loss: 1.2229 - classification_loss: 0.2344
 486/1500 [========>.....................] - ETA: 13:28 - loss: 1.4577 - regression_loss: 1.2233 - classification_loss: 0.2344
 487/1500 [========>.....................] - ETA: 13:27 - loss: 1.4581 - regression_loss: 1.2232 - classification_loss: 0.2349
 488/1500 [========>.....................] - ETA: 13:25 - loss: 1.4571 - regression_loss: 1.2226 - classification_loss: 0.2346
 489/1500 [========>.....................] - ETA: 13:23 - loss: 1.4574 - regression_loss: 1.2229 - classification_loss: 0.2345
 490/1500 [========>.....................] - ETA: 13:22 - loss: 1.4588 - regression_loss: 1.2241 - classification_loss: 0.2347
 491/1500 [========>.....................] - ETA: 13:20 - loss: 1.4578 - regression_loss: 1.2234 - classification_loss: 0.2344
 492/1500 [========>.....................] - ETA: 13:21 - loss: 1.4575 - regression_loss: 1.2233 - classification_loss: 0.2342
 493/1500 [========>.....................] - ETA: 13:19 - loss: 1.4567 - regression_loss: 1.2225 - classification_loss: 0.2342
 494/1500 [========>.....................] - ETA: 13:17 - loss: 1.4575 - regression_loss: 1.2234 - classification_loss: 0.2341
 495/1500 [========>.....................] - ETA: 13:16 - loss: 1.4576 - regression_loss: 1.2237 - classification_loss: 0.2338
 496/1500 [========>.....................] - ETA: 13:15 - loss: 1.4571 - regression_loss: 1.2235 - classification_loss: 0.2337
 497/1500 [========>.....................] - ETA: 13:14 - loss: 1.4589 - regression_loss: 1.2250 - classification_loss: 0.2339
 498/1500 [========>.....................] - ETA: 13:12 - loss: 1.4598 - regression_loss: 1.2255 - classification_loss: 0.2343
 499/1500 [========>.....................] - ETA: 13:11 - loss: 1.4626 - regression_loss: 1.2281 - classification_loss: 0.2345
 500/1500 [=========>....................] - ETA: 13:09 - loss: 1.4625 - regression_loss: 1.2280 - classification_loss: 0.2346
 501/1500 [=========>....................] - ETA: 13:08 - loss: 1.4646 - regression_loss: 1.2295 - classification_loss: 0.2351
 502/1500 [=========>....................] - ETA: 13:07 - loss: 1.4639 - regression_loss: 1.2289 - classification_loss: 0.2350
 503/1500 [=========>....................] - ETA: 13:06 - loss: 1.4632 - regression_loss: 1.2285 - classification_loss: 0.2347
 504/1500 [=========>....................] - ETA: 13:07 - loss: 1.4637 - regression_loss: 1.2288 - classification_loss: 0.2349
 505/1500 [=========>....................] - ETA: 13:09 - loss: 1.4619 - regression_loss: 1.2274 - classification_loss: 0.2345
 506/1500 [=========>....................] - ETA: 13:08 - loss: 1.4603 - regression_loss: 1.2262 - classification_loss: 0.2342
 507/1500 [=========>....................] - ETA: 13:07 - loss: 1.4591 - regression_loss: 1.2252 - classification_loss: 0.2340
 508/1500 [=========>....................] - ETA: 13:05 - loss: 1.4598 - regression_loss: 1.2260 - classification_loss: 0.2339
 509/1500 [=========>....................] - ETA: 13:04 - loss: 1.4586 - regression_loss: 1.2249 - classification_loss: 0.2337
 510/1500 [=========>....................] - ETA: 13:02 - loss: 1.4585 - regression_loss: 1.2247 - classification_loss: 0.2338
 511/1500 [=========>....................] - ETA: 13:01 - loss: 1.4610 - regression_loss: 1.2267 - classification_loss: 0.2343
 512/1500 [=========>....................] - ETA: 13:00 - loss: 1.4597 - regression_loss: 1.2257 - classification_loss: 0.2340
 513/1500 [=========>....................] - ETA: 13:00 - loss: 1.4607 - regression_loss: 1.2268 - classification_loss: 0.2339
 514/1500 [=========>....................] - ETA: 13:02 - loss: 1.4608 - regression_loss: 1.2267 - classification_loss: 0.2341
 515/1500 [=========>....................] - ETA: 13:01 - loss: 1.4623 - regression_loss: 1.2275 - classification_loss: 0.2348
 516/1500 [=========>....................] - ETA: 13:01 - loss: 1.4612 - regression_loss: 1.2266 - classification_loss: 0.2346
 517/1500 [=========>....................] - ETA: 13:01 - loss: 1.4614 - regression_loss: 1.2268 - classification_loss: 0.2345
 518/1500 [=========>....................] - ETA: 13:04 - loss: 1.4644 - regression_loss: 1.2289 - classification_loss: 0.2355
 519/1500 [=========>....................] - ETA: 13:03 - loss: 1.4643 - regression_loss: 1.2290 - classification_loss: 0.2353
 520/1500 [=========>....................] - ETA: 13:01 - loss: 1.4640 - regression_loss: 1.2286 - classification_loss: 0.2353
 521/1500 [=========>....................] - ETA: 13:00 - loss: 1.4650 - regression_loss: 1.2296 - classification_loss: 0.2355
 522/1500 [=========>....................] - ETA: 12:58 - loss: 1.4630 - regression_loss: 1.2279 - classification_loss: 0.2352
 523/1500 [=========>....................] - ETA: 12:57 - loss: 1.4617 - regression_loss: 1.2268 - classification_loss: 0.2349
 524/1500 [=========>....................] - ETA: 12:56 - loss: 1.4616 - regression_loss: 1.2267 - classification_loss: 0.2349
 525/1500 [=========>....................] - ETA: 12:55 - loss: 1.4606 - regression_loss: 1.2259 - classification_loss: 0.2347
 526/1500 [=========>....................] - ETA: 12:53 - loss: 1.4595 - regression_loss: 1.2250 - classification_loss: 0.2346
 527/1500 [=========>....................] - ETA: 12:52 - loss: 1.4584 - regression_loss: 1.2240 - classification_loss: 0.2344
 528/1500 [=========>....................] - ETA: 12:50 - loss: 1.4592 - regression_loss: 1.2247 - classification_loss: 0.2345
 529/1500 [=========>....................] - ETA: 12:49 - loss: 1.4583 - regression_loss: 1.2241 - classification_loss: 0.2342
 530/1500 [=========>....................] - ETA: 12:47 - loss: 1.4594 - regression_loss: 1.2251 - classification_loss: 0.2342
 531/1500 [=========>....................] - ETA: 12:46 - loss: 1.4593 - regression_loss: 1.2250 - classification_loss: 0.2342
 532/1500 [=========>....................] - ETA: 12:45 - loss: 1.4579 - regression_loss: 1.2239 - classification_loss: 0.2340
 533/1500 [=========>....................] - ETA: 12:44 - loss: 1.4577 - regression_loss: 1.2238 - classification_loss: 0.2339
 534/1500 [=========>....................] - ETA: 12:43 - loss: 1.4596 - regression_loss: 1.2256 - classification_loss: 0.2341
 535/1500 [=========>....................] - ETA: 12:41 - loss: 1.4598 - regression_loss: 1.2255 - classification_loss: 0.2343
 536/1500 [=========>....................] - ETA: 12:40 - loss: 1.4597 - regression_loss: 1.2254 - classification_loss: 0.2344
 537/1500 [=========>....................] - ETA: 12:38 - loss: 1.4602 - regression_loss: 1.2260 - classification_loss: 0.2342
 538/1500 [=========>....................] - ETA: 12:37 - loss: 1.4595 - regression_loss: 1.2255 - classification_loss: 0.2340
 539/1500 [=========>....................] - ETA: 12:36 - loss: 1.4591 - regression_loss: 1.2253 - classification_loss: 0.2338
 540/1500 [=========>....................] - ETA: 12:35 - loss: 1.4617 - regression_loss: 1.2274 - classification_loss: 0.2344
 541/1500 [=========>....................] - ETA: 12:34 - loss: 1.4610 - regression_loss: 1.2268 - classification_loss: 0.2342
 542/1500 [=========>....................] - ETA: 12:35 - loss: 1.4610 - regression_loss: 1.2267 - classification_loss: 0.2343
 543/1500 [=========>....................] - ETA: 12:37 - loss: 1.4604 - regression_loss: 1.2262 - classification_loss: 0.2342
 544/1500 [=========>....................] - ETA: 12:38 - loss: 1.4598 - regression_loss: 1.2257 - classification_loss: 0.2341
 545/1500 [=========>....................] - ETA: 12:37 - loss: 1.4594 - regression_loss: 1.2254 - classification_loss: 0.2341
 546/1500 [=========>....................] - ETA: 12:36 - loss: 1.4595 - regression_loss: 1.2255 - classification_loss: 0.2340
 547/1500 [=========>....................] - ETA: 12:36 - loss: 1.4611 - regression_loss: 1.2271 - classification_loss: 0.2340
 548/1500 [=========>....................] - ETA: 12:36 - loss: 1.4613 - regression_loss: 1.2273 - classification_loss: 0.2340
 549/1500 [=========>....................] - ETA: 12:34 - loss: 1.4610 - regression_loss: 1.2271 - classification_loss: 0.2338
 550/1500 [==========>...................] - ETA: 12:33 - loss: 1.4600 - regression_loss: 1.2264 - classification_loss: 0.2336
 551/1500 [==========>...................] - ETA: 12:32 - loss: 1.4608 - regression_loss: 1.2273 - classification_loss: 0.2335
 552/1500 [==========>...................] - ETA: 12:31 - loss: 1.4612 - regression_loss: 1.2275 - classification_loss: 0.2337
 553/1500 [==========>...................] - ETA: 12:30 - loss: 1.4599 - regression_loss: 1.2264 - classification_loss: 0.2335
 554/1500 [==========>...................] - ETA: 12:29 - loss: 1.4587 - regression_loss: 1.2254 - classification_loss: 0.2333
 555/1500 [==========>...................] - ETA: 12:27 - loss: 1.4597 - regression_loss: 1.2261 - classification_loss: 0.2336
 556/1500 [==========>...................] - ETA: 12:26 - loss: 1.4588 - regression_loss: 1.2255 - classification_loss: 0.2334
 557/1500 [==========>...................] - ETA: 12:24 - loss: 1.4610 - regression_loss: 1.2272 - classification_loss: 0.2338
 558/1500 [==========>...................] - ETA: 12:23 - loss: 1.4611 - regression_loss: 1.2270 - classification_loss: 0.2341
 559/1500 [==========>...................] - ETA: 12:22 - loss: 1.4622 - regression_loss: 1.2278 - classification_loss: 0.2344
 560/1500 [==========>...................] - ETA: 12:21 - loss: 1.4607 - regression_loss: 1.2266 - classification_loss: 0.2341
 561/1500 [==========>...................] - ETA: 12:20 - loss: 1.4606 - regression_loss: 1.2267 - classification_loss: 0.2340
 562/1500 [==========>...................] - ETA: 12:19 - loss: 1.4604 - regression_loss: 1.2265 - classification_loss: 0.2339
 563/1500 [==========>...................] - ETA: 12:17 - loss: 1.4612 - regression_loss: 1.2272 - classification_loss: 0.2340
 564/1500 [==========>...................] - ETA: 12:16 - loss: 1.4627 - regression_loss: 1.2285 - classification_loss: 0.2342
 565/1500 [==========>...................] - ETA: 12:15 - loss: 1.4612 - regression_loss: 1.2273 - classification_loss: 0.2339
 566/1500 [==========>...................] - ETA: 12:13 - loss: 1.4603 - regression_loss: 1.2267 - classification_loss: 0.2337
 567/1500 [==========>...................] - ETA: 12:12 - loss: 1.4607 - regression_loss: 1.2272 - classification_loss: 0.2335
 568/1500 [==========>...................] - ETA: 12:10 - loss: 1.4613 - regression_loss: 1.2278 - classification_loss: 0.2335
 569/1500 [==========>...................] - ETA: 12:10 - loss: 1.4607 - regression_loss: 1.2272 - classification_loss: 0.2335
 570/1500 [==========>...................] - ETA: 12:10 - loss: 1.4593 - regression_loss: 1.2260 - classification_loss: 0.2332
 571/1500 [==========>...................] - ETA: 12:08 - loss: 1.4591 - regression_loss: 1.2260 - classification_loss: 0.2331
 572/1500 [==========>...................] - ETA: 12:11 - loss: 1.4609 - regression_loss: 1.2275 - classification_loss: 0.2333
 573/1500 [==========>...................] - ETA: 12:10 - loss: 1.4623 - regression_loss: 1.2287 - classification_loss: 0.2335
 574/1500 [==========>...................] - ETA: 12:10 - loss: 1.4610 - regression_loss: 1.2276 - classification_loss: 0.2334
 575/1500 [==========>...................] - ETA: 12:09 - loss: 1.4600 - regression_loss: 1.2268 - classification_loss: 0.2332
 576/1500 [==========>...................] - ETA: 12:10 - loss: 1.4610 - regression_loss: 1.2277 - classification_loss: 0.2333
 577/1500 [==========>...................] - ETA: 12:08 - loss: 1.4615 - regression_loss: 1.2281 - classification_loss: 0.2334
 578/1500 [==========>...................] - ETA: 12:09 - loss: 1.4628 - regression_loss: 1.2292 - classification_loss: 0.2336
 579/1500 [==========>...................] - ETA: 12:07 - loss: 1.4618 - regression_loss: 1.2284 - classification_loss: 0.2334
 580/1500 [==========>...................] - ETA: 12:06 - loss: 1.4632 - regression_loss: 1.2292 - classification_loss: 0.2340
 581/1500 [==========>...................] - ETA: 12:05 - loss: 1.4646 - regression_loss: 1.2303 - classification_loss: 0.2343
 582/1500 [==========>...................] - ETA: 12:04 - loss: 1.4637 - regression_loss: 1.2297 - classification_loss: 0.2340
 583/1500 [==========>...................] - ETA: 12:05 - loss: 1.4643 - regression_loss: 1.2303 - classification_loss: 0.2340
 584/1500 [==========>...................] - ETA: 12:06 - loss: 1.4657 - regression_loss: 1.2313 - classification_loss: 0.2343
 585/1500 [==========>...................] - ETA: 12:05 - loss: 1.4655 - regression_loss: 1.2313 - classification_loss: 0.2343
 586/1500 [==========>...................] - ETA: 12:03 - loss: 1.4660 - regression_loss: 1.2317 - classification_loss: 0.2343
 587/1500 [==========>...................] - ETA: 12:02 - loss: 1.4646 - regression_loss: 1.2306 - classification_loss: 0.2340
 588/1500 [==========>...................] - ETA: 12:02 - loss: 1.4665 - regression_loss: 1.2321 - classification_loss: 0.2344
 589/1500 [==========>...................] - ETA: 12:01 - loss: 1.4659 - regression_loss: 1.2316 - classification_loss: 0.2343
 590/1500 [==========>...................] - ETA: 12:02 - loss: 1.4663 - regression_loss: 1.2318 - classification_loss: 0.2345
 591/1500 [==========>...................] - ETA: 12:00 - loss: 1.4665 - regression_loss: 1.2321 - classification_loss: 0.2344
 592/1500 [==========>...................] - ETA: 11:59 - loss: 1.4656 - regression_loss: 1.2314 - classification_loss: 0.2343
 593/1500 [==========>...................] - ETA: 11:58 - loss: 1.4664 - regression_loss: 1.2320 - classification_loss: 0.2344
 594/1500 [==========>...................] - ETA: 11:57 - loss: 1.4664 - regression_loss: 1.2320 - classification_loss: 0.2344
 595/1500 [==========>...................] - ETA: 11:58 - loss: 1.4669 - regression_loss: 1.2325 - classification_loss: 0.2344
 596/1500 [==========>...................] - ETA: 11:57 - loss: 1.4658 - regression_loss: 1.2315 - classification_loss: 0.2342
 597/1500 [==========>...................] - ETA: 11:55 - loss: 1.4654 - regression_loss: 1.2310 - classification_loss: 0.2344
 598/1500 [==========>...................] - ETA: 11:54 - loss: 1.4667 - regression_loss: 1.2321 - classification_loss: 0.2345
 599/1500 [==========>...................] - ETA: 11:56 - loss: 1.4661 - regression_loss: 1.2317 - classification_loss: 0.2344
 600/1500 [===========>..................] - ETA: 11:55 - loss: 1.4669 - regression_loss: 1.2323 - classification_loss: 0.2346
 601/1500 [===========>..................] - ETA: 11:54 - loss: 1.4668 - regression_loss: 1.2323 - classification_loss: 0.2345
 602/1500 [===========>..................] - ETA: 11:52 - loss: 1.4689 - regression_loss: 1.2335 - classification_loss: 0.2354
 603/1500 [===========>..................] - ETA: 11:51 - loss: 1.4709 - regression_loss: 1.2351 - classification_loss: 0.2358
 604/1500 [===========>..................] - ETA: 11:50 - loss: 1.4715 - regression_loss: 1.2359 - classification_loss: 0.2357
 605/1500 [===========>..................] - ETA: 11:50 - loss: 1.4720 - regression_loss: 1.2356 - classification_loss: 0.2364
 606/1500 [===========>..................] - ETA: 11:49 - loss: 1.4733 - regression_loss: 1.2367 - classification_loss: 0.2366
 607/1500 [===========>..................] - ETA: 11:48 - loss: 1.4730 - regression_loss: 1.2366 - classification_loss: 0.2364
 608/1500 [===========>..................] - ETA: 11:47 - loss: 1.4735 - regression_loss: 1.2369 - classification_loss: 0.2366
 609/1500 [===========>..................] - ETA: 11:47 - loss: 1.4738 - regression_loss: 1.2372 - classification_loss: 0.2366
 610/1500 [===========>..................] - ETA: 11:45 - loss: 1.4750 - regression_loss: 1.2381 - classification_loss: 0.2369
 611/1500 [===========>..................] - ETA: 11:45 - loss: 1.4746 - regression_loss: 1.2376 - classification_loss: 0.2370
 612/1500 [===========>..................] - ETA: 11:44 - loss: 1.4740 - regression_loss: 1.2372 - classification_loss: 0.2368
 613/1500 [===========>..................] - ETA: 11:42 - loss: 1.4737 - regression_loss: 1.2370 - classification_loss: 0.2367
 614/1500 [===========>..................] - ETA: 11:41 - loss: 1.4762 - regression_loss: 1.2388 - classification_loss: 0.2373
 615/1500 [===========>..................] - ETA: 11:40 - loss: 1.4752 - regression_loss: 1.2380 - classification_loss: 0.2372
 616/1500 [===========>..................] - ETA: 11:39 - loss: 1.4741 - regression_loss: 1.2371 - classification_loss: 0.2370
 617/1500 [===========>..................] - ETA: 11:38 - loss: 1.4728 - regression_loss: 1.2361 - classification_loss: 0.2367
 618/1500 [===========>..................] - ETA: 11:38 - loss: 1.4732 - regression_loss: 1.2365 - classification_loss: 0.2367
 619/1500 [===========>..................] - ETA: 11:36 - loss: 1.4731 - regression_loss: 1.2365 - classification_loss: 0.2366
 620/1500 [===========>..................] - ETA: 11:35 - loss: 1.4727 - regression_loss: 1.2363 - classification_loss: 0.2365
 621/1500 [===========>..................] - ETA: 11:34 - loss: 1.4724 - regression_loss: 1.2358 - classification_loss: 0.2366
 622/1500 [===========>..................] - ETA: 11:33 - loss: 1.4721 - regression_loss: 1.2356 - classification_loss: 0.2365
 623/1500 [===========>..................] - ETA: 11:32 - loss: 1.4721 - regression_loss: 1.2353 - classification_loss: 0.2369
 624/1500 [===========>..................] - ETA: 11:31 - loss: 1.4735 - regression_loss: 1.2363 - classification_loss: 0.2372
 625/1500 [===========>..................] - ETA: 11:32 - loss: 1.4731 - regression_loss: 1.2361 - classification_loss: 0.2370
 626/1500 [===========>..................] - ETA: 11:30 - loss: 1.4756 - regression_loss: 1.2380 - classification_loss: 0.2376
 627/1500 [===========>..................] - ETA: 11:29 - loss: 1.4744 - regression_loss: 1.2371 - classification_loss: 0.2374
 628/1500 [===========>..................] - ETA: 11:28 - loss: 1.4743 - regression_loss: 1.2369 - classification_loss: 0.2373
 629/1500 [===========>..................] - ETA: 11:26 - loss: 1.4730 - regression_loss: 1.2359 - classification_loss: 0.2371
 630/1500 [===========>..................] - ETA: 11:25 - loss: 1.4725 - regression_loss: 1.2352 - classification_loss: 0.2373
 631/1500 [===========>..................] - ETA: 11:27 - loss: 1.4728 - regression_loss: 1.2353 - classification_loss: 0.2375
 632/1500 [===========>..................] - ETA: 11:26 - loss: 1.4718 - regression_loss: 1.2346 - classification_loss: 0.2372
 633/1500 [===========>..................] - ETA: 11:25 - loss: 1.4709 - regression_loss: 1.2339 - classification_loss: 0.2370
 634/1500 [===========>..................] - ETA: 11:24 - loss: 1.4703 - regression_loss: 1.2334 - classification_loss: 0.2369
 635/1500 [===========>..................] - ETA: 11:24 - loss: 1.4697 - regression_loss: 1.2327 - classification_loss: 0.2370
 636/1500 [===========>..................] - ETA: 11:23 - loss: 1.4687 - regression_loss: 1.2319 - classification_loss: 0.2369
 637/1500 [===========>..................] - ETA: 11:24 - loss: 1.4686 - regression_loss: 1.2319 - classification_loss: 0.2367
 638/1500 [===========>..................] - ETA: 11:23 - loss: 1.4685 - regression_loss: 1.2319 - classification_loss: 0.2366
 639/1500 [===========>..................] - ETA: 11:21 - loss: 1.4675 - regression_loss: 1.2311 - classification_loss: 0.2364
 640/1500 [===========>..................] - ETA: 11:21 - loss: 1.4664 - regression_loss: 1.2302 - classification_loss: 0.2362
 641/1500 [===========>..................] - ETA: 11:20 - loss: 1.4662 - regression_loss: 1.2301 - classification_loss: 0.2360
 642/1500 [===========>..................] - ETA: 11:18 - loss: 1.4650 - regression_loss: 1.2291 - classification_loss: 0.2359
 643/1500 [===========>..................] - ETA: 11:18 - loss: 1.4657 - regression_loss: 1.2297 - classification_loss: 0.2359
 644/1500 [===========>..................] - ETA: 11:16 - loss: 1.4653 - regression_loss: 1.2295 - classification_loss: 0.2357
 645/1500 [===========>..................] - ETA: 11:15 - loss: 1.4657 - regression_loss: 1.2299 - classification_loss: 0.2358
 646/1500 [===========>..................] - ETA: 11:14 - loss: 1.4657 - regression_loss: 1.2300 - classification_loss: 0.2356
 647/1500 [===========>..................] - ETA: 11:13 - loss: 1.4674 - regression_loss: 1.2313 - classification_loss: 0.2361
 648/1500 [===========>..................] - ETA: 11:12 - loss: 1.4682 - regression_loss: 1.2319 - classification_loss: 0.2363
 649/1500 [===========>..................] - ETA: 11:11 - loss: 1.4682 - regression_loss: 1.2320 - classification_loss: 0.2362
 650/1500 [============>.................] - ETA: 11:10 - loss: 1.4667 - regression_loss: 1.2308 - classification_loss: 0.2359
 651/1500 [============>.................] - ETA: 11:09 - loss: 1.4668 - regression_loss: 1.2309 - classification_loss: 0.2358
 652/1500 [============>.................] - ETA: 11:09 - loss: 1.4658 - regression_loss: 1.2302 - classification_loss: 0.2356
 653/1500 [============>.................] - ETA: 11:08 - loss: 1.4665 - regression_loss: 1.2309 - classification_loss: 0.2356
 654/1500 [============>.................] - ETA: 11:07 - loss: 1.4668 - regression_loss: 1.2311 - classification_loss: 0.2356
 655/1500 [============>.................] - ETA: 11:06 - loss: 1.4681 - regression_loss: 1.2322 - classification_loss: 0.2360
 656/1500 [============>.................] - ETA: 11:05 - loss: 1.4681 - regression_loss: 1.2323 - classification_loss: 0.2358
 657/1500 [============>.................] - ETA: 11:03 - loss: 1.4670 - regression_loss: 1.2315 - classification_loss: 0.2356
 658/1500 [============>.................] - ETA: 11:02 - loss: 1.4655 - regression_loss: 1.2302 - classification_loss: 0.2353
 659/1500 [============>.................] - ETA: 11:01 - loss: 1.4669 - regression_loss: 1.2313 - classification_loss: 0.2356
 660/1500 [============>.................] - ETA: 11:01 - loss: 1.4668 - regression_loss: 1.2312 - classification_loss: 0.2356
 661/1500 [============>.................] - ETA: 11:00 - loss: 1.4657 - regression_loss: 1.2304 - classification_loss: 0.2353
 662/1500 [============>.................] - ETA: 10:59 - loss: 1.4657 - regression_loss: 1.2304 - classification_loss: 0.2352
 663/1500 [============>.................] - ETA: 10:58 - loss: 1.4650 - regression_loss: 1.2299 - classification_loss: 0.2352
 664/1500 [============>.................] - ETA: 10:57 - loss: 1.4644 - regression_loss: 1.2293 - classification_loss: 0.2351
 665/1500 [============>.................] - ETA: 10:55 - loss: 1.4647 - regression_loss: 1.2296 - classification_loss: 0.2351
 666/1500 [============>.................] - ETA: 10:55 - loss: 1.4647 - regression_loss: 1.2295 - classification_loss: 0.2352
 667/1500 [============>.................] - ETA: 10:55 - loss: 1.4644 - regression_loss: 1.2292 - classification_loss: 0.2352
 668/1500 [============>.................] - ETA: 10:54 - loss: 1.4638 - regression_loss: 1.2288 - classification_loss: 0.2350
 669/1500 [============>.................] - ETA: 10:55 - loss: 1.4650 - regression_loss: 1.2298 - classification_loss: 0.2352
 670/1500 [============>.................] - ETA: 10:53 - loss: 1.4638 - regression_loss: 1.2288 - classification_loss: 0.2350
 671/1500 [============>.................] - ETA: 10:52 - loss: 1.4637 - regression_loss: 1.2288 - classification_loss: 0.2349
 672/1500 [============>.................] - ETA: 10:51 - loss: 1.4627 - regression_loss: 1.2280 - classification_loss: 0.2347
 673/1500 [============>.................] - ETA: 10:50 - loss: 1.4622 - regression_loss: 1.2277 - classification_loss: 0.2346
 674/1500 [============>.................] - ETA: 10:49 - loss: 1.4618 - regression_loss: 1.2274 - classification_loss: 0.2344
 675/1500 [============>.................] - ETA: 10:48 - loss: 1.4616 - regression_loss: 1.2270 - classification_loss: 0.2347
 676/1500 [============>.................] - ETA: 10:47 - loss: 1.4617 - regression_loss: 1.2272 - classification_loss: 0.2345
 677/1500 [============>.................] - ETA: 10:47 - loss: 1.4623 - regression_loss: 1.2277 - classification_loss: 0.2346
 678/1500 [============>.................] - ETA: 10:46 - loss: 1.4612 - regression_loss: 1.2269 - classification_loss: 0.2343
 679/1500 [============>.................] - ETA: 10:46 - loss: 1.4607 - regression_loss: 1.2266 - classification_loss: 0.2341
 680/1500 [============>.................] - ETA: 10:44 - loss: 1.4605 - regression_loss: 1.2265 - classification_loss: 0.2340
 681/1500 [============>.................] - ETA: 10:43 - loss: 1.4619 - regression_loss: 1.2277 - classification_loss: 0.2342
 682/1500 [============>.................] - ETA: 10:42 - loss: 1.4610 - regression_loss: 1.2269 - classification_loss: 0.2341
 683/1500 [============>.................] - ETA: 10:40 - loss: 1.4609 - regression_loss: 1.2268 - classification_loss: 0.2341
 684/1500 [============>.................] - ETA: 10:39 - loss: 1.4602 - regression_loss: 1.2263 - classification_loss: 0.2339
 685/1500 [============>.................] - ETA: 10:39 - loss: 1.4590 - regression_loss: 1.2254 - classification_loss: 0.2336
 686/1500 [============>.................] - ETA: 10:38 - loss: 1.4589 - regression_loss: 1.2252 - classification_loss: 0.2337
 687/1500 [============>.................] - ETA: 10:37 - loss: 1.4599 - regression_loss: 1.2260 - classification_loss: 0.2339
 688/1500 [============>.................] - ETA: 10:36 - loss: 1.4600 - regression_loss: 1.2261 - classification_loss: 0.2339
 689/1500 [============>.................] - ETA: 10:35 - loss: 1.4612 - regression_loss: 1.2269 - classification_loss: 0.2342
 690/1500 [============>.................] - ETA: 10:34 - loss: 1.4608 - regression_loss: 1.2266 - classification_loss: 0.2341
 691/1500 [============>.................] - ETA: 10:33 - loss: 1.4596 - regression_loss: 1.2258 - classification_loss: 0.2339
 692/1500 [============>.................] - ETA: 10:32 - loss: 1.4601 - regression_loss: 1.2262 - classification_loss: 0.2339
 693/1500 [============>.................] - ETA: 10:33 - loss: 1.4609 - regression_loss: 1.2267 - classification_loss: 0.2343
 694/1500 [============>.................] - ETA: 10:33 - loss: 1.4618 - regression_loss: 1.2276 - classification_loss: 0.2342
 695/1500 [============>.................] - ETA: 10:32 - loss: 1.4609 - regression_loss: 1.2269 - classification_loss: 0.2340
 696/1500 [============>.................] - ETA: 10:31 - loss: 1.4611 - regression_loss: 1.2271 - classification_loss: 0.2340
 697/1500 [============>.................] - ETA: 10:30 - loss: 1.4603 - regression_loss: 1.2266 - classification_loss: 0.2337
 698/1500 [============>.................] - ETA: 10:29 - loss: 1.4600 - regression_loss: 1.2263 - classification_loss: 0.2337
 699/1500 [============>.................] - ETA: 10:28 - loss: 1.4603 - regression_loss: 1.2267 - classification_loss: 0.2336
 700/1500 [=============>................] - ETA: 10:26 - loss: 1.4593 - regression_loss: 1.2259 - classification_loss: 0.2334
 701/1500 [=============>................] - ETA: 10:26 - loss: 1.4582 - regression_loss: 1.2249 - classification_loss: 0.2333
 702/1500 [=============>................] - ETA: 10:25 - loss: 1.4573 - regression_loss: 1.2242 - classification_loss: 0.2331
 703/1500 [=============>................] - ETA: 10:25 - loss: 1.4563 - regression_loss: 1.2235 - classification_loss: 0.2328
 704/1500 [=============>................] - ETA: 10:23 - loss: 1.4559 - regression_loss: 1.2232 - classification_loss: 0.2327
 705/1500 [=============>................] - ETA: 10:23 - loss: 1.4575 - regression_loss: 1.2247 - classification_loss: 0.2329
 706/1500 [=============>................] - ETA: 10:22 - loss: 1.4571 - regression_loss: 1.2242 - classification_loss: 0.2329
 707/1500 [=============>................] - ETA: 10:21 - loss: 1.4572 - regression_loss: 1.2243 - classification_loss: 0.2330
 708/1500 [=============>................] - ETA: 10:21 - loss: 1.4587 - regression_loss: 1.2257 - classification_loss: 0.2330
 709/1500 [=============>................] - ETA: 10:23 - loss: 1.4598 - regression_loss: 1.2266 - classification_loss: 0.2332
 710/1500 [=============>................] - ETA: 10:23 - loss: 1.4592 - regression_loss: 1.2262 - classification_loss: 0.2331
 711/1500 [=============>................] - ETA: 10:21 - loss: 1.4585 - regression_loss: 1.2257 - classification_loss: 0.2329
 712/1500 [=============>................] - ETA: 10:22 - loss: 1.4599 - regression_loss: 1.2266 - classification_loss: 0.2333
 713/1500 [=============>................] - ETA: 10:22 - loss: 1.4618 - regression_loss: 1.2280 - classification_loss: 0.2338
 714/1500 [=============>................] - ETA: 10:22 - loss: 1.4616 - regression_loss: 1.2277 - classification_loss: 0.2339
 715/1500 [=============>................] - ETA: 10:21 - loss: 1.4630 - regression_loss: 1.2288 - classification_loss: 0.2343
 716/1500 [=============>................] - ETA: 10:20 - loss: 1.4621 - regression_loss: 1.2281 - classification_loss: 0.2340
 717/1500 [=============>................] - ETA: 10:19 - loss: 1.4614 - regression_loss: 1.2275 - classification_loss: 0.2339
 718/1500 [=============>................] - ETA: 10:18 - loss: 1.4627 - regression_loss: 1.2275 - classification_loss: 0.2351
 719/1500 [=============>................] - ETA: 10:18 - loss: 1.4623 - regression_loss: 1.2271 - classification_loss: 0.2352
 720/1500 [=============>................] - ETA: 10:17 - loss: 1.4614 - regression_loss: 1.2264 - classification_loss: 0.2350
 721/1500 [=============>................] - ETA: 10:16 - loss: 1.4606 - regression_loss: 1.2258 - classification_loss: 0.2348
 722/1500 [=============>................] - ETA: 10:15 - loss: 1.4617 - regression_loss: 1.2270 - classification_loss: 0.2347
 723/1500 [=============>................] - ETA: 10:14 - loss: 1.4621 - regression_loss: 1.2273 - classification_loss: 0.2348
 724/1500 [=============>................] - ETA: 10:13 - loss: 1.4622 - regression_loss: 1.2273 - classification_loss: 0.2348
 725/1500 [=============>................] - ETA: 10:12 - loss: 1.4614 - regression_loss: 1.2266 - classification_loss: 0.2348
 726/1500 [=============>................] - ETA: 10:12 - loss: 1.4607 - regression_loss: 1.2261 - classification_loss: 0.2347
 727/1500 [=============>................] - ETA: 10:11 - loss: 1.4601 - regression_loss: 1.2255 - classification_loss: 0.2346
 728/1500 [=============>................] - ETA: 10:10 - loss: 1.4610 - regression_loss: 1.2263 - classification_loss: 0.2347
 729/1500 [=============>................] - ETA: 10:11 - loss: 1.4625 - regression_loss: 1.2273 - classification_loss: 0.2351
 730/1500 [=============>................] - ETA: 10:10 - loss: 1.4631 - regression_loss: 1.2278 - classification_loss: 0.2354
 731/1500 [=============>................] - ETA: 10:09 - loss: 1.4621 - regression_loss: 1.2269 - classification_loss: 0.2352
 732/1500 [=============>................] - ETA: 10:08 - loss: 1.4614 - regression_loss: 1.2263 - classification_loss: 0.2351
 733/1500 [=============>................] - ETA: 10:07 - loss: 1.4609 - regression_loss: 1.2259 - classification_loss: 0.2350
 734/1500 [=============>................] - ETA: 10:05 - loss: 1.4603 - regression_loss: 1.2255 - classification_loss: 0.2348
 735/1500 [=============>................] - ETA: 10:04 - loss: 1.4598 - regression_loss: 1.2252 - classification_loss: 0.2346
 736/1500 [=============>................] - ETA: 10:03 - loss: 1.4593 - regression_loss: 1.2249 - classification_loss: 0.2344
 737/1500 [=============>................] - ETA: 10:02 - loss: 1.4597 - regression_loss: 1.2251 - classification_loss: 0.2345
 738/1500 [=============>................] - ETA: 10:01 - loss: 1.4606 - regression_loss: 1.2260 - classification_loss: 0.2345
 739/1500 [=============>................] - ETA: 10:00 - loss: 1.4602 - regression_loss: 1.2258 - classification_loss: 0.2344
 740/1500 [=============>................] - ETA: 9:59 - loss: 1.4597 - regression_loss: 1.2256 - classification_loss: 0.2342 
 741/1500 [=============>................] - ETA: 9:59 - loss: 1.4598 - regression_loss: 1.2256 - classification_loss: 0.2342
 742/1500 [=============>................] - ETA: 9:58 - loss: 1.4612 - regression_loss: 1.2269 - classification_loss: 0.2343
 743/1500 [=============>................] - ETA: 9:57 - loss: 1.4600 - regression_loss: 1.2259 - classification_loss: 0.2341
 744/1500 [=============>................] - ETA: 9:56 - loss: 1.4594 - regression_loss: 1.2254 - classification_loss: 0.2339
 745/1500 [=============>................] - ETA: 9:55 - loss: 1.4590 - regression_loss: 1.2253 - classification_loss: 0.2337
 746/1500 [=============>................] - ETA: 9:54 - loss: 1.4586 - regression_loss: 1.2249 - classification_loss: 0.2336
 747/1500 [=============>................] - ETA: 9:54 - loss: 1.4580 - regression_loss: 1.2245 - classification_loss: 0.2335
 748/1500 [=============>................] - ETA: 9:53 - loss: 1.4572 - regression_loss: 1.2238 - classification_loss: 0.2333
 749/1500 [=============>................] - ETA: 9:52 - loss: 1.4588 - regression_loss: 1.2250 - classification_loss: 0.2338
 750/1500 [==============>...............] - ETA: 9:51 - loss: 1.4588 - regression_loss: 1.2251 - classification_loss: 0.2337
 751/1500 [==============>...............] - ETA: 9:52 - loss: 1.4599 - regression_loss: 1.2260 - classification_loss: 0.2339
 752/1500 [==============>...............] - ETA: 9:51 - loss: 1.4590 - regression_loss: 1.2252 - classification_loss: 0.2338
 753/1500 [==============>...............] - ETA: 9:52 - loss: 1.4601 - regression_loss: 1.2260 - classification_loss: 0.2341
 754/1500 [==============>...............] - ETA: 9:51 - loss: 1.4601 - regression_loss: 1.2261 - classification_loss: 0.2340
 755/1500 [==============>...............] - ETA: 9:50 - loss: 1.4591 - regression_loss: 1.2245 - classification_loss: 0.2346
 756/1500 [==============>...............] - ETA: 9:49 - loss: 1.4590 - regression_loss: 1.2244 - classification_loss: 0.2346
 757/1500 [==============>...............] - ETA: 9:48 - loss: 1.4592 - regression_loss: 1.2247 - classification_loss: 0.2345
 758/1500 [==============>...............] - ETA: 9:48 - loss: 1.4588 - regression_loss: 1.2244 - classification_loss: 0.2344
 759/1500 [==============>...............] - ETA: 9:48 - loss: 1.4585 - regression_loss: 1.2243 - classification_loss: 0.2342
 760/1500 [==============>...............] - ETA: 9:47 - loss: 1.4589 - regression_loss: 1.2246 - classification_loss: 0.2343
 761/1500 [==============>...............] - ETA: 9:46 - loss: 1.4586 - regression_loss: 1.2244 - classification_loss: 0.2341
 762/1500 [==============>...............] - ETA: 9:45 - loss: 1.4589 - regression_loss: 1.2248 - classification_loss: 0.2341
 763/1500 [==============>...............] - ETA: 9:45 - loss: 1.4587 - regression_loss: 1.2245 - classification_loss: 0.2342
 764/1500 [==============>...............] - ETA: 9:44 - loss: 1.4581 - regression_loss: 1.2241 - classification_loss: 0.2340
 765/1500 [==============>...............] - ETA: 9:43 - loss: 1.4584 - regression_loss: 1.2243 - classification_loss: 0.2340
 766/1500 [==============>...............] - ETA: 9:43 - loss: 1.4575 - regression_loss: 1.2236 - classification_loss: 0.2339
 767/1500 [==============>...............] - ETA: 9:43 - loss: 1.4568 - regression_loss: 1.2231 - classification_loss: 0.2338
 768/1500 [==============>...............] - ETA: 9:42 - loss: 1.4573 - regression_loss: 1.2236 - classification_loss: 0.2337
 769/1500 [==============>...............] - ETA: 9:41 - loss: 1.4589 - regression_loss: 1.2247 - classification_loss: 0.2342
 770/1500 [==============>...............] - ETA: 9:40 - loss: 1.4591 - regression_loss: 1.2249 - classification_loss: 0.2342
 771/1500 [==============>...............] - ETA: 9:39 - loss: 1.4585 - regression_loss: 1.2245 - classification_loss: 0.2340
 772/1500 [==============>...............] - ETA: 9:38 - loss: 1.4584 - regression_loss: 1.2245 - classification_loss: 0.2339
 773/1500 [==============>...............] - ETA: 9:37 - loss: 1.4575 - regression_loss: 1.2238 - classification_loss: 0.2337
 774/1500 [==============>...............] - ETA: 9:36 - loss: 1.4580 - regression_loss: 1.2242 - classification_loss: 0.2338
 775/1500 [==============>...............] - ETA: 9:35 - loss: 1.4573 - regression_loss: 1.2237 - classification_loss: 0.2336
 776/1500 [==============>...............] - ETA: 9:35 - loss: 1.4584 - regression_loss: 1.2247 - classification_loss: 0.2337
 777/1500 [==============>...............] - ETA: 9:34 - loss: 1.4572 - regression_loss: 1.2238 - classification_loss: 0.2334
 778/1500 [==============>...............] - ETA: 9:33 - loss: 1.4565 - regression_loss: 1.2233 - classification_loss: 0.2332
 779/1500 [==============>...............] - ETA: 9:31 - loss: 1.4558 - regression_loss: 1.2227 - classification_loss: 0.2331
 780/1500 [==============>...............] - ETA: 9:30 - loss: 1.4570 - regression_loss: 1.2237 - classification_loss: 0.2333
 781/1500 [==============>...............] - ETA: 9:29 - loss: 1.4557 - regression_loss: 1.2227 - classification_loss: 0.2331
 782/1500 [==============>...............] - ETA: 9:29 - loss: 1.4549 - regression_loss: 1.2220 - classification_loss: 0.2329
 783/1500 [==============>...............] - ETA: 9:28 - loss: 1.4540 - regression_loss: 1.2214 - classification_loss: 0.2327
 784/1500 [==============>...............] - ETA: 9:28 - loss: 1.4544 - regression_loss: 1.2216 - classification_loss: 0.2329
 785/1500 [==============>...............] - ETA: 9:27 - loss: 1.4543 - regression_loss: 1.2216 - classification_loss: 0.2327
 786/1500 [==============>...............] - ETA: 9:26 - loss: 1.4539 - regression_loss: 1.2212 - classification_loss: 0.2326
 787/1500 [==============>...............] - ETA: 9:26 - loss: 1.4557 - regression_loss: 1.2227 - classification_loss: 0.2331
 788/1500 [==============>...............] - ETA: 9:25 - loss: 1.4554 - regression_loss: 1.2225 - classification_loss: 0.2329
 789/1500 [==============>...............] - ETA: 9:24 - loss: 1.4548 - regression_loss: 1.2222 - classification_loss: 0.2327
 790/1500 [==============>...............] - ETA: 9:23 - loss: 1.4545 - regression_loss: 1.2220 - classification_loss: 0.2326
 791/1500 [==============>...............] - ETA: 9:22 - loss: 1.4549 - regression_loss: 1.2224 - classification_loss: 0.2325
 792/1500 [==============>...............] - ETA: 9:21 - loss: 1.4543 - regression_loss: 1.2220 - classification_loss: 0.2324
 793/1500 [==============>...............] - ETA: 9:20 - loss: 1.4537 - regression_loss: 1.2215 - classification_loss: 0.2322
 794/1500 [==============>...............] - ETA: 9:20 - loss: 1.4548 - regression_loss: 1.2224 - classification_loss: 0.2324
 795/1500 [==============>...............] - ETA: 9:19 - loss: 1.4550 - regression_loss: 1.2223 - classification_loss: 0.2327
 796/1500 [==============>...............] - ETA: 9:18 - loss: 1.4554 - regression_loss: 1.2226 - classification_loss: 0.2328
 797/1500 [==============>...............] - ETA: 9:17 - loss: 1.4553 - regression_loss: 1.2227 - classification_loss: 0.2326
 798/1500 [==============>...............] - ETA: 9:17 - loss: 1.4559 - regression_loss: 1.2230 - classification_loss: 0.2329
 799/1500 [==============>...............] - ETA: 9:16 - loss: 1.4550 - regression_loss: 1.2223 - classification_loss: 0.2327
 800/1500 [===============>..............] - ETA: 9:16 - loss: 1.4548 - regression_loss: 1.2221 - classification_loss: 0.2327
 801/1500 [===============>..............] - ETA: 9:14 - loss: 1.4550 - regression_loss: 1.2221 - classification_loss: 0.2328
 802/1500 [===============>..............] - ETA: 9:13 - loss: 1.4545 - regression_loss: 1.2218 - classification_loss: 0.2327
 803/1500 [===============>..............] - ETA: 9:13 - loss: 1.4536 - regression_loss: 1.2210 - classification_loss: 0.2326
 804/1500 [===============>..............] - ETA: 9:12 - loss: 1.4529 - regression_loss: 1.2203 - classification_loss: 0.2326
 805/1500 [===============>..............] - ETA: 9:11 - loss: 1.4523 - regression_loss: 1.2199 - classification_loss: 0.2325
 806/1500 [===============>..............] - ETA: 9:10 - loss: 1.4520 - regression_loss: 1.2197 - classification_loss: 0.2323
 807/1500 [===============>..............] - ETA: 9:09 - loss: 1.4522 - regression_loss: 1.2198 - classification_loss: 0.2324
 808/1500 [===============>..............] - ETA: 9:09 - loss: 1.4540 - regression_loss: 1.2210 - classification_loss: 0.2329
 809/1500 [===============>..............] - ETA: 9:08 - loss: 1.4541 - regression_loss: 1.2211 - classification_loss: 0.2330
 810/1500 [===============>..............] - ETA: 9:07 - loss: 1.4545 - regression_loss: 1.2214 - classification_loss: 0.2331
 811/1500 [===============>..............] - ETA: 9:06 - loss: 1.4538 - regression_loss: 1.2209 - classification_loss: 0.2329
 812/1500 [===============>..............] - ETA: 9:05 - loss: 1.4541 - regression_loss: 1.2213 - classification_loss: 0.2328
 813/1500 [===============>..............] - ETA: 9:04 - loss: 1.4543 - regression_loss: 1.2216 - classification_loss: 0.2327
 814/1500 [===============>..............] - ETA: 9:04 - loss: 1.4550 - regression_loss: 1.2222 - classification_loss: 0.2328
 815/1500 [===============>..............] - ETA: 9:03 - loss: 1.4544 - regression_loss: 1.2218 - classification_loss: 0.2326
 816/1500 [===============>..............] - ETA: 9:02 - loss: 1.4545 - regression_loss: 1.2218 - classification_loss: 0.2327
 817/1500 [===============>..............] - ETA: 9:01 - loss: 1.4540 - regression_loss: 1.2215 - classification_loss: 0.2325
 818/1500 [===============>..............] - ETA: 9:00 - loss: 1.4545 - regression_loss: 1.2220 - classification_loss: 0.2324
 819/1500 [===============>..............] - ETA: 8:59 - loss: 1.4544 - regression_loss: 1.2221 - classification_loss: 0.2323
 820/1500 [===============>..............] - ETA: 8:58 - loss: 1.4547 - regression_loss: 1.2223 - classification_loss: 0.2324
 821/1500 [===============>..............] - ETA: 8:58 - loss: 1.4540 - regression_loss: 1.2217 - classification_loss: 0.2323
 822/1500 [===============>..............] - ETA: 8:57 - loss: 1.4558 - regression_loss: 1.2231 - classification_loss: 0.2327
 823/1500 [===============>..............] - ETA: 8:57 - loss: 1.4558 - regression_loss: 1.2231 - classification_loss: 0.2327
 824/1500 [===============>..............] - ETA: 8:56 - loss: 1.4563 - regression_loss: 1.2233 - classification_loss: 0.2330
 825/1500 [===============>..............] - ETA: 8:55 - loss: 1.4559 - regression_loss: 1.2230 - classification_loss: 0.2329
 826/1500 [===============>..............] - ETA: 8:54 - loss: 1.4570 - regression_loss: 1.2241 - classification_loss: 0.2329
 827/1500 [===============>..............] - ETA: 8:53 - loss: 1.4583 - regression_loss: 1.2252 - classification_loss: 0.2331
 828/1500 [===============>..............] - ETA: 8:52 - loss: 1.4582 - regression_loss: 1.2252 - classification_loss: 0.2330
 829/1500 [===============>..............] - ETA: 8:51 - loss: 1.4574 - regression_loss: 1.2245 - classification_loss: 0.2329
 830/1500 [===============>..............] - ETA: 8:50 - loss: 1.4564 - regression_loss: 1.2237 - classification_loss: 0.2327
 831/1500 [===============>..............] - ETA: 8:48 - loss: 1.4573 - regression_loss: 1.2244 - classification_loss: 0.2329
 832/1500 [===============>..............] - ETA: 8:47 - loss: 1.4576 - regression_loss: 1.2246 - classification_loss: 0.2330
 833/1500 [===============>..............] - ETA: 8:47 - loss: 1.4588 - regression_loss: 1.2255 - classification_loss: 0.2333
 834/1500 [===============>..............] - ETA: 8:46 - loss: 1.4594 - regression_loss: 1.2260 - classification_loss: 0.2334
 835/1500 [===============>..............] - ETA: 8:45 - loss: 1.4599 - regression_loss: 1.2262 - classification_loss: 0.2337
 836/1500 [===============>..............] - ETA: 8:44 - loss: 1.4605 - regression_loss: 1.2268 - classification_loss: 0.2337
 837/1500 [===============>..............] - ETA: 8:43 - loss: 1.4600 - regression_loss: 1.2264 - classification_loss: 0.2336
 838/1500 [===============>..............] - ETA: 8:42 - loss: 1.4603 - regression_loss: 1.2267 - classification_loss: 0.2336
 839/1500 [===============>..............] - ETA: 8:41 - loss: 1.4613 - regression_loss: 1.2276 - classification_loss: 0.2337
 840/1500 [===============>..............] - ETA: 8:40 - loss: 1.4618 - regression_loss: 1.2282 - classification_loss: 0.2337
 841/1500 [===============>..............] - ETA: 8:39 - loss: 1.4609 - regression_loss: 1.2274 - classification_loss: 0.2335
 842/1500 [===============>..............] - ETA: 8:38 - loss: 1.4601 - regression_loss: 1.2267 - classification_loss: 0.2334
 843/1500 [===============>..............] - ETA: 8:37 - loss: 1.4609 - regression_loss: 1.2275 - classification_loss: 0.2335
 844/1500 [===============>..............] - ETA: 8:37 - loss: 1.4612 - regression_loss: 1.2274 - classification_loss: 0.2338
 845/1500 [===============>..............] - ETA: 8:36 - loss: 1.4624 - regression_loss: 1.2284 - classification_loss: 0.2340
 846/1500 [===============>..............] - ETA: 8:36 - loss: 1.4617 - regression_loss: 1.2278 - classification_loss: 0.2339
 847/1500 [===============>..............] - ETA: 8:36 - loss: 1.4616 - regression_loss: 1.2276 - classification_loss: 0.2340
 848/1500 [===============>..............] - ETA: 8:35 - loss: 1.4623 - regression_loss: 1.2283 - classification_loss: 0.2340
 849/1500 [===============>..............] - ETA: 8:33 - loss: 1.4611 - regression_loss: 1.2273 - classification_loss: 0.2338
 850/1500 [================>.............] - ETA: 8:33 - loss: 1.4606 - regression_loss: 1.2269 - classification_loss: 0.2337
 851/1500 [================>.............] - ETA: 8:32 - loss: 1.4603 - regression_loss: 1.2267 - classification_loss: 0.2336
 852/1500 [================>.............] - ETA: 8:31 - loss: 1.4607 - regression_loss: 1.2270 - classification_loss: 0.2337
 853/1500 [================>.............] - ETA: 8:30 - loss: 1.4606 - regression_loss: 1.2269 - classification_loss: 0.2337
 854/1500 [================>.............] - ETA: 8:29 - loss: 1.4611 - regression_loss: 1.2266 - classification_loss: 0.2345
 855/1500 [================>.............] - ETA: 8:28 - loss: 1.4601 - regression_loss: 1.2256 - classification_loss: 0.2344
 856/1500 [================>.............] - ETA: 8:27 - loss: 1.4600 - regression_loss: 1.2255 - classification_loss: 0.2346
 857/1500 [================>.............] - ETA: 8:26 - loss: 1.4591 - regression_loss: 1.2247 - classification_loss: 0.2344
 858/1500 [================>.............] - ETA: 8:25 - loss: 1.4598 - regression_loss: 1.2252 - classification_loss: 0.2346
 859/1500 [================>.............] - ETA: 8:25 - loss: 1.4593 - regression_loss: 1.2249 - classification_loss: 0.2344
 860/1500 [================>.............] - ETA: 8:24 - loss: 1.4589 - regression_loss: 1.2247 - classification_loss: 0.2342
 861/1500 [================>.............] - ETA: 8:23 - loss: 1.4596 - regression_loss: 1.2252 - classification_loss: 0.2344
 862/1500 [================>.............] - ETA: 8:22 - loss: 1.4603 - regression_loss: 1.2257 - classification_loss: 0.2346
 863/1500 [================>.............] - ETA: 8:22 - loss: 1.4604 - regression_loss: 1.2258 - classification_loss: 0.2345
 864/1500 [================>.............] - ETA: 8:21 - loss: 1.4600 - regression_loss: 1.2255 - classification_loss: 0.2344
 865/1500 [================>.............] - ETA: 8:20 - loss: 1.4599 - regression_loss: 1.2254 - classification_loss: 0.2345
 866/1500 [================>.............] - ETA: 8:19 - loss: 1.4611 - regression_loss: 1.2264 - classification_loss: 0.2347
 867/1500 [================>.............] - ETA: 8:18 - loss: 1.4622 - regression_loss: 1.2274 - classification_loss: 0.2349
 868/1500 [================>.............] - ETA: 8:17 - loss: 1.4622 - regression_loss: 1.2273 - classification_loss: 0.2349
 869/1500 [================>.............] - ETA: 8:16 - loss: 1.4627 - regression_loss: 1.2275 - classification_loss: 0.2353
 870/1500 [================>.............] - ETA: 8:15 - loss: 1.4628 - regression_loss: 1.2275 - classification_loss: 0.2353
 871/1500 [================>.............] - ETA: 8:15 - loss: 1.4634 - regression_loss: 1.2281 - classification_loss: 0.2353
 872/1500 [================>.............] - ETA: 8:14 - loss: 1.4629 - regression_loss: 1.2278 - classification_loss: 0.2351
 873/1500 [================>.............] - ETA: 8:13 - loss: 1.4622 - regression_loss: 1.2273 - classification_loss: 0.2350
 874/1500 [================>.............] - ETA: 8:12 - loss: 1.4622 - regression_loss: 1.2273 - classification_loss: 0.2348
 875/1500 [================>.............] - ETA: 8:11 - loss: 1.4620 - regression_loss: 1.2273 - classification_loss: 0.2347
 876/1500 [================>.............] - ETA: 8:10 - loss: 1.4635 - regression_loss: 1.2285 - classification_loss: 0.2350
 877/1500 [================>.............] - ETA: 8:09 - loss: 1.4630 - regression_loss: 1.2280 - classification_loss: 0.2349
 878/1500 [================>.............] - ETA: 8:08 - loss: 1.4625 - regression_loss: 1.2276 - classification_loss: 0.2349
 879/1500 [================>.............] - ETA: 8:08 - loss: 1.4623 - regression_loss: 1.2274 - classification_loss: 0.2349
 880/1500 [================>.............] - ETA: 8:07 - loss: 1.4616 - regression_loss: 1.2268 - classification_loss: 0.2348
 881/1500 [================>.............] - ETA: 8:06 - loss: 1.4611 - regression_loss: 1.2262 - classification_loss: 0.2348
 882/1500 [================>.............] - ETA: 8:05 - loss: 1.4616 - regression_loss: 1.2266 - classification_loss: 0.2350
 883/1500 [================>.............] - ETA: 8:04 - loss: 1.4623 - regression_loss: 1.2273 - classification_loss: 0.2350
 884/1500 [================>.............] - ETA: 8:03 - loss: 1.4640 - regression_loss: 1.2286 - classification_loss: 0.2354
 885/1500 [================>.............] - ETA: 8:03 - loss: 1.4650 - regression_loss: 1.2293 - classification_loss: 0.2357
 886/1500 [================>.............] - ETA: 8:02 - loss: 1.4660 - regression_loss: 1.2301 - classification_loss: 0.2359
 887/1500 [================>.............] - ETA: 8:01 - loss: 1.4675 - regression_loss: 1.2311 - classification_loss: 0.2364
 888/1500 [================>.............] - ETA: 8:01 - loss: 1.4666 - regression_loss: 1.2304 - classification_loss: 0.2362
 889/1500 [================>.............] - ETA: 8:00 - loss: 1.4665 - regression_loss: 1.2304 - classification_loss: 0.2361
 890/1500 [================>.............] - ETA: 7:59 - loss: 1.4659 - regression_loss: 1.2300 - classification_loss: 0.2359
 891/1500 [================>.............] - ETA: 7:59 - loss: 1.4654 - regression_loss: 1.2296 - classification_loss: 0.2358
 892/1500 [================>.............] - ETA: 7:58 - loss: 1.4644 - regression_loss: 1.2288 - classification_loss: 0.2356
 893/1500 [================>.............] - ETA: 7:57 - loss: 1.4635 - regression_loss: 1.2281 - classification_loss: 0.2355
 894/1500 [================>.............] - ETA: 7:56 - loss: 1.4634 - regression_loss: 1.2279 - classification_loss: 0.2354
 895/1500 [================>.............] - ETA: 7:55 - loss: 1.4630 - regression_loss: 1.2277 - classification_loss: 0.2353
 896/1500 [================>.............] - ETA: 7:54 - loss: 1.4618 - regression_loss: 1.2267 - classification_loss: 0.2351
 897/1500 [================>.............] - ETA: 7:53 - loss: 1.4621 - regression_loss: 1.2271 - classification_loss: 0.2350
 898/1500 [================>.............] - ETA: 7:53 - loss: 1.4632 - regression_loss: 1.2280 - classification_loss: 0.2352
 899/1500 [================>.............] - ETA: 7:52 - loss: 1.4627 - regression_loss: 1.2276 - classification_loss: 0.2351
 900/1500 [=================>............] - ETA: 7:51 - loss: 1.4620 - regression_loss: 1.2271 - classification_loss: 0.2349
 901/1500 [=================>............] - ETA: 7:51 - loss: 1.4615 - regression_loss: 1.2267 - classification_loss: 0.2348
 902/1500 [=================>............] - ETA: 7:50 - loss: 1.4618 - regression_loss: 1.2271 - classification_loss: 0.2348
 903/1500 [=================>............] - ETA: 7:49 - loss: 1.4612 - regression_loss: 1.2266 - classification_loss: 0.2346
 904/1500 [=================>............] - ETA: 7:48 - loss: 1.4605 - regression_loss: 1.2259 - classification_loss: 0.2345
 905/1500 [=================>............] - ETA: 7:48 - loss: 1.4604 - regression_loss: 1.2258 - classification_loss: 0.2347
 906/1500 [=================>............] - ETA: 7:47 - loss: 1.4597 - regression_loss: 1.2251 - classification_loss: 0.2345
 907/1500 [=================>............] - ETA: 7:46 - loss: 1.4596 - regression_loss: 1.2251 - classification_loss: 0.2344
 908/1500 [=================>............] - ETA: 7:45 - loss: 1.4596 - regression_loss: 1.2251 - classification_loss: 0.2344
 909/1500 [=================>............] - ETA: 7:44 - loss: 1.4591 - regression_loss: 1.2245 - classification_loss: 0.2346
 910/1500 [=================>............] - ETA: 7:42 - loss: 1.4582 - regression_loss: 1.2238 - classification_loss: 0.2344
 911/1500 [=================>............] - ETA: 7:42 - loss: 1.4577 - regression_loss: 1.2234 - classification_loss: 0.2343
 912/1500 [=================>............] - ETA: 7:41 - loss: 1.4575 - regression_loss: 1.2234 - classification_loss: 0.2342
 913/1500 [=================>............] - ETA: 7:40 - loss: 1.4574 - regression_loss: 1.2234 - classification_loss: 0.2341
 914/1500 [=================>............] - ETA: 7:39 - loss: 1.4567 - regression_loss: 1.2229 - classification_loss: 0.2339
 915/1500 [=================>............] - ETA: 7:38 - loss: 1.4568 - regression_loss: 1.2229 - classification_loss: 0.2339
 916/1500 [=================>............] - ETA: 7:37 - loss: 1.4565 - regression_loss: 1.2228 - classification_loss: 0.2337
 917/1500 [=================>............] - ETA: 7:36 - loss: 1.4557 - regression_loss: 1.2221 - classification_loss: 0.2335
 918/1500 [=================>............] - ETA: 7:35 - loss: 1.4554 - regression_loss: 1.2220 - classification_loss: 0.2335
 919/1500 [=================>............] - ETA: 7:35 - loss: 1.4562 - regression_loss: 1.2224 - classification_loss: 0.2338
 920/1500 [=================>............] - ETA: 7:34 - loss: 1.4553 - regression_loss: 1.2217 - classification_loss: 0.2336
 921/1500 [=================>............] - ETA: 7:33 - loss: 1.4549 - regression_loss: 1.2214 - classification_loss: 0.2335
 922/1500 [=================>............] - ETA: 7:32 - loss: 1.4555 - regression_loss: 1.2221 - classification_loss: 0.2334
 923/1500 [=================>............] - ETA: 7:32 - loss: 1.4558 - regression_loss: 1.2225 - classification_loss: 0.2333
 924/1500 [=================>............] - ETA: 7:31 - loss: 1.4554 - regression_loss: 1.2221 - classification_loss: 0.2333
 925/1500 [=================>............] - ETA: 7:30 - loss: 1.4554 - regression_loss: 1.2220 - classification_loss: 0.2333
 926/1500 [=================>............] - ETA: 7:29 - loss: 1.4550 - regression_loss: 1.2218 - classification_loss: 0.2332
 927/1500 [=================>............] - ETA: 7:28 - loss: 1.4557 - regression_loss: 1.2223 - classification_loss: 0.2334
 928/1500 [=================>............] - ETA: 7:27 - loss: 1.4553 - regression_loss: 1.2220 - classification_loss: 0.2333
 929/1500 [=================>............] - ETA: 7:26 - loss: 1.4548 - regression_loss: 1.2216 - classification_loss: 0.2332
 930/1500 [=================>............] - ETA: 7:26 - loss: 1.4542 - regression_loss: 1.2210 - classification_loss: 0.2332
 931/1500 [=================>............] - ETA: 7:25 - loss: 1.4538 - regression_loss: 1.2208 - classification_loss: 0.2331
 932/1500 [=================>............] - ETA: 7:24 - loss: 1.4532 - regression_loss: 1.2202 - classification_loss: 0.2330
 933/1500 [=================>............] - ETA: 7:23 - loss: 1.4537 - regression_loss: 1.2207 - classification_loss: 0.2330
 934/1500 [=================>............] - ETA: 7:22 - loss: 1.4539 - regression_loss: 1.2208 - classification_loss: 0.2331
 935/1500 [=================>............] - ETA: 7:21 - loss: 1.4543 - regression_loss: 1.2213 - classification_loss: 0.2331
 936/1500 [=================>............] - ETA: 7:20 - loss: 1.4540 - regression_loss: 1.2210 - classification_loss: 0.2329
 937/1500 [=================>............] - ETA: 7:19 - loss: 1.4538 - regression_loss: 1.2210 - classification_loss: 0.2329
 938/1500 [=================>............] - ETA: 7:19 - loss: 1.4534 - regression_loss: 1.2206 - classification_loss: 0.2327
 939/1500 [=================>............] - ETA: 7:17 - loss: 1.4528 - regression_loss: 1.2201 - classification_loss: 0.2327
 940/1500 [=================>............] - ETA: 7:17 - loss: 1.4528 - regression_loss: 1.2199 - classification_loss: 0.2330
 941/1500 [=================>............] - ETA: 7:16 - loss: 1.4525 - regression_loss: 1.2197 - classification_loss: 0.2328
 942/1500 [=================>............] - ETA: 7:16 - loss: 1.4535 - regression_loss: 1.2203 - classification_loss: 0.2332
 943/1500 [=================>............] - ETA: 7:15 - loss: 1.4540 - regression_loss: 1.2206 - classification_loss: 0.2334
 944/1500 [=================>............] - ETA: 7:14 - loss: 1.4530 - regression_loss: 1.2199 - classification_loss: 0.2332
 945/1500 [=================>............] - ETA: 7:13 - loss: 1.4524 - regression_loss: 1.2193 - classification_loss: 0.2331
 946/1500 [=================>............] - ETA: 7:13 - loss: 1.4520 - regression_loss: 1.2190 - classification_loss: 0.2330
 947/1500 [=================>............] - ETA: 7:12 - loss: 1.4534 - regression_loss: 1.2199 - classification_loss: 0.2335
 948/1500 [=================>............] - ETA: 7:11 - loss: 1.4524 - regression_loss: 1.2191 - classification_loss: 0.2333
 949/1500 [=================>............] - ETA: 7:10 - loss: 1.4515 - regression_loss: 1.2183 - classification_loss: 0.2332
 950/1500 [==================>...........] - ETA: 7:09 - loss: 1.4510 - regression_loss: 1.2179 - classification_loss: 0.2331
 951/1500 [==================>...........] - ETA: 7:09 - loss: 1.4505 - regression_loss: 1.2175 - classification_loss: 0.2330
 952/1500 [==================>...........] - ETA: 7:08 - loss: 1.4504 - regression_loss: 1.2175 - classification_loss: 0.2329
 953/1500 [==================>...........] - ETA: 7:09 - loss: 1.4492 - regression_loss: 1.2165 - classification_loss: 0.2327
 954/1500 [==================>...........] - ETA: 7:08 - loss: 1.4487 - regression_loss: 1.2161 - classification_loss: 0.2326
 955/1500 [==================>...........] - ETA: 7:07 - loss: 1.4484 - regression_loss: 1.2160 - classification_loss: 0.2325
 956/1500 [==================>...........] - ETA: 7:06 - loss: 1.4475 - regression_loss: 1.2153 - classification_loss: 0.2323
 957/1500 [==================>...........] - ETA: 7:05 - loss: 1.4486 - regression_loss: 1.2150 - classification_loss: 0.2336
 958/1500 [==================>...........] - ETA: 7:05 - loss: 1.4478 - regression_loss: 1.2144 - classification_loss: 0.2334
 959/1500 [==================>...........] - ETA: 7:04 - loss: 1.4470 - regression_loss: 1.2137 - classification_loss: 0.2332
 960/1500 [==================>...........] - ETA: 7:03 - loss: 1.4460 - regression_loss: 1.2129 - classification_loss: 0.2331
 961/1500 [==================>...........] - ETA: 7:02 - loss: 1.4457 - regression_loss: 1.2127 - classification_loss: 0.2329
 962/1500 [==================>...........] - ETA: 7:01 - loss: 1.4463 - regression_loss: 1.2131 - classification_loss: 0.2331
 963/1500 [==================>...........] - ETA: 7:00 - loss: 1.4461 - regression_loss: 1.2130 - classification_loss: 0.2330
 964/1500 [==================>...........] - ETA: 6:59 - loss: 1.4468 - regression_loss: 1.2136 - classification_loss: 0.2332
 965/1500 [==================>...........] - ETA: 6:58 - loss: 1.4465 - regression_loss: 1.2134 - classification_loss: 0.2331
 966/1500 [==================>...........] - ETA: 6:57 - loss: 1.4467 - regression_loss: 1.2136 - classification_loss: 0.2331
 967/1500 [==================>...........] - ETA: 6:56 - loss: 1.4462 - regression_loss: 1.2132 - classification_loss: 0.2329
 968/1500 [==================>...........] - ETA: 6:55 - loss: 1.4463 - regression_loss: 1.2133 - classification_loss: 0.2331
 969/1500 [==================>...........] - ETA: 6:54 - loss: 1.4458 - regression_loss: 1.2129 - classification_loss: 0.2329
 970/1500 [==================>...........] - ETA: 6:53 - loss: 1.4471 - regression_loss: 1.2139 - classification_loss: 0.2332
 971/1500 [==================>...........] - ETA: 6:52 - loss: 1.4462 - regression_loss: 1.2131 - classification_loss: 0.2331
 972/1500 [==================>...........] - ETA: 6:52 - loss: 1.4463 - regression_loss: 1.2133 - classification_loss: 0.2331
 973/1500 [==================>...........] - ETA: 6:51 - loss: 1.4457 - regression_loss: 1.2127 - classification_loss: 0.2329
 974/1500 [==================>...........] - ETA: 6:50 - loss: 1.4454 - regression_loss: 1.2126 - classification_loss: 0.2328
 975/1500 [==================>...........] - ETA: 6:49 - loss: 1.4446 - regression_loss: 1.2120 - classification_loss: 0.2326
 976/1500 [==================>...........] - ETA: 6:48 - loss: 1.4444 - regression_loss: 1.2118 - classification_loss: 0.2326
 977/1500 [==================>...........] - ETA: 6:47 - loss: 1.4449 - regression_loss: 1.2123 - classification_loss: 0.2326
 978/1500 [==================>...........] - ETA: 6:46 - loss: 1.4443 - regression_loss: 1.2117 - classification_loss: 0.2326
 979/1500 [==================>...........] - ETA: 6:46 - loss: 1.4436 - regression_loss: 1.2111 - classification_loss: 0.2325
 980/1500 [==================>...........] - ETA: 6:45 - loss: 1.4433 - regression_loss: 1.2108 - classification_loss: 0.2324
 981/1500 [==================>...........] - ETA: 6:44 - loss: 1.4432 - regression_loss: 1.2108 - classification_loss: 0.2323
 982/1500 [==================>...........] - ETA: 6:44 - loss: 1.4425 - regression_loss: 1.2103 - classification_loss: 0.2322
 983/1500 [==================>...........] - ETA: 6:43 - loss: 1.4429 - regression_loss: 1.2107 - classification_loss: 0.2322
 984/1500 [==================>...........] - ETA: 6:42 - loss: 1.4429 - regression_loss: 1.2107 - classification_loss: 0.2322
 985/1500 [==================>...........] - ETA: 6:42 - loss: 1.4422 - regression_loss: 1.2102 - classification_loss: 0.2321
 986/1500 [==================>...........] - ETA: 6:41 - loss: 1.4419 - regression_loss: 1.2099 - classification_loss: 0.2320
 987/1500 [==================>...........] - ETA: 6:40 - loss: 1.4423 - regression_loss: 1.2101 - classification_loss: 0.2321
 988/1500 [==================>...........] - ETA: 6:39 - loss: 1.4418 - regression_loss: 1.2098 - classification_loss: 0.2320
 989/1500 [==================>...........] - ETA: 6:38 - loss: 1.4421 - regression_loss: 1.2100 - classification_loss: 0.2321
 990/1500 [==================>...........] - ETA: 6:37 - loss: 1.4417 - regression_loss: 1.2096 - classification_loss: 0.2321
 991/1500 [==================>...........] - ETA: 6:36 - loss: 1.4407 - regression_loss: 1.2088 - classification_loss: 0.2319
 992/1500 [==================>...........] - ETA: 6:35 - loss: 1.4402 - regression_loss: 1.2084 - classification_loss: 0.2317
 993/1500 [==================>...........] - ETA: 6:34 - loss: 1.4396 - regression_loss: 1.2080 - classification_loss: 0.2316
 994/1500 [==================>...........] - ETA: 6:34 - loss: 1.4389 - regression_loss: 1.2075 - classification_loss: 0.2315
 995/1500 [==================>...........] - ETA: 6:33 - loss: 1.4382 - regression_loss: 1.2068 - classification_loss: 0.2314
 996/1500 [==================>...........] - ETA: 6:32 - loss: 1.4386 - regression_loss: 1.2072 - classification_loss: 0.2314
 997/1500 [==================>...........] - ETA: 6:31 - loss: 1.4376 - regression_loss: 1.2064 - classification_loss: 0.2312
 998/1500 [==================>...........] - ETA: 6:30 - loss: 1.4375 - regression_loss: 1.2063 - classification_loss: 0.2312
 999/1500 [==================>...........] - ETA: 6:29 - loss: 1.4368 - regression_loss: 1.2057 - classification_loss: 0.2311
1000/1500 [===================>..........] - ETA: 6:28 - loss: 1.4364 - regression_loss: 1.2054 - classification_loss: 0.2309
1001/1500 [===================>..........] - ETA: 6:27 - loss: 1.4359 - regression_loss: 1.2051 - classification_loss: 0.2308
1002/1500 [===================>..........] - ETA: 6:26 - loss: 1.4355 - regression_loss: 1.2048 - classification_loss: 0.2307
1003/1500 [===================>..........] - ETA: 6:26 - loss: 1.4357 - regression_loss: 1.2051 - classification_loss: 0.2306
1004/1500 [===================>..........] - ETA: 6:25 - loss: 1.4351 - regression_loss: 1.2046 - classification_loss: 0.2305
1005/1500 [===================>..........] - ETA: 6:24 - loss: 1.4366 - regression_loss: 1.2056 - classification_loss: 0.2310
1006/1500 [===================>..........] - ETA: 6:23 - loss: 1.4372 - regression_loss: 1.2061 - classification_loss: 0.2312
1007/1500 [===================>..........] - ETA: 6:22 - loss: 1.4385 - regression_loss: 1.2070 - classification_loss: 0.2315
1008/1500 [===================>..........] - ETA: 6:21 - loss: 1.4389 - regression_loss: 1.2075 - classification_loss: 0.2314
1009/1500 [===================>..........] - ETA: 6:21 - loss: 1.4393 - regression_loss: 1.2078 - classification_loss: 0.2315
1010/1500 [===================>..........] - ETA: 6:21 - loss: 1.4391 - regression_loss: 1.2077 - classification_loss: 0.2314
1011/1500 [===================>..........] - ETA: 6:20 - loss: 1.4388 - regression_loss: 1.2074 - classification_loss: 0.2314
1012/1500 [===================>..........] - ETA: 6:19 - loss: 1.4397 - regression_loss: 1.2083 - classification_loss: 0.2314
1013/1500 [===================>..........] - ETA: 6:19 - loss: 1.4395 - regression_loss: 1.2081 - classification_loss: 0.2314
1014/1500 [===================>..........] - ETA: 6:18 - loss: 1.4404 - regression_loss: 1.2088 - classification_loss: 0.2316
1015/1500 [===================>..........] - ETA: 6:18 - loss: 1.4407 - regression_loss: 1.2091 - classification_loss: 0.2316
1016/1500 [===================>..........] - ETA: 6:17 - loss: 1.4407 - regression_loss: 1.2091 - classification_loss: 0.2316
1017/1500 [===================>..........] - ETA: 6:16 - loss: 1.4400 - regression_loss: 1.2086 - classification_loss: 0.2314
1018/1500 [===================>..........] - ETA: 6:16 - loss: 1.4401 - regression_loss: 1.2087 - classification_loss: 0.2314
1019/1500 [===================>..........] - ETA: 6:15 - loss: 1.4396 - regression_loss: 1.2083 - classification_loss: 0.2313
1020/1500 [===================>..........] - ETA: 6:15 - loss: 1.4393 - regression_loss: 1.2080 - classification_loss: 0.2313
1021/1500 [===================>..........] - ETA: 6:14 - loss: 1.4395 - regression_loss: 1.2082 - classification_loss: 0.2312
1022/1500 [===================>..........] - ETA: 6:13 - loss: 1.4389 - regression_loss: 1.2078 - classification_loss: 0.2312
1023/1500 [===================>..........] - ETA: 6:13 - loss: 1.4397 - regression_loss: 1.2083 - classification_loss: 0.2314
1024/1500 [===================>..........] - ETA: 6:12 - loss: 1.4392 - regression_loss: 1.2079 - classification_loss: 0.2313
1025/1500 [===================>..........] - ETA: 6:11 - loss: 1.4393 - regression_loss: 1.2081 - classification_loss: 0.2312
1026/1500 [===================>..........] - ETA: 6:10 - loss: 1.4399 - regression_loss: 1.2086 - classification_loss: 0.2313
1027/1500 [===================>..........] - ETA: 6:10 - loss: 1.4400 - regression_loss: 1.2088 - classification_loss: 0.2312
1028/1500 [===================>..........] - ETA: 6:09 - loss: 1.4402 - regression_loss: 1.2090 - classification_loss: 0.2312
1029/1500 [===================>..........] - ETA: 6:08 - loss: 1.4407 - regression_loss: 1.2094 - classification_loss: 0.2312
1030/1500 [===================>..........] - ETA: 6:07 - loss: 1.4411 - regression_loss: 1.2099 - classification_loss: 0.2312
1031/1500 [===================>..........] - ETA: 6:06 - loss: 1.4407 - regression_loss: 1.2096 - classification_loss: 0.2311
1032/1500 [===================>..........] - ETA: 6:06 - loss: 1.4404 - regression_loss: 1.2095 - classification_loss: 0.2310
1033/1500 [===================>..........] - ETA: 6:05 - loss: 1.4415 - regression_loss: 1.2102 - classification_loss: 0.2313
1034/1500 [===================>..........] - ETA: 6:05 - loss: 1.4418 - regression_loss: 1.2105 - classification_loss: 0.2313
1035/1500 [===================>..........] - ETA: 6:04 - loss: 1.4417 - regression_loss: 1.2106 - classification_loss: 0.2311
1036/1500 [===================>..........] - ETA: 6:03 - loss: 1.4422 - regression_loss: 1.2111 - classification_loss: 0.2311
1037/1500 [===================>..........] - ETA: 6:03 - loss: 1.4418 - regression_loss: 1.2109 - classification_loss: 0.2309
1038/1500 [===================>..........] - ETA: 6:02 - loss: 1.4409 - regression_loss: 1.2101 - classification_loss: 0.2308
1039/1500 [===================>..........] - ETA: 6:01 - loss: 1.4407 - regression_loss: 1.2100 - classification_loss: 0.2307
1040/1500 [===================>..........] - ETA: 6:00 - loss: 1.4409 - regression_loss: 1.2102 - classification_loss: 0.2307
1041/1500 [===================>..........] - ETA: 6:00 - loss: 1.4413 - regression_loss: 1.2106 - classification_loss: 0.2307
1042/1500 [===================>..........] - ETA: 5:59 - loss: 1.4410 - regression_loss: 1.2104 - classification_loss: 0.2306
1043/1500 [===================>..........] - ETA: 5:58 - loss: 1.4411 - regression_loss: 1.2106 - classification_loss: 0.2305
1044/1500 [===================>..........] - ETA: 5:57 - loss: 1.4423 - regression_loss: 1.2114 - classification_loss: 0.2308
1045/1500 [===================>..........] - ETA: 5:56 - loss: 1.4420 - regression_loss: 1.2112 - classification_loss: 0.2308
1046/1500 [===================>..........] - ETA: 5:55 - loss: 1.4430 - regression_loss: 1.2121 - classification_loss: 0.2310
1047/1500 [===================>..........] - ETA: 5:55 - loss: 1.4433 - regression_loss: 1.2123 - classification_loss: 0.2310
1048/1500 [===================>..........] - ETA: 5:54 - loss: 1.4434 - regression_loss: 1.2124 - classification_loss: 0.2310
1049/1500 [===================>..........] - ETA: 5:53 - loss: 1.4430 - regression_loss: 1.2121 - classification_loss: 0.2309
1050/1500 [====================>.........] - ETA: 5:52 - loss: 1.4441 - regression_loss: 1.2130 - classification_loss: 0.2311
1051/1500 [====================>.........] - ETA: 5:51 - loss: 1.4448 - regression_loss: 1.2137 - classification_loss: 0.2311
1052/1500 [====================>.........] - ETA: 5:51 - loss: 1.4461 - regression_loss: 1.2148 - classification_loss: 0.2313
1053/1500 [====================>.........] - ETA: 5:50 - loss: 1.4472 - regression_loss: 1.2155 - classification_loss: 0.2317
1054/1500 [====================>.........] - ETA: 5:49 - loss: 1.4472 - regression_loss: 1.2155 - classification_loss: 0.2317
1055/1500 [====================>.........] - ETA: 5:48 - loss: 1.4478 - regression_loss: 1.2160 - classification_loss: 0.2318
1056/1500 [====================>.........] - ETA: 5:47 - loss: 1.4473 - regression_loss: 1.2156 - classification_loss: 0.2317
1057/1500 [====================>.........] - ETA: 5:47 - loss: 1.4469 - regression_loss: 1.2153 - classification_loss: 0.2316
1058/1500 [====================>.........] - ETA: 5:46 - loss: 1.4472 - regression_loss: 1.2155 - classification_loss: 0.2316
1059/1500 [====================>.........] - ETA: 5:45 - loss: 1.4473 - regression_loss: 1.2156 - classification_loss: 0.2317
1060/1500 [====================>.........] - ETA: 5:44 - loss: 1.4473 - regression_loss: 1.2156 - classification_loss: 0.2317
1061/1500 [====================>.........] - ETA: 5:43 - loss: 1.4479 - regression_loss: 1.2162 - classification_loss: 0.2318
1062/1500 [====================>.........] - ETA: 5:43 - loss: 1.4487 - regression_loss: 1.2168 - classification_loss: 0.2319
1063/1500 [====================>.........] - ETA: 5:42 - loss: 1.4480 - regression_loss: 1.2163 - classification_loss: 0.2317
1064/1500 [====================>.........] - ETA: 5:41 - loss: 1.4480 - regression_loss: 1.2164 - classification_loss: 0.2316
1065/1500 [====================>.........] - ETA: 5:40 - loss: 1.4486 - regression_loss: 1.2167 - classification_loss: 0.2319
1066/1500 [====================>.........] - ETA: 5:40 - loss: 1.4498 - regression_loss: 1.2175 - classification_loss: 0.2323
1067/1500 [====================>.........] - ETA: 5:39 - loss: 1.4505 - regression_loss: 1.2180 - classification_loss: 0.2325
1068/1500 [====================>.........] - ETA: 5:38 - loss: 1.4520 - regression_loss: 1.2191 - classification_loss: 0.2330
1069/1500 [====================>.........] - ETA: 5:37 - loss: 1.4524 - regression_loss: 1.2194 - classification_loss: 0.2331
1070/1500 [====================>.........] - ETA: 5:36 - loss: 1.4516 - regression_loss: 1.2187 - classification_loss: 0.2329
1071/1500 [====================>.........] - ETA: 5:35 - loss: 1.4525 - regression_loss: 1.2192 - classification_loss: 0.2333
1072/1500 [====================>.........] - ETA: 5:35 - loss: 1.4524 - regression_loss: 1.2191 - classification_loss: 0.2333
1073/1500 [====================>.........] - ETA: 5:34 - loss: 1.4542 - regression_loss: 1.2200 - classification_loss: 0.2342
1074/1500 [====================>.........] - ETA: 5:33 - loss: 1.4551 - regression_loss: 1.2208 - classification_loss: 0.2343
1075/1500 [====================>.........] - ETA: 5:32 - loss: 1.4546 - regression_loss: 1.2205 - classification_loss: 0.2342
1076/1500 [====================>.........] - ETA: 5:31 - loss: 1.4544 - regression_loss: 1.2203 - classification_loss: 0.2341
1077/1500 [====================>.........] - ETA: 5:30 - loss: 1.4541 - regression_loss: 1.2201 - classification_loss: 0.2340
1078/1500 [====================>.........] - ETA: 5:30 - loss: 1.4533 - regression_loss: 1.2195 - classification_loss: 0.2338
1079/1500 [====================>.........] - ETA: 5:29 - loss: 1.4544 - regression_loss: 1.2205 - classification_loss: 0.2339
1080/1500 [====================>.........] - ETA: 5:28 - loss: 1.4543 - regression_loss: 1.2205 - classification_loss: 0.2338
1081/1500 [====================>.........] - ETA: 5:27 - loss: 1.4549 - regression_loss: 1.2210 - classification_loss: 0.2338
1082/1500 [====================>.........] - ETA: 5:27 - loss: 1.4549 - regression_loss: 1.2210 - classification_loss: 0.2339
1083/1500 [====================>.........] - ETA: 5:26 - loss: 1.4550 - regression_loss: 1.2210 - classification_loss: 0.2340
1084/1500 [====================>.........] - ETA: 5:25 - loss: 1.4546 - regression_loss: 1.2207 - classification_loss: 0.2339
1085/1500 [====================>.........] - ETA: 5:24 - loss: 1.4542 - regression_loss: 1.2204 - classification_loss: 0.2338
1086/1500 [====================>.........] - ETA: 5:24 - loss: 1.4536 - regression_loss: 1.2198 - classification_loss: 0.2337
1087/1500 [====================>.........] - ETA: 5:23 - loss: 1.4541 - regression_loss: 1.2203 - classification_loss: 0.2338
1088/1500 [====================>.........] - ETA: 5:22 - loss: 1.4535 - regression_loss: 1.2198 - classification_loss: 0.2337
1089/1500 [====================>.........] - ETA: 5:21 - loss: 1.4546 - regression_loss: 1.2205 - classification_loss: 0.2341
1090/1500 [====================>.........] - ETA: 5:20 - loss: 1.4539 - regression_loss: 1.2200 - classification_loss: 0.2339
1091/1500 [====================>.........] - ETA: 5:20 - loss: 1.4541 - regression_loss: 1.2201 - classification_loss: 0.2340
1092/1500 [====================>.........] - ETA: 5:19 - loss: 1.4538 - regression_loss: 1.2196 - classification_loss: 0.2341
1093/1500 [====================>.........] - ETA: 5:18 - loss: 1.4541 - regression_loss: 1.2199 - classification_loss: 0.2341
1094/1500 [====================>.........] - ETA: 5:17 - loss: 1.4539 - regression_loss: 1.2198 - classification_loss: 0.2341
1095/1500 [====================>.........] - ETA: 5:16 - loss: 1.4537 - regression_loss: 1.2197 - classification_loss: 0.2340
1096/1500 [====================>.........] - ETA: 5:15 - loss: 1.4540 - regression_loss: 1.2200 - classification_loss: 0.2340
1097/1500 [====================>.........] - ETA: 5:14 - loss: 1.4538 - regression_loss: 1.2197 - classification_loss: 0.2341
1098/1500 [====================>.........] - ETA: 5:14 - loss: 1.4537 - regression_loss: 1.2196 - classification_loss: 0.2341
1099/1500 [====================>.........] - ETA: 5:13 - loss: 1.4536 - regression_loss: 1.2195 - classification_loss: 0.2340
1100/1500 [=====================>........] - ETA: 5:12 - loss: 1.4543 - regression_loss: 1.2202 - classification_loss: 0.2341
1101/1500 [=====================>........] - ETA: 5:11 - loss: 1.4540 - regression_loss: 1.2200 - classification_loss: 0.2340
1102/1500 [=====================>........] - ETA: 5:10 - loss: 1.4542 - regression_loss: 1.2201 - classification_loss: 0.2341
1103/1500 [=====================>........] - ETA: 5:09 - loss: 1.4538 - regression_loss: 1.2198 - classification_loss: 0.2340
1104/1500 [=====================>........] - ETA: 5:09 - loss: 1.4542 - regression_loss: 1.2200 - classification_loss: 0.2341
1105/1500 [=====================>........] - ETA: 5:08 - loss: 1.4553 - regression_loss: 1.2209 - classification_loss: 0.2344
1106/1500 [=====================>........] - ETA: 5:07 - loss: 1.4549 - regression_loss: 1.2206 - classification_loss: 0.2343
1107/1500 [=====================>........] - ETA: 5:07 - loss: 1.4541 - regression_loss: 1.2199 - classification_loss: 0.2342
1108/1500 [=====================>........] - ETA: 5:06 - loss: 1.4546 - regression_loss: 1.2203 - classification_loss: 0.2343
1109/1500 [=====================>........] - ETA: 5:05 - loss: 1.4546 - regression_loss: 1.2203 - classification_loss: 0.2343
1110/1500 [=====================>........] - ETA: 5:04 - loss: 1.4539 - regression_loss: 1.2198 - classification_loss: 0.2341
1111/1500 [=====================>........] - ETA: 5:03 - loss: 1.4550 - regression_loss: 1.2206 - classification_loss: 0.2344
1112/1500 [=====================>........] - ETA: 5:02 - loss: 1.4552 - regression_loss: 1.2207 - classification_loss: 0.2345
1113/1500 [=====================>........] - ETA: 5:01 - loss: 1.4550 - regression_loss: 1.2205 - classification_loss: 0.2345
1114/1500 [=====================>........] - ETA: 5:01 - loss: 1.4551 - regression_loss: 1.2206 - classification_loss: 0.2345
1115/1500 [=====================>........] - ETA: 5:00 - loss: 1.4547 - regression_loss: 1.2203 - classification_loss: 0.2344
1116/1500 [=====================>........] - ETA: 4:59 - loss: 1.4543 - regression_loss: 1.2200 - classification_loss: 0.2342
1117/1500 [=====================>........] - ETA: 4:59 - loss: 1.4542 - regression_loss: 1.2200 - classification_loss: 0.2343
1118/1500 [=====================>........] - ETA: 4:58 - loss: 1.4555 - regression_loss: 1.2211 - classification_loss: 0.2344
1119/1500 [=====================>........] - ETA: 4:57 - loss: 1.4547 - regression_loss: 1.2204 - classification_loss: 0.2343
1120/1500 [=====================>........] - ETA: 4:57 - loss: 1.4546 - regression_loss: 1.2205 - classification_loss: 0.2342
1121/1500 [=====================>........] - ETA: 4:56 - loss: 1.4552 - regression_loss: 1.2210 - classification_loss: 0.2343
1122/1500 [=====================>........] - ETA: 4:55 - loss: 1.4545 - regression_loss: 1.2204 - classification_loss: 0.2341
1123/1500 [=====================>........] - ETA: 4:54 - loss: 1.4551 - regression_loss: 1.2209 - classification_loss: 0.2342
1124/1500 [=====================>........] - ETA: 4:53 - loss: 1.4546 - regression_loss: 1.2205 - classification_loss: 0.2341
1125/1500 [=====================>........] - ETA: 4:53 - loss: 1.4537 - regression_loss: 1.2198 - classification_loss: 0.2339
1126/1500 [=====================>........] - ETA: 4:52 - loss: 1.4532 - regression_loss: 1.2194 - classification_loss: 0.2338
1127/1500 [=====================>........] - ETA: 4:51 - loss: 1.4528 - regression_loss: 1.2191 - classification_loss: 0.2337
1128/1500 [=====================>........] - ETA: 4:50 - loss: 1.4536 - regression_loss: 1.2199 - classification_loss: 0.2337
1129/1500 [=====================>........] - ETA: 4:49 - loss: 1.4534 - regression_loss: 1.2197 - classification_loss: 0.2338
1130/1500 [=====================>........] - ETA: 4:48 - loss: 1.4544 - regression_loss: 1.2202 - classification_loss: 0.2342
1131/1500 [=====================>........] - ETA: 4:47 - loss: 1.4537 - regression_loss: 1.2197 - classification_loss: 0.2340
1132/1500 [=====================>........] - ETA: 4:46 - loss: 1.4529 - regression_loss: 1.2190 - classification_loss: 0.2339
1133/1500 [=====================>........] - ETA: 4:46 - loss: 1.4537 - regression_loss: 1.2196 - classification_loss: 0.2341
1134/1500 [=====================>........] - ETA: 4:45 - loss: 1.4541 - regression_loss: 1.2199 - classification_loss: 0.2342
1135/1500 [=====================>........] - ETA: 4:44 - loss: 1.4538 - regression_loss: 1.2198 - classification_loss: 0.2341
1136/1500 [=====================>........] - ETA: 4:43 - loss: 1.4537 - regression_loss: 1.2197 - classification_loss: 0.2340
1137/1500 [=====================>........] - ETA: 4:43 - loss: 1.4537 - regression_loss: 1.2195 - classification_loss: 0.2341
1138/1500 [=====================>........] - ETA: 4:42 - loss: 1.4540 - regression_loss: 1.2199 - classification_loss: 0.2342
1139/1500 [=====================>........] - ETA: 4:41 - loss: 1.4537 - regression_loss: 1.2196 - classification_loss: 0.2341
1140/1500 [=====================>........] - ETA: 4:40 - loss: 1.4547 - regression_loss: 1.2203 - classification_loss: 0.2343
1141/1500 [=====================>........] - ETA: 4:40 - loss: 1.4554 - regression_loss: 1.2208 - classification_loss: 0.2345
1142/1500 [=====================>........] - ETA: 4:39 - loss: 1.4554 - regression_loss: 1.2209 - classification_loss: 0.2345
1143/1500 [=====================>........] - ETA: 4:38 - loss: 1.4561 - regression_loss: 1.2216 - classification_loss: 0.2346
1144/1500 [=====================>........] - ETA: 4:38 - loss: 1.4563 - regression_loss: 1.2218 - classification_loss: 0.2345
1145/1500 [=====================>........] - ETA: 4:37 - loss: 1.4562 - regression_loss: 1.2216 - classification_loss: 0.2346
1146/1500 [=====================>........] - ETA: 4:36 - loss: 1.4560 - regression_loss: 1.2214 - classification_loss: 0.2345
1147/1500 [=====================>........] - ETA: 4:35 - loss: 1.4562 - regression_loss: 1.2216 - classification_loss: 0.2345
1148/1500 [=====================>........] - ETA: 4:34 - loss: 1.4554 - regression_loss: 1.2210 - classification_loss: 0.2344
1149/1500 [=====================>........] - ETA: 4:34 - loss: 1.4547 - regression_loss: 1.2205 - classification_loss: 0.2342
1150/1500 [======================>.......] - ETA: 4:33 - loss: 1.4552 - regression_loss: 1.2208 - classification_loss: 0.2343
1151/1500 [======================>.......] - ETA: 4:32 - loss: 1.4550 - regression_loss: 1.2208 - classification_loss: 0.2342
1152/1500 [======================>.......] - ETA: 4:31 - loss: 1.4555 - regression_loss: 1.2212 - classification_loss: 0.2343
1153/1500 [======================>.......] - ETA: 4:30 - loss: 1.4553 - regression_loss: 1.2210 - classification_loss: 0.2342
1154/1500 [======================>.......] - ETA: 4:30 - loss: 1.4548 - regression_loss: 1.2208 - classification_loss: 0.2341
1155/1500 [======================>.......] - ETA: 4:29 - loss: 1.4556 - regression_loss: 1.2215 - classification_loss: 0.2341
1156/1500 [======================>.......] - ETA: 4:28 - loss: 1.4560 - regression_loss: 1.2219 - classification_loss: 0.2341
1157/1500 [======================>.......] - ETA: 4:27 - loss: 1.4558 - regression_loss: 1.2218 - classification_loss: 0.2340
1158/1500 [======================>.......] - ETA: 4:26 - loss: 1.4563 - regression_loss: 1.2222 - classification_loss: 0.2341
1159/1500 [======================>.......] - ETA: 4:26 - loss: 1.4563 - regression_loss: 1.2223 - classification_loss: 0.2340
1160/1500 [======================>.......] - ETA: 4:25 - loss: 1.4565 - regression_loss: 1.2224 - classification_loss: 0.2341
1161/1500 [======================>.......] - ETA: 4:24 - loss: 1.4561 - regression_loss: 1.2221 - classification_loss: 0.2340
1162/1500 [======================>.......] - ETA: 4:23 - loss: 1.4562 - regression_loss: 1.2222 - classification_loss: 0.2340
1163/1500 [======================>.......] - ETA: 4:23 - loss: 1.4556 - regression_loss: 1.2216 - classification_loss: 0.2340
1164/1500 [======================>.......] - ETA: 4:22 - loss: 1.4563 - regression_loss: 1.2222 - classification_loss: 0.2341
1165/1500 [======================>.......] - ETA: 4:21 - loss: 1.4564 - regression_loss: 1.2223 - classification_loss: 0.2342
1166/1500 [======================>.......] - ETA: 4:21 - loss: 1.4572 - regression_loss: 1.2230 - classification_loss: 0.2342
1167/1500 [======================>.......] - ETA: 4:20 - loss: 1.4569 - regression_loss: 1.2227 - classification_loss: 0.2342
1168/1500 [======================>.......] - ETA: 4:19 - loss: 1.4570 - regression_loss: 1.2229 - classification_loss: 0.2342
1169/1500 [======================>.......] - ETA: 4:19 - loss: 1.4564 - regression_loss: 1.2224 - classification_loss: 0.2340
1170/1500 [======================>.......] - ETA: 4:18 - loss: 1.4560 - regression_loss: 1.2221 - classification_loss: 0.2339
1171/1500 [======================>.......] - ETA: 4:17 - loss: 1.4563 - regression_loss: 1.2223 - classification_loss: 0.2340
1172/1500 [======================>.......] - ETA: 4:16 - loss: 1.4568 - regression_loss: 1.2227 - classification_loss: 0.2341
1173/1500 [======================>.......] - ETA: 4:16 - loss: 1.4566 - regression_loss: 1.2226 - classification_loss: 0.2340
1174/1500 [======================>.......] - ETA: 4:15 - loss: 1.4562 - regression_loss: 1.2224 - classification_loss: 0.2339
1175/1500 [======================>.......] - ETA: 4:14 - loss: 1.4555 - regression_loss: 1.2218 - classification_loss: 0.2337
1176/1500 [======================>.......] - ETA: 4:13 - loss: 1.4556 - regression_loss: 1.2219 - classification_loss: 0.2338
1177/1500 [======================>.......] - ETA: 4:12 - loss: 1.4561 - regression_loss: 1.2221 - classification_loss: 0.2341
1178/1500 [======================>.......] - ETA: 4:11 - loss: 1.4554 - regression_loss: 1.2215 - classification_loss: 0.2339
1179/1500 [======================>.......] - ETA: 4:10 - loss: 1.4554 - regression_loss: 1.2215 - classification_loss: 0.2338
1180/1500 [======================>.......] - ETA: 4:09 - loss: 1.4547 - regression_loss: 1.2210 - classification_loss: 0.2337
1181/1500 [======================>.......] - ETA: 4:08 - loss: 1.4545 - regression_loss: 1.2209 - classification_loss: 0.2336
1182/1500 [======================>.......] - ETA: 4:08 - loss: 1.4538 - regression_loss: 1.2203 - classification_loss: 0.2335
1183/1500 [======================>.......] - ETA: 4:07 - loss: 1.4544 - regression_loss: 1.2206 - classification_loss: 0.2338
1184/1500 [======================>.......] - ETA: 4:06 - loss: 1.4543 - regression_loss: 1.2205 - classification_loss: 0.2338
1185/1500 [======================>.......] - ETA: 4:05 - loss: 1.4546 - regression_loss: 1.2207 - classification_loss: 0.2339
1186/1500 [======================>.......] - ETA: 4:04 - loss: 1.4549 - regression_loss: 1.2210 - classification_loss: 0.2339
1187/1500 [======================>.......] - ETA: 4:04 - loss: 1.4556 - regression_loss: 1.2216 - classification_loss: 0.2340
1188/1500 [======================>.......] - ETA: 4:03 - loss: 1.4561 - regression_loss: 1.2220 - classification_loss: 0.2341
1189/1500 [======================>.......] - ETA: 4:02 - loss: 1.4557 - regression_loss: 1.2217 - classification_loss: 0.2340
1190/1500 [======================>.......] - ETA: 4:01 - loss: 1.4550 - regression_loss: 1.2211 - classification_loss: 0.2339
1191/1500 [======================>.......] - ETA: 4:00 - loss: 1.4548 - regression_loss: 1.2210 - classification_loss: 0.2339
1192/1500 [======================>.......] - ETA: 3:59 - loss: 1.4546 - regression_loss: 1.2208 - classification_loss: 0.2337
1193/1500 [======================>.......] - ETA: 3:59 - loss: 1.4547 - regression_loss: 1.2210 - classification_loss: 0.2337
1194/1500 [======================>.......] - ETA: 3:58 - loss: 1.4544 - regression_loss: 1.2207 - classification_loss: 0.2337
1195/1500 [======================>.......] - ETA: 3:57 - loss: 1.4540 - regression_loss: 1.2204 - classification_loss: 0.2336
1196/1500 [======================>.......] - ETA: 3:56 - loss: 1.4547 - regression_loss: 1.2209 - classification_loss: 0.2337
1197/1500 [======================>.......] - ETA: 3:55 - loss: 1.4543 - regression_loss: 1.2206 - classification_loss: 0.2337
1198/1500 [======================>.......] - ETA: 3:55 - loss: 1.4544 - regression_loss: 1.2206 - classification_loss: 0.2338
1199/1500 [======================>.......] - ETA: 3:54 - loss: 1.4551 - regression_loss: 1.2212 - classification_loss: 0.2338
1200/1500 [=======================>......] - ETA: 3:53 - loss: 1.4544 - regression_loss: 1.2207 - classification_loss: 0.2337
1201/1500 [=======================>......] - ETA: 3:52 - loss: 1.4548 - regression_loss: 1.2211 - classification_loss: 0.2337
1202/1500 [=======================>......] - ETA: 3:52 - loss: 1.4546 - regression_loss: 1.2208 - classification_loss: 0.2337
1203/1500 [=======================>......] - ETA: 3:51 - loss: 1.4540 - regression_loss: 1.2204 - classification_loss: 0.2336
1204/1500 [=======================>......] - ETA: 3:50 - loss: 1.4541 - regression_loss: 1.2204 - classification_loss: 0.2337
1205/1500 [=======================>......] - ETA: 3:49 - loss: 1.4544 - regression_loss: 1.2207 - classification_loss: 0.2338
1206/1500 [=======================>......] - ETA: 3:49 - loss: 1.4547 - regression_loss: 1.2209 - classification_loss: 0.2338
1207/1500 [=======================>......] - ETA: 3:48 - loss: 1.4542 - regression_loss: 1.2204 - classification_loss: 0.2337
1208/1500 [=======================>......] - ETA: 3:47 - loss: 1.4542 - regression_loss: 1.2205 - classification_loss: 0.2337
1209/1500 [=======================>......] - ETA: 3:46 - loss: 1.4539 - regression_loss: 1.2203 - classification_loss: 0.2336
1210/1500 [=======================>......] - ETA: 3:45 - loss: 1.4538 - regression_loss: 1.2202 - classification_loss: 0.2336
1211/1500 [=======================>......] - ETA: 3:45 - loss: 1.4535 - regression_loss: 1.2199 - classification_loss: 0.2336
1212/1500 [=======================>......] - ETA: 3:44 - loss: 1.4539 - regression_loss: 1.2200 - classification_loss: 0.2338
1213/1500 [=======================>......] - ETA: 3:43 - loss: 1.4542 - regression_loss: 1.2203 - classification_loss: 0.2338
1214/1500 [=======================>......] - ETA: 3:42 - loss: 1.4538 - regression_loss: 1.2199 - classification_loss: 0.2339
1215/1500 [=======================>......] - ETA: 3:42 - loss: 1.4536 - regression_loss: 1.2198 - classification_loss: 0.2338
1216/1500 [=======================>......] - ETA: 3:41 - loss: 1.4533 - regression_loss: 1.2194 - classification_loss: 0.2338
1217/1500 [=======================>......] - ETA: 3:40 - loss: 1.4528 - regression_loss: 1.2192 - classification_loss: 0.2337
1218/1500 [=======================>......] - ETA: 3:39 - loss: 1.4533 - regression_loss: 1.2196 - classification_loss: 0.2338
1219/1500 [=======================>......] - ETA: 3:39 - loss: 1.4534 - regression_loss: 1.2197 - classification_loss: 0.2337
1220/1500 [=======================>......] - ETA: 3:38 - loss: 1.4533 - regression_loss: 1.2196 - classification_loss: 0.2336
1221/1500 [=======================>......] - ETA: 3:37 - loss: 1.4527 - regression_loss: 1.2192 - classification_loss: 0.2335
1222/1500 [=======================>......] - ETA: 3:37 - loss: 1.4525 - regression_loss: 1.2190 - classification_loss: 0.2335
1223/1500 [=======================>......] - ETA: 3:36 - loss: 1.4524 - regression_loss: 1.2188 - classification_loss: 0.2337
1224/1500 [=======================>......] - ETA: 3:35 - loss: 1.4526 - regression_loss: 1.2190 - classification_loss: 0.2336
1225/1500 [=======================>......] - ETA: 3:34 - loss: 1.4533 - regression_loss: 1.2194 - classification_loss: 0.2339
1226/1500 [=======================>......] - ETA: 3:33 - loss: 1.4541 - regression_loss: 1.2199 - classification_loss: 0.2341
1227/1500 [=======================>......] - ETA: 3:33 - loss: 1.4547 - regression_loss: 1.2204 - classification_loss: 0.2343
1228/1500 [=======================>......] - ETA: 3:32 - loss: 1.4547 - regression_loss: 1.2203 - classification_loss: 0.2344
1229/1500 [=======================>......] - ETA: 3:31 - loss: 1.4546 - regression_loss: 1.2202 - classification_loss: 0.2344
1230/1500 [=======================>......] - ETA: 3:31 - loss: 1.4545 - regression_loss: 1.2201 - classification_loss: 0.2344
1231/1500 [=======================>......] - ETA: 3:30 - loss: 1.4556 - regression_loss: 1.2209 - classification_loss: 0.2347
1232/1500 [=======================>......] - ETA: 3:29 - loss: 1.4549 - regression_loss: 1.2204 - classification_loss: 0.2345
1233/1500 [=======================>......] - ETA: 3:28 - loss: 1.4543 - regression_loss: 1.2200 - classification_loss: 0.2344
1234/1500 [=======================>......] - ETA: 3:27 - loss: 1.4551 - regression_loss: 1.2206 - classification_loss: 0.2345
1235/1500 [=======================>......] - ETA: 3:26 - loss: 1.4545 - regression_loss: 1.2201 - classification_loss: 0.2344
1236/1500 [=======================>......] - ETA: 3:26 - loss: 1.4542 - regression_loss: 1.2199 - classification_loss: 0.2343
1237/1500 [=======================>......] - ETA: 3:25 - loss: 1.4536 - regression_loss: 1.2195 - classification_loss: 0.2342
1238/1500 [=======================>......] - ETA: 3:24 - loss: 1.4534 - regression_loss: 1.2193 - classification_loss: 0.2341
1239/1500 [=======================>......] - ETA: 3:23 - loss: 1.4539 - regression_loss: 1.2198 - classification_loss: 0.2341
1240/1500 [=======================>......] - ETA: 3:23 - loss: 1.4539 - regression_loss: 1.2198 - classification_loss: 0.2341
1241/1500 [=======================>......] - ETA: 3:22 - loss: 1.4534 - regression_loss: 1.2194 - classification_loss: 0.2341
1242/1500 [=======================>......] - ETA: 3:21 - loss: 1.4532 - regression_loss: 1.2193 - classification_loss: 0.2340
1243/1500 [=======================>......] - ETA: 3:20 - loss: 1.4534 - regression_loss: 1.2194 - classification_loss: 0.2340
1244/1500 [=======================>......] - ETA: 3:19 - loss: 1.4529 - regression_loss: 1.2190 - classification_loss: 0.2339
1245/1500 [=======================>......] - ETA: 3:18 - loss: 1.4523 - regression_loss: 1.2185 - classification_loss: 0.2337
1246/1500 [=======================>......] - ETA: 3:18 - loss: 1.4524 - regression_loss: 1.2185 - classification_loss: 0.2339
1247/1500 [=======================>......] - ETA: 3:17 - loss: 1.4529 - regression_loss: 1.2188 - classification_loss: 0.2341
1248/1500 [=======================>......] - ETA: 3:16 - loss: 1.4528 - regression_loss: 1.2187 - classification_loss: 0.2342
1249/1500 [=======================>......] - ETA: 3:16 - loss: 1.4524 - regression_loss: 1.2183 - classification_loss: 0.2341
1250/1500 [========================>.....] - ETA: 3:15 - loss: 1.4522 - regression_loss: 1.2182 - classification_loss: 0.2340
1251/1500 [========================>.....] - ETA: 3:14 - loss: 1.4518 - regression_loss: 1.2179 - classification_loss: 0.2339
1252/1500 [========================>.....] - ETA: 3:13 - loss: 1.4514 - regression_loss: 1.2176 - classification_loss: 0.2339
1253/1500 [========================>.....] - ETA: 3:13 - loss: 1.4508 - regression_loss: 1.2170 - classification_loss: 0.2338
1254/1500 [========================>.....] - ETA: 3:12 - loss: 1.4502 - regression_loss: 1.2166 - classification_loss: 0.2336
1255/1500 [========================>.....] - ETA: 3:11 - loss: 1.4498 - regression_loss: 1.2162 - classification_loss: 0.2336
1256/1500 [========================>.....] - ETA: 3:10 - loss: 1.4502 - regression_loss: 1.2166 - classification_loss: 0.2336
1257/1500 [========================>.....] - ETA: 3:09 - loss: 1.4501 - regression_loss: 1.2165 - classification_loss: 0.2336
1258/1500 [========================>.....] - ETA: 3:09 - loss: 1.4500 - regression_loss: 1.2163 - classification_loss: 0.2337
1259/1500 [========================>.....] - ETA: 3:08 - loss: 1.4492 - regression_loss: 1.2157 - classification_loss: 0.2335
1260/1500 [========================>.....] - ETA: 3:07 - loss: 1.4492 - regression_loss: 1.2158 - classification_loss: 0.2335
1261/1500 [========================>.....] - ETA: 3:06 - loss: 1.4489 - regression_loss: 1.2155 - classification_loss: 0.2334
1262/1500 [========================>.....] - ETA: 3:05 - loss: 1.4486 - regression_loss: 1.2154 - classification_loss: 0.2333
1263/1500 [========================>.....] - ETA: 3:05 - loss: 1.4494 - regression_loss: 1.2159 - classification_loss: 0.2335
1264/1500 [========================>.....] - ETA: 3:04 - loss: 1.4490 - regression_loss: 1.2156 - classification_loss: 0.2334
1265/1500 [========================>.....] - ETA: 3:03 - loss: 1.4490 - regression_loss: 1.2157 - classification_loss: 0.2333
1266/1500 [========================>.....] - ETA: 3:02 - loss: 1.4490 - regression_loss: 1.2157 - classification_loss: 0.2333
1267/1500 [========================>.....] - ETA: 3:01 - loss: 1.4493 - regression_loss: 1.2160 - classification_loss: 0.2333
1268/1500 [========================>.....] - ETA: 3:00 - loss: 1.4489 - regression_loss: 1.2156 - classification_loss: 0.2333
1269/1500 [========================>.....] - ETA: 3:00 - loss: 1.4484 - regression_loss: 1.2151 - classification_loss: 0.2333
1270/1500 [========================>.....] - ETA: 2:59 - loss: 1.4482 - regression_loss: 1.2148 - classification_loss: 0.2334
1271/1500 [========================>.....] - ETA: 2:58 - loss: 1.4488 - regression_loss: 1.2152 - classification_loss: 0.2336
1272/1500 [========================>.....] - ETA: 2:57 - loss: 1.4485 - regression_loss: 1.2149 - classification_loss: 0.2336
1273/1500 [========================>.....] - ETA: 2:57 - loss: 1.4489 - regression_loss: 1.2152 - classification_loss: 0.2337
1274/1500 [========================>.....] - ETA: 2:56 - loss: 1.4482 - regression_loss: 1.2146 - classification_loss: 0.2336
1275/1500 [========================>.....] - ETA: 2:55 - loss: 1.4487 - regression_loss: 1.2151 - classification_loss: 0.2336
1276/1500 [========================>.....] - ETA: 2:54 - loss: 1.4482 - regression_loss: 1.2147 - classification_loss: 0.2335
1277/1500 [========================>.....] - ETA: 2:54 - loss: 1.4476 - regression_loss: 1.2142 - classification_loss: 0.2334
1278/1500 [========================>.....] - ETA: 2:53 - loss: 1.4478 - regression_loss: 1.2144 - classification_loss: 0.2334
1279/1500 [========================>.....] - ETA: 2:52 - loss: 1.4474 - regression_loss: 1.2141 - classification_loss: 0.2333
1280/1500 [========================>.....] - ETA: 2:51 - loss: 1.4466 - regression_loss: 1.2135 - classification_loss: 0.2332
1281/1500 [========================>.....] - ETA: 2:50 - loss: 1.4468 - regression_loss: 1.2137 - classification_loss: 0.2331
1282/1500 [========================>.....] - ETA: 2:50 - loss: 1.4464 - regression_loss: 1.2133 - classification_loss: 0.2330
1283/1500 [========================>.....] - ETA: 2:49 - loss: 1.4458 - regression_loss: 1.2129 - classification_loss: 0.2329
1284/1500 [========================>.....] - ETA: 2:48 - loss: 1.4459 - regression_loss: 1.2130 - classification_loss: 0.2329
1285/1500 [========================>.....] - ETA: 2:47 - loss: 1.4456 - regression_loss: 1.2128 - classification_loss: 0.2328
1286/1500 [========================>.....] - ETA: 2:46 - loss: 1.4455 - regression_loss: 1.2127 - classification_loss: 0.2327
1287/1500 [========================>.....] - ETA: 2:46 - loss: 1.4449 - regression_loss: 1.2123 - classification_loss: 0.2326
1288/1500 [========================>.....] - ETA: 2:45 - loss: 1.4448 - regression_loss: 1.2123 - classification_loss: 0.2326
1289/1500 [========================>.....] - ETA: 2:44 - loss: 1.4441 - regression_loss: 1.2117 - classification_loss: 0.2324
1290/1500 [========================>.....] - ETA: 2:43 - loss: 1.4440 - regression_loss: 1.2117 - classification_loss: 0.2324
1291/1500 [========================>.....] - ETA: 2:42 - loss: 1.4444 - regression_loss: 1.2120 - classification_loss: 0.2324
1292/1500 [========================>.....] - ETA: 2:42 - loss: 1.4441 - regression_loss: 1.2117 - classification_loss: 0.2324
1293/1500 [========================>.....] - ETA: 2:41 - loss: 1.4439 - regression_loss: 1.2116 - classification_loss: 0.2323
1294/1500 [========================>.....] - ETA: 2:40 - loss: 1.4434 - regression_loss: 1.2111 - classification_loss: 0.2322
1295/1500 [========================>.....] - ETA: 2:39 - loss: 1.4428 - regression_loss: 1.2107 - classification_loss: 0.2321
1296/1500 [========================>.....] - ETA: 2:38 - loss: 1.4425 - regression_loss: 1.2105 - classification_loss: 0.2320
1297/1500 [========================>.....] - ETA: 2:38 - loss: 1.4417 - regression_loss: 1.2099 - classification_loss: 0.2318
1298/1500 [========================>.....] - ETA: 2:37 - loss: 1.4411 - regression_loss: 1.2094 - classification_loss: 0.2318
1299/1500 [========================>.....] - ETA: 2:36 - loss: 1.4407 - regression_loss: 1.2090 - classification_loss: 0.2317
1300/1500 [=========================>....] - ETA: 2:35 - loss: 1.4404 - regression_loss: 1.2087 - classification_loss: 0.2317
1301/1500 [=========================>....] - ETA: 2:34 - loss: 1.4401 - regression_loss: 1.2085 - classification_loss: 0.2316
1302/1500 [=========================>....] - ETA: 2:34 - loss: 1.4397 - regression_loss: 1.2082 - classification_loss: 0.2315
1303/1500 [=========================>....] - ETA: 2:33 - loss: 1.4391 - regression_loss: 1.2077 - classification_loss: 0.2314
1304/1500 [=========================>....] - ETA: 2:32 - loss: 1.4384 - regression_loss: 1.2071 - classification_loss: 0.2313
1305/1500 [=========================>....] - ETA: 2:31 - loss: 1.4382 - regression_loss: 1.2069 - classification_loss: 0.2313
1306/1500 [=========================>....] - ETA: 2:30 - loss: 1.4379 - regression_loss: 1.2067 - classification_loss: 0.2313
1307/1500 [=========================>....] - ETA: 2:30 - loss: 1.4379 - regression_loss: 1.2065 - classification_loss: 0.2314
1308/1500 [=========================>....] - ETA: 2:29 - loss: 1.4377 - regression_loss: 1.2064 - classification_loss: 0.2314
1309/1500 [=========================>....] - ETA: 2:28 - loss: 1.4371 - regression_loss: 1.2058 - classification_loss: 0.2313
1310/1500 [=========================>....] - ETA: 2:27 - loss: 1.4367 - regression_loss: 1.2056 - classification_loss: 0.2311
1311/1500 [=========================>....] - ETA: 2:27 - loss: 1.4367 - regression_loss: 1.2056 - classification_loss: 0.2311
1312/1500 [=========================>....] - ETA: 2:26 - loss: 1.4362 - regression_loss: 1.2052 - classification_loss: 0.2310
1313/1500 [=========================>....] - ETA: 2:25 - loss: 1.4361 - regression_loss: 1.2052 - classification_loss: 0.2309
1314/1500 [=========================>....] - ETA: 2:24 - loss: 1.4357 - regression_loss: 1.2049 - classification_loss: 0.2308
1315/1500 [=========================>....] - ETA: 2:24 - loss: 1.4351 - regression_loss: 1.2044 - classification_loss: 0.2307
1316/1500 [=========================>....] - ETA: 2:23 - loss: 1.4347 - regression_loss: 1.2041 - classification_loss: 0.2306
1317/1500 [=========================>....] - ETA: 2:22 - loss: 1.4344 - regression_loss: 1.2039 - classification_loss: 0.2305
1318/1500 [=========================>....] - ETA: 2:21 - loss: 1.4342 - regression_loss: 1.2038 - classification_loss: 0.2304
1319/1500 [=========================>....] - ETA: 2:20 - loss: 1.4341 - regression_loss: 1.2036 - classification_loss: 0.2304
1320/1500 [=========================>....] - ETA: 2:20 - loss: 1.4334 - regression_loss: 1.2031 - classification_loss: 0.2303
1321/1500 [=========================>....] - ETA: 2:19 - loss: 1.4339 - regression_loss: 1.2034 - classification_loss: 0.2304
1322/1500 [=========================>....] - ETA: 2:18 - loss: 1.4335 - regression_loss: 1.2032 - classification_loss: 0.2304
1323/1500 [=========================>....] - ETA: 2:17 - loss: 1.4336 - regression_loss: 1.2034 - classification_loss: 0.2303
1324/1500 [=========================>....] - ETA: 2:17 - loss: 1.4351 - regression_loss: 1.2047 - classification_loss: 0.2304
1325/1500 [=========================>....] - ETA: 2:16 - loss: 1.4346 - regression_loss: 1.2043 - classification_loss: 0.2303
1326/1500 [=========================>....] - ETA: 2:15 - loss: 1.4346 - regression_loss: 1.2044 - classification_loss: 0.2302
1327/1500 [=========================>....] - ETA: 2:14 - loss: 1.4344 - regression_loss: 1.2043 - classification_loss: 0.2302
1328/1500 [=========================>....] - ETA: 2:13 - loss: 1.4338 - regression_loss: 1.2038 - classification_loss: 0.2300
1329/1500 [=========================>....] - ETA: 2:13 - loss: 1.4334 - regression_loss: 1.2035 - classification_loss: 0.2299
1330/1500 [=========================>....] - ETA: 2:12 - loss: 1.4328 - regression_loss: 1.2030 - classification_loss: 0.2298
1331/1500 [=========================>....] - ETA: 2:11 - loss: 1.4322 - regression_loss: 1.2025 - classification_loss: 0.2297
1332/1500 [=========================>....] - ETA: 2:10 - loss: 1.4316 - regression_loss: 1.2020 - classification_loss: 0.2296
1333/1500 [=========================>....] - ETA: 2:09 - loss: 1.4309 - regression_loss: 1.2014 - classification_loss: 0.2295
1334/1500 [=========================>....] - ETA: 2:09 - loss: 1.4306 - regression_loss: 1.2011 - classification_loss: 0.2295
1335/1500 [=========================>....] - ETA: 2:08 - loss: 1.4302 - regression_loss: 1.2008 - classification_loss: 0.2294
1336/1500 [=========================>....] - ETA: 2:07 - loss: 1.4304 - regression_loss: 1.2010 - classification_loss: 0.2294
1337/1500 [=========================>....] - ETA: 2:06 - loss: 1.4313 - regression_loss: 1.2017 - classification_loss: 0.2296
1338/1500 [=========================>....] - ETA: 2:06 - loss: 1.4316 - regression_loss: 1.2018 - classification_loss: 0.2297
1339/1500 [=========================>....] - ETA: 2:05 - loss: 1.4310 - regression_loss: 1.2014 - classification_loss: 0.2296
1340/1500 [=========================>....] - ETA: 2:04 - loss: 1.4316 - regression_loss: 1.2019 - classification_loss: 0.2298
1341/1500 [=========================>....] - ETA: 2:03 - loss: 1.4319 - regression_loss: 1.2020 - classification_loss: 0.2299
1342/1500 [=========================>....] - ETA: 2:02 - loss: 1.4319 - regression_loss: 1.2020 - classification_loss: 0.2299
1343/1500 [=========================>....] - ETA: 2:02 - loss: 1.4318 - regression_loss: 1.2019 - classification_loss: 0.2299
1344/1500 [=========================>....] - ETA: 2:01 - loss: 1.4313 - regression_loss: 1.2015 - classification_loss: 0.2298
1345/1500 [=========================>....] - ETA: 2:00 - loss: 1.4308 - regression_loss: 1.2011 - classification_loss: 0.2296
1346/1500 [=========================>....] - ETA: 1:59 - loss: 1.4310 - regression_loss: 1.2013 - classification_loss: 0.2296
1347/1500 [=========================>....] - ETA: 1:59 - loss: 1.4309 - regression_loss: 1.2013 - classification_loss: 0.2296
1348/1500 [=========================>....] - ETA: 1:58 - loss: 1.4307 - regression_loss: 1.2011 - classification_loss: 0.2296
1349/1500 [=========================>....] - ETA: 1:57 - loss: 1.4307 - regression_loss: 1.2012 - classification_loss: 0.2295
1350/1500 [==========================>...] - ETA: 1:56 - loss: 1.4324 - regression_loss: 1.2022 - classification_loss: 0.2302
1351/1500 [==========================>...] - ETA: 1:55 - loss: 1.4323 - regression_loss: 1.2022 - classification_loss: 0.2301
1352/1500 [==========================>...] - ETA: 1:55 - loss: 1.4319 - regression_loss: 1.2019 - classification_loss: 0.2300
1353/1500 [==========================>...] - ETA: 1:54 - loss: 1.4318 - regression_loss: 1.2017 - classification_loss: 0.2301
1354/1500 [==========================>...] - ETA: 1:53 - loss: 1.4324 - regression_loss: 1.2023 - classification_loss: 0.2301
1355/1500 [==========================>...] - ETA: 1:52 - loss: 1.4331 - regression_loss: 1.2028 - classification_loss: 0.2303
1356/1500 [==========================>...] - ETA: 1:52 - loss: 1.4333 - regression_loss: 1.2030 - classification_loss: 0.2303
1357/1500 [==========================>...] - ETA: 1:51 - loss: 1.4331 - regression_loss: 1.2028 - classification_loss: 0.2302
1358/1500 [==========================>...] - ETA: 1:50 - loss: 1.4332 - regression_loss: 1.2029 - classification_loss: 0.2304
1359/1500 [==========================>...] - ETA: 1:49 - loss: 1.4340 - regression_loss: 1.2036 - classification_loss: 0.2304
1360/1500 [==========================>...] - ETA: 1:48 - loss: 1.4338 - regression_loss: 1.2034 - classification_loss: 0.2304
1361/1500 [==========================>...] - ETA: 1:48 - loss: 1.4338 - regression_loss: 1.2035 - classification_loss: 0.2303
1362/1500 [==========================>...] - ETA: 1:47 - loss: 1.4336 - regression_loss: 1.2034 - classification_loss: 0.2302
1363/1500 [==========================>...] - ETA: 1:46 - loss: 1.4341 - regression_loss: 1.2039 - classification_loss: 0.2302
1364/1500 [==========================>...] - ETA: 1:45 - loss: 1.4338 - regression_loss: 1.2036 - classification_loss: 0.2302
1365/1500 [==========================>...] - ETA: 1:44 - loss: 1.4342 - regression_loss: 1.2040 - classification_loss: 0.2302
1366/1500 [==========================>...] - ETA: 1:44 - loss: 1.4336 - regression_loss: 1.2036 - classification_loss: 0.2301
1367/1500 [==========================>...] - ETA: 1:43 - loss: 1.4333 - regression_loss: 1.2032 - classification_loss: 0.2300
1368/1500 [==========================>...] - ETA: 1:42 - loss: 1.4332 - regression_loss: 1.2032 - classification_loss: 0.2299
1369/1500 [==========================>...] - ETA: 1:41 - loss: 1.4330 - regression_loss: 1.2032 - classification_loss: 0.2298
1370/1500 [==========================>...] - ETA: 1:41 - loss: 1.4329 - regression_loss: 1.2031 - classification_loss: 0.2298
1371/1500 [==========================>...] - ETA: 1:40 - loss: 1.4324 - regression_loss: 1.2027 - classification_loss: 0.2297
1372/1500 [==========================>...] - ETA: 1:39 - loss: 1.4323 - regression_loss: 1.2026 - classification_loss: 0.2297
1373/1500 [==========================>...] - ETA: 1:38 - loss: 1.4318 - regression_loss: 1.2022 - classification_loss: 0.2296
1374/1500 [==========================>...] - ETA: 1:38 - loss: 1.4320 - regression_loss: 1.2024 - classification_loss: 0.2296
1375/1500 [==========================>...] - ETA: 1:37 - loss: 1.4325 - regression_loss: 1.2029 - classification_loss: 0.2296
1376/1500 [==========================>...] - ETA: 1:36 - loss: 1.4321 - regression_loss: 1.2026 - classification_loss: 0.2295
1377/1500 [==========================>...] - ETA: 1:35 - loss: 1.4317 - regression_loss: 1.2022 - classification_loss: 0.2295
1378/1500 [==========================>...] - ETA: 1:34 - loss: 1.4315 - regression_loss: 1.2021 - classification_loss: 0.2295
1379/1500 [==========================>...] - ETA: 1:34 - loss: 1.4309 - regression_loss: 1.2016 - classification_loss: 0.2293
1380/1500 [==========================>...] - ETA: 1:33 - loss: 1.4315 - regression_loss: 1.2021 - classification_loss: 0.2294
1381/1500 [==========================>...] - ETA: 1:32 - loss: 1.4311 - regression_loss: 1.2018 - classification_loss: 0.2293
1382/1500 [==========================>...] - ETA: 1:31 - loss: 1.4310 - regression_loss: 1.2017 - classification_loss: 0.2292
1383/1500 [==========================>...] - ETA: 1:30 - loss: 1.4308 - regression_loss: 1.2015 - classification_loss: 0.2293
1384/1500 [==========================>...] - ETA: 1:30 - loss: 1.4309 - regression_loss: 1.2015 - classification_loss: 0.2293
1385/1500 [==========================>...] - ETA: 1:29 - loss: 1.4305 - regression_loss: 1.2012 - classification_loss: 0.2293
1386/1500 [==========================>...] - ETA: 1:28 - loss: 1.4300 - regression_loss: 1.2008 - classification_loss: 0.2292
1387/1500 [==========================>...] - ETA: 1:27 - loss: 1.4302 - regression_loss: 1.2011 - classification_loss: 0.2292
1388/1500 [==========================>...] - ETA: 1:26 - loss: 1.4306 - regression_loss: 1.2013 - classification_loss: 0.2292
1389/1500 [==========================>...] - ETA: 1:26 - loss: 1.4300 - regression_loss: 1.2009 - classification_loss: 0.2291
1390/1500 [==========================>...] - ETA: 1:25 - loss: 1.4307 - regression_loss: 1.2014 - classification_loss: 0.2293
1391/1500 [==========================>...] - ETA: 1:24 - loss: 1.4300 - regression_loss: 1.2009 - classification_loss: 0.2292
1392/1500 [==========================>...] - ETA: 1:23 - loss: 1.4301 - regression_loss: 1.2009 - classification_loss: 0.2292
1393/1500 [==========================>...] - ETA: 1:23 - loss: 1.4300 - regression_loss: 1.2008 - classification_loss: 0.2292
1394/1500 [==========================>...] - ETA: 1:22 - loss: 1.4303 - regression_loss: 1.2011 - classification_loss: 0.2292
1395/1500 [==========================>...] - ETA: 1:21 - loss: 1.4310 - regression_loss: 1.2017 - classification_loss: 0.2293
1396/1500 [==========================>...] - ETA: 1:20 - loss: 1.4315 - regression_loss: 1.2022 - classification_loss: 0.2293
1397/1500 [==========================>...] - ETA: 1:19 - loss: 1.4315 - regression_loss: 1.2022 - classification_loss: 0.2293
1398/1500 [==========================>...] - ETA: 1:19 - loss: 1.4311 - regression_loss: 1.2019 - classification_loss: 0.2292
1399/1500 [==========================>...] - ETA: 1:18 - loss: 1.4320 - regression_loss: 1.2024 - classification_loss: 0.2296
1400/1500 [===========================>..] - ETA: 1:17 - loss: 1.4321 - regression_loss: 1.2025 - classification_loss: 0.2296
1401/1500 [===========================>..] - ETA: 1:16 - loss: 1.4315 - regression_loss: 1.2020 - classification_loss: 0.2295
1402/1500 [===========================>..] - ETA: 1:16 - loss: 1.4313 - regression_loss: 1.2019 - classification_loss: 0.2294
1403/1500 [===========================>..] - ETA: 1:15 - loss: 1.4313 - regression_loss: 1.2019 - classification_loss: 0.2294
1404/1500 [===========================>..] - ETA: 1:14 - loss: 1.4310 - regression_loss: 1.2017 - classification_loss: 0.2293
1405/1500 [===========================>..] - ETA: 1:13 - loss: 1.4312 - regression_loss: 1.2018 - classification_loss: 0.2293
1406/1500 [===========================>..] - ETA: 1:12 - loss: 1.4310 - regression_loss: 1.2016 - classification_loss: 0.2293
1407/1500 [===========================>..] - ETA: 1:12 - loss: 1.4309 - regression_loss: 1.2016 - classification_loss: 0.2293
1408/1500 [===========================>..] - ETA: 1:11 - loss: 1.4307 - regression_loss: 1.2015 - classification_loss: 0.2292
1409/1500 [===========================>..] - ETA: 1:10 - loss: 1.4301 - regression_loss: 1.2010 - classification_loss: 0.2291
1410/1500 [===========================>..] - ETA: 1:10 - loss: 1.4296 - regression_loss: 1.2007 - classification_loss: 0.2290
1411/1500 [===========================>..] - ETA: 1:09 - loss: 1.4294 - regression_loss: 1.2005 - classification_loss: 0.2289
1412/1500 [===========================>..] - ETA: 1:08 - loss: 1.4290 - regression_loss: 1.2003 - classification_loss: 0.2287
1413/1500 [===========================>..] - ETA: 1:07 - loss: 1.4294 - regression_loss: 1.2006 - classification_loss: 0.2288
1414/1500 [===========================>..] - ETA: 1:06 - loss: 1.4288 - regression_loss: 1.2001 - classification_loss: 0.2287
1415/1500 [===========================>..] - ETA: 1:06 - loss: 1.4281 - regression_loss: 1.1995 - classification_loss: 0.2286
1416/1500 [===========================>..] - ETA: 1:05 - loss: 1.4283 - regression_loss: 1.1997 - classification_loss: 0.2286
1417/1500 [===========================>..] - ETA: 1:04 - loss: 1.4278 - regression_loss: 1.1993 - classification_loss: 0.2285
1418/1500 [===========================>..] - ETA: 1:03 - loss: 1.4280 - regression_loss: 1.1994 - classification_loss: 0.2286
1419/1500 [===========================>..] - ETA: 1:02 - loss: 1.4275 - regression_loss: 1.1989 - classification_loss: 0.2285
1420/1500 [===========================>..] - ETA: 1:02 - loss: 1.4269 - regression_loss: 1.1985 - classification_loss: 0.2284
1421/1500 [===========================>..] - ETA: 1:01 - loss: 1.4269 - regression_loss: 1.1985 - classification_loss: 0.2283
1422/1500 [===========================>..] - ETA: 1:00 - loss: 1.4269 - regression_loss: 1.1986 - classification_loss: 0.2283
1423/1500 [===========================>..] - ETA: 59s - loss: 1.4267 - regression_loss: 1.1984 - classification_loss: 0.2282 
1424/1500 [===========================>..] - ETA: 59s - loss: 1.4278 - regression_loss: 1.1994 - classification_loss: 0.2284
1425/1500 [===========================>..] - ETA: 58s - loss: 1.4282 - regression_loss: 1.1996 - classification_loss: 0.2286
1426/1500 [===========================>..] - ETA: 57s - loss: 1.4285 - regression_loss: 1.1999 - classification_loss: 0.2286
1427/1500 [===========================>..] - ETA: 56s - loss: 1.4287 - regression_loss: 1.2001 - classification_loss: 0.2286
1428/1500 [===========================>..] - ETA: 56s - loss: 1.4284 - regression_loss: 1.1999 - classification_loss: 0.2286
1429/1500 [===========================>..] - ETA: 55s - loss: 1.4286 - regression_loss: 1.2000 - classification_loss: 0.2286
1430/1500 [===========================>..] - ETA: 54s - loss: 1.4285 - regression_loss: 1.1999 - classification_loss: 0.2286
1431/1500 [===========================>..] - ETA: 53s - loss: 1.4284 - regression_loss: 1.1999 - classification_loss: 0.2285
1432/1500 [===========================>..] - ETA: 52s - loss: 1.4286 - regression_loss: 1.2001 - classification_loss: 0.2285
1433/1500 [===========================>..] - ETA: 52s - loss: 1.4283 - regression_loss: 1.1998 - classification_loss: 0.2285
1434/1500 [===========================>..] - ETA: 51s - loss: 1.4285 - regression_loss: 1.2000 - classification_loss: 0.2284
1435/1500 [===========================>..] - ETA: 50s - loss: 1.4280 - regression_loss: 1.1996 - classification_loss: 0.2283
1436/1500 [===========================>..] - ETA: 49s - loss: 1.4275 - regression_loss: 1.1993 - classification_loss: 0.2282
1437/1500 [===========================>..] - ETA: 49s - loss: 1.4269 - regression_loss: 1.1988 - classification_loss: 0.2281
1438/1500 [===========================>..] - ETA: 48s - loss: 1.4276 - regression_loss: 1.1994 - classification_loss: 0.2282
1439/1500 [===========================>..] - ETA: 47s - loss: 1.4275 - regression_loss: 1.1993 - classification_loss: 0.2282
1440/1500 [===========================>..] - ETA: 46s - loss: 1.4274 - regression_loss: 1.1993 - classification_loss: 0.2281
1441/1500 [===========================>..] - ETA: 45s - loss: 1.4281 - regression_loss: 1.1998 - classification_loss: 0.2284
1442/1500 [===========================>..] - ETA: 45s - loss: 1.4285 - regression_loss: 1.2001 - classification_loss: 0.2285
1443/1500 [===========================>..] - ETA: 44s - loss: 1.4283 - regression_loss: 1.1999 - classification_loss: 0.2284
1444/1500 [===========================>..] - ETA: 43s - loss: 1.4287 - regression_loss: 1.2002 - classification_loss: 0.2285
1445/1500 [===========================>..] - ETA: 42s - loss: 1.4291 - regression_loss: 1.2006 - classification_loss: 0.2285
1446/1500 [===========================>..] - ETA: 42s - loss: 1.4300 - regression_loss: 1.2013 - classification_loss: 0.2287
1447/1500 [===========================>..] - ETA: 41s - loss: 1.4297 - regression_loss: 1.2012 - classification_loss: 0.2286
1448/1500 [===========================>..] - ETA: 40s - loss: 1.4296 - regression_loss: 1.2010 - classification_loss: 0.2286
1449/1500 [===========================>..] - ETA: 39s - loss: 1.4296 - regression_loss: 1.2010 - classification_loss: 0.2285
1450/1500 [============================>.] - ETA: 39s - loss: 1.4303 - regression_loss: 1.2017 - classification_loss: 0.2286
1451/1500 [============================>.] - ETA: 38s - loss: 1.4305 - regression_loss: 1.2018 - classification_loss: 0.2287
1452/1500 [============================>.] - ETA: 37s - loss: 1.4305 - regression_loss: 1.2018 - classification_loss: 0.2287
1453/1500 [============================>.] - ETA: 36s - loss: 1.4309 - regression_loss: 1.2021 - classification_loss: 0.2288
1454/1500 [============================>.] - ETA: 35s - loss: 1.4316 - regression_loss: 1.2026 - classification_loss: 0.2289
1455/1500 [============================>.] - ETA: 35s - loss: 1.4316 - regression_loss: 1.2027 - classification_loss: 0.2289
1456/1500 [============================>.] - ETA: 34s - loss: 1.4317 - regression_loss: 1.2028 - classification_loss: 0.2289
1457/1500 [============================>.] - ETA: 33s - loss: 1.4323 - regression_loss: 1.2033 - classification_loss: 0.2289
1458/1500 [============================>.] - ETA: 32s - loss: 1.4317 - regression_loss: 1.2029 - classification_loss: 0.2289
1459/1500 [============================>.] - ETA: 31s - loss: 1.4320 - regression_loss: 1.2032 - classification_loss: 0.2288
1460/1500 [============================>.] - ETA: 31s - loss: 1.4317 - regression_loss: 1.2030 - classification_loss: 0.2287
1461/1500 [============================>.] - ETA: 30s - loss: 1.4316 - regression_loss: 1.2030 - classification_loss: 0.2287
1462/1500 [============================>.] - ETA: 29s - loss: 1.4311 - regression_loss: 1.2025 - classification_loss: 0.2286
1463/1500 [============================>.] - ETA: 28s - loss: 1.4309 - regression_loss: 1.2023 - classification_loss: 0.2285
1464/1500 [============================>.] - ETA: 28s - loss: 1.4308 - regression_loss: 1.2023 - classification_loss: 0.2285
1465/1500 [============================>.] - ETA: 27s - loss: 1.4302 - regression_loss: 1.2017 - classification_loss: 0.2284
1466/1500 [============================>.] - ETA: 26s - loss: 1.4299 - regression_loss: 1.2016 - classification_loss: 0.2284
1467/1500 [============================>.] - ETA: 25s - loss: 1.4298 - regression_loss: 1.2015 - classification_loss: 0.2283
1468/1500 [============================>.] - ETA: 24s - loss: 1.4293 - regression_loss: 1.2011 - classification_loss: 0.2282
1469/1500 [============================>.] - ETA: 24s - loss: 1.4293 - regression_loss: 1.2012 - classification_loss: 0.2282
1470/1500 [============================>.] - ETA: 23s - loss: 1.4292 - regression_loss: 1.2011 - classification_loss: 0.2281
1471/1500 [============================>.] - ETA: 22s - loss: 1.4294 - regression_loss: 1.2013 - classification_loss: 0.2281
1472/1500 [============================>.] - ETA: 21s - loss: 1.4298 - regression_loss: 1.2016 - classification_loss: 0.2281
1473/1500 [============================>.] - ETA: 21s - loss: 1.4298 - regression_loss: 1.2017 - classification_loss: 0.2281
1474/1500 [============================>.] - ETA: 20s - loss: 1.4301 - regression_loss: 1.2019 - classification_loss: 0.2282
1475/1500 [============================>.] - ETA: 19s - loss: 1.4308 - regression_loss: 1.2025 - classification_loss: 0.2283
1476/1500 [============================>.] - ETA: 18s - loss: 1.4310 - regression_loss: 1.2027 - classification_loss: 0.2283
1477/1500 [============================>.] - ETA: 17s - loss: 1.4313 - regression_loss: 1.2030 - classification_loss: 0.2283
1478/1500 [============================>.] - ETA: 17s - loss: 1.4313 - regression_loss: 1.2030 - classification_loss: 0.2283
1479/1500 [============================>.] - ETA: 16s - loss: 1.4310 - regression_loss: 1.2027 - classification_loss: 0.2282
1480/1500 [============================>.] - ETA: 15s - loss: 1.4312 - regression_loss: 1.2027 - classification_loss: 0.2284
1481/1500 [============================>.] - ETA: 14s - loss: 1.4310 - regression_loss: 1.2027 - classification_loss: 0.2284
1482/1500 [============================>.] - ETA: 14s - loss: 1.4313 - regression_loss: 1.2029 - classification_loss: 0.2284
1483/1500 [============================>.] - ETA: 13s - loss: 1.4308 - regression_loss: 1.2025 - classification_loss: 0.2283
1484/1500 [============================>.] - ETA: 12s - loss: 1.4307 - regression_loss: 1.2025 - classification_loss: 0.2282
1485/1500 [============================>.] - ETA: 11s - loss: 1.4302 - regression_loss: 1.2021 - classification_loss: 0.2281
1486/1500 [============================>.] - ETA: 10s - loss: 1.4298 - regression_loss: 1.2018 - classification_loss: 0.2280
1487/1500 [============================>.] - ETA: 10s - loss: 1.4295 - regression_loss: 1.2016 - classification_loss: 0.2280
1488/1500 [============================>.] - ETA: 9s - loss: 1.4299 - regression_loss: 1.2018 - classification_loss: 0.2281 
1489/1500 [============================>.] - ETA: 8s - loss: 1.4297 - regression_loss: 1.2017 - classification_loss: 0.2280
1490/1500 [============================>.] - ETA: 7s - loss: 1.4291 - regression_loss: 1.2012 - classification_loss: 0.2279
1491/1500 [============================>.] - ETA: 7s - loss: 1.4289 - regression_loss: 1.2010 - classification_loss: 0.2279
1492/1500 [============================>.] - ETA: 6s - loss: 1.4295 - regression_loss: 1.2015 - classification_loss: 0.2280
1493/1500 [============================>.] - ETA: 5s - loss: 1.4303 - regression_loss: 1.2021 - classification_loss: 0.2282
1494/1500 [============================>.] - ETA: 4s - loss: 1.4303 - regression_loss: 1.2020 - classification_loss: 0.2282
1495/1500 [============================>.] - ETA: 3s - loss: 1.4305 - regression_loss: 1.2020 - classification_loss: 0.2285
1496/1500 [============================>.] - ETA: 3s - loss: 1.4304 - regression_loss: 1.2019 - classification_loss: 0.2285
1497/1500 [============================>.] - ETA: 2s - loss: 1.4303 - regression_loss: 1.2018 - classification_loss: 0.2284
1498/1500 [============================>.] - ETA: 1s - loss: 1.4301 - regression_loss: 1.2017 - classification_loss: 0.2285
1499/1500 [============================>.] - ETA: 0s - loss: 1.4302 - regression_loss: 1.2016 - classification_loss: 0.2286
1500/1500 [==============================] - 1170s 780ms/step - loss: 1.4303 - regression_loss: 1.2017 - classification_loss: 0.2286

Epoch 00006: saving model to ./snapshots/resnet50_csv_06.h5
Epoch 7/10

   1/1500 [..............................] - ETA: 9:53 - loss: 2.0039 - regression_loss: 1.5576 - classification_loss: 0.4463
   2/1500 [..............................] - ETA: 8:59 - loss: 2.7255 - regression_loss: 2.2925 - classification_loss: 0.4330
   3/1500 [..............................] - ETA: 14:15 - loss: 2.5678 - regression_loss: 1.9739 - classification_loss: 0.5939
   4/1500 [..............................] - ETA: 13:15 - loss: 2.1840 - regression_loss: 1.7086 - classification_loss: 0.4754
   5/1500 [..............................] - ETA: 20:46 - loss: 1.8392 - regression_loss: 1.4485 - classification_loss: 0.3906
   6/1500 [..............................] - ETA: 19:27 - loss: 1.9070 - regression_loss: 1.5310 - classification_loss: 0.3760
   7/1500 [..............................] - ETA: 18:31 - loss: 1.7744 - regression_loss: 1.4409 - classification_loss: 0.3335
   8/1500 [..............................] - ETA: 20:10 - loss: 1.7398 - regression_loss: 1.4149 - classification_loss: 0.3248
   9/1500 [..............................] - ETA: 23:18 - loss: 1.7153 - regression_loss: 1.3855 - classification_loss: 0.3298
  10/1500 [..............................] - ETA: 22:37 - loss: 1.6314 - regression_loss: 1.3284 - classification_loss: 0.3030
  11/1500 [..............................] - ETA: 21:27 - loss: 1.5977 - regression_loss: 1.2986 - classification_loss: 0.2991
  12/1500 [..............................] - ETA: 20:28 - loss: 1.5433 - regression_loss: 1.2577 - classification_loss: 0.2856
  13/1500 [..............................] - ETA: 20:29 - loss: 1.5132 - regression_loss: 1.2300 - classification_loss: 0.2832
  14/1500 [..............................] - ETA: 19:38 - loss: 1.4589 - regression_loss: 1.1889 - classification_loss: 0.2700
  15/1500 [..............................] - ETA: 21:40 - loss: 1.5138 - regression_loss: 1.2290 - classification_loss: 0.2848
  16/1500 [..............................] - ETA: 21:22 - loss: 1.5188 - regression_loss: 1.2310 - classification_loss: 0.2879
  17/1500 [..............................] - ETA: 23:04 - loss: 1.5227 - regression_loss: 1.2315 - classification_loss: 0.2912
  18/1500 [..............................] - ETA: 22:22 - loss: 1.5614 - regression_loss: 1.2651 - classification_loss: 0.2963
  19/1500 [..............................] - ETA: 21:43 - loss: 1.5356 - regression_loss: 1.2508 - classification_loss: 0.2848
  20/1500 [..............................] - ETA: 21:35 - loss: 1.5715 - regression_loss: 1.2831 - classification_loss: 0.2884
  21/1500 [..............................] - ETA: 21:11 - loss: 1.5574 - regression_loss: 1.2757 - classification_loss: 0.2817
  22/1500 [..............................] - ETA: 21:11 - loss: 1.5390 - regression_loss: 1.2657 - classification_loss: 0.2734
  23/1500 [..............................] - ETA: 20:51 - loss: 1.5393 - regression_loss: 1.2718 - classification_loss: 0.2676
  24/1500 [..............................] - ETA: 21:44 - loss: 1.5525 - regression_loss: 1.2776 - classification_loss: 0.2749
  25/1500 [..............................] - ETA: 21:30 - loss: 1.5249 - regression_loss: 1.2551 - classification_loss: 0.2698
  26/1500 [..............................] - ETA: 22:02 - loss: 1.5268 - regression_loss: 1.2540 - classification_loss: 0.2728
  27/1500 [..............................] - ETA: 21:55 - loss: 1.5073 - regression_loss: 1.2388 - classification_loss: 0.2685
  28/1500 [..............................] - ETA: 21:24 - loss: 1.5212 - regression_loss: 1.2507 - classification_loss: 0.2705
  29/1500 [..............................] - ETA: 21:02 - loss: 1.4941 - regression_loss: 1.2287 - classification_loss: 0.2654
  30/1500 [..............................] - ETA: 20:38 - loss: 1.4839 - regression_loss: 1.2241 - classification_loss: 0.2599
  31/1500 [..............................] - ETA: 20:18 - loss: 1.5188 - regression_loss: 1.2581 - classification_loss: 0.2608
  32/1500 [..............................] - ETA: 19:57 - loss: 1.5231 - regression_loss: 1.2640 - classification_loss: 0.2591
  33/1500 [..............................] - ETA: 19:39 - loss: 1.5259 - regression_loss: 1.2688 - classification_loss: 0.2571
  34/1500 [..............................] - ETA: 19:35 - loss: 1.5179 - regression_loss: 1.2595 - classification_loss: 0.2584
  35/1500 [..............................] - ETA: 19:23 - loss: 1.5002 - regression_loss: 1.2458 - classification_loss: 0.2543
  36/1500 [..............................] - ETA: 19:06 - loss: 1.4911 - regression_loss: 1.2384 - classification_loss: 0.2527
  37/1500 [..............................] - ETA: 19:05 - loss: 1.4665 - regression_loss: 1.2186 - classification_loss: 0.2480
  38/1500 [..............................] - ETA: 18:51 - loss: 1.4631 - regression_loss: 1.2159 - classification_loss: 0.2472
  39/1500 [..............................] - ETA: 18:54 - loss: 1.4661 - regression_loss: 1.2185 - classification_loss: 0.2476
  40/1500 [..............................] - ETA: 19:08 - loss: 1.4526 - regression_loss: 1.2087 - classification_loss: 0.2439
  41/1500 [..............................] - ETA: 19:01 - loss: 1.4625 - regression_loss: 1.2174 - classification_loss: 0.2451
  42/1500 [..............................] - ETA: 18:45 - loss: 1.4431 - regression_loss: 1.2019 - classification_loss: 0.2413
  43/1500 [..............................] - ETA: 19:10 - loss: 1.4330 - regression_loss: 1.1951 - classification_loss: 0.2379
  44/1500 [..............................] - ETA: 18:55 - loss: 1.4398 - regression_loss: 1.2001 - classification_loss: 0.2397
  45/1500 [..............................] - ETA: 18:55 - loss: 1.4260 - regression_loss: 1.1873 - classification_loss: 0.2387
  46/1500 [..............................] - ETA: 18:54 - loss: 1.4227 - regression_loss: 1.1870 - classification_loss: 0.2357
  47/1500 [..............................] - ETA: 19:14 - loss: 1.4075 - regression_loss: 1.1757 - classification_loss: 0.2318
  48/1500 [..............................] - ETA: 19:26 - loss: 1.4205 - regression_loss: 1.1889 - classification_loss: 0.2317
  49/1500 [..............................] - ETA: 19:17 - loss: 1.4064 - regression_loss: 1.1764 - classification_loss: 0.2300
  50/1500 [>.............................] - ETA: 19:05 - loss: 1.4060 - regression_loss: 1.1764 - classification_loss: 0.2297
  51/1500 [>.............................] - ETA: 19:10 - loss: 1.4085 - regression_loss: 1.1797 - classification_loss: 0.2288
  52/1500 [>.............................] - ETA: 18:59 - loss: 1.3956 - regression_loss: 1.1690 - classification_loss: 0.2265
  53/1500 [>.............................] - ETA: 19:55 - loss: 1.4117 - regression_loss: 1.1823 - classification_loss: 0.2294
  54/1500 [>.............................] - ETA: 19:56 - loss: 1.3954 - regression_loss: 1.1687 - classification_loss: 0.2267
  55/1500 [>.............................] - ETA: 20:05 - loss: 1.3947 - regression_loss: 1.1672 - classification_loss: 0.2275
  56/1500 [>.............................] - ETA: 20:09 - loss: 1.3837 - regression_loss: 1.1589 - classification_loss: 0.2248
  57/1500 [>.............................] - ETA: 20:19 - loss: 1.3715 - regression_loss: 1.1492 - classification_loss: 0.2223
  58/1500 [>.............................] - ETA: 20:05 - loss: 1.3744 - regression_loss: 1.1520 - classification_loss: 0.2224
  59/1500 [>.............................] - ETA: 19:55 - loss: 1.3606 - regression_loss: 1.1399 - classification_loss: 0.2207
  60/1500 [>.............................] - ETA: 19:49 - loss: 1.3534 - regression_loss: 1.1350 - classification_loss: 0.2184
  61/1500 [>.............................] - ETA: 19:51 - loss: 1.3712 - regression_loss: 1.1487 - classification_loss: 0.2226
  62/1500 [>.............................] - ETA: 20:14 - loss: 1.3701 - regression_loss: 1.1471 - classification_loss: 0.2230
  63/1500 [>.............................] - ETA: 20:03 - loss: 1.3584 - regression_loss: 1.1381 - classification_loss: 0.2203
  64/1500 [>.............................] - ETA: 20:01 - loss: 1.3744 - regression_loss: 1.1544 - classification_loss: 0.2201
  65/1500 [>.............................] - ETA: 19:52 - loss: 1.3704 - regression_loss: 1.1523 - classification_loss: 0.2181
  66/1500 [>.............................] - ETA: 19:43 - loss: 1.3698 - regression_loss: 1.1519 - classification_loss: 0.2179
  67/1500 [>.............................] - ETA: 19:33 - loss: 1.3716 - regression_loss: 1.1528 - classification_loss: 0.2188
  68/1500 [>.............................] - ETA: 19:43 - loss: 1.3730 - regression_loss: 1.1542 - classification_loss: 0.2188
  69/1500 [>.............................] - ETA: 19:34 - loss: 1.3627 - regression_loss: 1.1460 - classification_loss: 0.2167
  70/1500 [>.............................] - ETA: 19:25 - loss: 1.3732 - regression_loss: 1.1553 - classification_loss: 0.2179
  71/1500 [>.............................] - ETA: 19:23 - loss: 1.3652 - regression_loss: 1.1484 - classification_loss: 0.2168
  72/1500 [>.............................] - ETA: 19:26 - loss: 1.3707 - regression_loss: 1.1531 - classification_loss: 0.2176
  73/1500 [>.............................] - ETA: 19:17 - loss: 1.3623 - regression_loss: 1.1462 - classification_loss: 0.2161
  74/1500 [>.............................] - ETA: 19:31 - loss: 1.3530 - regression_loss: 1.1389 - classification_loss: 0.2141
  75/1500 [>.............................] - ETA: 19:42 - loss: 1.3776 - regression_loss: 1.1461 - classification_loss: 0.2316
  76/1500 [>.............................] - ETA: 19:37 - loss: 1.3758 - regression_loss: 1.1453 - classification_loss: 0.2305
  77/1500 [>.............................] - ETA: 19:27 - loss: 1.3736 - regression_loss: 1.1434 - classification_loss: 0.2302
  78/1500 [>.............................] - ETA: 19:23 - loss: 1.3813 - regression_loss: 1.1468 - classification_loss: 0.2345
  79/1500 [>.............................] - ETA: 19:24 - loss: 1.3757 - regression_loss: 1.1425 - classification_loss: 0.2332
  80/1500 [>.............................] - ETA: 19:17 - loss: 1.3684 - regression_loss: 1.1368 - classification_loss: 0.2316
  81/1500 [>.............................] - ETA: 19:12 - loss: 1.3712 - regression_loss: 1.1405 - classification_loss: 0.2307
  82/1500 [>.............................] - ETA: 19:08 - loss: 1.3656 - regression_loss: 1.1364 - classification_loss: 0.2292
  83/1500 [>.............................] - ETA: 19:01 - loss: 1.3560 - regression_loss: 1.1288 - classification_loss: 0.2272
  84/1500 [>.............................] - ETA: 19:03 - loss: 1.3658 - regression_loss: 1.1372 - classification_loss: 0.2286
  85/1500 [>.............................] - ETA: 18:55 - loss: 1.3607 - regression_loss: 1.1332 - classification_loss: 0.2275
  86/1500 [>.............................] - ETA: 18:48 - loss: 1.3565 - regression_loss: 1.1294 - classification_loss: 0.2270
  87/1500 [>.............................] - ETA: 18:41 - loss: 1.3567 - regression_loss: 1.1290 - classification_loss: 0.2277
  88/1500 [>.............................] - ETA: 18:35 - loss: 1.3637 - regression_loss: 1.1350 - classification_loss: 0.2286
  89/1500 [>.............................] - ETA: 18:27 - loss: 1.3575 - regression_loss: 1.1306 - classification_loss: 0.2269
  90/1500 [>.............................] - ETA: 18:28 - loss: 1.3521 - regression_loss: 1.1264 - classification_loss: 0.2257
  91/1500 [>.............................] - ETA: 18:22 - loss: 1.3450 - regression_loss: 1.1208 - classification_loss: 0.2242
  92/1500 [>.............................] - ETA: 18:23 - loss: 1.3438 - regression_loss: 1.1203 - classification_loss: 0.2235
  93/1500 [>.............................] - ETA: 18:17 - loss: 1.3460 - regression_loss: 1.1237 - classification_loss: 0.2223
  94/1500 [>.............................] - ETA: 18:23 - loss: 1.3528 - regression_loss: 1.1310 - classification_loss: 0.2219
  95/1500 [>.............................] - ETA: 18:16 - loss: 1.3549 - regression_loss: 1.1334 - classification_loss: 0.2215
  96/1500 [>.............................] - ETA: 18:22 - loss: 1.3460 - regression_loss: 1.1261 - classification_loss: 0.2198
  97/1500 [>.............................] - ETA: 18:17 - loss: 1.3462 - regression_loss: 1.1269 - classification_loss: 0.2193
  98/1500 [>.............................] - ETA: 18:17 - loss: 1.3434 - regression_loss: 1.1250 - classification_loss: 0.2183
  99/1500 [>.............................] - ETA: 18:22 - loss: 1.3460 - regression_loss: 1.1276 - classification_loss: 0.2184
 100/1500 [=>............................] - ETA: 18:30 - loss: 1.3415 - regression_loss: 1.1245 - classification_loss: 0.2170
 101/1500 [=>............................] - ETA: 18:23 - loss: 1.3364 - regression_loss: 1.1212 - classification_loss: 0.2152
 102/1500 [=>............................] - ETA: 18:18 - loss: 1.3358 - regression_loss: 1.1205 - classification_loss: 0.2152
 103/1500 [=>............................] - ETA: 18:13 - loss: 1.3332 - regression_loss: 1.1192 - classification_loss: 0.2140
 104/1500 [=>............................] - ETA: 18:14 - loss: 1.3299 - regression_loss: 1.1169 - classification_loss: 0.2131
 105/1500 [=>............................] - ETA: 18:15 - loss: 1.3321 - regression_loss: 1.1179 - classification_loss: 0.2142
 106/1500 [=>............................] - ETA: 18:16 - loss: 1.3285 - regression_loss: 1.1149 - classification_loss: 0.2136
 107/1500 [=>............................] - ETA: 18:10 - loss: 1.3377 - regression_loss: 1.1237 - classification_loss: 0.2140
 108/1500 [=>............................] - ETA: 18:05 - loss: 1.3466 - regression_loss: 1.1320 - classification_loss: 0.2146
 109/1500 [=>............................] - ETA: 18:30 - loss: 1.3474 - regression_loss: 1.1331 - classification_loss: 0.2144
 110/1500 [=>............................] - ETA: 18:24 - loss: 1.3457 - regression_loss: 1.1290 - classification_loss: 0.2167
 111/1500 [=>............................] - ETA: 18:18 - loss: 1.3408 - regression_loss: 1.1256 - classification_loss: 0.2152
 112/1500 [=>............................] - ETA: 18:15 - loss: 1.3421 - regression_loss: 1.1277 - classification_loss: 0.2144
 113/1500 [=>............................] - ETA: 18:10 - loss: 1.3399 - regression_loss: 1.1262 - classification_loss: 0.2136
 114/1500 [=>............................] - ETA: 18:05 - loss: 1.3404 - regression_loss: 1.1270 - classification_loss: 0.2134
 115/1500 [=>............................] - ETA: 18:00 - loss: 1.3350 - regression_loss: 1.1228 - classification_loss: 0.2122
 116/1500 [=>............................] - ETA: 17:54 - loss: 1.3307 - regression_loss: 1.1187 - classification_loss: 0.2119
 117/1500 [=>............................] - ETA: 17:52 - loss: 1.3289 - regression_loss: 1.1176 - classification_loss: 0.2113
 118/1500 [=>............................] - ETA: 17:58 - loss: 1.3411 - regression_loss: 1.1265 - classification_loss: 0.2147
 119/1500 [=>............................] - ETA: 17:52 - loss: 1.3399 - regression_loss: 1.1259 - classification_loss: 0.2140
 120/1500 [=>............................] - ETA: 17:47 - loss: 1.3468 - regression_loss: 1.1308 - classification_loss: 0.2160
 121/1500 [=>............................] - ETA: 17:49 - loss: 1.3503 - regression_loss: 1.1338 - classification_loss: 0.2165
 122/1500 [=>............................] - ETA: 17:47 - loss: 1.3582 - regression_loss: 1.1400 - classification_loss: 0.2182
 123/1500 [=>............................] - ETA: 17:46 - loss: 1.3506 - regression_loss: 1.1339 - classification_loss: 0.2168
 124/1500 [=>............................] - ETA: 17:44 - loss: 1.3425 - regression_loss: 1.1270 - classification_loss: 0.2155
 125/1500 [=>............................] - ETA: 17:43 - loss: 1.3477 - regression_loss: 1.1319 - classification_loss: 0.2158
 126/1500 [=>............................] - ETA: 17:55 - loss: 1.3506 - regression_loss: 1.1331 - classification_loss: 0.2175
 127/1500 [=>............................] - ETA: 17:51 - loss: 1.3501 - regression_loss: 1.1329 - classification_loss: 0.2172
 128/1500 [=>............................] - ETA: 17:49 - loss: 1.3581 - regression_loss: 1.1397 - classification_loss: 0.2184
 129/1500 [=>............................] - ETA: 17:44 - loss: 1.3616 - regression_loss: 1.1430 - classification_loss: 0.2186
 130/1500 [=>............................] - ETA: 17:45 - loss: 1.3598 - regression_loss: 1.1421 - classification_loss: 0.2177
 131/1500 [=>............................] - ETA: 17:40 - loss: 1.3561 - regression_loss: 1.1385 - classification_loss: 0.2176
 132/1500 [=>............................] - ETA: 17:36 - loss: 1.3582 - regression_loss: 1.1413 - classification_loss: 0.2170
 133/1500 [=>............................] - ETA: 17:37 - loss: 1.3561 - regression_loss: 1.1400 - classification_loss: 0.2162
 134/1500 [=>............................] - ETA: 17:37 - loss: 1.3539 - regression_loss: 1.1372 - classification_loss: 0.2167
 135/1500 [=>............................] - ETA: 17:43 - loss: 1.3516 - regression_loss: 1.1355 - classification_loss: 0.2160
 136/1500 [=>............................] - ETA: 17:42 - loss: 1.3460 - regression_loss: 1.1312 - classification_loss: 0.2148
 137/1500 [=>............................] - ETA: 17:37 - loss: 1.3455 - regression_loss: 1.1309 - classification_loss: 0.2146
 138/1500 [=>............................] - ETA: 17:33 - loss: 1.3411 - regression_loss: 1.1274 - classification_loss: 0.2137
 139/1500 [=>............................] - ETA: 17:41 - loss: 1.3457 - regression_loss: 1.1313 - classification_loss: 0.2145
 140/1500 [=>............................] - ETA: 17:37 - loss: 1.3438 - regression_loss: 1.1301 - classification_loss: 0.2137
 141/1500 [=>............................] - ETA: 17:32 - loss: 1.3436 - regression_loss: 1.1304 - classification_loss: 0.2132
 142/1500 [=>............................] - ETA: 17:36 - loss: 1.3420 - regression_loss: 1.1290 - classification_loss: 0.2130
 143/1500 [=>............................] - ETA: 17:34 - loss: 1.3497 - regression_loss: 1.1344 - classification_loss: 0.2153
 144/1500 [=>............................] - ETA: 17:29 - loss: 1.3500 - regression_loss: 1.1342 - classification_loss: 0.2159
 145/1500 [=>............................] - ETA: 17:30 - loss: 1.3451 - regression_loss: 1.1304 - classification_loss: 0.2147
 146/1500 [=>............................] - ETA: 17:26 - loss: 1.3402 - regression_loss: 1.1267 - classification_loss: 0.2135
 147/1500 [=>............................] - ETA: 17:22 - loss: 1.3392 - regression_loss: 1.1264 - classification_loss: 0.2128
 148/1500 [=>............................] - ETA: 17:18 - loss: 1.3455 - regression_loss: 1.1309 - classification_loss: 0.2145
 149/1500 [=>............................] - ETA: 17:23 - loss: 1.3500 - regression_loss: 1.1352 - classification_loss: 0.2149
 150/1500 [==>...........................] - ETA: 17:27 - loss: 1.3473 - regression_loss: 1.1334 - classification_loss: 0.2139
 151/1500 [==>...........................] - ETA: 17:29 - loss: 1.3488 - regression_loss: 1.1355 - classification_loss: 0.2133
 152/1500 [==>...........................] - ETA: 17:25 - loss: 1.3470 - regression_loss: 1.1345 - classification_loss: 0.2126
 153/1500 [==>...........................] - ETA: 17:26 - loss: 1.3442 - regression_loss: 1.1317 - classification_loss: 0.2125
 154/1500 [==>...........................] - ETA: 17:26 - loss: 1.3472 - regression_loss: 1.1347 - classification_loss: 0.2125
 155/1500 [==>...........................] - ETA: 17:23 - loss: 1.3448 - regression_loss: 1.1329 - classification_loss: 0.2119
 156/1500 [==>...........................] - ETA: 17:24 - loss: 1.3429 - regression_loss: 1.1316 - classification_loss: 0.2113
 157/1500 [==>...........................] - ETA: 17:24 - loss: 1.3374 - regression_loss: 1.1244 - classification_loss: 0.2130
 158/1500 [==>...........................] - ETA: 17:27 - loss: 1.3485 - regression_loss: 1.1324 - classification_loss: 0.2160
 159/1500 [==>...........................] - ETA: 17:24 - loss: 1.3478 - regression_loss: 1.1324 - classification_loss: 0.2154
 160/1500 [==>...........................] - ETA: 17:20 - loss: 1.3465 - regression_loss: 1.1317 - classification_loss: 0.2148
 161/1500 [==>...........................] - ETA: 17:19 - loss: 1.3448 - regression_loss: 1.1303 - classification_loss: 0.2145
 162/1500 [==>...........................] - ETA: 17:18 - loss: 1.3507 - regression_loss: 1.1343 - classification_loss: 0.2164
 163/1500 [==>...........................] - ETA: 17:18 - loss: 1.3535 - regression_loss: 1.1361 - classification_loss: 0.2174
 164/1500 [==>...........................] - ETA: 17:26 - loss: 1.3528 - regression_loss: 1.1346 - classification_loss: 0.2182
 165/1500 [==>...........................] - ETA: 17:22 - loss: 1.3495 - regression_loss: 1.1319 - classification_loss: 0.2177
 166/1500 [==>...........................] - ETA: 17:37 - loss: 1.3592 - regression_loss: 1.1388 - classification_loss: 0.2204
 167/1500 [==>...........................] - ETA: 17:35 - loss: 1.3564 - regression_loss: 1.1367 - classification_loss: 0.2197
 168/1500 [==>...........................] - ETA: 17:38 - loss: 1.3633 - regression_loss: 1.1415 - classification_loss: 0.2218
 169/1500 [==>...........................] - ETA: 17:37 - loss: 1.3622 - regression_loss: 1.1410 - classification_loss: 0.2213
 170/1500 [==>...........................] - ETA: 17:38 - loss: 1.3615 - regression_loss: 1.1403 - classification_loss: 0.2212
 171/1500 [==>...........................] - ETA: 17:36 - loss: 1.3627 - regression_loss: 1.1414 - classification_loss: 0.2213
 172/1500 [==>...........................] - ETA: 17:34 - loss: 1.3616 - regression_loss: 1.1409 - classification_loss: 0.2207
 173/1500 [==>...........................] - ETA: 17:30 - loss: 1.3638 - regression_loss: 1.1436 - classification_loss: 0.2203
 174/1500 [==>...........................] - ETA: 17:30 - loss: 1.3623 - regression_loss: 1.1428 - classification_loss: 0.2195
 175/1500 [==>...........................] - ETA: 17:30 - loss: 1.3661 - regression_loss: 1.1463 - classification_loss: 0.2198
 176/1500 [==>...........................] - ETA: 17:28 - loss: 1.3614 - regression_loss: 1.1425 - classification_loss: 0.2189
 177/1500 [==>...........................] - ETA: 17:25 - loss: 1.3564 - regression_loss: 1.1382 - classification_loss: 0.2182
 178/1500 [==>...........................] - ETA: 17:24 - loss: 1.3557 - regression_loss: 1.1381 - classification_loss: 0.2175
 179/1500 [==>...........................] - ETA: 17:20 - loss: 1.3589 - regression_loss: 1.1406 - classification_loss: 0.2182
 180/1500 [==>...........................] - ETA: 17:16 - loss: 1.3570 - regression_loss: 1.1391 - classification_loss: 0.2179
 181/1500 [==>...........................] - ETA: 17:13 - loss: 1.3528 - regression_loss: 1.1358 - classification_loss: 0.2170
 182/1500 [==>...........................] - ETA: 17:09 - loss: 1.3567 - regression_loss: 1.1391 - classification_loss: 0.2176
 183/1500 [==>...........................] - ETA: 17:06 - loss: 1.3524 - regression_loss: 1.1355 - classification_loss: 0.2169
 184/1500 [==>...........................] - ETA: 17:03 - loss: 1.3525 - regression_loss: 1.1361 - classification_loss: 0.2164
 185/1500 [==>...........................] - ETA: 17:00 - loss: 1.3547 - regression_loss: 1.1381 - classification_loss: 0.2166
 186/1500 [==>...........................] - ETA: 16:58 - loss: 1.3538 - regression_loss: 1.1374 - classification_loss: 0.2164
 187/1500 [==>...........................] - ETA: 17:03 - loss: 1.3578 - regression_loss: 1.1406 - classification_loss: 0.2171
 188/1500 [==>...........................] - ETA: 17:05 - loss: 1.3593 - regression_loss: 1.1415 - classification_loss: 0.2178
 189/1500 [==>...........................] - ETA: 17:01 - loss: 1.3581 - regression_loss: 1.1401 - classification_loss: 0.2180
 190/1500 [==>...........................] - ETA: 16:58 - loss: 1.3532 - regression_loss: 1.1359 - classification_loss: 0.2173
 191/1500 [==>...........................] - ETA: 16:54 - loss: 1.3501 - regression_loss: 1.1331 - classification_loss: 0.2171
 192/1500 [==>...........................] - ETA: 16:53 - loss: 1.3518 - regression_loss: 1.1345 - classification_loss: 0.2173
 193/1500 [==>...........................] - ETA: 16:49 - loss: 1.3518 - regression_loss: 1.1347 - classification_loss: 0.2170
 194/1500 [==>...........................] - ETA: 16:46 - loss: 1.3601 - regression_loss: 1.1410 - classification_loss: 0.2191
 195/1500 [==>...........................] - ETA: 16:43 - loss: 1.3611 - regression_loss: 1.1415 - classification_loss: 0.2196
 196/1500 [==>...........................] - ETA: 16:40 - loss: 1.3633 - regression_loss: 1.1440 - classification_loss: 0.2193
 197/1500 [==>...........................] - ETA: 16:41 - loss: 1.3675 - regression_loss: 1.1472 - classification_loss: 0.2203
 198/1500 [==>...........................] - ETA: 16:40 - loss: 1.3635 - regression_loss: 1.1439 - classification_loss: 0.2196
 199/1500 [==>...........................] - ETA: 16:38 - loss: 1.3656 - regression_loss: 1.1449 - classification_loss: 0.2207
 200/1500 [===>..........................] - ETA: 16:36 - loss: 1.3671 - regression_loss: 1.1466 - classification_loss: 0.2206
 201/1500 [===>..........................] - ETA: 16:39 - loss: 1.3653 - regression_loss: 1.1453 - classification_loss: 0.2200
 202/1500 [===>..........................] - ETA: 16:35 - loss: 1.3653 - regression_loss: 1.1455 - classification_loss: 0.2198
 203/1500 [===>..........................] - ETA: 16:32 - loss: 1.3640 - regression_loss: 1.1446 - classification_loss: 0.2195
 204/1500 [===>..........................] - ETA: 16:29 - loss: 1.3678 - regression_loss: 1.1473 - classification_loss: 0.2205
 205/1500 [===>..........................] - ETA: 16:32 - loss: 1.3730 - regression_loss: 1.1513 - classification_loss: 0.2218
 206/1500 [===>..........................] - ETA: 16:29 - loss: 1.3742 - regression_loss: 1.1523 - classification_loss: 0.2218
 207/1500 [===>..........................] - ETA: 16:26 - loss: 1.3749 - regression_loss: 1.1531 - classification_loss: 0.2218
 208/1500 [===>..........................] - ETA: 16:23 - loss: 1.3748 - regression_loss: 1.1533 - classification_loss: 0.2214
 209/1500 [===>..........................] - ETA: 16:20 - loss: 1.3766 - regression_loss: 1.1553 - classification_loss: 0.2213
 210/1500 [===>..........................] - ETA: 16:17 - loss: 1.3766 - regression_loss: 1.1550 - classification_loss: 0.2216
 211/1500 [===>..........................] - ETA: 16:14 - loss: 1.3782 - regression_loss: 1.1563 - classification_loss: 0.2220
 212/1500 [===>..........................] - ETA: 16:11 - loss: 1.3742 - regression_loss: 1.1532 - classification_loss: 0.2211
 213/1500 [===>..........................] - ETA: 16:08 - loss: 1.3779 - regression_loss: 1.1547 - classification_loss: 0.2232
 214/1500 [===>..........................] - ETA: 16:05 - loss: 1.3776 - regression_loss: 1.1545 - classification_loss: 0.2231
 215/1500 [===>..........................] - ETA: 16:08 - loss: 1.3777 - regression_loss: 1.1549 - classification_loss: 0.2227
 216/1500 [===>..........................] - ETA: 16:06 - loss: 1.3767 - regression_loss: 1.1543 - classification_loss: 0.2224
 217/1500 [===>..........................] - ETA: 16:07 - loss: 1.3762 - regression_loss: 1.1538 - classification_loss: 0.2224
 218/1500 [===>..........................] - ETA: 16:09 - loss: 1.3755 - regression_loss: 1.1535 - classification_loss: 0.2220
 219/1500 [===>..........................] - ETA: 16:11 - loss: 1.3795 - regression_loss: 1.1567 - classification_loss: 0.2229
 220/1500 [===>..........................] - ETA: 16:08 - loss: 1.3757 - regression_loss: 1.1535 - classification_loss: 0.2222
 221/1500 [===>..........................] - ETA: 16:08 - loss: 1.3736 - regression_loss: 1.1519 - classification_loss: 0.2217
 222/1500 [===>..........................] - ETA: 16:14 - loss: 1.3779 - regression_loss: 1.1555 - classification_loss: 0.2224
 223/1500 [===>..........................] - ETA: 16:12 - loss: 1.3801 - regression_loss: 1.1576 - classification_loss: 0.2225
 224/1500 [===>..........................] - ETA: 16:09 - loss: 1.3781 - regression_loss: 1.1559 - classification_loss: 0.2223
 225/1500 [===>..........................] - ETA: 16:07 - loss: 1.3796 - regression_loss: 1.1566 - classification_loss: 0.2229
 226/1500 [===>..........................] - ETA: 16:08 - loss: 1.3801 - regression_loss: 1.1573 - classification_loss: 0.2228
 227/1500 [===>..........................] - ETA: 16:07 - loss: 1.3781 - regression_loss: 1.1559 - classification_loss: 0.2222
 228/1500 [===>..........................] - ETA: 16:04 - loss: 1.3764 - regression_loss: 1.1547 - classification_loss: 0.2217
 229/1500 [===>..........................] - ETA: 16:02 - loss: 1.3773 - regression_loss: 1.1555 - classification_loss: 0.2218
 230/1500 [===>..........................] - ETA: 16:01 - loss: 1.3739 - regression_loss: 1.1527 - classification_loss: 0.2212
 231/1500 [===>..........................] - ETA: 15:58 - loss: 1.3717 - regression_loss: 1.1511 - classification_loss: 0.2206
 232/1500 [===>..........................] - ETA: 16:00 - loss: 1.3685 - regression_loss: 1.1485 - classification_loss: 0.2200
 233/1500 [===>..........................] - ETA: 16:04 - loss: 1.3651 - regression_loss: 1.1457 - classification_loss: 0.2194
 234/1500 [===>..........................] - ETA: 16:04 - loss: 1.3630 - regression_loss: 1.1440 - classification_loss: 0.2190
 235/1500 [===>..........................] - ETA: 16:01 - loss: 1.3643 - regression_loss: 1.1453 - classification_loss: 0.2191
 236/1500 [===>..........................] - ETA: 16:03 - loss: 1.3696 - regression_loss: 1.1492 - classification_loss: 0.2204
 237/1500 [===>..........................] - ETA: 16:01 - loss: 1.3670 - regression_loss: 1.1471 - classification_loss: 0.2199
 238/1500 [===>..........................] - ETA: 15:58 - loss: 1.3658 - regression_loss: 1.1462 - classification_loss: 0.2195
 239/1500 [===>..........................] - ETA: 15:56 - loss: 1.3652 - regression_loss: 1.1458 - classification_loss: 0.2194
 240/1500 [===>..........................] - ETA: 15:53 - loss: 1.3629 - regression_loss: 1.1440 - classification_loss: 0.2190
 241/1500 [===>..........................] - ETA: 15:50 - loss: 1.3613 - regression_loss: 1.1429 - classification_loss: 0.2185
 242/1500 [===>..........................] - ETA: 15:48 - loss: 1.3582 - regression_loss: 1.1404 - classification_loss: 0.2178
 243/1500 [===>..........................] - ETA: 15:50 - loss: 1.3568 - regression_loss: 1.1383 - classification_loss: 0.2185
 244/1500 [===>..........................] - ETA: 15:48 - loss: 1.3552 - regression_loss: 1.1365 - classification_loss: 0.2187
 245/1500 [===>..........................] - ETA: 15:45 - loss: 1.3568 - regression_loss: 1.1373 - classification_loss: 0.2195
 246/1500 [===>..........................] - ETA: 15:43 - loss: 1.3549 - regression_loss: 1.1357 - classification_loss: 0.2192
 247/1500 [===>..........................] - ETA: 15:40 - loss: 1.3537 - regression_loss: 1.1349 - classification_loss: 0.2188
 248/1500 [===>..........................] - ETA: 15:38 - loss: 1.3536 - regression_loss: 1.1348 - classification_loss: 0.2187
 249/1500 [===>..........................] - ETA: 15:38 - loss: 1.3551 - regression_loss: 1.1361 - classification_loss: 0.2189
 250/1500 [====>.........................] - ETA: 15:35 - loss: 1.3528 - regression_loss: 1.1345 - classification_loss: 0.2184
 251/1500 [====>.........................] - ETA: 15:43 - loss: 1.3506 - regression_loss: 1.1326 - classification_loss: 0.2180
 252/1500 [====>.........................] - ETA: 15:47 - loss: 1.3512 - regression_loss: 1.1324 - classification_loss: 0.2188
 253/1500 [====>.........................] - ETA: 15:44 - loss: 1.3524 - regression_loss: 1.1333 - classification_loss: 0.2191
 254/1500 [====>.........................] - ETA: 15:51 - loss: 1.3561 - regression_loss: 1.1359 - classification_loss: 0.2203
 255/1500 [====>.........................] - ETA: 15:50 - loss: 1.3540 - regression_loss: 1.1343 - classification_loss: 0.2197
 256/1500 [====>.........................] - ETA: 15:52 - loss: 1.3585 - regression_loss: 1.1376 - classification_loss: 0.2209
 257/1500 [====>.........................] - ETA: 15:59 - loss: 1.3603 - regression_loss: 1.1393 - classification_loss: 0.2210
 258/1500 [====>.........................] - ETA: 15:56 - loss: 1.3577 - regression_loss: 1.1373 - classification_loss: 0.2204
 259/1500 [====>.........................] - ETA: 15:54 - loss: 1.3579 - regression_loss: 1.1373 - classification_loss: 0.2206
 260/1500 [====>.........................] - ETA: 15:54 - loss: 1.3601 - regression_loss: 1.1397 - classification_loss: 0.2204
 261/1500 [====>.........................] - ETA: 15:51 - loss: 1.3562 - regression_loss: 1.1364 - classification_loss: 0.2197
 262/1500 [====>.........................] - ETA: 15:49 - loss: 1.3599 - regression_loss: 1.1392 - classification_loss: 0.2207
 263/1500 [====>.........................] - ETA: 15:47 - loss: 1.3607 - regression_loss: 1.1387 - classification_loss: 0.2220
 264/1500 [====>.........................] - ETA: 15:44 - loss: 1.3608 - regression_loss: 1.1386 - classification_loss: 0.2222
 265/1500 [====>.........................] - ETA: 15:49 - loss: 1.3641 - regression_loss: 1.1411 - classification_loss: 0.2230
 266/1500 [====>.........................] - ETA: 15:47 - loss: 1.3645 - regression_loss: 1.1418 - classification_loss: 0.2227
 267/1500 [====>.........................] - ETA: 15:55 - loss: 1.3635 - regression_loss: 1.1410 - classification_loss: 0.2225
 268/1500 [====>.........................] - ETA: 15:56 - loss: 1.3645 - regression_loss: 1.1417 - classification_loss: 0.2228
 269/1500 [====>.........................] - ETA: 15:54 - loss: 1.3678 - regression_loss: 1.1443 - classification_loss: 0.2235
 270/1500 [====>.........................] - ETA: 15:51 - loss: 1.3717 - regression_loss: 1.1477 - classification_loss: 0.2241
 271/1500 [====>.........................] - ETA: 15:52 - loss: 1.3733 - regression_loss: 1.1495 - classification_loss: 0.2239
 272/1500 [====>.........................] - ETA: 15:49 - loss: 1.3732 - regression_loss: 1.1494 - classification_loss: 0.2237
 273/1500 [====>.........................] - ETA: 15:47 - loss: 1.3715 - regression_loss: 1.1480 - classification_loss: 0.2235
 274/1500 [====>.........................] - ETA: 15:45 - loss: 1.3711 - regression_loss: 1.1477 - classification_loss: 0.2234
 275/1500 [====>.........................] - ETA: 15:48 - loss: 1.3731 - regression_loss: 1.1497 - classification_loss: 0.2234
 276/1500 [====>.........................] - ETA: 15:47 - loss: 1.3745 - regression_loss: 1.1510 - classification_loss: 0.2235
 277/1500 [====>.........................] - ETA: 15:44 - loss: 1.3742 - regression_loss: 1.1510 - classification_loss: 0.2232
 278/1500 [====>.........................] - ETA: 15:44 - loss: 1.3751 - regression_loss: 1.1517 - classification_loss: 0.2234
 279/1500 [====>.........................] - ETA: 15:41 - loss: 1.3743 - regression_loss: 1.1513 - classification_loss: 0.2230
 280/1500 [====>.........................] - ETA: 15:41 - loss: 1.3737 - regression_loss: 1.1509 - classification_loss: 0.2228
 281/1500 [====>.........................] - ETA: 15:42 - loss: 1.3706 - regression_loss: 1.1484 - classification_loss: 0.2222
 282/1500 [====>.........................] - ETA: 15:40 - loss: 1.3718 - regression_loss: 1.1497 - classification_loss: 0.2221
 283/1500 [====>.........................] - ETA: 15:38 - loss: 1.3715 - regression_loss: 1.1495 - classification_loss: 0.2220
 284/1500 [====>.........................] - ETA: 15:40 - loss: 1.3701 - regression_loss: 1.1484 - classification_loss: 0.2217
 285/1500 [====>.........................] - ETA: 15:37 - loss: 1.3673 - regression_loss: 1.1461 - classification_loss: 0.2213
 286/1500 [====>.........................] - ETA: 15:37 - loss: 1.3654 - regression_loss: 1.1445 - classification_loss: 0.2210
 287/1500 [====>.........................] - ETA: 15:35 - loss: 1.3636 - regression_loss: 1.1425 - classification_loss: 0.2211
 288/1500 [====>.........................] - ETA: 15:33 - loss: 1.3659 - regression_loss: 1.1445 - classification_loss: 0.2213
 289/1500 [====>.........................] - ETA: 15:35 - loss: 1.3685 - regression_loss: 1.1467 - classification_loss: 0.2218
 290/1500 [====>.........................] - ETA: 15:33 - loss: 1.3694 - regression_loss: 1.1472 - classification_loss: 0.2222
 291/1500 [====>.........................] - ETA: 15:30 - loss: 1.3708 - regression_loss: 1.1485 - classification_loss: 0.2223
 292/1500 [====>.........................] - ETA: 15:32 - loss: 1.3716 - regression_loss: 1.1491 - classification_loss: 0.2225
 293/1500 [====>.........................] - ETA: 15:30 - loss: 1.3696 - regression_loss: 1.1475 - classification_loss: 0.2221
 294/1500 [====>.........................] - ETA: 15:28 - loss: 1.3682 - regression_loss: 1.1466 - classification_loss: 0.2216
 295/1500 [====>.........................] - ETA: 15:26 - loss: 1.3731 - regression_loss: 1.1488 - classification_loss: 0.2243
 296/1500 [====>.........................] - ETA: 15:27 - loss: 1.3780 - regression_loss: 1.1526 - classification_loss: 0.2254
 297/1500 [====>.........................] - ETA: 15:25 - loss: 1.3773 - regression_loss: 1.1521 - classification_loss: 0.2252
 298/1500 [====>.........................] - ETA: 15:23 - loss: 1.3758 - regression_loss: 1.1509 - classification_loss: 0.2250
 299/1500 [====>.........................] - ETA: 15:30 - loss: 1.3753 - regression_loss: 1.1506 - classification_loss: 0.2248
 300/1500 [=====>........................] - ETA: 15:31 - loss: 1.3758 - regression_loss: 1.1515 - classification_loss: 0.2244
 301/1500 [=====>........................] - ETA: 15:30 - loss: 1.3754 - regression_loss: 1.1515 - classification_loss: 0.2240
 302/1500 [=====>........................] - ETA: 15:27 - loss: 1.3743 - regression_loss: 1.1503 - classification_loss: 0.2239
 303/1500 [=====>........................] - ETA: 15:25 - loss: 1.3780 - regression_loss: 1.1538 - classification_loss: 0.2242
 304/1500 [=====>........................] - ETA: 15:24 - loss: 1.3768 - regression_loss: 1.1531 - classification_loss: 0.2237
 305/1500 [=====>........................] - ETA: 15:23 - loss: 1.3771 - regression_loss: 1.1536 - classification_loss: 0.2235
 306/1500 [=====>........................] - ETA: 15:24 - loss: 1.3759 - regression_loss: 1.1527 - classification_loss: 0.2232
 307/1500 [=====>........................] - ETA: 15:25 - loss: 1.3772 - regression_loss: 1.1533 - classification_loss: 0.2238
 308/1500 [=====>........................] - ETA: 15:25 - loss: 1.3767 - regression_loss: 1.1530 - classification_loss: 0.2237
 309/1500 [=====>........................] - ETA: 15:25 - loss: 1.3775 - regression_loss: 1.1537 - classification_loss: 0.2238
 310/1500 [=====>........................] - ETA: 15:23 - loss: 1.3761 - regression_loss: 1.1524 - classification_loss: 0.2237
 311/1500 [=====>........................] - ETA: 15:21 - loss: 1.3754 - regression_loss: 1.1516 - classification_loss: 0.2239
 312/1500 [=====>........................] - ETA: 15:19 - loss: 1.3770 - regression_loss: 1.1525 - classification_loss: 0.2245
 313/1500 [=====>........................] - ETA: 15:19 - loss: 1.3766 - regression_loss: 1.1521 - classification_loss: 0.2245
 314/1500 [=====>........................] - ETA: 15:17 - loss: 1.3758 - regression_loss: 1.1515 - classification_loss: 0.2243
 315/1500 [=====>........................] - ETA: 15:15 - loss: 1.3746 - regression_loss: 1.1506 - classification_loss: 0.2240
 316/1500 [=====>........................] - ETA: 15:13 - loss: 1.3735 - regression_loss: 1.1497 - classification_loss: 0.2238
 317/1500 [=====>........................] - ETA: 15:11 - loss: 1.3746 - regression_loss: 1.1509 - classification_loss: 0.2237
 318/1500 [=====>........................] - ETA: 15:09 - loss: 1.3757 - regression_loss: 1.1519 - classification_loss: 0.2238
 319/1500 [=====>........................] - ETA: 15:12 - loss: 1.3737 - regression_loss: 1.1503 - classification_loss: 0.2234
 320/1500 [=====>........................] - ETA: 15:10 - loss: 1.3716 - regression_loss: 1.1487 - classification_loss: 0.2229
 321/1500 [=====>........................] - ETA: 15:09 - loss: 1.3726 - regression_loss: 1.1499 - classification_loss: 0.2227
 322/1500 [=====>........................] - ETA: 15:09 - loss: 1.3744 - regression_loss: 1.1517 - classification_loss: 0.2227
 323/1500 [=====>........................] - ETA: 15:09 - loss: 1.3751 - regression_loss: 1.1522 - classification_loss: 0.2229
 324/1500 [=====>........................] - ETA: 15:08 - loss: 1.3743 - regression_loss: 1.1518 - classification_loss: 0.2224
 325/1500 [=====>........................] - ETA: 15:06 - loss: 1.3735 - regression_loss: 1.1512 - classification_loss: 0.2224
 326/1500 [=====>........................] - ETA: 15:04 - loss: 1.3742 - regression_loss: 1.1520 - classification_loss: 0.2222
 327/1500 [=====>........................] - ETA: 15:03 - loss: 1.3757 - regression_loss: 1.1535 - classification_loss: 0.2222
 328/1500 [=====>........................] - ETA: 15:01 - loss: 1.3777 - regression_loss: 1.1547 - classification_loss: 0.2230
 329/1500 [=====>........................] - ETA: 15:01 - loss: 1.3759 - regression_loss: 1.1533 - classification_loss: 0.2226
 330/1500 [=====>........................] - ETA: 14:59 - loss: 1.3773 - regression_loss: 1.1548 - classification_loss: 0.2225
 331/1500 [=====>........................] - ETA: 14:57 - loss: 1.3767 - regression_loss: 1.1541 - classification_loss: 0.2225
 332/1500 [=====>........................] - ETA: 14:55 - loss: 1.3765 - regression_loss: 1.1531 - classification_loss: 0.2235
 333/1500 [=====>........................] - ETA: 15:02 - loss: 1.3768 - regression_loss: 1.1534 - classification_loss: 0.2234
 334/1500 [=====>........................] - ETA: 15:00 - loss: 1.3748 - regression_loss: 1.1518 - classification_loss: 0.2230
 335/1500 [=====>........................] - ETA: 14:58 - loss: 1.3779 - regression_loss: 1.1539 - classification_loss: 0.2240
 336/1500 [=====>........................] - ETA: 15:01 - loss: 1.3780 - regression_loss: 1.1539 - classification_loss: 0.2242
 337/1500 [=====>........................] - ETA: 15:03 - loss: 1.3761 - regression_loss: 1.1525 - classification_loss: 0.2236
 338/1500 [=====>........................] - ETA: 15:01 - loss: 1.3736 - regression_loss: 1.1505 - classification_loss: 0.2231
 339/1500 [=====>........................] - ETA: 14:59 - loss: 1.3749 - regression_loss: 1.1518 - classification_loss: 0.2232
 340/1500 [=====>........................] - ETA: 15:00 - loss: 1.3745 - regression_loss: 1.1517 - classification_loss: 0.2227
 341/1500 [=====>........................] - ETA: 15:02 - loss: 1.3738 - regression_loss: 1.1510 - classification_loss: 0.2229
 342/1500 [=====>........................] - ETA: 14:59 - loss: 1.3761 - regression_loss: 1.1526 - classification_loss: 0.2235
 343/1500 [=====>........................] - ETA: 14:57 - loss: 1.3769 - regression_loss: 1.1531 - classification_loss: 0.2238
 344/1500 [=====>........................] - ETA: 14:59 - loss: 1.3744 - regression_loss: 1.1510 - classification_loss: 0.2233
 345/1500 [=====>........................] - ETA: 14:58 - loss: 1.3749 - regression_loss: 1.1512 - classification_loss: 0.2236
 346/1500 [=====>........................] - ETA: 15:01 - loss: 1.3772 - regression_loss: 1.1527 - classification_loss: 0.2245
 347/1500 [=====>........................] - ETA: 14:59 - loss: 1.3780 - regression_loss: 1.1534 - classification_loss: 0.2246
 348/1500 [=====>........................] - ETA: 14:57 - loss: 1.3783 - regression_loss: 1.1537 - classification_loss: 0.2246
 349/1500 [=====>........................] - ETA: 14:55 - loss: 1.3772 - regression_loss: 1.1529 - classification_loss: 0.2243
 350/1500 [======>.......................] - ETA: 14:54 - loss: 1.3769 - regression_loss: 1.1526 - classification_loss: 0.2243
 351/1500 [======>.......................] - ETA: 14:57 - loss: 1.3757 - regression_loss: 1.1517 - classification_loss: 0.2241
 352/1500 [======>.......................] - ETA: 14:57 - loss: 1.3738 - regression_loss: 1.1501 - classification_loss: 0.2237
 353/1500 [======>.......................] - ETA: 14:55 - loss: 1.3718 - regression_loss: 1.1485 - classification_loss: 0.2233
 354/1500 [======>.......................] - ETA: 14:54 - loss: 1.3706 - regression_loss: 1.1475 - classification_loss: 0.2231
 355/1500 [======>.......................] - ETA: 14:52 - loss: 1.3719 - regression_loss: 1.1490 - classification_loss: 0.2230
 356/1500 [======>.......................] - ETA: 14:50 - loss: 1.3706 - regression_loss: 1.1477 - classification_loss: 0.2229
 357/1500 [======>.......................] - ETA: 14:49 - loss: 1.3691 - regression_loss: 1.1467 - classification_loss: 0.2224
 358/1500 [======>.......................] - ETA: 14:48 - loss: 1.3680 - regression_loss: 1.1455 - classification_loss: 0.2225
 359/1500 [======>.......................] - ETA: 14:47 - loss: 1.3655 - regression_loss: 1.1434 - classification_loss: 0.2221
 360/1500 [======>.......................] - ETA: 14:45 - loss: 1.3631 - regression_loss: 1.1414 - classification_loss: 0.2217
 361/1500 [======>.......................] - ETA: 14:47 - loss: 1.3648 - regression_loss: 1.1430 - classification_loss: 0.2218
 362/1500 [======>.......................] - ETA: 14:51 - loss: 1.3654 - regression_loss: 1.1435 - classification_loss: 0.2219
 363/1500 [======>.......................] - ETA: 14:49 - loss: 1.3672 - regression_loss: 1.1451 - classification_loss: 0.2222
 364/1500 [======>.......................] - ETA: 14:48 - loss: 1.3671 - regression_loss: 1.1451 - classification_loss: 0.2220
 365/1500 [======>.......................] - ETA: 14:46 - loss: 1.3670 - regression_loss: 1.1450 - classification_loss: 0.2220
 366/1500 [======>.......................] - ETA: 14:43 - loss: 1.3656 - regression_loss: 1.1438 - classification_loss: 0.2218
 367/1500 [======>.......................] - ETA: 14:43 - loss: 1.3677 - regression_loss: 1.1455 - classification_loss: 0.2222
 368/1500 [======>.......................] - ETA: 14:41 - loss: 1.3670 - regression_loss: 1.1447 - classification_loss: 0.2223
 369/1500 [======>.......................] - ETA: 14:40 - loss: 1.3652 - regression_loss: 1.1433 - classification_loss: 0.2219
 370/1500 [======>.......................] - ETA: 14:38 - loss: 1.3637 - regression_loss: 1.1422 - classification_loss: 0.2214
 371/1500 [======>.......................] - ETA: 14:37 - loss: 1.3626 - regression_loss: 1.1413 - classification_loss: 0.2212
 372/1500 [======>.......................] - ETA: 14:35 - loss: 1.3623 - regression_loss: 1.1408 - classification_loss: 0.2215
 373/1500 [======>.......................] - ETA: 14:33 - loss: 1.3621 - regression_loss: 1.1405 - classification_loss: 0.2216
 374/1500 [======>.......................] - ETA: 14:37 - loss: 1.3617 - regression_loss: 1.1402 - classification_loss: 0.2215
 375/1500 [======>.......................] - ETA: 14:35 - loss: 1.3601 - regression_loss: 1.1390 - classification_loss: 0.2211
 376/1500 [======>.......................] - ETA: 14:33 - loss: 1.3606 - regression_loss: 1.1391 - classification_loss: 0.2215
 377/1500 [======>.......................] - ETA: 14:34 - loss: 1.3593 - regression_loss: 1.1374 - classification_loss: 0.2219
 378/1500 [======>.......................] - ETA: 14:34 - loss: 1.3598 - regression_loss: 1.1379 - classification_loss: 0.2219
 379/1500 [======>.......................] - ETA: 14:32 - loss: 1.3601 - regression_loss: 1.1385 - classification_loss: 0.2216
 380/1500 [======>.......................] - ETA: 14:30 - loss: 1.3600 - regression_loss: 1.1386 - classification_loss: 0.2214
 381/1500 [======>.......................] - ETA: 14:29 - loss: 1.3596 - regression_loss: 1.1385 - classification_loss: 0.2211
 382/1500 [======>.......................] - ETA: 14:27 - loss: 1.3580 - regression_loss: 1.1373 - classification_loss: 0.2207
 383/1500 [======>.......................] - ETA: 14:29 - loss: 1.3616 - regression_loss: 1.1401 - classification_loss: 0.2214
 384/1500 [======>.......................] - ETA: 14:31 - loss: 1.3606 - regression_loss: 1.1394 - classification_loss: 0.2212
 385/1500 [======>.......................] - ETA: 14:31 - loss: 1.3613 - regression_loss: 1.1393 - classification_loss: 0.2220
 386/1500 [======>.......................] - ETA: 14:29 - loss: 1.3623 - regression_loss: 1.1404 - classification_loss: 0.2219
 387/1500 [======>.......................] - ETA: 14:28 - loss: 1.3609 - regression_loss: 1.1394 - classification_loss: 0.2215
 388/1500 [======>.......................] - ETA: 14:27 - loss: 1.3589 - regression_loss: 1.1379 - classification_loss: 0.2210
 389/1500 [======>.......................] - ETA: 14:25 - loss: 1.3569 - regression_loss: 1.1363 - classification_loss: 0.2206
 390/1500 [======>.......................] - ETA: 14:23 - loss: 1.3559 - regression_loss: 1.1355 - classification_loss: 0.2204
 391/1500 [======>.......................] - ETA: 14:23 - loss: 1.3567 - regression_loss: 1.1363 - classification_loss: 0.2204
 392/1500 [======>.......................] - ETA: 14:22 - loss: 1.3555 - regression_loss: 1.1354 - classification_loss: 0.2201
 393/1500 [======>.......................] - ETA: 14:21 - loss: 1.3569 - regression_loss: 1.1364 - classification_loss: 0.2205
 394/1500 [======>.......................] - ETA: 14:20 - loss: 1.3573 - regression_loss: 1.1357 - classification_loss: 0.2216
 395/1500 [======>.......................] - ETA: 14:19 - loss: 1.3568 - regression_loss: 1.1353 - classification_loss: 0.2215
 396/1500 [======>.......................] - ETA: 14:18 - loss: 1.3557 - regression_loss: 1.1344 - classification_loss: 0.2212
 397/1500 [======>.......................] - ETA: 14:21 - loss: 1.3542 - regression_loss: 1.1334 - classification_loss: 0.2208
 398/1500 [======>.......................] - ETA: 14:19 - loss: 1.3531 - regression_loss: 1.1326 - classification_loss: 0.2205
 399/1500 [======>.......................] - ETA: 14:17 - loss: 1.3527 - regression_loss: 1.1324 - classification_loss: 0.2203
 400/1500 [=======>......................] - ETA: 14:18 - loss: 1.3510 - regression_loss: 1.1310 - classification_loss: 0.2200
 401/1500 [=======>......................] - ETA: 14:18 - loss: 1.3522 - regression_loss: 1.1315 - classification_loss: 0.2206
 402/1500 [=======>......................] - ETA: 14:17 - loss: 1.3526 - regression_loss: 1.1320 - classification_loss: 0.2206
 403/1500 [=======>......................] - ETA: 14:20 - loss: 1.3511 - regression_loss: 1.1309 - classification_loss: 0.2203
 404/1500 [=======>......................] - ETA: 14:18 - loss: 1.3501 - regression_loss: 1.1300 - classification_loss: 0.2201
 405/1500 [=======>......................] - ETA: 14:17 - loss: 1.3503 - regression_loss: 1.1301 - classification_loss: 0.2202
 406/1500 [=======>......................] - ETA: 14:17 - loss: 1.3494 - regression_loss: 1.1296 - classification_loss: 0.2197
 407/1500 [=======>......................] - ETA: 14:17 - loss: 1.3508 - regression_loss: 1.1310 - classification_loss: 0.2199
 408/1500 [=======>......................] - ETA: 14:15 - loss: 1.3518 - regression_loss: 1.1320 - classification_loss: 0.2199
 409/1500 [=======>......................] - ETA: 14:16 - loss: 1.3560 - regression_loss: 1.1348 - classification_loss: 0.2211
 410/1500 [=======>......................] - ETA: 14:16 - loss: 1.3593 - regression_loss: 1.1379 - classification_loss: 0.2214
 411/1500 [=======>......................] - ETA: 14:16 - loss: 1.3589 - regression_loss: 1.1372 - classification_loss: 0.2217
 412/1500 [=======>......................] - ETA: 14:17 - loss: 1.3594 - regression_loss: 1.1375 - classification_loss: 0.2218
 413/1500 [=======>......................] - ETA: 14:15 - loss: 1.3577 - regression_loss: 1.1358 - classification_loss: 0.2219
 414/1500 [=======>......................] - ETA: 14:16 - loss: 1.3564 - regression_loss: 1.1348 - classification_loss: 0.2216
 415/1500 [=======>......................] - ETA: 14:14 - loss: 1.3550 - regression_loss: 1.1337 - classification_loss: 0.2213
 416/1500 [=======>......................] - ETA: 14:13 - loss: 1.3527 - regression_loss: 1.1319 - classification_loss: 0.2208
 417/1500 [=======>......................] - ETA: 14:11 - loss: 1.3524 - regression_loss: 1.1317 - classification_loss: 0.2207
 418/1500 [=======>......................] - ETA: 14:09 - loss: 1.3520 - regression_loss: 1.1313 - classification_loss: 0.2206
 419/1500 [=======>......................] - ETA: 14:07 - loss: 1.3506 - regression_loss: 1.1303 - classification_loss: 0.2202
 420/1500 [=======>......................] - ETA: 14:07 - loss: 1.3506 - regression_loss: 1.1306 - classification_loss: 0.2201
 421/1500 [=======>......................] - ETA: 14:07 - loss: 1.3501 - regression_loss: 1.1302 - classification_loss: 0.2198
 422/1500 [=======>......................] - ETA: 14:06 - loss: 1.3532 - regression_loss: 1.1328 - classification_loss: 0.2204
 423/1500 [=======>......................] - ETA: 14:06 - loss: 1.3515 - regression_loss: 1.1315 - classification_loss: 0.2201
 424/1500 [=======>......................] - ETA: 14:05 - loss: 1.3507 - regression_loss: 1.1309 - classification_loss: 0.2199
 425/1500 [=======>......................] - ETA: 14:08 - loss: 1.3517 - regression_loss: 1.1315 - classification_loss: 0.2202
 426/1500 [=======>......................] - ETA: 14:06 - loss: 1.3517 - regression_loss: 1.1315 - classification_loss: 0.2202
 427/1500 [=======>......................] - ETA: 14:06 - loss: 1.3506 - regression_loss: 1.1306 - classification_loss: 0.2199
 428/1500 [=======>......................] - ETA: 14:06 - loss: 1.3489 - regression_loss: 1.1292 - classification_loss: 0.2197
 429/1500 [=======>......................] - ETA: 14:04 - loss: 1.3478 - regression_loss: 1.1284 - classification_loss: 0.2194
 430/1500 [=======>......................] - ETA: 14:02 - loss: 1.3480 - regression_loss: 1.1288 - classification_loss: 0.2192
 431/1500 [=======>......................] - ETA: 14:02 - loss: 1.3473 - regression_loss: 1.1283 - classification_loss: 0.2190
 432/1500 [=======>......................] - ETA: 14:01 - loss: 1.3469 - regression_loss: 1.1281 - classification_loss: 0.2187
 433/1500 [=======>......................] - ETA: 13:59 - loss: 1.3458 - regression_loss: 1.1274 - classification_loss: 0.2184
 434/1500 [=======>......................] - ETA: 13:58 - loss: 1.3445 - regression_loss: 1.1263 - classification_loss: 0.2182
 435/1500 [=======>......................] - ETA: 13:57 - loss: 1.3434 - regression_loss: 1.1255 - classification_loss: 0.2178
 436/1500 [=======>......................] - ETA: 13:56 - loss: 1.3413 - regression_loss: 1.1238 - classification_loss: 0.2175
 437/1500 [=======>......................] - ETA: 13:57 - loss: 1.3438 - regression_loss: 1.1256 - classification_loss: 0.2181
 438/1500 [=======>......................] - ETA: 13:57 - loss: 1.3449 - regression_loss: 1.1267 - classification_loss: 0.2182
 439/1500 [=======>......................] - ETA: 13:56 - loss: 1.3471 - regression_loss: 1.1285 - classification_loss: 0.2186
 440/1500 [=======>......................] - ETA: 13:54 - loss: 1.3476 - regression_loss: 1.1293 - classification_loss: 0.2184
 441/1500 [=======>......................] - ETA: 13:53 - loss: 1.3466 - regression_loss: 1.1285 - classification_loss: 0.2180
 442/1500 [=======>......................] - ETA: 13:52 - loss: 1.3473 - regression_loss: 1.1289 - classification_loss: 0.2183
 443/1500 [=======>......................] - ETA: 13:54 - loss: 1.3481 - regression_loss: 1.1298 - classification_loss: 0.2183
 444/1500 [=======>......................] - ETA: 13:55 - loss: 1.3484 - regression_loss: 1.1302 - classification_loss: 0.2182
 445/1500 [=======>......................] - ETA: 13:57 - loss: 1.3480 - regression_loss: 1.1300 - classification_loss: 0.2179
 446/1500 [=======>......................] - ETA: 13:58 - loss: 1.3496 - regression_loss: 1.1312 - classification_loss: 0.2183
 447/1500 [=======>......................] - ETA: 13:59 - loss: 1.3512 - regression_loss: 1.1327 - classification_loss: 0.2185
 448/1500 [=======>......................] - ETA: 13:58 - loss: 1.3502 - regression_loss: 1.1320 - classification_loss: 0.2182
 449/1500 [=======>......................] - ETA: 13:57 - loss: 1.3501 - regression_loss: 1.1321 - classification_loss: 0.2180
 450/1500 [========>.....................] - ETA: 13:58 - loss: 1.3489 - regression_loss: 1.1312 - classification_loss: 0.2177
 451/1500 [========>.....................] - ETA: 13:56 - loss: 1.3479 - regression_loss: 1.1305 - classification_loss: 0.2174
 452/1500 [========>.....................] - ETA: 13:55 - loss: 1.3482 - regression_loss: 1.1306 - classification_loss: 0.2175
 453/1500 [========>.....................] - ETA: 13:53 - loss: 1.3469 - regression_loss: 1.1298 - classification_loss: 0.2171
 454/1500 [========>.....................] - ETA: 13:51 - loss: 1.3478 - regression_loss: 1.1306 - classification_loss: 0.2171
 455/1500 [========>.....................] - ETA: 13:50 - loss: 1.3470 - regression_loss: 1.1302 - classification_loss: 0.2168
 456/1500 [========>.....................] - ETA: 13:49 - loss: 1.3499 - regression_loss: 1.1321 - classification_loss: 0.2178
 457/1500 [========>.....................] - ETA: 13:49 - loss: 1.3475 - regression_loss: 1.1301 - classification_loss: 0.2174
 458/1500 [========>.....................] - ETA: 13:47 - loss: 1.3467 - regression_loss: 1.1295 - classification_loss: 0.2171
 459/1500 [========>.....................] - ETA: 13:46 - loss: 1.3450 - regression_loss: 1.1282 - classification_loss: 0.2168
 460/1500 [========>.....................] - ETA: 13:44 - loss: 1.3442 - regression_loss: 1.1277 - classification_loss: 0.2165
 461/1500 [========>.....................] - ETA: 13:44 - loss: 1.3449 - regression_loss: 1.1283 - classification_loss: 0.2166
 462/1500 [========>.....................] - ETA: 13:42 - loss: 1.3444 - regression_loss: 1.1279 - classification_loss: 0.2165
 463/1500 [========>.....................] - ETA: 13:41 - loss: 1.3463 - regression_loss: 1.1299 - classification_loss: 0.2163
 464/1500 [========>.....................] - ETA: 13:40 - loss: 1.3455 - regression_loss: 1.1293 - classification_loss: 0.2162
 465/1500 [========>.....................] - ETA: 13:39 - loss: 1.3440 - regression_loss: 1.1281 - classification_loss: 0.2159
 466/1500 [========>.....................] - ETA: 13:41 - loss: 1.3434 - regression_loss: 1.1278 - classification_loss: 0.2156
 467/1500 [========>.....................] - ETA: 13:39 - loss: 1.3430 - regression_loss: 1.1276 - classification_loss: 0.2155
 468/1500 [========>.....................] - ETA: 13:39 - loss: 1.3419 - regression_loss: 1.1267 - classification_loss: 0.2152
 469/1500 [========>.....................] - ETA: 13:37 - loss: 1.3405 - regression_loss: 1.1257 - classification_loss: 0.2148
 470/1500 [========>.....................] - ETA: 13:38 - loss: 1.3435 - regression_loss: 1.1281 - classification_loss: 0.2154
 471/1500 [========>.....................] - ETA: 13:36 - loss: 1.3431 - regression_loss: 1.1279 - classification_loss: 0.2152
 472/1500 [========>.....................] - ETA: 13:35 - loss: 1.3426 - regression_loss: 1.1274 - classification_loss: 0.2152
 473/1500 [========>.....................] - ETA: 13:35 - loss: 1.3441 - regression_loss: 1.1286 - classification_loss: 0.2155
 474/1500 [========>.....................] - ETA: 13:33 - loss: 1.3441 - regression_loss: 1.1285 - classification_loss: 0.2156
 475/1500 [========>.....................] - ETA: 13:31 - loss: 1.3426 - regression_loss: 1.1273 - classification_loss: 0.2153
 476/1500 [========>.....................] - ETA: 13:30 - loss: 1.3419 - regression_loss: 1.1268 - classification_loss: 0.2152
 477/1500 [========>.....................] - ETA: 13:35 - loss: 1.3435 - regression_loss: 1.1282 - classification_loss: 0.2153
 478/1500 [========>.....................] - ETA: 13:34 - loss: 1.3416 - regression_loss: 1.1267 - classification_loss: 0.2149
 479/1500 [========>.....................] - ETA: 13:33 - loss: 1.3407 - regression_loss: 1.1260 - classification_loss: 0.2147
 480/1500 [========>.....................] - ETA: 13:33 - loss: 1.3407 - regression_loss: 1.1260 - classification_loss: 0.2146
 481/1500 [========>.....................] - ETA: 13:32 - loss: 1.3395 - regression_loss: 1.1251 - classification_loss: 0.2144
 482/1500 [========>.....................] - ETA: 13:31 - loss: 1.3400 - regression_loss: 1.1255 - classification_loss: 0.2145
 483/1500 [========>.....................] - ETA: 13:30 - loss: 1.3399 - regression_loss: 1.1257 - classification_loss: 0.2142
 484/1500 [========>.....................] - ETA: 13:29 - loss: 1.3422 - regression_loss: 1.1275 - classification_loss: 0.2146
 485/1500 [========>.....................] - ETA: 13:27 - loss: 1.3419 - regression_loss: 1.1274 - classification_loss: 0.2145
 486/1500 [========>.....................] - ETA: 13:27 - loss: 1.3426 - regression_loss: 1.1280 - classification_loss: 0.2145
 487/1500 [========>.....................] - ETA: 13:26 - loss: 1.3446 - regression_loss: 1.1297 - classification_loss: 0.2149
 488/1500 [========>.....................] - ETA: 13:24 - loss: 1.3441 - regression_loss: 1.1293 - classification_loss: 0.2148
 489/1500 [========>.....................] - ETA: 13:28 - loss: 1.3466 - regression_loss: 1.1312 - classification_loss: 0.2154
 490/1500 [========>.....................] - ETA: 13:29 - loss: 1.3450 - regression_loss: 1.1298 - classification_loss: 0.2151
 491/1500 [========>.....................] - ETA: 13:29 - loss: 1.3467 - regression_loss: 1.1313 - classification_loss: 0.2155
 492/1500 [========>.....................] - ETA: 13:28 - loss: 1.3479 - regression_loss: 1.1322 - classification_loss: 0.2156
 493/1500 [========>.....................] - ETA: 13:26 - loss: 1.3480 - regression_loss: 1.1326 - classification_loss: 0.2154
 494/1500 [========>.....................] - ETA: 13:24 - loss: 1.3474 - regression_loss: 1.1322 - classification_loss: 0.2152
 495/1500 [========>.....................] - ETA: 13:24 - loss: 1.3506 - regression_loss: 1.1328 - classification_loss: 0.2177
 496/1500 [========>.....................] - ETA: 13:23 - loss: 1.3493 - regression_loss: 1.1319 - classification_loss: 0.2175
 497/1500 [========>.....................] - ETA: 13:25 - loss: 1.3507 - regression_loss: 1.1327 - classification_loss: 0.2180
 498/1500 [========>.....................] - ETA: 13:24 - loss: 1.3523 - regression_loss: 1.1341 - classification_loss: 0.2182
 499/1500 [========>.....................] - ETA: 13:24 - loss: 1.3527 - regression_loss: 1.1341 - classification_loss: 0.2185
 500/1500 [=========>....................] - ETA: 13:24 - loss: 1.3522 - regression_loss: 1.1338 - classification_loss: 0.2184
 501/1500 [=========>....................] - ETA: 13:25 - loss: 1.3515 - regression_loss: 1.1333 - classification_loss: 0.2182
 502/1500 [=========>....................] - ETA: 13:24 - loss: 1.3521 - regression_loss: 1.1338 - classification_loss: 0.2183
 503/1500 [=========>....................] - ETA: 13:23 - loss: 1.3528 - regression_loss: 1.1341 - classification_loss: 0.2187
 504/1500 [=========>....................] - ETA: 13:21 - loss: 1.3514 - regression_loss: 1.1331 - classification_loss: 0.2184
 505/1500 [=========>....................] - ETA: 13:20 - loss: 1.3506 - regression_loss: 1.1324 - classification_loss: 0.2182
 506/1500 [=========>....................] - ETA: 13:18 - loss: 1.3529 - regression_loss: 1.1343 - classification_loss: 0.2187
 507/1500 [=========>....................] - ETA: 13:17 - loss: 1.3543 - regression_loss: 1.1352 - classification_loss: 0.2191
 508/1500 [=========>....................] - ETA: 13:15 - loss: 1.3552 - regression_loss: 1.1361 - classification_loss: 0.2191
 509/1500 [=========>....................] - ETA: 13:13 - loss: 1.3560 - regression_loss: 1.1368 - classification_loss: 0.2191
 510/1500 [=========>....................] - ETA: 13:12 - loss: 1.3562 - regression_loss: 1.1370 - classification_loss: 0.2191
 511/1500 [=========>....................] - ETA: 13:10 - loss: 1.3557 - regression_loss: 1.1369 - classification_loss: 0.2189
 512/1500 [=========>....................] - ETA: 13:09 - loss: 1.3555 - regression_loss: 1.1367 - classification_loss: 0.2189
 513/1500 [=========>....................] - ETA: 13:09 - loss: 1.3552 - regression_loss: 1.1364 - classification_loss: 0.2187
 514/1500 [=========>....................] - ETA: 13:07 - loss: 1.3536 - regression_loss: 1.1351 - classification_loss: 0.2185
 515/1500 [=========>....................] - ETA: 13:06 - loss: 1.3531 - regression_loss: 1.1349 - classification_loss: 0.2182
 516/1500 [=========>....................] - ETA: 13:05 - loss: 1.3523 - regression_loss: 1.1342 - classification_loss: 0.2181
 517/1500 [=========>....................] - ETA: 13:03 - loss: 1.3547 - regression_loss: 1.1363 - classification_loss: 0.2184
 518/1500 [=========>....................] - ETA: 13:03 - loss: 1.3570 - regression_loss: 1.1385 - classification_loss: 0.2186
 519/1500 [=========>....................] - ETA: 13:03 - loss: 1.3581 - regression_loss: 1.1394 - classification_loss: 0.2187
 520/1500 [=========>....................] - ETA: 13:04 - loss: 1.3600 - regression_loss: 1.1411 - classification_loss: 0.2189
 521/1500 [=========>....................] - ETA: 13:04 - loss: 1.3595 - regression_loss: 1.1409 - classification_loss: 0.2187
 522/1500 [=========>....................] - ETA: 13:04 - loss: 1.3590 - regression_loss: 1.1405 - classification_loss: 0.2185
 523/1500 [=========>....................] - ETA: 13:02 - loss: 1.3579 - regression_loss: 1.1396 - classification_loss: 0.2182
 524/1500 [=========>....................] - ETA: 13:01 - loss: 1.3597 - regression_loss: 1.1412 - classification_loss: 0.2185
 525/1500 [=========>....................] - ETA: 13:00 - loss: 1.3600 - regression_loss: 1.1415 - classification_loss: 0.2185
 526/1500 [=========>....................] - ETA: 12:59 - loss: 1.3598 - regression_loss: 1.1411 - classification_loss: 0.2187
 527/1500 [=========>....................] - ETA: 12:58 - loss: 1.3599 - regression_loss: 1.1413 - classification_loss: 0.2186
 528/1500 [=========>....................] - ETA: 12:56 - loss: 1.3584 - regression_loss: 1.1401 - classification_loss: 0.2183
 529/1500 [=========>....................] - ETA: 12:56 - loss: 1.3599 - regression_loss: 1.1417 - classification_loss: 0.2182
 530/1500 [=========>....................] - ETA: 12:55 - loss: 1.3611 - regression_loss: 1.1428 - classification_loss: 0.2183
 531/1500 [=========>....................] - ETA: 12:53 - loss: 1.3607 - regression_loss: 1.1425 - classification_loss: 0.2182
 532/1500 [=========>....................] - ETA: 12:52 - loss: 1.3596 - regression_loss: 1.1415 - classification_loss: 0.2180
 533/1500 [=========>....................] - ETA: 12:51 - loss: 1.3586 - regression_loss: 1.1408 - classification_loss: 0.2178
 534/1500 [=========>....................] - ETA: 12:49 - loss: 1.3581 - regression_loss: 1.1404 - classification_loss: 0.2177
 535/1500 [=========>....................] - ETA: 12:48 - loss: 1.3567 - regression_loss: 1.1393 - classification_loss: 0.2174
 536/1500 [=========>....................] - ETA: 12:46 - loss: 1.3549 - regression_loss: 1.1377 - classification_loss: 0.2172
 537/1500 [=========>....................] - ETA: 12:46 - loss: 1.3566 - regression_loss: 1.1390 - classification_loss: 0.2176
 538/1500 [=========>....................] - ETA: 12:45 - loss: 1.3567 - regression_loss: 1.1391 - classification_loss: 0.2176
 539/1500 [=========>....................] - ETA: 12:43 - loss: 1.3567 - regression_loss: 1.1388 - classification_loss: 0.2179
 540/1500 [=========>....................] - ETA: 12:42 - loss: 1.3557 - regression_loss: 1.1380 - classification_loss: 0.2177
 541/1500 [=========>....................] - ETA: 12:41 - loss: 1.3546 - regression_loss: 1.1370 - classification_loss: 0.2176
 542/1500 [=========>....................] - ETA: 12:39 - loss: 1.3561 - regression_loss: 1.1381 - classification_loss: 0.2179
 543/1500 [=========>....................] - ETA: 12:39 - loss: 1.3561 - regression_loss: 1.1381 - classification_loss: 0.2180
 544/1500 [=========>....................] - ETA: 12:38 - loss: 1.3553 - regression_loss: 1.1376 - classification_loss: 0.2177
 545/1500 [=========>....................] - ETA: 12:37 - loss: 1.3542 - regression_loss: 1.1364 - classification_loss: 0.2177
 546/1500 [=========>....................] - ETA: 12:36 - loss: 1.3567 - regression_loss: 1.1383 - classification_loss: 0.2183
 547/1500 [=========>....................] - ETA: 12:36 - loss: 1.3558 - regression_loss: 1.1376 - classification_loss: 0.2182
 548/1500 [=========>....................] - ETA: 12:35 - loss: 1.3547 - regression_loss: 1.1368 - classification_loss: 0.2179
 549/1500 [=========>....................] - ETA: 12:33 - loss: 1.3533 - regression_loss: 1.1358 - classification_loss: 0.2175
 550/1500 [==========>...................] - ETA: 12:32 - loss: 1.3548 - regression_loss: 1.1369 - classification_loss: 0.2179
 551/1500 [==========>...................] - ETA: 12:32 - loss: 1.3545 - regression_loss: 1.1367 - classification_loss: 0.2178
 552/1500 [==========>...................] - ETA: 12:32 - loss: 1.3552 - regression_loss: 1.1375 - classification_loss: 0.2176
 553/1500 [==========>...................] - ETA: 12:30 - loss: 1.3563 - regression_loss: 1.1384 - classification_loss: 0.2179
 554/1500 [==========>...................] - ETA: 12:30 - loss: 1.3557 - regression_loss: 1.1380 - classification_loss: 0.2177
 555/1500 [==========>...................] - ETA: 12:28 - loss: 1.3564 - regression_loss: 1.1383 - classification_loss: 0.2180
 556/1500 [==========>...................] - ETA: 12:27 - loss: 1.3568 - regression_loss: 1.1386 - classification_loss: 0.2182
 557/1500 [==========>...................] - ETA: 12:25 - loss: 1.3558 - regression_loss: 1.1379 - classification_loss: 0.2179
 558/1500 [==========>...................] - ETA: 12:25 - loss: 1.3559 - regression_loss: 1.1379 - classification_loss: 0.2180
 559/1500 [==========>...................] - ETA: 12:25 - loss: 1.3562 - regression_loss: 1.1382 - classification_loss: 0.2180
 560/1500 [==========>...................] - ETA: 12:25 - loss: 1.3576 - regression_loss: 1.1395 - classification_loss: 0.2182
 561/1500 [==========>...................] - ETA: 12:24 - loss: 1.3575 - regression_loss: 1.1396 - classification_loss: 0.2179
 562/1500 [==========>...................] - ETA: 12:22 - loss: 1.3578 - regression_loss: 1.1397 - classification_loss: 0.2181
 563/1500 [==========>...................] - ETA: 12:21 - loss: 1.3566 - regression_loss: 1.1386 - classification_loss: 0.2180
 564/1500 [==========>...................] - ETA: 12:20 - loss: 1.3571 - regression_loss: 1.1391 - classification_loss: 0.2180
 565/1500 [==========>...................] - ETA: 12:18 - loss: 1.3573 - regression_loss: 1.1395 - classification_loss: 0.2178
 566/1500 [==========>...................] - ETA: 12:17 - loss: 1.3585 - regression_loss: 1.1404 - classification_loss: 0.2181
 567/1500 [==========>...................] - ETA: 12:16 - loss: 1.3618 - regression_loss: 1.1404 - classification_loss: 0.2214
 568/1500 [==========>...................] - ETA: 12:15 - loss: 1.3624 - regression_loss: 1.1409 - classification_loss: 0.2214
 569/1500 [==========>...................] - ETA: 12:14 - loss: 1.3615 - regression_loss: 1.1403 - classification_loss: 0.2212
 570/1500 [==========>...................] - ETA: 12:13 - loss: 1.3620 - regression_loss: 1.1406 - classification_loss: 0.2213
 571/1500 [==========>...................] - ETA: 12:11 - loss: 1.3650 - regression_loss: 1.1430 - classification_loss: 0.2219
 572/1500 [==========>...................] - ETA: 12:10 - loss: 1.3655 - regression_loss: 1.1435 - classification_loss: 0.2219
 573/1500 [==========>...................] - ETA: 12:12 - loss: 1.3666 - regression_loss: 1.1446 - classification_loss: 0.2220
 574/1500 [==========>...................] - ETA: 12:10 - loss: 1.3662 - regression_loss: 1.1444 - classification_loss: 0.2218
 575/1500 [==========>...................] - ETA: 12:09 - loss: 1.3667 - regression_loss: 1.1447 - classification_loss: 0.2219
 576/1500 [==========>...................] - ETA: 12:08 - loss: 1.3688 - regression_loss: 1.1462 - classification_loss: 0.2226
 577/1500 [==========>...................] - ETA: 12:06 - loss: 1.3686 - regression_loss: 1.1462 - classification_loss: 0.2224
 578/1500 [==========>...................] - ETA: 12:05 - loss: 1.3679 - regression_loss: 1.1457 - classification_loss: 0.2222
 579/1500 [==========>...................] - ETA: 12:04 - loss: 1.3668 - regression_loss: 1.1447 - classification_loss: 0.2220
 580/1500 [==========>...................] - ETA: 12:03 - loss: 1.3671 - regression_loss: 1.1451 - classification_loss: 0.2220
 581/1500 [==========>...................] - ETA: 12:02 - loss: 1.3672 - regression_loss: 1.1454 - classification_loss: 0.2218
 582/1500 [==========>...................] - ETA: 12:01 - loss: 1.3667 - regression_loss: 1.1450 - classification_loss: 0.2217
 583/1500 [==========>...................] - ETA: 12:00 - loss: 1.3658 - regression_loss: 1.1443 - classification_loss: 0.2214
 584/1500 [==========>...................] - ETA: 11:58 - loss: 1.3668 - regression_loss: 1.1448 - classification_loss: 0.2220
 585/1500 [==========>...................] - ETA: 11:57 - loss: 1.3674 - regression_loss: 1.1454 - classification_loss: 0.2220
 586/1500 [==========>...................] - ETA: 11:57 - loss: 1.3671 - regression_loss: 1.1451 - classification_loss: 0.2220
 587/1500 [==========>...................] - ETA: 11:58 - loss: 1.3678 - regression_loss: 1.1455 - classification_loss: 0.2223
 588/1500 [==========>...................] - ETA: 12:00 - loss: 1.3662 - regression_loss: 1.1442 - classification_loss: 0.2220
 589/1500 [==========>...................] - ETA: 11:58 - loss: 1.3660 - regression_loss: 1.1442 - classification_loss: 0.2219
 590/1500 [==========>...................] - ETA: 11:57 - loss: 1.3644 - regression_loss: 1.1428 - classification_loss: 0.2216
 591/1500 [==========>...................] - ETA: 11:56 - loss: 1.3628 - regression_loss: 1.1414 - classification_loss: 0.2214
 592/1500 [==========>...................] - ETA: 11:55 - loss: 1.3618 - regression_loss: 1.1407 - classification_loss: 0.2211
 593/1500 [==========>...................] - ETA: 11:54 - loss: 1.3618 - regression_loss: 1.1408 - classification_loss: 0.2210
 594/1500 [==========>...................] - ETA: 11:53 - loss: 1.3632 - regression_loss: 1.1412 - classification_loss: 0.2220
 595/1500 [==========>...................] - ETA: 11:52 - loss: 1.3645 - regression_loss: 1.1422 - classification_loss: 0.2223
 596/1500 [==========>...................] - ETA: 11:52 - loss: 1.3637 - regression_loss: 1.1414 - classification_loss: 0.2223
 597/1500 [==========>...................] - ETA: 11:51 - loss: 1.3654 - regression_loss: 1.1429 - classification_loss: 0.2224
 598/1500 [==========>...................] - ETA: 11:51 - loss: 1.3648 - regression_loss: 1.1424 - classification_loss: 0.2224
 599/1500 [==========>...................] - ETA: 11:50 - loss: 1.3633 - regression_loss: 1.1412 - classification_loss: 0.2222
 600/1500 [===========>..................] - ETA: 11:49 - loss: 1.3633 - regression_loss: 1.1413 - classification_loss: 0.2220
 601/1500 [===========>..................] - ETA: 11:48 - loss: 1.3636 - regression_loss: 1.1415 - classification_loss: 0.2221
 602/1500 [===========>..................] - ETA: 11:48 - loss: 1.3626 - regression_loss: 1.1407 - classification_loss: 0.2219
 603/1500 [===========>..................] - ETA: 11:47 - loss: 1.3626 - regression_loss: 1.1409 - classification_loss: 0.2217
 604/1500 [===========>..................] - ETA: 11:47 - loss: 1.3638 - regression_loss: 1.1419 - classification_loss: 0.2218
 605/1500 [===========>..................] - ETA: 11:46 - loss: 1.3656 - regression_loss: 1.1433 - classification_loss: 0.2223
 606/1500 [===========>..................] - ETA: 11:44 - loss: 1.3651 - regression_loss: 1.1429 - classification_loss: 0.2222
 607/1500 [===========>..................] - ETA: 11:43 - loss: 1.3652 - regression_loss: 1.1431 - classification_loss: 0.2221
 608/1500 [===========>..................] - ETA: 11:42 - loss: 1.3644 - regression_loss: 1.1426 - classification_loss: 0.2218
 609/1500 [===========>..................] - ETA: 11:40 - loss: 1.3640 - regression_loss: 1.1424 - classification_loss: 0.2216
 610/1500 [===========>..................] - ETA: 11:40 - loss: 1.3655 - regression_loss: 1.1437 - classification_loss: 0.2218
 611/1500 [===========>..................] - ETA: 11:40 - loss: 1.3667 - regression_loss: 1.1447 - classification_loss: 0.2221
 612/1500 [===========>..................] - ETA: 11:39 - loss: 1.3663 - regression_loss: 1.1443 - classification_loss: 0.2220
 613/1500 [===========>..................] - ETA: 11:38 - loss: 1.3669 - regression_loss: 1.1450 - classification_loss: 0.2219
 614/1500 [===========>..................] - ETA: 11:37 - loss: 1.3676 - regression_loss: 1.1456 - classification_loss: 0.2220
 615/1500 [===========>..................] - ETA: 11:36 - loss: 1.3667 - regression_loss: 1.1448 - classification_loss: 0.2218
 616/1500 [===========>..................] - ETA: 11:35 - loss: 1.3675 - regression_loss: 1.1454 - classification_loss: 0.2221
 617/1500 [===========>..................] - ETA: 11:35 - loss: 1.3680 - regression_loss: 1.1460 - classification_loss: 0.2220
 618/1500 [===========>..................] - ETA: 11:34 - loss: 1.3668 - regression_loss: 1.1451 - classification_loss: 0.2217
 619/1500 [===========>..................] - ETA: 11:33 - loss: 1.3672 - regression_loss: 1.1454 - classification_loss: 0.2217
 620/1500 [===========>..................] - ETA: 11:34 - loss: 1.3661 - regression_loss: 1.1445 - classification_loss: 0.2216
 621/1500 [===========>..................] - ETA: 11:33 - loss: 1.3653 - regression_loss: 1.1438 - classification_loss: 0.2214
 622/1500 [===========>..................] - ETA: 11:33 - loss: 1.3669 - regression_loss: 1.1453 - classification_loss: 0.2217
 623/1500 [===========>..................] - ETA: 11:32 - loss: 1.3669 - regression_loss: 1.1454 - classification_loss: 0.2215
 624/1500 [===========>..................] - ETA: 11:33 - loss: 1.3683 - regression_loss: 1.1464 - classification_loss: 0.2219
 625/1500 [===========>..................] - ETA: 11:32 - loss: 1.3677 - regression_loss: 1.1459 - classification_loss: 0.2218
 626/1500 [===========>..................] - ETA: 11:32 - loss: 1.3678 - regression_loss: 1.1457 - classification_loss: 0.2221
 627/1500 [===========>..................] - ETA: 11:31 - loss: 1.3681 - regression_loss: 1.1461 - classification_loss: 0.2220
 628/1500 [===========>..................] - ETA: 11:31 - loss: 1.3711 - regression_loss: 1.1470 - classification_loss: 0.2241
 629/1500 [===========>..................] - ETA: 11:30 - loss: 1.3718 - regression_loss: 1.1476 - classification_loss: 0.2241
 630/1500 [===========>..................] - ETA: 11:29 - loss: 1.3704 - regression_loss: 1.1465 - classification_loss: 0.2239
 631/1500 [===========>..................] - ETA: 11:28 - loss: 1.3722 - regression_loss: 1.1478 - classification_loss: 0.2243
 632/1500 [===========>..................] - ETA: 11:28 - loss: 1.3730 - regression_loss: 1.1484 - classification_loss: 0.2245
 633/1500 [===========>..................] - ETA: 11:28 - loss: 1.3716 - regression_loss: 1.1473 - classification_loss: 0.2243
 634/1500 [===========>..................] - ETA: 11:28 - loss: 1.3714 - regression_loss: 1.1468 - classification_loss: 0.2246
 635/1500 [===========>..................] - ETA: 11:27 - loss: 1.3708 - regression_loss: 1.1464 - classification_loss: 0.2244
 636/1500 [===========>..................] - ETA: 11:26 - loss: 1.3693 - regression_loss: 1.1452 - classification_loss: 0.2241
 637/1500 [===========>..................] - ETA: 11:26 - loss: 1.3710 - regression_loss: 1.1465 - classification_loss: 0.2245
 638/1500 [===========>..................] - ETA: 11:25 - loss: 1.3700 - regression_loss: 1.1455 - classification_loss: 0.2245
 639/1500 [===========>..................] - ETA: 11:24 - loss: 1.3703 - regression_loss: 1.1458 - classification_loss: 0.2245
 640/1500 [===========>..................] - ETA: 11:23 - loss: 1.3703 - regression_loss: 1.1458 - classification_loss: 0.2245
 641/1500 [===========>..................] - ETA: 11:21 - loss: 1.3709 - regression_loss: 1.1464 - classification_loss: 0.2246
 642/1500 [===========>..................] - ETA: 11:22 - loss: 1.3726 - regression_loss: 1.1479 - classification_loss: 0.2247
 643/1500 [===========>..................] - ETA: 11:20 - loss: 1.3727 - regression_loss: 1.1482 - classification_loss: 0.2245
 644/1500 [===========>..................] - ETA: 11:20 - loss: 1.3723 - regression_loss: 1.1479 - classification_loss: 0.2244
 645/1500 [===========>..................] - ETA: 11:18 - loss: 1.3713 - regression_loss: 1.1471 - classification_loss: 0.2242
 646/1500 [===========>..................] - ETA: 11:18 - loss: 1.3712 - regression_loss: 1.1469 - classification_loss: 0.2244
 647/1500 [===========>..................] - ETA: 11:17 - loss: 1.3705 - regression_loss: 1.1464 - classification_loss: 0.2241
 648/1500 [===========>..................] - ETA: 11:16 - loss: 1.3695 - regression_loss: 1.1456 - classification_loss: 0.2239
 649/1500 [===========>..................] - ETA: 11:15 - loss: 1.3705 - regression_loss: 1.1466 - classification_loss: 0.2239
 650/1500 [============>.................] - ETA: 11:14 - loss: 1.3697 - regression_loss: 1.1460 - classification_loss: 0.2237
 651/1500 [============>.................] - ETA: 11:14 - loss: 1.3686 - regression_loss: 1.1450 - classification_loss: 0.2236
 652/1500 [============>.................] - ETA: 11:12 - loss: 1.3686 - regression_loss: 1.1451 - classification_loss: 0.2235
 653/1500 [============>.................] - ETA: 11:12 - loss: 1.3673 - regression_loss: 1.1441 - classification_loss: 0.2232
 654/1500 [============>.................] - ETA: 11:12 - loss: 1.3687 - regression_loss: 1.1452 - classification_loss: 0.2234
 655/1500 [============>.................] - ETA: 11:11 - loss: 1.3677 - regression_loss: 1.1445 - classification_loss: 0.2232
 656/1500 [============>.................] - ETA: 11:09 - loss: 1.3672 - regression_loss: 1.1442 - classification_loss: 0.2230
 657/1500 [============>.................] - ETA: 11:08 - loss: 1.3671 - regression_loss: 1.1442 - classification_loss: 0.2229
 658/1500 [============>.................] - ETA: 11:07 - loss: 1.3659 - regression_loss: 1.1433 - classification_loss: 0.2226
 659/1500 [============>.................] - ETA: 11:06 - loss: 1.3664 - regression_loss: 1.1436 - classification_loss: 0.2228
 660/1500 [============>.................] - ETA: 11:05 - loss: 1.3659 - regression_loss: 1.1434 - classification_loss: 0.2225
 661/1500 [============>.................] - ETA: 11:04 - loss: 1.3654 - regression_loss: 1.1430 - classification_loss: 0.2223
 662/1500 [============>.................] - ETA: 11:03 - loss: 1.3664 - regression_loss: 1.1437 - classification_loss: 0.2227
 663/1500 [============>.................] - ETA: 11:01 - loss: 1.3674 - regression_loss: 1.1446 - classification_loss: 0.2228
 664/1500 [============>.................] - ETA: 11:00 - loss: 1.3673 - regression_loss: 1.1446 - classification_loss: 0.2227
 665/1500 [============>.................] - ETA: 10:59 - loss: 1.3690 - regression_loss: 1.1442 - classification_loss: 0.2247
 666/1500 [============>.................] - ETA: 10:58 - loss: 1.3711 - regression_loss: 1.1458 - classification_loss: 0.2254
 667/1500 [============>.................] - ETA: 10:57 - loss: 1.3702 - regression_loss: 1.1451 - classification_loss: 0.2251
 668/1500 [============>.................] - ETA: 10:57 - loss: 1.3704 - regression_loss: 1.1448 - classification_loss: 0.2257
 669/1500 [============>.................] - ETA: 10:55 - loss: 1.3696 - regression_loss: 1.1440 - classification_loss: 0.2256
 670/1500 [============>.................] - ETA: 10:54 - loss: 1.3690 - regression_loss: 1.1436 - classification_loss: 0.2254
 671/1500 [============>.................] - ETA: 10:53 - loss: 1.3686 - regression_loss: 1.1433 - classification_loss: 0.2253
 672/1500 [============>.................] - ETA: 10:52 - loss: 1.3683 - regression_loss: 1.1432 - classification_loss: 0.2251
 673/1500 [============>.................] - ETA: 10:51 - loss: 1.3691 - regression_loss: 1.1435 - classification_loss: 0.2256
 674/1500 [============>.................] - ETA: 10:50 - loss: 1.3689 - regression_loss: 1.1436 - classification_loss: 0.2254
 675/1500 [============>.................] - ETA: 10:48 - loss: 1.3699 - regression_loss: 1.1442 - classification_loss: 0.2257
 676/1500 [============>.................] - ETA: 10:47 - loss: 1.3694 - regression_loss: 1.1439 - classification_loss: 0.2255
 677/1500 [============>.................] - ETA: 10:46 - loss: 1.3691 - regression_loss: 1.1436 - classification_loss: 0.2254
 678/1500 [============>.................] - ETA: 10:45 - loss: 1.3693 - regression_loss: 1.1440 - classification_loss: 0.2253
 679/1500 [============>.................] - ETA: 10:44 - loss: 1.3686 - regression_loss: 1.1435 - classification_loss: 0.2251
 680/1500 [============>.................] - ETA: 10:44 - loss: 1.3682 - regression_loss: 1.1432 - classification_loss: 0.2251
 681/1500 [============>.................] - ETA: 10:43 - loss: 1.3692 - regression_loss: 1.1440 - classification_loss: 0.2251
 682/1500 [============>.................] - ETA: 10:42 - loss: 1.3699 - regression_loss: 1.1448 - classification_loss: 0.2251
 683/1500 [============>.................] - ETA: 10:41 - loss: 1.3714 - regression_loss: 1.1460 - classification_loss: 0.2255
 684/1500 [============>.................] - ETA: 10:41 - loss: 1.3705 - regression_loss: 1.1453 - classification_loss: 0.2252
 685/1500 [============>.................] - ETA: 10:39 - loss: 1.3712 - regression_loss: 1.1458 - classification_loss: 0.2254
 686/1500 [============>.................] - ETA: 10:38 - loss: 1.3714 - regression_loss: 1.1459 - classification_loss: 0.2254
 687/1500 [============>.................] - ETA: 10:37 - loss: 1.3711 - regression_loss: 1.1456 - classification_loss: 0.2254
 688/1500 [============>.................] - ETA: 10:37 - loss: 1.3716 - regression_loss: 1.1460 - classification_loss: 0.2256
 689/1500 [============>.................] - ETA: 10:36 - loss: 1.3726 - regression_loss: 1.1468 - classification_loss: 0.2258
 690/1500 [============>.................] - ETA: 10:36 - loss: 1.3724 - regression_loss: 1.1468 - classification_loss: 0.2257
 691/1500 [============>.................] - ETA: 10:37 - loss: 1.3727 - regression_loss: 1.1471 - classification_loss: 0.2257
 692/1500 [============>.................] - ETA: 10:36 - loss: 1.3723 - regression_loss: 1.1466 - classification_loss: 0.2256
 693/1500 [============>.................] - ETA: 10:36 - loss: 1.3733 - regression_loss: 1.1474 - classification_loss: 0.2259
 694/1500 [============>.................] - ETA: 10:36 - loss: 1.3742 - regression_loss: 1.1482 - classification_loss: 0.2260
 695/1500 [============>.................] - ETA: 10:35 - loss: 1.3738 - regression_loss: 1.1478 - classification_loss: 0.2260
 696/1500 [============>.................] - ETA: 10:34 - loss: 1.3732 - regression_loss: 1.1474 - classification_loss: 0.2258
 697/1500 [============>.................] - ETA: 10:33 - loss: 1.3726 - regression_loss: 1.1470 - classification_loss: 0.2256
 698/1500 [============>.................] - ETA: 10:32 - loss: 1.3716 - regression_loss: 1.1462 - classification_loss: 0.2254
 699/1500 [============>.................] - ETA: 10:31 - loss: 1.3717 - regression_loss: 1.1464 - classification_loss: 0.2253
 700/1500 [=============>................] - ETA: 10:30 - loss: 1.3718 - regression_loss: 1.1467 - classification_loss: 0.2251
 701/1500 [=============>................] - ETA: 10:29 - loss: 1.3711 - regression_loss: 1.1460 - classification_loss: 0.2251
 702/1500 [=============>................] - ETA: 10:28 - loss: 1.3719 - regression_loss: 1.1469 - classification_loss: 0.2250
 703/1500 [=============>................] - ETA: 10:27 - loss: 1.3719 - regression_loss: 1.1471 - classification_loss: 0.2248
 704/1500 [=============>................] - ETA: 10:26 - loss: 1.3717 - regression_loss: 1.1469 - classification_loss: 0.2248
 705/1500 [=============>................] - ETA: 10:24 - loss: 1.3714 - regression_loss: 1.1467 - classification_loss: 0.2247
 706/1500 [=============>................] - ETA: 10:24 - loss: 1.3731 - regression_loss: 1.1481 - classification_loss: 0.2251
 707/1500 [=============>................] - ETA: 10:23 - loss: 1.3732 - regression_loss: 1.1483 - classification_loss: 0.2249
 708/1500 [=============>................] - ETA: 10:22 - loss: 1.3736 - regression_loss: 1.1488 - classification_loss: 0.2248
 709/1500 [=============>................] - ETA: 10:21 - loss: 1.3736 - regression_loss: 1.1487 - classification_loss: 0.2249
 710/1500 [=============>................] - ETA: 10:19 - loss: 1.3727 - regression_loss: 1.1481 - classification_loss: 0.2246
 711/1500 [=============>................] - ETA: 10:19 - loss: 1.3726 - regression_loss: 1.1481 - classification_loss: 0.2245
 712/1500 [=============>................] - ETA: 10:17 - loss: 1.3733 - regression_loss: 1.1488 - classification_loss: 0.2245
 713/1500 [=============>................] - ETA: 10:16 - loss: 1.3736 - regression_loss: 1.1492 - classification_loss: 0.2244
 714/1500 [=============>................] - ETA: 10:15 - loss: 1.3747 - regression_loss: 1.1501 - classification_loss: 0.2246
 715/1500 [=============>................] - ETA: 10:15 - loss: 1.3763 - regression_loss: 1.1515 - classification_loss: 0.2248
 716/1500 [=============>................] - ETA: 10:14 - loss: 1.3760 - regression_loss: 1.1513 - classification_loss: 0.2246
 717/1500 [=============>................] - ETA: 10:13 - loss: 1.3754 - regression_loss: 1.1509 - classification_loss: 0.2245
 718/1500 [=============>................] - ETA: 10:12 - loss: 1.3752 - regression_loss: 1.1508 - classification_loss: 0.2244
 719/1500 [=============>................] - ETA: 10:13 - loss: 1.3757 - regression_loss: 1.1515 - classification_loss: 0.2242
 720/1500 [=============>................] - ETA: 10:11 - loss: 1.3760 - regression_loss: 1.1518 - classification_loss: 0.2242
 721/1500 [=============>................] - ETA: 10:11 - loss: 1.3755 - regression_loss: 1.1511 - classification_loss: 0.2244
 722/1500 [=============>................] - ETA: 10:11 - loss: 1.3763 - regression_loss: 1.1519 - classification_loss: 0.2244
 723/1500 [=============>................] - ETA: 10:10 - loss: 1.3763 - regression_loss: 1.1520 - classification_loss: 0.2244
 724/1500 [=============>................] - ETA: 10:10 - loss: 1.3760 - regression_loss: 1.1517 - classification_loss: 0.2243
 725/1500 [=============>................] - ETA: 10:09 - loss: 1.3760 - regression_loss: 1.1517 - classification_loss: 0.2243
 726/1500 [=============>................] - ETA: 10:08 - loss: 1.3762 - regression_loss: 1.1518 - classification_loss: 0.2244
 727/1500 [=============>................] - ETA: 10:07 - loss: 1.3754 - regression_loss: 1.1511 - classification_loss: 0.2243
 728/1500 [=============>................] - ETA: 10:06 - loss: 1.3746 - regression_loss: 1.1504 - classification_loss: 0.2243
 729/1500 [=============>................] - ETA: 10:05 - loss: 1.3743 - regression_loss: 1.1501 - classification_loss: 0.2242
 730/1500 [=============>................] - ETA: 10:05 - loss: 1.3729 - regression_loss: 1.1490 - classification_loss: 0.2239
 731/1500 [=============>................] - ETA: 10:04 - loss: 1.3723 - regression_loss: 1.1485 - classification_loss: 0.2237
 732/1500 [=============>................] - ETA: 10:03 - loss: 1.3734 - regression_loss: 1.1496 - classification_loss: 0.2238
 733/1500 [=============>................] - ETA: 10:02 - loss: 1.3736 - regression_loss: 1.1496 - classification_loss: 0.2240
 734/1500 [=============>................] - ETA: 10:01 - loss: 1.3736 - regression_loss: 1.1495 - classification_loss: 0.2241
 735/1500 [=============>................] - ETA: 9:59 - loss: 1.3726 - regression_loss: 1.1487 - classification_loss: 0.2239 
 736/1500 [=============>................] - ETA: 10:00 - loss: 1.3723 - regression_loss: 1.1485 - classification_loss: 0.2238
 737/1500 [=============>................] - ETA: 9:59 - loss: 1.3719 - regression_loss: 1.1480 - classification_loss: 0.2239 
 738/1500 [=============>................] - ETA: 9:58 - loss: 1.3711 - regression_loss: 1.1474 - classification_loss: 0.2237
 739/1500 [=============>................] - ETA: 9:57 - loss: 1.3702 - regression_loss: 1.1466 - classification_loss: 0.2236
 740/1500 [=============>................] - ETA: 9:56 - loss: 1.3706 - regression_loss: 1.1471 - classification_loss: 0.2234
 741/1500 [=============>................] - ETA: 9:56 - loss: 1.3700 - regression_loss: 1.1467 - classification_loss: 0.2233
 742/1500 [=============>................] - ETA: 9:54 - loss: 1.3698 - regression_loss: 1.1466 - classification_loss: 0.2232
 743/1500 [=============>................] - ETA: 9:53 - loss: 1.3703 - regression_loss: 1.1468 - classification_loss: 0.2235
 744/1500 [=============>................] - ETA: 9:53 - loss: 1.3705 - regression_loss: 1.1468 - classification_loss: 0.2237
 745/1500 [=============>................] - ETA: 9:53 - loss: 1.3693 - regression_loss: 1.1459 - classification_loss: 0.2235
 746/1500 [=============>................] - ETA: 9:52 - loss: 1.3711 - regression_loss: 1.1472 - classification_loss: 0.2239
 747/1500 [=============>................] - ETA: 9:50 - loss: 1.3707 - regression_loss: 1.1468 - classification_loss: 0.2239
 748/1500 [=============>................] - ETA: 9:49 - loss: 1.3709 - regression_loss: 1.1464 - classification_loss: 0.2245
 749/1500 [=============>................] - ETA: 9:48 - loss: 1.3718 - regression_loss: 1.1471 - classification_loss: 0.2247
 750/1500 [==============>...............] - ETA: 9:47 - loss: 1.3719 - regression_loss: 1.1471 - classification_loss: 0.2248
 751/1500 [==============>...............] - ETA: 9:47 - loss: 1.3721 - regression_loss: 1.1473 - classification_loss: 0.2248
 752/1500 [==============>...............] - ETA: 9:46 - loss: 1.3716 - regression_loss: 1.1469 - classification_loss: 0.2248
 753/1500 [==============>...............] - ETA: 9:45 - loss: 1.3711 - regression_loss: 1.1465 - classification_loss: 0.2246
 754/1500 [==============>...............] - ETA: 9:44 - loss: 1.3713 - regression_loss: 1.1468 - classification_loss: 0.2245
 755/1500 [==============>...............] - ETA: 9:42 - loss: 1.3709 - regression_loss: 1.1464 - classification_loss: 0.2245
 756/1500 [==============>...............] - ETA: 9:41 - loss: 1.3704 - regression_loss: 1.1460 - classification_loss: 0.2244
 757/1500 [==============>...............] - ETA: 9:40 - loss: 1.3696 - regression_loss: 1.1454 - classification_loss: 0.2242
 758/1500 [==============>...............] - ETA: 9:39 - loss: 1.3705 - regression_loss: 1.1460 - classification_loss: 0.2245
 759/1500 [==============>...............] - ETA: 9:39 - loss: 1.3700 - regression_loss: 1.1456 - classification_loss: 0.2244
 760/1500 [==============>...............] - ETA: 9:38 - loss: 1.3709 - regression_loss: 1.1462 - classification_loss: 0.2247
 761/1500 [==============>...............] - ETA: 9:36 - loss: 1.3707 - regression_loss: 1.1462 - classification_loss: 0.2245
 762/1500 [==============>...............] - ETA: 9:37 - loss: 1.3709 - regression_loss: 1.1463 - classification_loss: 0.2246
 763/1500 [==============>...............] - ETA: 9:36 - loss: 1.3722 - regression_loss: 1.1465 - classification_loss: 0.2257
 764/1500 [==============>...............] - ETA: 9:35 - loss: 1.3726 - regression_loss: 1.1469 - classification_loss: 0.2258
 765/1500 [==============>...............] - ETA: 9:34 - loss: 1.3718 - regression_loss: 1.1463 - classification_loss: 0.2255
 766/1500 [==============>...............] - ETA: 9:33 - loss: 1.3713 - regression_loss: 1.1458 - classification_loss: 0.2255
 767/1500 [==============>...............] - ETA: 9:32 - loss: 1.3705 - regression_loss: 1.1453 - classification_loss: 0.2253
 768/1500 [==============>...............] - ETA: 9:31 - loss: 1.3705 - regression_loss: 1.1453 - classification_loss: 0.2252
 769/1500 [==============>...............] - ETA: 9:30 - loss: 1.3707 - regression_loss: 1.1455 - classification_loss: 0.2251
 770/1500 [==============>...............] - ETA: 9:29 - loss: 1.3711 - regression_loss: 1.1459 - classification_loss: 0.2252
 771/1500 [==============>...............] - ETA: 9:28 - loss: 1.3711 - regression_loss: 1.1459 - classification_loss: 0.2252
 772/1500 [==============>...............] - ETA: 9:27 - loss: 1.3712 - regression_loss: 1.1459 - classification_loss: 0.2253
 773/1500 [==============>...............] - ETA: 9:26 - loss: 1.3709 - regression_loss: 1.1457 - classification_loss: 0.2252
 774/1500 [==============>...............] - ETA: 9:26 - loss: 1.3709 - regression_loss: 1.1459 - classification_loss: 0.2250
 775/1500 [==============>...............] - ETA: 9:25 - loss: 1.3708 - regression_loss: 1.1459 - classification_loss: 0.2249
 776/1500 [==============>...............] - ETA: 9:24 - loss: 1.3710 - regression_loss: 1.1461 - classification_loss: 0.2249
 777/1500 [==============>...............] - ETA: 9:23 - loss: 1.3724 - regression_loss: 1.1475 - classification_loss: 0.2250
 778/1500 [==============>...............] - ETA: 9:21 - loss: 1.3727 - regression_loss: 1.1478 - classification_loss: 0.2249
 779/1500 [==============>...............] - ETA: 9:23 - loss: 1.3717 - regression_loss: 1.1471 - classification_loss: 0.2247
 780/1500 [==============>...............] - ETA: 9:22 - loss: 1.3726 - regression_loss: 1.1478 - classification_loss: 0.2248
 781/1500 [==============>...............] - ETA: 9:21 - loss: 1.3731 - regression_loss: 1.1482 - classification_loss: 0.2249
 782/1500 [==============>...............] - ETA: 9:19 - loss: 1.3729 - regression_loss: 1.1481 - classification_loss: 0.2248
 783/1500 [==============>...............] - ETA: 9:18 - loss: 1.3743 - regression_loss: 1.1492 - classification_loss: 0.2251
 784/1500 [==============>...............] - ETA: 9:17 - loss: 1.3734 - regression_loss: 1.1484 - classification_loss: 0.2250
 785/1500 [==============>...............] - ETA: 9:17 - loss: 1.3726 - regression_loss: 1.1477 - classification_loss: 0.2249
 786/1500 [==============>...............] - ETA: 9:16 - loss: 1.3731 - regression_loss: 1.1482 - classification_loss: 0.2249
 787/1500 [==============>...............] - ETA: 9:16 - loss: 1.3729 - regression_loss: 1.1481 - classification_loss: 0.2248
 788/1500 [==============>...............] - ETA: 9:15 - loss: 1.3745 - regression_loss: 1.1495 - classification_loss: 0.2250
 789/1500 [==============>...............] - ETA: 9:14 - loss: 1.3741 - regression_loss: 1.1493 - classification_loss: 0.2248
 790/1500 [==============>...............] - ETA: 9:13 - loss: 1.3739 - regression_loss: 1.1491 - classification_loss: 0.2248
 791/1500 [==============>...............] - ETA: 9:12 - loss: 1.3755 - regression_loss: 1.1504 - classification_loss: 0.2250
 792/1500 [==============>...............] - ETA: 9:11 - loss: 1.3758 - regression_loss: 1.1509 - classification_loss: 0.2249
 793/1500 [==============>...............] - ETA: 9:11 - loss: 1.3753 - regression_loss: 1.1505 - classification_loss: 0.2248
 794/1500 [==============>...............] - ETA: 9:10 - loss: 1.3743 - regression_loss: 1.1497 - classification_loss: 0.2247
 795/1500 [==============>...............] - ETA: 9:09 - loss: 1.3743 - regression_loss: 1.1496 - classification_loss: 0.2247
 796/1500 [==============>...............] - ETA: 9:09 - loss: 1.3731 - regression_loss: 1.1486 - classification_loss: 0.2245
 797/1500 [==============>...............] - ETA: 9:08 - loss: 1.3726 - regression_loss: 1.1482 - classification_loss: 0.2244
 798/1500 [==============>...............] - ETA: 9:08 - loss: 1.3742 - regression_loss: 1.1495 - classification_loss: 0.2247
 799/1500 [==============>...............] - ETA: 9:08 - loss: 1.3739 - regression_loss: 1.1493 - classification_loss: 0.2246
 800/1500 [===============>..............] - ETA: 9:07 - loss: 1.3731 - regression_loss: 1.1488 - classification_loss: 0.2243
 801/1500 [===============>..............] - ETA: 9:06 - loss: 1.3731 - regression_loss: 1.1488 - classification_loss: 0.2243
 802/1500 [===============>..............] - ETA: 9:05 - loss: 1.3723 - regression_loss: 1.1482 - classification_loss: 0.2241
 803/1500 [===============>..............] - ETA: 9:04 - loss: 1.3728 - regression_loss: 1.1486 - classification_loss: 0.2242
 804/1500 [===============>..............] - ETA: 9:03 - loss: 1.3738 - regression_loss: 1.1495 - classification_loss: 0.2243
 805/1500 [===============>..............] - ETA: 9:02 - loss: 1.3735 - regression_loss: 1.1493 - classification_loss: 0.2242
 806/1500 [===============>..............] - ETA: 9:01 - loss: 1.3735 - regression_loss: 1.1494 - classification_loss: 0.2241
 807/1500 [===============>..............] - ETA: 9:00 - loss: 1.3731 - regression_loss: 1.1491 - classification_loss: 0.2240
 808/1500 [===============>..............] - ETA: 9:00 - loss: 1.3729 - regression_loss: 1.1490 - classification_loss: 0.2239
 809/1500 [===============>..............] - ETA: 8:59 - loss: 1.3727 - regression_loss: 1.1488 - classification_loss: 0.2240
 810/1500 [===============>..............] - ETA: 8:58 - loss: 1.3729 - regression_loss: 1.1489 - classification_loss: 0.2240
 811/1500 [===============>..............] - ETA: 8:58 - loss: 1.3722 - regression_loss: 1.1483 - classification_loss: 0.2239
 812/1500 [===============>..............] - ETA: 8:57 - loss: 1.3730 - regression_loss: 1.1490 - classification_loss: 0.2240
 813/1500 [===============>..............] - ETA: 8:56 - loss: 1.3726 - regression_loss: 1.1487 - classification_loss: 0.2240
 814/1500 [===============>..............] - ETA: 8:55 - loss: 1.3740 - regression_loss: 1.1499 - classification_loss: 0.2240
 815/1500 [===============>..............] - ETA: 8:54 - loss: 1.3754 - regression_loss: 1.1508 - classification_loss: 0.2246
 816/1500 [===============>..............] - ETA: 8:53 - loss: 1.3756 - regression_loss: 1.1512 - classification_loss: 0.2245
 817/1500 [===============>..............] - ETA: 8:52 - loss: 1.3751 - regression_loss: 1.1508 - classification_loss: 0.2244
 818/1500 [===============>..............] - ETA: 8:51 - loss: 1.3755 - regression_loss: 1.1511 - classification_loss: 0.2244
 819/1500 [===============>..............] - ETA: 8:50 - loss: 1.3744 - regression_loss: 1.1503 - classification_loss: 0.2242
 820/1500 [===============>..............] - ETA: 8:49 - loss: 1.3748 - regression_loss: 1.1506 - classification_loss: 0.2242
 821/1500 [===============>..............] - ETA: 8:48 - loss: 1.3745 - regression_loss: 1.1504 - classification_loss: 0.2241
 822/1500 [===============>..............] - ETA: 8:47 - loss: 1.3741 - regression_loss: 1.1501 - classification_loss: 0.2240
 823/1500 [===============>..............] - ETA: 8:47 - loss: 1.3741 - regression_loss: 1.1503 - classification_loss: 0.2239
 824/1500 [===============>..............] - ETA: 8:46 - loss: 1.3753 - regression_loss: 1.1512 - classification_loss: 0.2241
 825/1500 [===============>..............] - ETA: 8:45 - loss: 1.3760 - regression_loss: 1.1519 - classification_loss: 0.2241
 826/1500 [===============>..............] - ETA: 8:45 - loss: 1.3752 - regression_loss: 1.1512 - classification_loss: 0.2240
 827/1500 [===============>..............] - ETA: 8:44 - loss: 1.3743 - regression_loss: 1.1504 - classification_loss: 0.2238
 828/1500 [===============>..............] - ETA: 8:44 - loss: 1.3740 - regression_loss: 1.1502 - classification_loss: 0.2238
 829/1500 [===============>..............] - ETA: 8:44 - loss: 1.3730 - regression_loss: 1.1494 - classification_loss: 0.2236
 830/1500 [===============>..............] - ETA: 8:43 - loss: 1.3724 - regression_loss: 1.1489 - classification_loss: 0.2234
 831/1500 [===============>..............] - ETA: 8:43 - loss: 1.3717 - regression_loss: 1.1485 - classification_loss: 0.2232
 832/1500 [===============>..............] - ETA: 8:42 - loss: 1.3717 - regression_loss: 1.1485 - classification_loss: 0.2233
 833/1500 [===============>..............] - ETA: 8:41 - loss: 1.3711 - regression_loss: 1.1480 - classification_loss: 0.2231
 834/1500 [===============>..............] - ETA: 8:41 - loss: 1.3722 - regression_loss: 1.1489 - classification_loss: 0.2233
 835/1500 [===============>..............] - ETA: 8:40 - loss: 1.3720 - regression_loss: 1.1487 - classification_loss: 0.2233
 836/1500 [===============>..............] - ETA: 8:39 - loss: 1.3717 - regression_loss: 1.1485 - classification_loss: 0.2232
 837/1500 [===============>..............] - ETA: 8:38 - loss: 1.3721 - regression_loss: 1.1490 - classification_loss: 0.2231
 838/1500 [===============>..............] - ETA: 8:37 - loss: 1.3722 - regression_loss: 1.1491 - classification_loss: 0.2231
 839/1500 [===============>..............] - ETA: 8:37 - loss: 1.3718 - regression_loss: 1.1489 - classification_loss: 0.2230
 840/1500 [===============>..............] - ETA: 8:36 - loss: 1.3710 - regression_loss: 1.1482 - classification_loss: 0.2228
 841/1500 [===============>..............] - ETA: 8:36 - loss: 1.3706 - regression_loss: 1.1480 - classification_loss: 0.2226
 842/1500 [===============>..............] - ETA: 8:36 - loss: 1.3719 - regression_loss: 1.1489 - classification_loss: 0.2229
 843/1500 [===============>..............] - ETA: 8:35 - loss: 1.3718 - regression_loss: 1.1489 - classification_loss: 0.2229
 844/1500 [===============>..............] - ETA: 8:34 - loss: 1.3719 - regression_loss: 1.1490 - classification_loss: 0.2229
 845/1500 [===============>..............] - ETA: 8:33 - loss: 1.3719 - regression_loss: 1.1493 - classification_loss: 0.2227
 846/1500 [===============>..............] - ETA: 8:32 - loss: 1.3713 - regression_loss: 1.1488 - classification_loss: 0.2225
 847/1500 [===============>..............] - ETA: 8:32 - loss: 1.3719 - regression_loss: 1.1492 - classification_loss: 0.2227
 848/1500 [===============>..............] - ETA: 8:31 - loss: 1.3715 - regression_loss: 1.1490 - classification_loss: 0.2226
 849/1500 [===============>..............] - ETA: 8:30 - loss: 1.3714 - regression_loss: 1.1489 - classification_loss: 0.2225
 850/1500 [================>.............] - ETA: 8:29 - loss: 1.3717 - regression_loss: 1.1491 - classification_loss: 0.2226
 851/1500 [================>.............] - ETA: 8:28 - loss: 1.3707 - regression_loss: 1.1483 - classification_loss: 0.2224
 852/1500 [================>.............] - ETA: 8:28 - loss: 1.3706 - regression_loss: 1.1482 - classification_loss: 0.2224
 853/1500 [================>.............] - ETA: 8:27 - loss: 1.3710 - regression_loss: 1.1486 - classification_loss: 0.2223
 854/1500 [================>.............] - ETA: 8:26 - loss: 1.3703 - regression_loss: 1.1481 - classification_loss: 0.2222
 855/1500 [================>.............] - ETA: 8:25 - loss: 1.3701 - regression_loss: 1.1481 - classification_loss: 0.2221
 856/1500 [================>.............] - ETA: 8:25 - loss: 1.3701 - regression_loss: 1.1482 - classification_loss: 0.2220
 857/1500 [================>.............] - ETA: 8:24 - loss: 1.3695 - regression_loss: 1.1477 - classification_loss: 0.2218
 858/1500 [================>.............] - ETA: 8:23 - loss: 1.3697 - regression_loss: 1.1480 - classification_loss: 0.2217
 859/1500 [================>.............] - ETA: 8:22 - loss: 1.3701 - regression_loss: 1.1485 - classification_loss: 0.2216
 860/1500 [================>.............] - ETA: 8:21 - loss: 1.3713 - regression_loss: 1.1495 - classification_loss: 0.2219
 861/1500 [================>.............] - ETA: 8:20 - loss: 1.3720 - regression_loss: 1.1502 - classification_loss: 0.2218
 862/1500 [================>.............] - ETA: 8:19 - loss: 1.3731 - regression_loss: 1.1512 - classification_loss: 0.2219
 863/1500 [================>.............] - ETA: 8:18 - loss: 1.3728 - regression_loss: 1.1510 - classification_loss: 0.2218
 864/1500 [================>.............] - ETA: 8:17 - loss: 1.3720 - regression_loss: 1.1504 - classification_loss: 0.2217
 865/1500 [================>.............] - ETA: 8:16 - loss: 1.3713 - regression_loss: 1.1498 - classification_loss: 0.2215
 866/1500 [================>.............] - ETA: 8:15 - loss: 1.3708 - regression_loss: 1.1494 - classification_loss: 0.2214
 867/1500 [================>.............] - ETA: 8:14 - loss: 1.3699 - regression_loss: 1.1488 - classification_loss: 0.2212
 868/1500 [================>.............] - ETA: 8:13 - loss: 1.3693 - regression_loss: 1.1482 - classification_loss: 0.2211
 869/1500 [================>.............] - ETA: 8:12 - loss: 1.3697 - regression_loss: 1.1487 - classification_loss: 0.2211
 870/1500 [================>.............] - ETA: 8:12 - loss: 1.3713 - regression_loss: 1.1498 - classification_loss: 0.2214
 871/1500 [================>.............] - ETA: 8:12 - loss: 1.3709 - regression_loss: 1.1495 - classification_loss: 0.2214
 872/1500 [================>.............] - ETA: 8:11 - loss: 1.3705 - regression_loss: 1.1494 - classification_loss: 0.2212
 873/1500 [================>.............] - ETA: 8:10 - loss: 1.3700 - regression_loss: 1.1490 - classification_loss: 0.2210
 874/1500 [================>.............] - ETA: 8:09 - loss: 1.3699 - regression_loss: 1.1490 - classification_loss: 0.2209
 875/1500 [================>.............] - ETA: 8:08 - loss: 1.3696 - regression_loss: 1.1489 - classification_loss: 0.2207
 876/1500 [================>.............] - ETA: 8:07 - loss: 1.3695 - regression_loss: 1.1489 - classification_loss: 0.2206
 877/1500 [================>.............] - ETA: 8:06 - loss: 1.3706 - regression_loss: 1.1498 - classification_loss: 0.2208
 878/1500 [================>.............] - ETA: 8:05 - loss: 1.3707 - regression_loss: 1.1499 - classification_loss: 0.2208
 879/1500 [================>.............] - ETA: 8:06 - loss: 1.3695 - regression_loss: 1.1489 - classification_loss: 0.2206
 880/1500 [================>.............] - ETA: 8:05 - loss: 1.3691 - regression_loss: 1.1486 - classification_loss: 0.2205
 881/1500 [================>.............] - ETA: 8:04 - loss: 1.3700 - regression_loss: 1.1492 - classification_loss: 0.2208
 882/1500 [================>.............] - ETA: 8:03 - loss: 1.3705 - regression_loss: 1.1497 - classification_loss: 0.2208
 883/1500 [================>.............] - ETA: 8:02 - loss: 1.3710 - regression_loss: 1.1500 - classification_loss: 0.2209
 884/1500 [================>.............] - ETA: 8:01 - loss: 1.3711 - regression_loss: 1.1502 - classification_loss: 0.2210
 885/1500 [================>.............] - ETA: 8:02 - loss: 1.3708 - regression_loss: 1.1499 - classification_loss: 0.2208
 886/1500 [================>.............] - ETA: 8:01 - loss: 1.3699 - regression_loss: 1.1492 - classification_loss: 0.2207
 887/1500 [================>.............] - ETA: 8:00 - loss: 1.3702 - regression_loss: 1.1496 - classification_loss: 0.2206
 888/1500 [================>.............] - ETA: 7:59 - loss: 1.3699 - regression_loss: 1.1494 - classification_loss: 0.2205
 889/1500 [================>.............] - ETA: 7:58 - loss: 1.3694 - regression_loss: 1.1490 - classification_loss: 0.2204
 890/1500 [================>.............] - ETA: 7:58 - loss: 1.3697 - regression_loss: 1.1492 - classification_loss: 0.2205
 891/1500 [================>.............] - ETA: 7:57 - loss: 1.3691 - regression_loss: 1.1488 - classification_loss: 0.2204
 892/1500 [================>.............] - ETA: 7:56 - loss: 1.3694 - regression_loss: 1.1488 - classification_loss: 0.2206
 893/1500 [================>.............] - ETA: 7:55 - loss: 1.3689 - regression_loss: 1.1485 - classification_loss: 0.2204
 894/1500 [================>.............] - ETA: 7:54 - loss: 1.3694 - regression_loss: 1.1489 - classification_loss: 0.2204
 895/1500 [================>.............] - ETA: 7:53 - loss: 1.3686 - regression_loss: 1.1483 - classification_loss: 0.2203
 896/1500 [================>.............] - ETA: 7:52 - loss: 1.3677 - regression_loss: 1.1476 - classification_loss: 0.2201
 897/1500 [================>.............] - ETA: 7:52 - loss: 1.3675 - regression_loss: 1.1475 - classification_loss: 0.2200
 898/1500 [================>.............] - ETA: 7:51 - loss: 1.3682 - regression_loss: 1.1481 - classification_loss: 0.2201
 899/1500 [================>.............] - ETA: 7:50 - loss: 1.3675 - regression_loss: 1.1475 - classification_loss: 0.2200
 900/1500 [=================>............] - ETA: 7:50 - loss: 1.3683 - regression_loss: 1.1480 - classification_loss: 0.2203
 901/1500 [=================>............] - ETA: 7:50 - loss: 1.3679 - regression_loss: 1.1477 - classification_loss: 0.2202
 902/1500 [=================>............] - ETA: 7:49 - loss: 1.3674 - regression_loss: 1.1473 - classification_loss: 0.2201
 903/1500 [=================>............] - ETA: 7:48 - loss: 1.3666 - regression_loss: 1.1467 - classification_loss: 0.2199
 904/1500 [=================>............] - ETA: 7:47 - loss: 1.3677 - regression_loss: 1.1475 - classification_loss: 0.2202
 905/1500 [=================>............] - ETA: 7:46 - loss: 1.3693 - regression_loss: 1.1489 - classification_loss: 0.2204
 906/1500 [=================>............] - ETA: 7:45 - loss: 1.3692 - regression_loss: 1.1489 - classification_loss: 0.2203
 907/1500 [=================>............] - ETA: 7:44 - loss: 1.3696 - regression_loss: 1.1492 - classification_loss: 0.2204
 908/1500 [=================>............] - ETA: 7:44 - loss: 1.3707 - regression_loss: 1.1501 - classification_loss: 0.2206
 909/1500 [=================>............] - ETA: 7:43 - loss: 1.3710 - regression_loss: 1.1504 - classification_loss: 0.2207
 910/1500 [=================>............] - ETA: 7:43 - loss: 1.3716 - regression_loss: 1.1508 - classification_loss: 0.2208
 911/1500 [=================>............] - ETA: 7:42 - loss: 1.3727 - regression_loss: 1.1516 - classification_loss: 0.2211
 912/1500 [=================>............] - ETA: 7:41 - loss: 1.3728 - regression_loss: 1.1518 - classification_loss: 0.2209
 913/1500 [=================>............] - ETA: 7:40 - loss: 1.3726 - regression_loss: 1.1517 - classification_loss: 0.2208
 914/1500 [=================>............] - ETA: 7:39 - loss: 1.3726 - regression_loss: 1.1518 - classification_loss: 0.2208
 915/1500 [=================>............] - ETA: 7:38 - loss: 1.3728 - regression_loss: 1.1520 - classification_loss: 0.2208
 916/1500 [=================>............] - ETA: 7:37 - loss: 1.3720 - regression_loss: 1.1513 - classification_loss: 0.2207
 917/1500 [=================>............] - ETA: 7:36 - loss: 1.3716 - regression_loss: 1.1511 - classification_loss: 0.2205
 918/1500 [=================>............] - ETA: 7:35 - loss: 1.3713 - regression_loss: 1.1507 - classification_loss: 0.2206
 919/1500 [=================>............] - ETA: 7:35 - loss: 1.3722 - regression_loss: 1.1515 - classification_loss: 0.2206
 920/1500 [=================>............] - ETA: 7:35 - loss: 1.3719 - regression_loss: 1.1514 - classification_loss: 0.2205
 921/1500 [=================>............] - ETA: 7:34 - loss: 1.3725 - regression_loss: 1.1519 - classification_loss: 0.2206
 922/1500 [=================>............] - ETA: 7:32 - loss: 1.3724 - regression_loss: 1.1519 - classification_loss: 0.2205
 923/1500 [=================>............] - ETA: 7:32 - loss: 1.3724 - regression_loss: 1.1520 - classification_loss: 0.2203
 924/1500 [=================>............] - ETA: 7:31 - loss: 1.3717 - regression_loss: 1.1515 - classification_loss: 0.2202
 925/1500 [=================>............] - ETA: 7:30 - loss: 1.3715 - regression_loss: 1.1514 - classification_loss: 0.2201
 926/1500 [=================>............] - ETA: 7:30 - loss: 1.3733 - regression_loss: 1.1530 - classification_loss: 0.2203
 927/1500 [=================>............] - ETA: 7:30 - loss: 1.3729 - regression_loss: 1.1525 - classification_loss: 0.2204
 928/1500 [=================>............] - ETA: 7:29 - loss: 1.3729 - regression_loss: 1.1526 - classification_loss: 0.2204
 929/1500 [=================>............] - ETA: 7:28 - loss: 1.3732 - regression_loss: 1.1529 - classification_loss: 0.2203
 930/1500 [=================>............] - ETA: 7:27 - loss: 1.3723 - regression_loss: 1.1522 - classification_loss: 0.2201
 931/1500 [=================>............] - ETA: 7:26 - loss: 1.3726 - regression_loss: 1.1525 - classification_loss: 0.2201
 932/1500 [=================>............] - ETA: 7:25 - loss: 1.3723 - regression_loss: 1.1523 - classification_loss: 0.2200
 933/1500 [=================>............] - ETA: 7:25 - loss: 1.3726 - regression_loss: 1.1527 - classification_loss: 0.2200
 934/1500 [=================>............] - ETA: 7:24 - loss: 1.3726 - regression_loss: 1.1527 - classification_loss: 0.2199
 935/1500 [=================>............] - ETA: 7:23 - loss: 1.3722 - regression_loss: 1.1524 - classification_loss: 0.2198
 936/1500 [=================>............] - ETA: 7:22 - loss: 1.3720 - regression_loss: 1.1521 - classification_loss: 0.2199
 937/1500 [=================>............] - ETA: 7:22 - loss: 1.3711 - regression_loss: 1.1514 - classification_loss: 0.2197
 938/1500 [=================>............] - ETA: 7:21 - loss: 1.3704 - regression_loss: 1.1508 - classification_loss: 0.2195
 939/1500 [=================>............] - ETA: 7:20 - loss: 1.3696 - regression_loss: 1.1503 - classification_loss: 0.2193
 940/1500 [=================>............] - ETA: 7:19 - loss: 1.3695 - regression_loss: 1.1502 - classification_loss: 0.2193
 941/1500 [=================>............] - ETA: 7:18 - loss: 1.3685 - regression_loss: 1.1495 - classification_loss: 0.2191
 942/1500 [=================>............] - ETA: 7:18 - loss: 1.3690 - regression_loss: 1.1499 - classification_loss: 0.2191
 943/1500 [=================>............] - ETA: 7:17 - loss: 1.3697 - regression_loss: 1.1506 - classification_loss: 0.2192
 944/1500 [=================>............] - ETA: 7:16 - loss: 1.3698 - regression_loss: 1.1507 - classification_loss: 0.2191
 945/1500 [=================>............] - ETA: 7:16 - loss: 1.3695 - regression_loss: 1.1505 - classification_loss: 0.2190
 946/1500 [=================>............] - ETA: 7:15 - loss: 1.3693 - regression_loss: 1.1503 - classification_loss: 0.2189
 947/1500 [=================>............] - ETA: 7:14 - loss: 1.3688 - regression_loss: 1.1499 - classification_loss: 0.2189
 948/1500 [=================>............] - ETA: 7:13 - loss: 1.3678 - regression_loss: 1.1491 - classification_loss: 0.2187
 949/1500 [=================>............] - ETA: 7:12 - loss: 1.3686 - regression_loss: 1.1499 - classification_loss: 0.2187
 950/1500 [==================>...........] - ETA: 7:11 - loss: 1.3686 - regression_loss: 1.1499 - classification_loss: 0.2188
 951/1500 [==================>...........] - ETA: 7:11 - loss: 1.3679 - regression_loss: 1.1493 - classification_loss: 0.2186
 952/1500 [==================>...........] - ETA: 7:10 - loss: 1.3672 - regression_loss: 1.1487 - classification_loss: 0.2185
 953/1500 [==================>...........] - ETA: 7:09 - loss: 1.3676 - regression_loss: 1.1491 - classification_loss: 0.2185
 954/1500 [==================>...........] - ETA: 7:09 - loss: 1.3670 - regression_loss: 1.1484 - classification_loss: 0.2185
 955/1500 [==================>...........] - ETA: 7:08 - loss: 1.3675 - regression_loss: 1.1488 - classification_loss: 0.2188
 956/1500 [==================>...........] - ETA: 7:07 - loss: 1.3671 - regression_loss: 1.1484 - classification_loss: 0.2187
 957/1500 [==================>...........] - ETA: 7:07 - loss: 1.3673 - regression_loss: 1.1484 - classification_loss: 0.2189
 958/1500 [==================>...........] - ETA: 7:06 - loss: 1.3668 - regression_loss: 1.1480 - classification_loss: 0.2187
 959/1500 [==================>...........] - ETA: 7:05 - loss: 1.3670 - regression_loss: 1.1482 - classification_loss: 0.2188
 960/1500 [==================>...........] - ETA: 7:04 - loss: 1.3673 - regression_loss: 1.1485 - classification_loss: 0.2188
 961/1500 [==================>...........] - ETA: 7:03 - loss: 1.3670 - regression_loss: 1.1483 - classification_loss: 0.2187
 962/1500 [==================>...........] - ETA: 7:02 - loss: 1.3670 - regression_loss: 1.1483 - classification_loss: 0.2187
 963/1500 [==================>...........] - ETA: 7:02 - loss: 1.3679 - regression_loss: 1.1489 - classification_loss: 0.2189
 964/1500 [==================>...........] - ETA: 7:01 - loss: 1.3691 - regression_loss: 1.1498 - classification_loss: 0.2192
 965/1500 [==================>...........] - ETA: 7:00 - loss: 1.3688 - regression_loss: 1.1497 - classification_loss: 0.2191
 966/1500 [==================>...........] - ETA: 6:59 - loss: 1.3699 - regression_loss: 1.1506 - classification_loss: 0.2193
 967/1500 [==================>...........] - ETA: 6:58 - loss: 1.3704 - regression_loss: 1.1511 - classification_loss: 0.2193
 968/1500 [==================>...........] - ETA: 6:58 - loss: 1.3701 - regression_loss: 1.1509 - classification_loss: 0.2192
 969/1500 [==================>...........] - ETA: 6:57 - loss: 1.3697 - regression_loss: 1.1507 - classification_loss: 0.2190
 970/1500 [==================>...........] - ETA: 6:56 - loss: 1.3730 - regression_loss: 1.1503 - classification_loss: 0.2227
 971/1500 [==================>...........] - ETA: 6:56 - loss: 1.3725 - regression_loss: 1.1500 - classification_loss: 0.2225
 972/1500 [==================>...........] - ETA: 6:55 - loss: 1.3727 - regression_loss: 1.1502 - classification_loss: 0.2225
 973/1500 [==================>...........] - ETA: 6:54 - loss: 1.3732 - regression_loss: 1.1507 - classification_loss: 0.2225
 974/1500 [==================>...........] - ETA: 6:53 - loss: 1.3730 - regression_loss: 1.1505 - classification_loss: 0.2225
 975/1500 [==================>...........] - ETA: 6:52 - loss: 1.3723 - regression_loss: 1.1500 - classification_loss: 0.2223
 976/1500 [==================>...........] - ETA: 6:51 - loss: 1.3717 - regression_loss: 1.1495 - classification_loss: 0.2222
 977/1500 [==================>...........] - ETA: 6:51 - loss: 1.3714 - regression_loss: 1.1491 - classification_loss: 0.2223
 978/1500 [==================>...........] - ETA: 6:50 - loss: 1.3716 - regression_loss: 1.1494 - classification_loss: 0.2223
 979/1500 [==================>...........] - ETA: 6:49 - loss: 1.3709 - regression_loss: 1.1488 - classification_loss: 0.2221
 980/1500 [==================>...........] - ETA: 6:48 - loss: 1.3718 - regression_loss: 1.1496 - classification_loss: 0.2222
 981/1500 [==================>...........] - ETA: 6:47 - loss: 1.3723 - regression_loss: 1.1501 - classification_loss: 0.2222
 982/1500 [==================>...........] - ETA: 6:46 - loss: 1.3720 - regression_loss: 1.1499 - classification_loss: 0.2221
 983/1500 [==================>...........] - ETA: 6:45 - loss: 1.3720 - regression_loss: 1.1500 - classification_loss: 0.2220
 984/1500 [==================>...........] - ETA: 6:45 - loss: 1.3724 - regression_loss: 1.1504 - classification_loss: 0.2220
 985/1500 [==================>...........] - ETA: 6:44 - loss: 1.3724 - regression_loss: 1.1503 - classification_loss: 0.2220
 986/1500 [==================>...........] - ETA: 6:43 - loss: 1.3715 - regression_loss: 1.1496 - classification_loss: 0.2219
 987/1500 [==================>...........] - ETA: 6:42 - loss: 1.3717 - regression_loss: 1.1494 - classification_loss: 0.2223
 988/1500 [==================>...........] - ETA: 6:41 - loss: 1.3730 - regression_loss: 1.1505 - classification_loss: 0.2225
 989/1500 [==================>...........] - ETA: 6:40 - loss: 1.3730 - regression_loss: 1.1504 - classification_loss: 0.2225
 990/1500 [==================>...........] - ETA: 6:39 - loss: 1.3741 - regression_loss: 1.1514 - classification_loss: 0.2227
 991/1500 [==================>...........] - ETA: 6:39 - loss: 1.3738 - regression_loss: 1.1512 - classification_loss: 0.2226
 992/1500 [==================>...........] - ETA: 6:38 - loss: 1.3737 - regression_loss: 1.1512 - classification_loss: 0.2225
 993/1500 [==================>...........] - ETA: 6:37 - loss: 1.3738 - regression_loss: 1.1510 - classification_loss: 0.2227
 994/1500 [==================>...........] - ETA: 6:37 - loss: 1.3727 - regression_loss: 1.1502 - classification_loss: 0.2225
 995/1500 [==================>...........] - ETA: 6:36 - loss: 1.3722 - regression_loss: 1.1499 - classification_loss: 0.2224
 996/1500 [==================>...........] - ETA: 6:35 - loss: 1.3723 - regression_loss: 1.1500 - classification_loss: 0.2223
 997/1500 [==================>...........] - ETA: 6:34 - loss: 1.3731 - regression_loss: 1.1507 - classification_loss: 0.2224
 998/1500 [==================>...........] - ETA: 6:33 - loss: 1.3726 - regression_loss: 1.1503 - classification_loss: 0.2223
 999/1500 [==================>...........] - ETA: 6:33 - loss: 1.3732 - regression_loss: 1.1506 - classification_loss: 0.2225
1000/1500 [===================>..........] - ETA: 6:32 - loss: 1.3724 - regression_loss: 1.1500 - classification_loss: 0.2224
1001/1500 [===================>..........] - ETA: 6:31 - loss: 1.3719 - regression_loss: 1.1496 - classification_loss: 0.2223
1002/1500 [===================>..........] - ETA: 6:30 - loss: 1.3713 - regression_loss: 1.1492 - classification_loss: 0.2221
1003/1500 [===================>..........] - ETA: 6:29 - loss: 1.3703 - regression_loss: 1.1484 - classification_loss: 0.2219
1004/1500 [===================>..........] - ETA: 6:29 - loss: 1.3711 - regression_loss: 1.1490 - classification_loss: 0.2221
1005/1500 [===================>..........] - ETA: 6:28 - loss: 1.3706 - regression_loss: 1.1486 - classification_loss: 0.2220
1006/1500 [===================>..........] - ETA: 6:27 - loss: 1.3702 - regression_loss: 1.1482 - classification_loss: 0.2220
1007/1500 [===================>..........] - ETA: 6:26 - loss: 1.3702 - regression_loss: 1.1482 - classification_loss: 0.2219
1008/1500 [===================>..........] - ETA: 6:25 - loss: 1.3705 - regression_loss: 1.1484 - classification_loss: 0.2221
1009/1500 [===================>..........] - ETA: 6:24 - loss: 1.3696 - regression_loss: 1.1477 - classification_loss: 0.2219
1010/1500 [===================>..........] - ETA: 6:24 - loss: 1.3700 - regression_loss: 1.1481 - classification_loss: 0.2218
1011/1500 [===================>..........] - ETA: 6:23 - loss: 1.3708 - regression_loss: 1.1489 - classification_loss: 0.2219
1012/1500 [===================>..........] - ETA: 6:22 - loss: 1.3703 - regression_loss: 1.1484 - classification_loss: 0.2219
1013/1500 [===================>..........] - ETA: 6:21 - loss: 1.3705 - regression_loss: 1.1485 - classification_loss: 0.2220
1014/1500 [===================>..........] - ETA: 6:20 - loss: 1.3700 - regression_loss: 1.1480 - classification_loss: 0.2220
1015/1500 [===================>..........] - ETA: 6:20 - loss: 1.3713 - regression_loss: 1.1490 - classification_loss: 0.2224
1016/1500 [===================>..........] - ETA: 6:19 - loss: 1.3708 - regression_loss: 1.1486 - classification_loss: 0.2222
1017/1500 [===================>..........] - ETA: 6:18 - loss: 1.3703 - regression_loss: 1.1482 - classification_loss: 0.2221
1018/1500 [===================>..........] - ETA: 6:17 - loss: 1.3705 - regression_loss: 1.1480 - classification_loss: 0.2225
1019/1500 [===================>..........] - ETA: 6:17 - loss: 1.3711 - regression_loss: 1.1485 - classification_loss: 0.2226
1020/1500 [===================>..........] - ETA: 6:16 - loss: 1.3711 - regression_loss: 1.1486 - classification_loss: 0.2225
1021/1500 [===================>..........] - ETA: 6:15 - loss: 1.3716 - regression_loss: 1.1490 - classification_loss: 0.2227
1022/1500 [===================>..........] - ETA: 6:15 - loss: 1.3717 - regression_loss: 1.1490 - classification_loss: 0.2227
1023/1500 [===================>..........] - ETA: 6:14 - loss: 1.3715 - regression_loss: 1.1490 - classification_loss: 0.2226
1024/1500 [===================>..........] - ETA: 6:13 - loss: 1.3708 - regression_loss: 1.1484 - classification_loss: 0.2224
1025/1500 [===================>..........] - ETA: 6:13 - loss: 1.3707 - regression_loss: 1.1484 - classification_loss: 0.2223
1026/1500 [===================>..........] - ETA: 6:12 - loss: 1.3709 - regression_loss: 1.1485 - classification_loss: 0.2223
1027/1500 [===================>..........] - ETA: 6:11 - loss: 1.3711 - regression_loss: 1.1487 - classification_loss: 0.2224
1028/1500 [===================>..........] - ETA: 6:10 - loss: 1.3714 - regression_loss: 1.1490 - classification_loss: 0.2224
1029/1500 [===================>..........] - ETA: 6:09 - loss: 1.3710 - regression_loss: 1.1487 - classification_loss: 0.2223
1030/1500 [===================>..........] - ETA: 6:08 - loss: 1.3713 - regression_loss: 1.1488 - classification_loss: 0.2225
1031/1500 [===================>..........] - ETA: 6:07 - loss: 1.3713 - regression_loss: 1.1489 - classification_loss: 0.2224
1032/1500 [===================>..........] - ETA: 6:06 - loss: 1.3706 - regression_loss: 1.1483 - classification_loss: 0.2223
1033/1500 [===================>..........] - ETA: 6:06 - loss: 1.3702 - regression_loss: 1.1481 - classification_loss: 0.2222
1034/1500 [===================>..........] - ETA: 6:05 - loss: 1.3709 - regression_loss: 1.1488 - classification_loss: 0.2221
1035/1500 [===================>..........] - ETA: 6:04 - loss: 1.3701 - regression_loss: 1.1482 - classification_loss: 0.2220
1036/1500 [===================>..........] - ETA: 6:03 - loss: 1.3695 - regression_loss: 1.1476 - classification_loss: 0.2219
1037/1500 [===================>..........] - ETA: 6:02 - loss: 1.3693 - regression_loss: 1.1475 - classification_loss: 0.2218
1038/1500 [===================>..........] - ETA: 6:01 - loss: 1.3698 - regression_loss: 1.1479 - classification_loss: 0.2219
1039/1500 [===================>..........] - ETA: 6:00 - loss: 1.3709 - regression_loss: 1.1488 - classification_loss: 0.2221
1040/1500 [===================>..........] - ETA: 5:59 - loss: 1.3709 - regression_loss: 1.1489 - classification_loss: 0.2220
1041/1500 [===================>..........] - ETA: 5:58 - loss: 1.3710 - regression_loss: 1.1489 - classification_loss: 0.2221
1042/1500 [===================>..........] - ETA: 5:57 - loss: 1.3711 - regression_loss: 1.1491 - classification_loss: 0.2220
1043/1500 [===================>..........] - ETA: 5:57 - loss: 1.3705 - regression_loss: 1.1486 - classification_loss: 0.2218
1044/1500 [===================>..........] - ETA: 5:56 - loss: 1.3700 - regression_loss: 1.1482 - classification_loss: 0.2218
1045/1500 [===================>..........] - ETA: 5:56 - loss: 1.3703 - regression_loss: 1.1484 - classification_loss: 0.2219
1046/1500 [===================>..........] - ETA: 5:55 - loss: 1.3697 - regression_loss: 1.1480 - classification_loss: 0.2217
1047/1500 [===================>..........] - ETA: 5:54 - loss: 1.3697 - regression_loss: 1.1480 - classification_loss: 0.2217
1048/1500 [===================>..........] - ETA: 5:53 - loss: 1.3702 - regression_loss: 1.1486 - classification_loss: 0.2216
1049/1500 [===================>..........] - ETA: 5:52 - loss: 1.3716 - regression_loss: 1.1497 - classification_loss: 0.2220
1050/1500 [====================>.........] - ETA: 5:51 - loss: 1.3715 - regression_loss: 1.1496 - classification_loss: 0.2220
1051/1500 [====================>.........] - ETA: 5:50 - loss: 1.3728 - regression_loss: 1.1504 - classification_loss: 0.2224
1052/1500 [====================>.........] - ETA: 5:49 - loss: 1.3723 - regression_loss: 1.1500 - classification_loss: 0.2223
1053/1500 [====================>.........] - ETA: 5:49 - loss: 1.3718 - regression_loss: 1.1496 - classification_loss: 0.2222
1054/1500 [====================>.........] - ETA: 5:48 - loss: 1.3719 - regression_loss: 1.1495 - classification_loss: 0.2224
1055/1500 [====================>.........] - ETA: 5:47 - loss: 1.3714 - regression_loss: 1.1491 - classification_loss: 0.2224
1056/1500 [====================>.........] - ETA: 5:46 - loss: 1.3708 - regression_loss: 1.1485 - classification_loss: 0.2222
1057/1500 [====================>.........] - ETA: 5:45 - loss: 1.3705 - regression_loss: 1.1483 - classification_loss: 0.2222
1058/1500 [====================>.........] - ETA: 5:45 - loss: 1.3702 - regression_loss: 1.1482 - classification_loss: 0.2221
1059/1500 [====================>.........] - ETA: 5:44 - loss: 1.3704 - regression_loss: 1.1483 - classification_loss: 0.2221
1060/1500 [====================>.........] - ETA: 5:43 - loss: 1.3703 - regression_loss: 1.1482 - classification_loss: 0.2222
1061/1500 [====================>.........] - ETA: 5:42 - loss: 1.3699 - regression_loss: 1.1479 - classification_loss: 0.2220
1062/1500 [====================>.........] - ETA: 5:41 - loss: 1.3695 - regression_loss: 1.1476 - classification_loss: 0.2219
1063/1500 [====================>.........] - ETA: 5:41 - loss: 1.3692 - regression_loss: 1.1474 - classification_loss: 0.2218
1064/1500 [====================>.........] - ETA: 5:40 - loss: 1.3686 - regression_loss: 1.1470 - classification_loss: 0.2217
1065/1500 [====================>.........] - ETA: 5:39 - loss: 1.3682 - regression_loss: 1.1466 - classification_loss: 0.2215
1066/1500 [====================>.........] - ETA: 5:38 - loss: 1.3685 - regression_loss: 1.1467 - classification_loss: 0.2218
1067/1500 [====================>.........] - ETA: 5:37 - loss: 1.3683 - regression_loss: 1.1465 - classification_loss: 0.2218
1068/1500 [====================>.........] - ETA: 5:36 - loss: 1.3676 - regression_loss: 1.1460 - classification_loss: 0.2216
1069/1500 [====================>.........] - ETA: 5:36 - loss: 1.3680 - regression_loss: 1.1464 - classification_loss: 0.2216
1070/1500 [====================>.........] - ETA: 5:36 - loss: 1.3682 - regression_loss: 1.1465 - classification_loss: 0.2216
1071/1500 [====================>.........] - ETA: 5:35 - loss: 1.3675 - regression_loss: 1.1460 - classification_loss: 0.2215
1072/1500 [====================>.........] - ETA: 5:34 - loss: 1.3671 - regression_loss: 1.1457 - classification_loss: 0.2214
1073/1500 [====================>.........] - ETA: 5:33 - loss: 1.3668 - regression_loss: 1.1455 - classification_loss: 0.2213
1074/1500 [====================>.........] - ETA: 5:33 - loss: 1.3664 - regression_loss: 1.1452 - classification_loss: 0.2212
1075/1500 [====================>.........] - ETA: 5:32 - loss: 1.3667 - regression_loss: 1.1455 - classification_loss: 0.2212
1076/1500 [====================>.........] - ETA: 5:31 - loss: 1.3676 - regression_loss: 1.1463 - classification_loss: 0.2213
1077/1500 [====================>.........] - ETA: 5:30 - loss: 1.3670 - regression_loss: 1.1459 - classification_loss: 0.2212
1078/1500 [====================>.........] - ETA: 5:29 - loss: 1.3673 - regression_loss: 1.1462 - classification_loss: 0.2211
1079/1500 [====================>.........] - ETA: 5:29 - loss: 1.3670 - regression_loss: 1.1459 - classification_loss: 0.2211
1080/1500 [====================>.........] - ETA: 5:28 - loss: 1.3670 - regression_loss: 1.1460 - classification_loss: 0.2211
1081/1500 [====================>.........] - ETA: 5:28 - loss: 1.3681 - regression_loss: 1.1468 - classification_loss: 0.2213
1082/1500 [====================>.........] - ETA: 5:27 - loss: 1.3690 - regression_loss: 1.1473 - classification_loss: 0.2217
1083/1500 [====================>.........] - ETA: 5:26 - loss: 1.3692 - regression_loss: 1.1475 - classification_loss: 0.2217
1084/1500 [====================>.........] - ETA: 5:25 - loss: 1.3689 - regression_loss: 1.1473 - classification_loss: 0.2216
1085/1500 [====================>.........] - ETA: 5:24 - loss: 1.3692 - regression_loss: 1.1469 - classification_loss: 0.2223
1086/1500 [====================>.........] - ETA: 5:24 - loss: 1.3690 - regression_loss: 1.1468 - classification_loss: 0.2222
1087/1500 [====================>.........] - ETA: 5:23 - loss: 1.3682 - regression_loss: 1.1462 - classification_loss: 0.2221
1088/1500 [====================>.........] - ETA: 5:22 - loss: 1.3686 - regression_loss: 1.1465 - classification_loss: 0.2220
1089/1500 [====================>.........] - ETA: 5:21 - loss: 1.3686 - regression_loss: 1.1466 - classification_loss: 0.2220
1090/1500 [====================>.........] - ETA: 5:20 - loss: 1.3688 - regression_loss: 1.1469 - classification_loss: 0.2219
1091/1500 [====================>.........] - ETA: 5:19 - loss: 1.3684 - regression_loss: 1.1466 - classification_loss: 0.2218
1092/1500 [====================>.........] - ETA: 5:19 - loss: 1.3676 - regression_loss: 1.1460 - classification_loss: 0.2217
1093/1500 [====================>.........] - ETA: 5:18 - loss: 1.3688 - regression_loss: 1.1468 - classification_loss: 0.2220
1094/1500 [====================>.........] - ETA: 5:17 - loss: 1.3693 - regression_loss: 1.1474 - classification_loss: 0.2219
1095/1500 [====================>.........] - ETA: 5:16 - loss: 1.3693 - regression_loss: 1.1474 - classification_loss: 0.2219
1096/1500 [====================>.........] - ETA: 5:16 - loss: 1.3697 - regression_loss: 1.1478 - classification_loss: 0.2219
1097/1500 [====================>.........] - ETA: 5:15 - loss: 1.3697 - regression_loss: 1.1478 - classification_loss: 0.2219
1098/1500 [====================>.........] - ETA: 5:14 - loss: 1.3698 - regression_loss: 1.1479 - classification_loss: 0.2219
1099/1500 [====================>.........] - ETA: 5:13 - loss: 1.3694 - regression_loss: 1.1475 - classification_loss: 0.2219
1100/1500 [=====================>........] - ETA: 5:13 - loss: 1.3695 - regression_loss: 1.1477 - classification_loss: 0.2218
1101/1500 [=====================>........] - ETA: 5:12 - loss: 1.3711 - regression_loss: 1.1488 - classification_loss: 0.2223
1102/1500 [=====================>........] - ETA: 5:11 - loss: 1.3707 - regression_loss: 1.1486 - classification_loss: 0.2221
1103/1500 [=====================>........] - ETA: 5:10 - loss: 1.3714 - regression_loss: 1.1491 - classification_loss: 0.2223
1104/1500 [=====================>........] - ETA: 5:10 - loss: 1.3724 - regression_loss: 1.1500 - classification_loss: 0.2224
1105/1500 [=====================>........] - ETA: 5:09 - loss: 1.3724 - regression_loss: 1.1500 - classification_loss: 0.2223
1106/1500 [=====================>........] - ETA: 5:08 - loss: 1.3726 - regression_loss: 1.1502 - classification_loss: 0.2224
1107/1500 [=====================>........] - ETA: 5:07 - loss: 1.3729 - regression_loss: 1.1505 - classification_loss: 0.2223
1108/1500 [=====================>........] - ETA: 5:06 - loss: 1.3720 - regression_loss: 1.1498 - classification_loss: 0.2222
1109/1500 [=====================>........] - ETA: 5:06 - loss: 1.3715 - regression_loss: 1.1495 - classification_loss: 0.2221
1110/1500 [=====================>........] - ETA: 5:05 - loss: 1.3712 - regression_loss: 1.1493 - classification_loss: 0.2220
1111/1500 [=====================>........] - ETA: 5:04 - loss: 1.3721 - regression_loss: 1.1500 - classification_loss: 0.2221
1112/1500 [=====================>........] - ETA: 5:03 - loss: 1.3722 - regression_loss: 1.1501 - classification_loss: 0.2221
1113/1500 [=====================>........] - ETA: 5:02 - loss: 1.3716 - regression_loss: 1.1496 - classification_loss: 0.2220
1114/1500 [=====================>........] - ETA: 5:01 - loss: 1.3710 - regression_loss: 1.1492 - classification_loss: 0.2218
1115/1500 [=====================>........] - ETA: 5:01 - loss: 1.3703 - regression_loss: 1.1487 - classification_loss: 0.2217
1116/1500 [=====================>........] - ETA: 5:00 - loss: 1.3705 - regression_loss: 1.1488 - classification_loss: 0.2217
1117/1500 [=====================>........] - ETA: 4:59 - loss: 1.3702 - regression_loss: 1.1486 - classification_loss: 0.2216
1118/1500 [=====================>........] - ETA: 4:58 - loss: 1.3699 - regression_loss: 1.1485 - classification_loss: 0.2214
1119/1500 [=====================>........] - ETA: 4:57 - loss: 1.3694 - regression_loss: 1.1481 - classification_loss: 0.2213
1120/1500 [=====================>........] - ETA: 4:57 - loss: 1.3700 - regression_loss: 1.1488 - classification_loss: 0.2213
1121/1500 [=====================>........] - ETA: 4:56 - loss: 1.3695 - regression_loss: 1.1482 - classification_loss: 0.2213
1122/1500 [=====================>........] - ETA: 4:55 - loss: 1.3702 - regression_loss: 1.1487 - classification_loss: 0.2214
1123/1500 [=====================>........] - ETA: 4:54 - loss: 1.3707 - regression_loss: 1.1491 - classification_loss: 0.2216
1124/1500 [=====================>........] - ETA: 4:53 - loss: 1.3707 - regression_loss: 1.1491 - classification_loss: 0.2216
1125/1500 [=====================>........] - ETA: 4:52 - loss: 1.3707 - regression_loss: 1.1492 - classification_loss: 0.2215
1126/1500 [=====================>........] - ETA: 4:52 - loss: 1.3718 - regression_loss: 1.1500 - classification_loss: 0.2218
1127/1500 [=====================>........] - ETA: 4:51 - loss: 1.3719 - regression_loss: 1.1501 - classification_loss: 0.2218
1128/1500 [=====================>........] - ETA: 4:50 - loss: 1.3713 - regression_loss: 1.1496 - classification_loss: 0.2217
1129/1500 [=====================>........] - ETA: 4:49 - loss: 1.3723 - regression_loss: 1.1504 - classification_loss: 0.2219
1130/1500 [=====================>........] - ETA: 4:49 - loss: 1.3719 - regression_loss: 1.1501 - classification_loss: 0.2218
1131/1500 [=====================>........] - ETA: 4:48 - loss: 1.3726 - regression_loss: 1.1508 - classification_loss: 0.2217
1132/1500 [=====================>........] - ETA: 4:47 - loss: 1.3720 - regression_loss: 1.1504 - classification_loss: 0.2216
1133/1500 [=====================>........] - ETA: 4:46 - loss: 1.3716 - regression_loss: 1.1500 - classification_loss: 0.2216
1134/1500 [=====================>........] - ETA: 4:45 - loss: 1.3708 - regression_loss: 1.1493 - classification_loss: 0.2215
1135/1500 [=====================>........] - ETA: 4:45 - loss: 1.3700 - regression_loss: 1.1487 - classification_loss: 0.2213
1136/1500 [=====================>........] - ETA: 4:44 - loss: 1.3706 - regression_loss: 1.1492 - classification_loss: 0.2214
1137/1500 [=====================>........] - ETA: 4:43 - loss: 1.3705 - regression_loss: 1.1492 - classification_loss: 0.2214
1138/1500 [=====================>........] - ETA: 4:42 - loss: 1.3701 - regression_loss: 1.1488 - classification_loss: 0.2213
1139/1500 [=====================>........] - ETA: 4:41 - loss: 1.3702 - regression_loss: 1.1489 - classification_loss: 0.2213
1140/1500 [=====================>........] - ETA: 4:40 - loss: 1.3701 - regression_loss: 1.1488 - classification_loss: 0.2213
1141/1500 [=====================>........] - ETA: 4:40 - loss: 1.3703 - regression_loss: 1.1490 - classification_loss: 0.2213
1142/1500 [=====================>........] - ETA: 4:39 - loss: 1.3702 - regression_loss: 1.1489 - classification_loss: 0.2213
1143/1500 [=====================>........] - ETA: 4:38 - loss: 1.3711 - regression_loss: 1.1497 - classification_loss: 0.2214
1144/1500 [=====================>........] - ETA: 4:37 - loss: 1.3720 - regression_loss: 1.1505 - classification_loss: 0.2216
1145/1500 [=====================>........] - ETA: 4:36 - loss: 1.3719 - regression_loss: 1.1503 - classification_loss: 0.2215
1146/1500 [=====================>........] - ETA: 4:36 - loss: 1.3719 - regression_loss: 1.1503 - classification_loss: 0.2216
1147/1500 [=====================>........] - ETA: 4:35 - loss: 1.3715 - regression_loss: 1.1500 - classification_loss: 0.2215
1148/1500 [=====================>........] - ETA: 4:34 - loss: 1.3723 - regression_loss: 1.1503 - classification_loss: 0.2221
1149/1500 [=====================>........] - ETA: 4:33 - loss: 1.3723 - regression_loss: 1.1504 - classification_loss: 0.2220
1150/1500 [======================>.......] - ETA: 4:32 - loss: 1.3719 - regression_loss: 1.1500 - classification_loss: 0.2219
1151/1500 [======================>.......] - ETA: 4:32 - loss: 1.3714 - regression_loss: 1.1496 - classification_loss: 0.2218
1152/1500 [======================>.......] - ETA: 4:31 - loss: 1.3709 - regression_loss: 1.1491 - classification_loss: 0.2218
1153/1500 [======================>.......] - ETA: 4:30 - loss: 1.3706 - regression_loss: 1.1489 - classification_loss: 0.2217
1154/1500 [======================>.......] - ETA: 4:29 - loss: 1.3709 - regression_loss: 1.1490 - classification_loss: 0.2218
1155/1500 [======================>.......] - ETA: 4:29 - loss: 1.3702 - regression_loss: 1.1485 - classification_loss: 0.2217
1156/1500 [======================>.......] - ETA: 4:28 - loss: 1.3703 - regression_loss: 1.1486 - classification_loss: 0.2217
1157/1500 [======================>.......] - ETA: 4:27 - loss: 1.3701 - regression_loss: 1.1485 - classification_loss: 0.2215
1158/1500 [======================>.......] - ETA: 4:26 - loss: 1.3696 - regression_loss: 1.1481 - classification_loss: 0.2215
1159/1500 [======================>.......] - ETA: 4:25 - loss: 1.3690 - regression_loss: 1.1476 - classification_loss: 0.2213
1160/1500 [======================>.......] - ETA: 4:25 - loss: 1.3688 - regression_loss: 1.1475 - classification_loss: 0.2213
1161/1500 [======================>.......] - ETA: 4:24 - loss: 1.3684 - regression_loss: 1.1472 - classification_loss: 0.2212
1162/1500 [======================>.......] - ETA: 4:23 - loss: 1.3693 - regression_loss: 1.1479 - classification_loss: 0.2215
1163/1500 [======================>.......] - ETA: 4:23 - loss: 1.3693 - regression_loss: 1.1479 - classification_loss: 0.2214
1164/1500 [======================>.......] - ETA: 4:22 - loss: 1.3696 - regression_loss: 1.1482 - classification_loss: 0.2214
1165/1500 [======================>.......] - ETA: 4:21 - loss: 1.3695 - regression_loss: 1.1482 - classification_loss: 0.2213
1166/1500 [======================>.......] - ETA: 4:20 - loss: 1.3695 - regression_loss: 1.1483 - classification_loss: 0.2212
1167/1500 [======================>.......] - ETA: 4:19 - loss: 1.3705 - regression_loss: 1.1491 - classification_loss: 0.2214
1168/1500 [======================>.......] - ETA: 4:19 - loss: 1.3702 - regression_loss: 1.1489 - classification_loss: 0.2213
1169/1500 [======================>.......] - ETA: 4:18 - loss: 1.3712 - regression_loss: 1.1498 - classification_loss: 0.2214
1170/1500 [======================>.......] - ETA: 4:17 - loss: 1.3713 - regression_loss: 1.1500 - classification_loss: 0.2213
1171/1500 [======================>.......] - ETA: 4:16 - loss: 1.3707 - regression_loss: 1.1495 - classification_loss: 0.2212
1172/1500 [======================>.......] - ETA: 4:16 - loss: 1.3714 - regression_loss: 1.1501 - classification_loss: 0.2213
1173/1500 [======================>.......] - ETA: 4:15 - loss: 1.3707 - regression_loss: 1.1495 - classification_loss: 0.2212
1174/1500 [======================>.......] - ETA: 4:14 - loss: 1.3704 - regression_loss: 1.1493 - classification_loss: 0.2211
1175/1500 [======================>.......] - ETA: 4:13 - loss: 1.3705 - regression_loss: 1.1494 - classification_loss: 0.2211
1176/1500 [======================>.......] - ETA: 4:12 - loss: 1.3706 - regression_loss: 1.1495 - classification_loss: 0.2211
1177/1500 [======================>.......] - ETA: 4:11 - loss: 1.3701 - regression_loss: 1.1491 - classification_loss: 0.2211
1178/1500 [======================>.......] - ETA: 4:11 - loss: 1.3702 - regression_loss: 1.1492 - classification_loss: 0.2210
1179/1500 [======================>.......] - ETA: 4:10 - loss: 1.3708 - regression_loss: 1.1496 - classification_loss: 0.2212
1180/1500 [======================>.......] - ETA: 4:09 - loss: 1.3701 - regression_loss: 1.1491 - classification_loss: 0.2210
1181/1500 [======================>.......] - ETA: 4:08 - loss: 1.3701 - regression_loss: 1.1491 - classification_loss: 0.2210
1182/1500 [======================>.......] - ETA: 4:08 - loss: 1.3696 - regression_loss: 1.1487 - classification_loss: 0.2209
1183/1500 [======================>.......] - ETA: 4:07 - loss: 1.3705 - regression_loss: 1.1494 - classification_loss: 0.2211
1184/1500 [======================>.......] - ETA: 4:06 - loss: 1.3707 - regression_loss: 1.1494 - classification_loss: 0.2213
1185/1500 [======================>.......] - ETA: 4:05 - loss: 1.3704 - regression_loss: 1.1492 - classification_loss: 0.2212
1186/1500 [======================>.......] - ETA: 4:04 - loss: 1.3711 - regression_loss: 1.1493 - classification_loss: 0.2218
1187/1500 [======================>.......] - ETA: 4:03 - loss: 1.3708 - regression_loss: 1.1490 - classification_loss: 0.2218
1188/1500 [======================>.......] - ETA: 4:03 - loss: 1.3710 - regression_loss: 1.1491 - classification_loss: 0.2219
1189/1500 [======================>.......] - ETA: 4:02 - loss: 1.3710 - regression_loss: 1.1492 - classification_loss: 0.2219
1190/1500 [======================>.......] - ETA: 4:01 - loss: 1.3705 - regression_loss: 1.1488 - classification_loss: 0.2218
1191/1500 [======================>.......] - ETA: 4:00 - loss: 1.3708 - regression_loss: 1.1491 - classification_loss: 0.2217
1192/1500 [======================>.......] - ETA: 3:59 - loss: 1.3706 - regression_loss: 1.1490 - classification_loss: 0.2217
1193/1500 [======================>.......] - ETA: 3:58 - loss: 1.3708 - regression_loss: 1.1491 - classification_loss: 0.2217
1194/1500 [======================>.......] - ETA: 3:58 - loss: 1.3702 - regression_loss: 1.1486 - classification_loss: 0.2216
1195/1500 [======================>.......] - ETA: 3:57 - loss: 1.3699 - regression_loss: 1.1484 - classification_loss: 0.2215
1196/1500 [======================>.......] - ETA: 3:56 - loss: 1.3703 - regression_loss: 1.1487 - classification_loss: 0.2216
1197/1500 [======================>.......] - ETA: 3:56 - loss: 1.3705 - regression_loss: 1.1490 - classification_loss: 0.2215
1198/1500 [======================>.......] - ETA: 3:55 - loss: 1.3706 - regression_loss: 1.1490 - classification_loss: 0.2216
1199/1500 [======================>.......] - ETA: 3:54 - loss: 1.3717 - regression_loss: 1.1498 - classification_loss: 0.2219
1200/1500 [=======================>......] - ETA: 3:54 - loss: 1.3709 - regression_loss: 1.1492 - classification_loss: 0.2218
1201/1500 [=======================>......] - ETA: 3:53 - loss: 1.3705 - regression_loss: 1.1489 - classification_loss: 0.2216
1202/1500 [=======================>......] - ETA: 3:52 - loss: 1.3707 - regression_loss: 1.1491 - classification_loss: 0.2217
1203/1500 [=======================>......] - ETA: 3:51 - loss: 1.3718 - regression_loss: 1.1499 - classification_loss: 0.2219
1204/1500 [=======================>......] - ETA: 3:51 - loss: 1.3718 - regression_loss: 1.1500 - classification_loss: 0.2219
1205/1500 [=======================>......] - ETA: 3:50 - loss: 1.3714 - regression_loss: 1.1496 - classification_loss: 0.2218
1206/1500 [=======================>......] - ETA: 3:49 - loss: 1.3727 - regression_loss: 1.1506 - classification_loss: 0.2222
1207/1500 [=======================>......] - ETA: 3:48 - loss: 1.3726 - regression_loss: 1.1505 - classification_loss: 0.2222
1208/1500 [=======================>......] - ETA: 3:47 - loss: 1.3727 - regression_loss: 1.1506 - classification_loss: 0.2221
1209/1500 [=======================>......] - ETA: 3:47 - loss: 1.3726 - regression_loss: 1.1505 - classification_loss: 0.2221
1210/1500 [=======================>......] - ETA: 3:46 - loss: 1.3726 - regression_loss: 1.1505 - classification_loss: 0.2221
1211/1500 [=======================>......] - ETA: 3:45 - loss: 1.3725 - regression_loss: 1.1504 - classification_loss: 0.2221
1212/1500 [=======================>......] - ETA: 3:44 - loss: 1.3724 - regression_loss: 1.1504 - classification_loss: 0.2220
1213/1500 [=======================>......] - ETA: 3:43 - loss: 1.3720 - regression_loss: 1.1501 - classification_loss: 0.2219
1214/1500 [=======================>......] - ETA: 3:42 - loss: 1.3721 - regression_loss: 1.1502 - classification_loss: 0.2219
1215/1500 [=======================>......] - ETA: 3:42 - loss: 1.3720 - regression_loss: 1.1500 - classification_loss: 0.2220
1216/1500 [=======================>......] - ETA: 3:41 - loss: 1.3721 - regression_loss: 1.1502 - classification_loss: 0.2218
1217/1500 [=======================>......] - ETA: 3:40 - loss: 1.3719 - regression_loss: 1.1502 - classification_loss: 0.2217
1218/1500 [=======================>......] - ETA: 3:39 - loss: 1.3720 - regression_loss: 1.1503 - classification_loss: 0.2217
1219/1500 [=======================>......] - ETA: 3:39 - loss: 1.3726 - regression_loss: 1.1507 - classification_loss: 0.2219
1220/1500 [=======================>......] - ETA: 3:38 - loss: 1.3721 - regression_loss: 1.1503 - classification_loss: 0.2218
1221/1500 [=======================>......] - ETA: 3:37 - loss: 1.3734 - regression_loss: 1.1514 - classification_loss: 0.2221
1222/1500 [=======================>......] - ETA: 3:36 - loss: 1.3736 - regression_loss: 1.1515 - classification_loss: 0.2221
1223/1500 [=======================>......] - ETA: 3:36 - loss: 1.3734 - regression_loss: 1.1514 - classification_loss: 0.2221
1224/1500 [=======================>......] - ETA: 3:35 - loss: 1.3728 - regression_loss: 1.1508 - classification_loss: 0.2219
1225/1500 [=======================>......] - ETA: 3:34 - loss: 1.3736 - regression_loss: 1.1516 - classification_loss: 0.2220
1226/1500 [=======================>......] - ETA: 3:33 - loss: 1.3744 - regression_loss: 1.1522 - classification_loss: 0.2221
1227/1500 [=======================>......] - ETA: 3:32 - loss: 1.3748 - regression_loss: 1.1527 - classification_loss: 0.2222
1228/1500 [=======================>......] - ETA: 3:32 - loss: 1.3747 - regression_loss: 1.1525 - classification_loss: 0.2222
1229/1500 [=======================>......] - ETA: 3:31 - loss: 1.3742 - regression_loss: 1.1522 - classification_loss: 0.2221
1230/1500 [=======================>......] - ETA: 3:30 - loss: 1.3743 - regression_loss: 1.1523 - classification_loss: 0.2220
1231/1500 [=======================>......] - ETA: 3:29 - loss: 1.3743 - regression_loss: 1.1524 - classification_loss: 0.2219
1232/1500 [=======================>......] - ETA: 3:28 - loss: 1.3739 - regression_loss: 1.1519 - classification_loss: 0.2220
1233/1500 [=======================>......] - ETA: 3:28 - loss: 1.3734 - regression_loss: 1.1514 - classification_loss: 0.2220
1234/1500 [=======================>......] - ETA: 3:27 - loss: 1.3737 - regression_loss: 1.1517 - classification_loss: 0.2220
1235/1500 [=======================>......] - ETA: 3:26 - loss: 1.3742 - regression_loss: 1.1521 - classification_loss: 0.2221
1236/1500 [=======================>......] - ETA: 3:25 - loss: 1.3742 - regression_loss: 1.1522 - classification_loss: 0.2221
1237/1500 [=======================>......] - ETA: 3:24 - loss: 1.3737 - regression_loss: 1.1518 - classification_loss: 0.2219
1238/1500 [=======================>......] - ETA: 3:24 - loss: 1.3740 - regression_loss: 1.1521 - classification_loss: 0.2219
1239/1500 [=======================>......] - ETA: 3:23 - loss: 1.3738 - regression_loss: 1.1520 - classification_loss: 0.2218
1240/1500 [=======================>......] - ETA: 3:22 - loss: 1.3742 - regression_loss: 1.1524 - classification_loss: 0.2218
1241/1500 [=======================>......] - ETA: 3:21 - loss: 1.3740 - regression_loss: 1.1523 - classification_loss: 0.2217
1242/1500 [=======================>......] - ETA: 3:21 - loss: 1.3734 - regression_loss: 1.1518 - classification_loss: 0.2216
1243/1500 [=======================>......] - ETA: 3:20 - loss: 1.3730 - regression_loss: 1.1515 - classification_loss: 0.2215
1244/1500 [=======================>......] - ETA: 3:19 - loss: 1.3727 - regression_loss: 1.1513 - classification_loss: 0.2215
1245/1500 [=======================>......] - ETA: 3:18 - loss: 1.3724 - regression_loss: 1.1510 - classification_loss: 0.2214
1246/1500 [=======================>......] - ETA: 3:17 - loss: 1.3723 - regression_loss: 1.1509 - classification_loss: 0.2214
1247/1500 [=======================>......] - ETA: 3:17 - loss: 1.3724 - regression_loss: 1.1510 - classification_loss: 0.2215
1248/1500 [=======================>......] - ETA: 3:16 - loss: 1.3731 - regression_loss: 1.1515 - classification_loss: 0.2217
1249/1500 [=======================>......] - ETA: 3:15 - loss: 1.3736 - regression_loss: 1.1520 - classification_loss: 0.2216
1250/1500 [========================>.....] - ETA: 3:14 - loss: 1.3740 - regression_loss: 1.1523 - classification_loss: 0.2217
1251/1500 [========================>.....] - ETA: 3:14 - loss: 1.3737 - regression_loss: 1.1521 - classification_loss: 0.2216
1252/1500 [========================>.....] - ETA: 3:13 - loss: 1.3735 - regression_loss: 1.1520 - classification_loss: 0.2215
1253/1500 [========================>.....] - ETA: 3:12 - loss: 1.3734 - regression_loss: 1.1519 - classification_loss: 0.2215
1254/1500 [========================>.....] - ETA: 3:11 - loss: 1.3735 - regression_loss: 1.1521 - classification_loss: 0.2215
1255/1500 [========================>.....] - ETA: 3:10 - loss: 1.3732 - regression_loss: 1.1517 - classification_loss: 0.2215
1256/1500 [========================>.....] - ETA: 3:10 - loss: 1.3725 - regression_loss: 1.1511 - classification_loss: 0.2214
1257/1500 [========================>.....] - ETA: 3:09 - loss: 1.3730 - regression_loss: 1.1516 - classification_loss: 0.2215
1258/1500 [========================>.....] - ETA: 3:08 - loss: 1.3736 - regression_loss: 1.1520 - classification_loss: 0.2216
1259/1500 [========================>.....] - ETA: 3:08 - loss: 1.3734 - regression_loss: 1.1518 - classification_loss: 0.2216
1260/1500 [========================>.....] - ETA: 3:07 - loss: 1.3733 - regression_loss: 1.1518 - classification_loss: 0.2215
1261/1500 [========================>.....] - ETA: 3:06 - loss: 1.3736 - regression_loss: 1.1520 - classification_loss: 0.2215
1262/1500 [========================>.....] - ETA: 3:05 - loss: 1.3739 - regression_loss: 1.1524 - classification_loss: 0.2215
1263/1500 [========================>.....] - ETA: 3:04 - loss: 1.3751 - regression_loss: 1.1532 - classification_loss: 0.2219
1264/1500 [========================>.....] - ETA: 3:04 - loss: 1.3750 - regression_loss: 1.1532 - classification_loss: 0.2218
1265/1500 [========================>.....] - ETA: 3:03 - loss: 1.3748 - regression_loss: 1.1531 - classification_loss: 0.2217
1266/1500 [========================>.....] - ETA: 3:02 - loss: 1.3744 - regression_loss: 1.1527 - classification_loss: 0.2217
1267/1500 [========================>.....] - ETA: 3:01 - loss: 1.3747 - regression_loss: 1.1531 - classification_loss: 0.2217
1268/1500 [========================>.....] - ETA: 3:01 - loss: 1.3746 - regression_loss: 1.1531 - classification_loss: 0.2215
1269/1500 [========================>.....] - ETA: 3:00 - loss: 1.3744 - regression_loss: 1.1529 - classification_loss: 0.2215
1270/1500 [========================>.....] - ETA: 2:59 - loss: 1.3746 - regression_loss: 1.1530 - classification_loss: 0.2216
1271/1500 [========================>.....] - ETA: 2:58 - loss: 1.3742 - regression_loss: 1.1527 - classification_loss: 0.2215
1272/1500 [========================>.....] - ETA: 2:57 - loss: 1.3742 - regression_loss: 1.1527 - classification_loss: 0.2215
1273/1500 [========================>.....] - ETA: 2:57 - loss: 1.3747 - regression_loss: 1.1531 - classification_loss: 0.2216
1274/1500 [========================>.....] - ETA: 2:56 - loss: 1.3754 - regression_loss: 1.1535 - classification_loss: 0.2219
1275/1500 [========================>.....] - ETA: 2:55 - loss: 1.3752 - regression_loss: 1.1533 - classification_loss: 0.2219
1276/1500 [========================>.....] - ETA: 2:54 - loss: 1.3758 - regression_loss: 1.1540 - classification_loss: 0.2218
1277/1500 [========================>.....] - ETA: 2:53 - loss: 1.3771 - regression_loss: 1.1549 - classification_loss: 0.2222
1278/1500 [========================>.....] - ETA: 2:53 - loss: 1.3769 - regression_loss: 1.1547 - classification_loss: 0.2222
1279/1500 [========================>.....] - ETA: 2:52 - loss: 1.3763 - regression_loss: 1.1542 - classification_loss: 0.2221
1280/1500 [========================>.....] - ETA: 2:51 - loss: 1.3760 - regression_loss: 1.1540 - classification_loss: 0.2220
1281/1500 [========================>.....] - ETA: 2:50 - loss: 1.3761 - regression_loss: 1.1541 - classification_loss: 0.2220
1282/1500 [========================>.....] - ETA: 2:49 - loss: 1.3762 - regression_loss: 1.1543 - classification_loss: 0.2219
1283/1500 [========================>.....] - ETA: 2:49 - loss: 1.3758 - regression_loss: 1.1540 - classification_loss: 0.2218
1284/1500 [========================>.....] - ETA: 2:48 - loss: 1.3764 - regression_loss: 1.1546 - classification_loss: 0.2219
1285/1500 [========================>.....] - ETA: 2:47 - loss: 1.3767 - regression_loss: 1.1548 - classification_loss: 0.2219
1286/1500 [========================>.....] - ETA: 2:47 - loss: 1.3762 - regression_loss: 1.1543 - classification_loss: 0.2218
1287/1500 [========================>.....] - ETA: 2:46 - loss: 1.3762 - regression_loss: 1.1544 - classification_loss: 0.2218
1288/1500 [========================>.....] - ETA: 2:45 - loss: 1.3756 - regression_loss: 1.1539 - classification_loss: 0.2217
1289/1500 [========================>.....] - ETA: 2:44 - loss: 1.3758 - regression_loss: 1.1542 - classification_loss: 0.2216
1290/1500 [========================>.....] - ETA: 2:43 - loss: 1.3755 - regression_loss: 1.1539 - classification_loss: 0.2216
1291/1500 [========================>.....] - ETA: 2:43 - loss: 1.3750 - regression_loss: 1.1534 - classification_loss: 0.2215
1292/1500 [========================>.....] - ETA: 2:42 - loss: 1.3747 - regression_loss: 1.1533 - classification_loss: 0.2214
1293/1500 [========================>.....] - ETA: 2:41 - loss: 1.3753 - regression_loss: 1.1538 - classification_loss: 0.2215
1294/1500 [========================>.....] - ETA: 2:40 - loss: 1.3749 - regression_loss: 1.1534 - classification_loss: 0.2215
1295/1500 [========================>.....] - ETA: 2:40 - loss: 1.3752 - regression_loss: 1.1535 - classification_loss: 0.2217
1296/1500 [========================>.....] - ETA: 2:39 - loss: 1.3754 - regression_loss: 1.1537 - classification_loss: 0.2218
1297/1500 [========================>.....] - ETA: 2:38 - loss: 1.3756 - regression_loss: 1.1539 - classification_loss: 0.2217
1298/1500 [========================>.....] - ETA: 2:37 - loss: 1.3762 - regression_loss: 1.1544 - classification_loss: 0.2218
1299/1500 [========================>.....] - ETA: 2:37 - loss: 1.3762 - regression_loss: 1.1545 - classification_loss: 0.2218
1300/1500 [=========================>....] - ETA: 2:36 - loss: 1.3764 - regression_loss: 1.1545 - classification_loss: 0.2219
1301/1500 [=========================>....] - ETA: 2:35 - loss: 1.3772 - regression_loss: 1.1552 - classification_loss: 0.2220
1302/1500 [=========================>....] - ETA: 2:34 - loss: 1.3769 - regression_loss: 1.1550 - classification_loss: 0.2219
1303/1500 [=========================>....] - ETA: 2:34 - loss: 1.3775 - regression_loss: 1.1556 - classification_loss: 0.2219
1304/1500 [=========================>....] - ETA: 2:33 - loss: 1.3780 - regression_loss: 1.1561 - classification_loss: 0.2219
1305/1500 [=========================>....] - ETA: 2:32 - loss: 1.3778 - regression_loss: 1.1560 - classification_loss: 0.2218
1306/1500 [=========================>....] - ETA: 2:31 - loss: 1.3782 - regression_loss: 1.1563 - classification_loss: 0.2219
1307/1500 [=========================>....] - ETA: 2:30 - loss: 1.3784 - regression_loss: 1.1564 - classification_loss: 0.2219
1308/1500 [=========================>....] - ETA: 2:30 - loss: 1.3778 - regression_loss: 1.1560 - classification_loss: 0.2218
1309/1500 [=========================>....] - ETA: 2:29 - loss: 1.3773 - regression_loss: 1.1556 - classification_loss: 0.2217
1310/1500 [=========================>....] - ETA: 2:28 - loss: 1.3775 - regression_loss: 1.1556 - classification_loss: 0.2219
1311/1500 [=========================>....] - ETA: 2:28 - loss: 1.3771 - regression_loss: 1.1553 - classification_loss: 0.2218
1312/1500 [=========================>....] - ETA: 2:27 - loss: 1.3775 - regression_loss: 1.1557 - classification_loss: 0.2218
1313/1500 [=========================>....] - ETA: 2:26 - loss: 1.3772 - regression_loss: 1.1554 - classification_loss: 0.2218
1314/1500 [=========================>....] - ETA: 2:25 - loss: 1.3771 - regression_loss: 1.1554 - classification_loss: 0.2217
1315/1500 [=========================>....] - ETA: 2:24 - loss: 1.3765 - regression_loss: 1.1550 - classification_loss: 0.2216
1316/1500 [=========================>....] - ETA: 2:23 - loss: 1.3761 - regression_loss: 1.1546 - classification_loss: 0.2215
1317/1500 [=========================>....] - ETA: 2:23 - loss: 1.3760 - regression_loss: 1.1546 - classification_loss: 0.2214
1318/1500 [=========================>....] - ETA: 2:22 - loss: 1.3770 - regression_loss: 1.1553 - classification_loss: 0.2217
1319/1500 [=========================>....] - ETA: 2:21 - loss: 1.3772 - regression_loss: 1.1555 - classification_loss: 0.2217
1320/1500 [=========================>....] - ETA: 2:20 - loss: 1.3768 - regression_loss: 1.1552 - classification_loss: 0.2216
1321/1500 [=========================>....] - ETA: 2:19 - loss: 1.3765 - regression_loss: 1.1549 - classification_loss: 0.2215
1322/1500 [=========================>....] - ETA: 2:19 - loss: 1.3769 - regression_loss: 1.1551 - classification_loss: 0.2217
1323/1500 [=========================>....] - ETA: 2:18 - loss: 1.3764 - regression_loss: 1.1548 - classification_loss: 0.2216
1324/1500 [=========================>....] - ETA: 2:17 - loss: 1.3760 - regression_loss: 1.1545 - classification_loss: 0.2215
1325/1500 [=========================>....] - ETA: 2:16 - loss: 1.3759 - regression_loss: 1.1544 - classification_loss: 0.2215
1326/1500 [=========================>....] - ETA: 2:15 - loss: 1.3754 - regression_loss: 1.1540 - classification_loss: 0.2214
1327/1500 [=========================>....] - ETA: 2:15 - loss: 1.3755 - regression_loss: 1.1542 - classification_loss: 0.2214
1328/1500 [=========================>....] - ETA: 2:14 - loss: 1.3757 - regression_loss: 1.1542 - classification_loss: 0.2215
1329/1500 [=========================>....] - ETA: 2:13 - loss: 1.3754 - regression_loss: 1.1539 - classification_loss: 0.2215
1330/1500 [=========================>....] - ETA: 2:12 - loss: 1.3749 - regression_loss: 1.1535 - classification_loss: 0.2214
1331/1500 [=========================>....] - ETA: 2:11 - loss: 1.3743 - regression_loss: 1.1530 - classification_loss: 0.2213
1332/1500 [=========================>....] - ETA: 2:11 - loss: 1.3739 - regression_loss: 1.1527 - classification_loss: 0.2212
1333/1500 [=========================>....] - ETA: 2:10 - loss: 1.3735 - regression_loss: 1.1524 - classification_loss: 0.2211
1334/1500 [=========================>....] - ETA: 2:09 - loss: 1.3738 - regression_loss: 1.1527 - classification_loss: 0.2211
1335/1500 [=========================>....] - ETA: 2:08 - loss: 1.3740 - regression_loss: 1.1527 - classification_loss: 0.2213
1336/1500 [=========================>....] - ETA: 2:07 - loss: 1.3736 - regression_loss: 1.1524 - classification_loss: 0.2212
1337/1500 [=========================>....] - ETA: 2:07 - loss: 1.3732 - regression_loss: 1.1521 - classification_loss: 0.2212
1338/1500 [=========================>....] - ETA: 2:06 - loss: 1.3734 - regression_loss: 1.1523 - classification_loss: 0.2211
1339/1500 [=========================>....] - ETA: 2:05 - loss: 1.3733 - regression_loss: 1.1522 - classification_loss: 0.2211
1340/1500 [=========================>....] - ETA: 2:05 - loss: 1.3731 - regression_loss: 1.1521 - classification_loss: 0.2210
1341/1500 [=========================>....] - ETA: 2:04 - loss: 1.3730 - regression_loss: 1.1519 - classification_loss: 0.2211
1342/1500 [=========================>....] - ETA: 2:03 - loss: 1.3730 - regression_loss: 1.1519 - classification_loss: 0.2212
1343/1500 [=========================>....] - ETA: 2:02 - loss: 1.3725 - regression_loss: 1.1515 - classification_loss: 0.2211
1344/1500 [=========================>....] - ETA: 2:01 - loss: 1.3723 - regression_loss: 1.1513 - classification_loss: 0.2211
1345/1500 [=========================>....] - ETA: 2:01 - loss: 1.3729 - regression_loss: 1.1517 - classification_loss: 0.2212
1346/1500 [=========================>....] - ETA: 2:00 - loss: 1.3735 - regression_loss: 1.1522 - classification_loss: 0.2212
1347/1500 [=========================>....] - ETA: 1:59 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2214
1348/1500 [=========================>....] - ETA: 1:58 - loss: 1.3737 - regression_loss: 1.1524 - classification_loss: 0.2213
1349/1500 [=========================>....] - ETA: 1:58 - loss: 1.3736 - regression_loss: 1.1524 - classification_loss: 0.2212
1350/1500 [==========================>...] - ETA: 1:57 - loss: 1.3733 - regression_loss: 1.1522 - classification_loss: 0.2211
1351/1500 [==========================>...] - ETA: 1:56 - loss: 1.3732 - regression_loss: 1.1521 - classification_loss: 0.2212
1352/1500 [==========================>...] - ETA: 1:55 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2214
1353/1500 [==========================>...] - ETA: 1:55 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2214
1354/1500 [==========================>...] - ETA: 1:54 - loss: 1.3739 - regression_loss: 1.1525 - classification_loss: 0.2214
1355/1500 [==========================>...] - ETA: 1:53 - loss: 1.3745 - regression_loss: 1.1529 - classification_loss: 0.2215
1356/1500 [==========================>...] - ETA: 1:52 - loss: 1.3743 - regression_loss: 1.1529 - classification_loss: 0.2214
1357/1500 [==========================>...] - ETA: 1:52 - loss: 1.3746 - regression_loss: 1.1532 - classification_loss: 0.2215
1358/1500 [==========================>...] - ETA: 1:51 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2214
1359/1500 [==========================>...] - ETA: 1:50 - loss: 1.3737 - regression_loss: 1.1524 - classification_loss: 0.2214
1360/1500 [==========================>...] - ETA: 1:49 - loss: 1.3745 - regression_loss: 1.1530 - classification_loss: 0.2215
1361/1500 [==========================>...] - ETA: 1:48 - loss: 1.3741 - regression_loss: 1.1527 - classification_loss: 0.2214
1362/1500 [==========================>...] - ETA: 1:48 - loss: 1.3741 - regression_loss: 1.1528 - classification_loss: 0.2213
1363/1500 [==========================>...] - ETA: 1:47 - loss: 1.3747 - regression_loss: 1.1534 - classification_loss: 0.2213
1364/1500 [==========================>...] - ETA: 1:46 - loss: 1.3752 - regression_loss: 1.1536 - classification_loss: 0.2215
1365/1500 [==========================>...] - ETA: 1:45 - loss: 1.3747 - regression_loss: 1.1533 - classification_loss: 0.2214
1366/1500 [==========================>...] - ETA: 1:45 - loss: 1.3744 - regression_loss: 1.1530 - classification_loss: 0.2214
1367/1500 [==========================>...] - ETA: 1:44 - loss: 1.3740 - regression_loss: 1.1527 - classification_loss: 0.2213
1368/1500 [==========================>...] - ETA: 1:43 - loss: 1.3741 - regression_loss: 1.1528 - classification_loss: 0.2213
1369/1500 [==========================>...] - ETA: 1:42 - loss: 1.3742 - regression_loss: 1.1529 - classification_loss: 0.2213
1370/1500 [==========================>...] - ETA: 1:41 - loss: 1.3739 - regression_loss: 1.1526 - classification_loss: 0.2212
1371/1500 [==========================>...] - ETA: 1:41 - loss: 1.3745 - regression_loss: 1.1532 - classification_loss: 0.2213
1372/1500 [==========================>...] - ETA: 1:40 - loss: 1.3742 - regression_loss: 1.1529 - classification_loss: 0.2213
1373/1500 [==========================>...] - ETA: 1:39 - loss: 1.3746 - regression_loss: 1.1533 - classification_loss: 0.2213
1374/1500 [==========================>...] - ETA: 1:38 - loss: 1.3745 - regression_loss: 1.1532 - classification_loss: 0.2213
1375/1500 [==========================>...] - ETA: 1:38 - loss: 1.3742 - regression_loss: 1.1530 - classification_loss: 0.2212
1376/1500 [==========================>...] - ETA: 1:37 - loss: 1.3739 - regression_loss: 1.1527 - classification_loss: 0.2212
1377/1500 [==========================>...] - ETA: 1:36 - loss: 1.3751 - regression_loss: 1.1536 - classification_loss: 0.2215
1378/1500 [==========================>...] - ETA: 1:35 - loss: 1.3747 - regression_loss: 1.1532 - classification_loss: 0.2215
1379/1500 [==========================>...] - ETA: 1:34 - loss: 1.3748 - regression_loss: 1.1532 - classification_loss: 0.2216
1380/1500 [==========================>...] - ETA: 1:34 - loss: 1.3746 - regression_loss: 1.1530 - classification_loss: 0.2216
1381/1500 [==========================>...] - ETA: 1:33 - loss: 1.3744 - regression_loss: 1.1528 - classification_loss: 0.2216
1382/1500 [==========================>...] - ETA: 1:32 - loss: 1.3753 - regression_loss: 1.1536 - classification_loss: 0.2217
1383/1500 [==========================>...] - ETA: 1:31 - loss: 1.3749 - regression_loss: 1.1533 - classification_loss: 0.2216
1384/1500 [==========================>...] - ETA: 1:31 - loss: 1.3744 - regression_loss: 1.1529 - classification_loss: 0.2215
1385/1500 [==========================>...] - ETA: 1:30 - loss: 1.3741 - regression_loss: 1.1527 - classification_loss: 0.2214
1386/1500 [==========================>...] - ETA: 1:29 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2215
1387/1500 [==========================>...] - ETA: 1:28 - loss: 1.3745 - regression_loss: 1.1530 - classification_loss: 0.2215
1388/1500 [==========================>...] - ETA: 1:28 - loss: 1.3750 - regression_loss: 1.1534 - classification_loss: 0.2216
1389/1500 [==========================>...] - ETA: 1:27 - loss: 1.3750 - regression_loss: 1.1534 - classification_loss: 0.2216
1390/1500 [==========================>...] - ETA: 1:26 - loss: 1.3749 - regression_loss: 1.1534 - classification_loss: 0.2215
1391/1500 [==========================>...] - ETA: 1:25 - loss: 1.3747 - regression_loss: 1.1532 - classification_loss: 0.2215
1392/1500 [==========================>...] - ETA: 1:24 - loss: 1.3753 - regression_loss: 1.1537 - classification_loss: 0.2216
1393/1500 [==========================>...] - ETA: 1:24 - loss: 1.3757 - regression_loss: 1.1541 - classification_loss: 0.2216
1394/1500 [==========================>...] - ETA: 1:23 - loss: 1.3756 - regression_loss: 1.1540 - classification_loss: 0.2216
1395/1500 [==========================>...] - ETA: 1:22 - loss: 1.3759 - regression_loss: 1.1542 - classification_loss: 0.2216
1396/1500 [==========================>...] - ETA: 1:21 - loss: 1.3758 - regression_loss: 1.1542 - classification_loss: 0.2216
1397/1500 [==========================>...] - ETA: 1:21 - loss: 1.3759 - regression_loss: 1.1543 - classification_loss: 0.2216
1398/1500 [==========================>...] - ETA: 1:20 - loss: 1.3755 - regression_loss: 1.1540 - classification_loss: 0.2215
1399/1500 [==========================>...] - ETA: 1:19 - loss: 1.3760 - regression_loss: 1.1543 - classification_loss: 0.2217
1400/1500 [===========================>..] - ETA: 1:18 - loss: 1.3766 - regression_loss: 1.1547 - classification_loss: 0.2218
1401/1500 [===========================>..] - ETA: 1:17 - loss: 1.3768 - regression_loss: 1.1548 - classification_loss: 0.2220
1402/1500 [===========================>..] - ETA: 1:17 - loss: 1.3762 - regression_loss: 1.1544 - classification_loss: 0.2218
1403/1500 [===========================>..] - ETA: 1:16 - loss: 1.3768 - regression_loss: 1.1548 - classification_loss: 0.2220
1404/1500 [===========================>..] - ETA: 1:15 - loss: 1.3762 - regression_loss: 1.1544 - classification_loss: 0.2218
1405/1500 [===========================>..] - ETA: 1:14 - loss: 1.3762 - regression_loss: 1.1545 - classification_loss: 0.2217
1406/1500 [===========================>..] - ETA: 1:13 - loss: 1.3758 - regression_loss: 1.1542 - classification_loss: 0.2217
1407/1500 [===========================>..] - ETA: 1:13 - loss: 1.3763 - regression_loss: 1.1545 - classification_loss: 0.2218
1408/1500 [===========================>..] - ETA: 1:12 - loss: 1.3764 - regression_loss: 1.1547 - classification_loss: 0.2218
1409/1500 [===========================>..] - ETA: 1:11 - loss: 1.3758 - regression_loss: 1.1541 - classification_loss: 0.2217
1410/1500 [===========================>..] - ETA: 1:10 - loss: 1.3752 - regression_loss: 1.1537 - classification_loss: 0.2216
1411/1500 [===========================>..] - ETA: 1:10 - loss: 1.3756 - regression_loss: 1.1540 - classification_loss: 0.2216
1412/1500 [===========================>..] - ETA: 1:09 - loss: 1.3750 - regression_loss: 1.1536 - classification_loss: 0.2214
1413/1500 [===========================>..] - ETA: 1:08 - loss: 1.3752 - regression_loss: 1.1537 - classification_loss: 0.2215
1414/1500 [===========================>..] - ETA: 1:07 - loss: 1.3749 - regression_loss: 1.1535 - classification_loss: 0.2214
1415/1500 [===========================>..] - ETA: 1:06 - loss: 1.3746 - regression_loss: 1.1531 - classification_loss: 0.2215
1416/1500 [===========================>..] - ETA: 1:05 - loss: 1.3742 - regression_loss: 1.1528 - classification_loss: 0.2214
1417/1500 [===========================>..] - ETA: 1:05 - loss: 1.3742 - regression_loss: 1.1529 - classification_loss: 0.2213
1418/1500 [===========================>..] - ETA: 1:04 - loss: 1.3744 - regression_loss: 1.1531 - classification_loss: 0.2212
1419/1500 [===========================>..] - ETA: 1:03 - loss: 1.3743 - regression_loss: 1.1531 - classification_loss: 0.2213
1420/1500 [===========================>..] - ETA: 1:02 - loss: 1.3740 - regression_loss: 1.1528 - classification_loss: 0.2212
1421/1500 [===========================>..] - ETA: 1:01 - loss: 1.3745 - regression_loss: 1.1532 - classification_loss: 0.2212
1422/1500 [===========================>..] - ETA: 1:01 - loss: 1.3739 - regression_loss: 1.1527 - classification_loss: 0.2211
1423/1500 [===========================>..] - ETA: 1:00 - loss: 1.3736 - regression_loss: 1.1525 - classification_loss: 0.2210
1424/1500 [===========================>..] - ETA: 59s - loss: 1.3732 - regression_loss: 1.1521 - classification_loss: 0.2211 
1425/1500 [===========================>..] - ETA: 58s - loss: 1.3737 - regression_loss: 1.1518 - classification_loss: 0.2218
1426/1500 [===========================>..] - ETA: 58s - loss: 1.3751 - regression_loss: 1.1524 - classification_loss: 0.2227
1427/1500 [===========================>..] - ETA: 57s - loss: 1.3749 - regression_loss: 1.1523 - classification_loss: 0.2226
1428/1500 [===========================>..] - ETA: 56s - loss: 1.3750 - regression_loss: 1.1523 - classification_loss: 0.2227
1429/1500 [===========================>..] - ETA: 55s - loss: 1.3753 - regression_loss: 1.1525 - classification_loss: 0.2228
1430/1500 [===========================>..] - ETA: 54s - loss: 1.3749 - regression_loss: 1.1522 - classification_loss: 0.2227
1431/1500 [===========================>..] - ETA: 54s - loss: 1.3749 - regression_loss: 1.1519 - classification_loss: 0.2230
1432/1500 [===========================>..] - ETA: 53s - loss: 1.3747 - regression_loss: 1.1516 - classification_loss: 0.2231
1433/1500 [===========================>..] - ETA: 52s - loss: 1.3746 - regression_loss: 1.1516 - classification_loss: 0.2230
1434/1500 [===========================>..] - ETA: 51s - loss: 1.3745 - regression_loss: 1.1515 - classification_loss: 0.2230
1435/1500 [===========================>..] - ETA: 51s - loss: 1.3749 - regression_loss: 1.1518 - classification_loss: 0.2230
1436/1500 [===========================>..] - ETA: 50s - loss: 1.3752 - regression_loss: 1.1522 - classification_loss: 0.2230
1437/1500 [===========================>..] - ETA: 49s - loss: 1.3754 - regression_loss: 1.1523 - classification_loss: 0.2231
1438/1500 [===========================>..] - ETA: 48s - loss: 1.3757 - regression_loss: 1.1526 - classification_loss: 0.2232
1439/1500 [===========================>..] - ETA: 47s - loss: 1.3763 - regression_loss: 1.1530 - classification_loss: 0.2233
1440/1500 [===========================>..] - ETA: 47s - loss: 1.3767 - regression_loss: 1.1532 - classification_loss: 0.2235
1441/1500 [===========================>..] - ETA: 46s - loss: 1.3774 - regression_loss: 1.1538 - classification_loss: 0.2236
1442/1500 [===========================>..] - ETA: 45s - loss: 1.3770 - regression_loss: 1.1535 - classification_loss: 0.2235
1443/1500 [===========================>..] - ETA: 44s - loss: 1.3775 - regression_loss: 1.1539 - classification_loss: 0.2235
1444/1500 [===========================>..] - ETA: 44s - loss: 1.3771 - regression_loss: 1.1536 - classification_loss: 0.2235
1445/1500 [===========================>..] - ETA: 43s - loss: 1.3765 - regression_loss: 1.1531 - classification_loss: 0.2234
1446/1500 [===========================>..] - ETA: 42s - loss: 1.3761 - regression_loss: 1.1528 - classification_loss: 0.2233
1447/1500 [===========================>..] - ETA: 41s - loss: 1.3757 - regression_loss: 1.1525 - classification_loss: 0.2232
1448/1500 [===========================>..] - ETA: 40s - loss: 1.3753 - regression_loss: 1.1521 - classification_loss: 0.2231
1449/1500 [===========================>..] - ETA: 40s - loss: 1.3747 - regression_loss: 1.1517 - classification_loss: 0.2231
1450/1500 [============================>.] - ETA: 39s - loss: 1.3744 - regression_loss: 1.1514 - classification_loss: 0.2230
1451/1500 [============================>.] - ETA: 38s - loss: 1.3743 - regression_loss: 1.1513 - classification_loss: 0.2230
1452/1500 [============================>.] - ETA: 37s - loss: 1.3741 - regression_loss: 1.1512 - classification_loss: 0.2229
1453/1500 [============================>.] - ETA: 36s - loss: 1.3736 - regression_loss: 1.1508 - classification_loss: 0.2228
1454/1500 [============================>.] - ETA: 36s - loss: 1.3733 - regression_loss: 1.1505 - classification_loss: 0.2227
1455/1500 [============================>.] - ETA: 35s - loss: 1.3729 - regression_loss: 1.1502 - classification_loss: 0.2227
1456/1500 [============================>.] - ETA: 34s - loss: 1.3737 - regression_loss: 1.1509 - classification_loss: 0.2228
1457/1500 [============================>.] - ETA: 33s - loss: 1.3735 - regression_loss: 1.1507 - classification_loss: 0.2228
1458/1500 [============================>.] - ETA: 32s - loss: 1.3736 - regression_loss: 1.1508 - classification_loss: 0.2228
1459/1500 [============================>.] - ETA: 32s - loss: 1.3740 - regression_loss: 1.1513 - classification_loss: 0.2227
1460/1500 [============================>.] - ETA: 31s - loss: 1.3733 - regression_loss: 1.1507 - classification_loss: 0.2226
1461/1500 [============================>.] - ETA: 30s - loss: 1.3732 - regression_loss: 1.1507 - classification_loss: 0.2225
1462/1500 [============================>.] - ETA: 29s - loss: 1.3729 - regression_loss: 1.1505 - classification_loss: 0.2224
1463/1500 [============================>.] - ETA: 28s - loss: 1.3724 - regression_loss: 1.1501 - classification_loss: 0.2223
1464/1500 [============================>.] - ETA: 28s - loss: 1.3720 - regression_loss: 1.1498 - classification_loss: 0.2223
1465/1500 [============================>.] - ETA: 27s - loss: 1.3718 - regression_loss: 1.1496 - classification_loss: 0.2222
1466/1500 [============================>.] - ETA: 26s - loss: 1.3714 - regression_loss: 1.1492 - classification_loss: 0.2222
1467/1500 [============================>.] - ETA: 25s - loss: 1.3713 - regression_loss: 1.1491 - classification_loss: 0.2222
1468/1500 [============================>.] - ETA: 25s - loss: 1.3720 - regression_loss: 1.1497 - classification_loss: 0.2222
1469/1500 [============================>.] - ETA: 24s - loss: 1.3717 - regression_loss: 1.1495 - classification_loss: 0.2222
1470/1500 [============================>.] - ETA: 23s - loss: 1.3718 - regression_loss: 1.1496 - classification_loss: 0.2222
1471/1500 [============================>.] - ETA: 22s - loss: 1.3718 - regression_loss: 1.1496 - classification_loss: 0.2222
1472/1500 [============================>.] - ETA: 21s - loss: 1.3719 - regression_loss: 1.1496 - classification_loss: 0.2223
1473/1500 [============================>.] - ETA: 21s - loss: 1.3720 - regression_loss: 1.1497 - classification_loss: 0.2223
1474/1500 [============================>.] - ETA: 20s - loss: 1.3725 - regression_loss: 1.1501 - classification_loss: 0.2224
1475/1500 [============================>.] - ETA: 19s - loss: 1.3724 - regression_loss: 1.1500 - classification_loss: 0.2224
1476/1500 [============================>.] - ETA: 18s - loss: 1.3719 - regression_loss: 1.1496 - classification_loss: 0.2223
1477/1500 [============================>.] - ETA: 17s - loss: 1.3717 - regression_loss: 1.1495 - classification_loss: 0.2222
1478/1500 [============================>.] - ETA: 17s - loss: 1.3716 - regression_loss: 1.1495 - classification_loss: 0.2221
1479/1500 [============================>.] - ETA: 16s - loss: 1.3712 - regression_loss: 1.1491 - classification_loss: 0.2220
1480/1500 [============================>.] - ETA: 15s - loss: 1.3712 - regression_loss: 1.1491 - classification_loss: 0.2221
1481/1500 [============================>.] - ETA: 14s - loss: 1.3718 - regression_loss: 1.1495 - classification_loss: 0.2222
1482/1500 [============================>.] - ETA: 14s - loss: 1.3723 - regression_loss: 1.1499 - classification_loss: 0.2224
1483/1500 [============================>.] - ETA: 13s - loss: 1.3717 - regression_loss: 1.1494 - classification_loss: 0.2223
1484/1500 [============================>.] - ETA: 12s - loss: 1.3714 - regression_loss: 1.1492 - classification_loss: 0.2222
1485/1500 [============================>.] - ETA: 11s - loss: 1.3710 - regression_loss: 1.1489 - classification_loss: 0.2221
1486/1500 [============================>.] - ETA: 10s - loss: 1.3704 - regression_loss: 1.1484 - classification_loss: 0.2220
1487/1500 [============================>.] - ETA: 10s - loss: 1.3701 - regression_loss: 1.1482 - classification_loss: 0.2219
1488/1500 [============================>.] - ETA: 9s - loss: 1.3701 - regression_loss: 1.1482 - classification_loss: 0.2219 
1489/1500 [============================>.] - ETA: 8s - loss: 1.3699 - regression_loss: 1.1481 - classification_loss: 0.2218
1490/1500 [============================>.] - ETA: 7s - loss: 1.3702 - regression_loss: 1.1484 - classification_loss: 0.2219
1491/1500 [============================>.] - ETA: 7s - loss: 1.3708 - regression_loss: 1.1489 - classification_loss: 0.2220
1492/1500 [============================>.] - ETA: 6s - loss: 1.3708 - regression_loss: 1.1487 - classification_loss: 0.2221
1493/1500 [============================>.] - ETA: 5s - loss: 1.3711 - regression_loss: 1.1489 - classification_loss: 0.2222
1494/1500 [============================>.] - ETA: 4s - loss: 1.3712 - regression_loss: 1.1490 - classification_loss: 0.2221
1495/1500 [============================>.] - ETA: 3s - loss: 1.3717 - regression_loss: 1.1494 - classification_loss: 0.2223
1496/1500 [============================>.] - ETA: 3s - loss: 1.3718 - regression_loss: 1.1495 - classification_loss: 0.2223
1497/1500 [============================>.] - ETA: 2s - loss: 1.3714 - regression_loss: 1.1492 - classification_loss: 0.2222
1498/1500 [============================>.] - ETA: 1s - loss: 1.3709 - regression_loss: 1.1488 - classification_loss: 0.2221
1499/1500 [============================>.] - ETA: 0s - loss: 1.3702 - regression_loss: 1.1483 - classification_loss: 0.2219
1500/1500 [==============================] - 1173s 782ms/step - loss: 1.3697 - regression_loss: 1.1478 - classification_loss: 0.2219

Epoch 00007: saving model to ./snapshots/resnet50_csv_07.h5
Epoch 8/10

   1/1500 [..............................] - ETA: 10:15 - loss: 1.5098 - regression_loss: 1.3444 - classification_loss: 0.1654
   2/1500 [..............................] - ETA: 10:18 - loss: 1.2685 - regression_loss: 1.1070 - classification_loss: 0.1615
   3/1500 [..............................] - ETA: 10:13 - loss: 1.1497 - regression_loss: 1.0090 - classification_loss: 0.1407
   4/1500 [..............................] - ETA: 11:42 - loss: 1.1851 - regression_loss: 0.9967 - classification_loss: 0.1885
   5/1500 [..............................] - ETA: 11:20 - loss: 1.0617 - regression_loss: 0.8947 - classification_loss: 0.1670
   6/1500 [..............................] - ETA: 14:14 - loss: 1.0224 - regression_loss: 0.8754 - classification_loss: 0.1469
   7/1500 [..............................] - ETA: 18:16 - loss: 0.9627 - regression_loss: 0.8178 - classification_loss: 0.1449
   8/1500 [..............................] - ETA: 17:21 - loss: 1.1079 - regression_loss: 0.9355 - classification_loss: 0.1723
   9/1500 [..............................] - ETA: 17:38 - loss: 1.0878 - regression_loss: 0.9171 - classification_loss: 0.1707
  10/1500 [..............................] - ETA: 19:45 - loss: 1.1405 - regression_loss: 0.9664 - classification_loss: 0.1742
  11/1500 [..............................] - ETA: 18:50 - loss: 1.1294 - regression_loss: 0.9588 - classification_loss: 0.1706
  12/1500 [..............................] - ETA: 18:06 - loss: 1.1156 - regression_loss: 0.9459 - classification_loss: 0.1697
  13/1500 [..............................] - ETA: 17:24 - loss: 1.0891 - regression_loss: 0.9261 - classification_loss: 0.1630
  14/1500 [..............................] - ETA: 16:50 - loss: 1.0580 - regression_loss: 0.9021 - classification_loss: 0.1559
  15/1500 [..............................] - ETA: 16:55 - loss: 1.1050 - regression_loss: 0.9395 - classification_loss: 0.1655
  16/1500 [..............................] - ETA: 17:17 - loss: 1.1353 - regression_loss: 0.9675 - classification_loss: 0.1678
  17/1500 [..............................] - ETA: 17:05 - loss: 1.1276 - regression_loss: 0.9631 - classification_loss: 0.1645
  18/1500 [..............................] - ETA: 17:53 - loss: 1.1224 - regression_loss: 0.9551 - classification_loss: 0.1672
  19/1500 [..............................] - ETA: 17:27 - loss: 1.1258 - regression_loss: 0.9603 - classification_loss: 0.1655
  20/1500 [..............................] - ETA: 17:12 - loss: 1.1264 - regression_loss: 0.9641 - classification_loss: 0.1622
  21/1500 [..............................] - ETA: 16:52 - loss: 1.1167 - regression_loss: 0.9569 - classification_loss: 0.1597
  22/1500 [..............................] - ETA: 16:34 - loss: 1.1076 - regression_loss: 0.9481 - classification_loss: 0.1595
  23/1500 [..............................] - ETA: 16:15 - loss: 1.0876 - regression_loss: 0.9322 - classification_loss: 0.1553
  24/1500 [..............................] - ETA: 15:58 - loss: 1.0874 - regression_loss: 0.9320 - classification_loss: 0.1554
  25/1500 [..............................] - ETA: 16:46 - loss: 1.1032 - regression_loss: 0.9370 - classification_loss: 0.1663
  26/1500 [..............................] - ETA: 16:26 - loss: 1.1455 - regression_loss: 0.9719 - classification_loss: 0.1736
  27/1500 [..............................] - ETA: 16:44 - loss: 1.1523 - regression_loss: 0.9754 - classification_loss: 0.1769
  28/1500 [..............................] - ETA: 16:27 - loss: 1.1428 - regression_loss: 0.9693 - classification_loss: 0.1735
  29/1500 [..............................] - ETA: 16:57 - loss: 1.1419 - regression_loss: 0.9694 - classification_loss: 0.1725
  30/1500 [..............................] - ETA: 17:01 - loss: 1.1291 - regression_loss: 0.9581 - classification_loss: 0.1710
  31/1500 [..............................] - ETA: 17:42 - loss: 1.1889 - regression_loss: 0.9628 - classification_loss: 0.2260
  32/1500 [..............................] - ETA: 18:18 - loss: 1.1725 - regression_loss: 0.9475 - classification_loss: 0.2251
  33/1500 [..............................] - ETA: 19:06 - loss: 1.1541 - regression_loss: 0.9335 - classification_loss: 0.2206
  34/1500 [..............................] - ETA: 19:33 - loss: 1.2017 - regression_loss: 0.9727 - classification_loss: 0.2290
  35/1500 [..............................] - ETA: 19:12 - loss: 1.2076 - regression_loss: 0.9788 - classification_loss: 0.2288
  36/1500 [..............................] - ETA: 19:28 - loss: 1.1876 - regression_loss: 0.9633 - classification_loss: 0.2242
  37/1500 [..............................] - ETA: 19:24 - loss: 1.1684 - regression_loss: 0.9488 - classification_loss: 0.2196
  38/1500 [..............................] - ETA: 19:18 - loss: 1.1517 - regression_loss: 0.9360 - classification_loss: 0.2157
  39/1500 [..............................] - ETA: 19:03 - loss: 1.1558 - regression_loss: 0.9396 - classification_loss: 0.2162
  40/1500 [..............................] - ETA: 18:57 - loss: 1.1445 - regression_loss: 0.9316 - classification_loss: 0.2129
  41/1500 [..............................] - ETA: 19:23 - loss: 1.1556 - regression_loss: 0.9443 - classification_loss: 0.2114
  42/1500 [..............................] - ETA: 19:09 - loss: 1.1470 - regression_loss: 0.9381 - classification_loss: 0.2089
  43/1500 [..............................] - ETA: 19:27 - loss: 1.1390 - regression_loss: 0.9335 - classification_loss: 0.2055
  44/1500 [..............................] - ETA: 19:50 - loss: 1.1750 - regression_loss: 0.9605 - classification_loss: 0.2145
  45/1500 [..............................] - ETA: 19:35 - loss: 1.1619 - regression_loss: 0.9497 - classification_loss: 0.2122
  46/1500 [..............................] - ETA: 19:33 - loss: 1.1448 - regression_loss: 0.9360 - classification_loss: 0.2088
  47/1500 [..............................] - ETA: 19:20 - loss: 1.1577 - regression_loss: 0.9486 - classification_loss: 0.2091
  48/1500 [..............................] - ETA: 19:07 - loss: 1.1488 - regression_loss: 0.9412 - classification_loss: 0.2076
  49/1500 [..............................] - ETA: 19:23 - loss: 1.1558 - regression_loss: 0.9465 - classification_loss: 0.2094
  50/1500 [>.............................] - ETA: 20:02 - loss: 1.1412 - regression_loss: 0.9352 - classification_loss: 0.2061
  51/1500 [>.............................] - ETA: 19:47 - loss: 1.1799 - regression_loss: 0.9694 - classification_loss: 0.2106
  52/1500 [>.............................] - ETA: 19:34 - loss: 1.1804 - regression_loss: 0.9705 - classification_loss: 0.2100
  53/1500 [>.............................] - ETA: 19:54 - loss: 1.1815 - regression_loss: 0.9701 - classification_loss: 0.2114
  54/1500 [>.............................] - ETA: 20:45 - loss: 1.1866 - regression_loss: 0.9743 - classification_loss: 0.2123
  55/1500 [>.............................] - ETA: 20:33 - loss: 1.1916 - regression_loss: 0.9800 - classification_loss: 0.2116
  56/1500 [>.............................] - ETA: 20:44 - loss: 1.2126 - regression_loss: 0.9975 - classification_loss: 0.2151
  57/1500 [>.............................] - ETA: 20:36 - loss: 1.2202 - regression_loss: 1.0034 - classification_loss: 0.2168
  58/1500 [>.............................] - ETA: 20:25 - loss: 1.2158 - regression_loss: 1.0018 - classification_loss: 0.2140
  59/1500 [>.............................] - ETA: 20:14 - loss: 1.2164 - regression_loss: 1.0039 - classification_loss: 0.2125
  60/1500 [>.............................] - ETA: 20:02 - loss: 1.2150 - regression_loss: 1.0026 - classification_loss: 0.2125
  61/1500 [>.............................] - ETA: 19:49 - loss: 1.2213 - regression_loss: 1.0062 - classification_loss: 0.2151
  62/1500 [>.............................] - ETA: 19:44 - loss: 1.2188 - regression_loss: 1.0044 - classification_loss: 0.2144
  63/1500 [>.............................] - ETA: 19:34 - loss: 1.2129 - regression_loss: 0.9997 - classification_loss: 0.2132
  64/1500 [>.............................] - ETA: 19:24 - loss: 1.2026 - regression_loss: 0.9912 - classification_loss: 0.2114
  65/1500 [>.............................] - ETA: 19:15 - loss: 1.2234 - regression_loss: 1.0080 - classification_loss: 0.2154
  66/1500 [>.............................] - ETA: 19:39 - loss: 1.2150 - regression_loss: 1.0021 - classification_loss: 0.2128
  67/1500 [>.............................] - ETA: 19:32 - loss: 1.2188 - regression_loss: 1.0066 - classification_loss: 0.2121
  68/1500 [>.............................] - ETA: 19:45 - loss: 1.2149 - regression_loss: 1.0033 - classification_loss: 0.2116
  69/1500 [>.............................] - ETA: 19:51 - loss: 1.2049 - regression_loss: 0.9939 - classification_loss: 0.2110
  70/1500 [>.............................] - ETA: 19:48 - loss: 1.1996 - regression_loss: 0.9895 - classification_loss: 0.2101
  71/1500 [>.............................] - ETA: 19:39 - loss: 1.2163 - regression_loss: 1.0028 - classification_loss: 0.2135
  72/1500 [>.............................] - ETA: 19:33 - loss: 1.2245 - regression_loss: 1.0102 - classification_loss: 0.2143
  73/1500 [>.............................] - ETA: 20:25 - loss: 1.2243 - regression_loss: 1.0111 - classification_loss: 0.2132
  74/1500 [>.............................] - ETA: 20:14 - loss: 1.2183 - regression_loss: 1.0070 - classification_loss: 0.2113
  75/1500 [>.............................] - ETA: 20:05 - loss: 1.2180 - regression_loss: 1.0075 - classification_loss: 0.2105
  76/1500 [>.............................] - ETA: 19:57 - loss: 1.2336 - regression_loss: 1.0214 - classification_loss: 0.2123
  77/1500 [>.............................] - ETA: 19:50 - loss: 1.2393 - regression_loss: 1.0264 - classification_loss: 0.2128
  78/1500 [>.............................] - ETA: 19:58 - loss: 1.2544 - regression_loss: 1.0392 - classification_loss: 0.2152
  79/1500 [>.............................] - ETA: 19:58 - loss: 1.2554 - regression_loss: 1.0396 - classification_loss: 0.2158
  80/1500 [>.............................] - ETA: 20:04 - loss: 1.2527 - regression_loss: 1.0378 - classification_loss: 0.2150
  81/1500 [>.............................] - ETA: 20:24 - loss: 1.2562 - regression_loss: 1.0413 - classification_loss: 0.2149
  82/1500 [>.............................] - ETA: 20:22 - loss: 1.2630 - regression_loss: 1.0463 - classification_loss: 0.2167
  83/1500 [>.............................] - ETA: 20:27 - loss: 1.2725 - regression_loss: 1.0551 - classification_loss: 0.2174
  84/1500 [>.............................] - ETA: 20:25 - loss: 1.2654 - regression_loss: 1.0496 - classification_loss: 0.2157
  85/1500 [>.............................] - ETA: 20:16 - loss: 1.2623 - regression_loss: 1.0482 - classification_loss: 0.2142
  86/1500 [>.............................] - ETA: 20:11 - loss: 1.2528 - regression_loss: 1.0407 - classification_loss: 0.2121
  87/1500 [>.............................] - ETA: 20:03 - loss: 1.2434 - regression_loss: 1.0333 - classification_loss: 0.2101
  88/1500 [>.............................] - ETA: 20:09 - loss: 1.2383 - regression_loss: 1.0296 - classification_loss: 0.2087
  89/1500 [>.............................] - ETA: 20:00 - loss: 1.2398 - regression_loss: 1.0304 - classification_loss: 0.2094
  90/1500 [>.............................] - ETA: 19:51 - loss: 1.2473 - regression_loss: 1.0362 - classification_loss: 0.2111
  91/1500 [>.............................] - ETA: 19:47 - loss: 1.2497 - regression_loss: 1.0399 - classification_loss: 0.2098
  92/1500 [>.............................] - ETA: 19:49 - loss: 1.2569 - regression_loss: 1.0444 - classification_loss: 0.2125
  93/1500 [>.............................] - ETA: 19:40 - loss: 1.2616 - regression_loss: 1.0490 - classification_loss: 0.2126
  94/1500 [>.............................] - ETA: 19:37 - loss: 1.2540 - regression_loss: 1.0424 - classification_loss: 0.2117
  95/1500 [>.............................] - ETA: 19:29 - loss: 1.2520 - regression_loss: 1.0409 - classification_loss: 0.2111
  96/1500 [>.............................] - ETA: 19:38 - loss: 1.2460 - regression_loss: 1.0364 - classification_loss: 0.2095
  97/1500 [>.............................] - ETA: 19:42 - loss: 1.2430 - regression_loss: 1.0343 - classification_loss: 0.2087
  98/1500 [>.............................] - ETA: 19:38 - loss: 1.2523 - regression_loss: 1.0423 - classification_loss: 0.2100
  99/1500 [>.............................] - ETA: 19:32 - loss: 1.2543 - regression_loss: 1.0449 - classification_loss: 0.2094
 100/1500 [=>............................] - ETA: 19:36 - loss: 1.2531 - regression_loss: 1.0438 - classification_loss: 0.2093
 101/1500 [=>............................] - ETA: 19:54 - loss: 1.2549 - regression_loss: 1.0441 - classification_loss: 0.2108
 102/1500 [=>............................] - ETA: 19:53 - loss: 1.2560 - regression_loss: 1.0457 - classification_loss: 0.2103
 103/1500 [=>............................] - ETA: 19:47 - loss: 1.2552 - regression_loss: 1.0460 - classification_loss: 0.2092
 104/1500 [=>............................] - ETA: 19:40 - loss: 1.2597 - regression_loss: 1.0467 - classification_loss: 0.2130
 105/1500 [=>............................] - ETA: 19:37 - loss: 1.2662 - regression_loss: 1.0520 - classification_loss: 0.2143
 106/1500 [=>............................] - ETA: 19:30 - loss: 1.2605 - regression_loss: 1.0479 - classification_loss: 0.2127
 107/1500 [=>............................] - ETA: 19:23 - loss: 1.2590 - regression_loss: 1.0472 - classification_loss: 0.2118
 108/1500 [=>............................] - ETA: 19:17 - loss: 1.2558 - regression_loss: 1.0442 - classification_loss: 0.2116
 109/1500 [=>............................] - ETA: 19:16 - loss: 1.2546 - regression_loss: 1.0441 - classification_loss: 0.2105
 110/1500 [=>............................] - ETA: 19:22 - loss: 1.2479 - regression_loss: 1.0384 - classification_loss: 0.2094
 111/1500 [=>............................] - ETA: 19:16 - loss: 1.2446 - regression_loss: 1.0362 - classification_loss: 0.2085
 112/1500 [=>............................] - ETA: 19:12 - loss: 1.2499 - regression_loss: 1.0410 - classification_loss: 0.2088
 113/1500 [=>............................] - ETA: 19:08 - loss: 1.2512 - regression_loss: 1.0425 - classification_loss: 0.2086
 114/1500 [=>............................] - ETA: 19:07 - loss: 1.2603 - regression_loss: 1.0512 - classification_loss: 0.2091
 115/1500 [=>............................] - ETA: 19:02 - loss: 1.2560 - regression_loss: 1.0482 - classification_loss: 0.2078
 116/1500 [=>............................] - ETA: 18:58 - loss: 1.2499 - regression_loss: 1.0433 - classification_loss: 0.2066
 117/1500 [=>............................] - ETA: 19:06 - loss: 1.2511 - regression_loss: 1.0437 - classification_loss: 0.2074
 118/1500 [=>............................] - ETA: 19:00 - loss: 1.2459 - regression_loss: 1.0393 - classification_loss: 0.2067
 119/1500 [=>............................] - ETA: 19:00 - loss: 1.2426 - regression_loss: 1.0365 - classification_loss: 0.2061
 120/1500 [=>............................] - ETA: 18:56 - loss: 1.2552 - regression_loss: 1.0464 - classification_loss: 0.2088
 121/1500 [=>............................] - ETA: 18:50 - loss: 1.2670 - regression_loss: 1.0553 - classification_loss: 0.2117
 122/1500 [=>............................] - ETA: 18:53 - loss: 1.2620 - regression_loss: 1.0514 - classification_loss: 0.2106
 123/1500 [=>............................] - ETA: 18:52 - loss: 1.2620 - regression_loss: 1.0517 - classification_loss: 0.2103
 124/1500 [=>............................] - ETA: 18:47 - loss: 1.2571 - regression_loss: 1.0479 - classification_loss: 0.2092
 125/1500 [=>............................] - ETA: 18:44 - loss: 1.2620 - regression_loss: 1.0524 - classification_loss: 0.2095
 126/1500 [=>............................] - ETA: 18:53 - loss: 1.2581 - regression_loss: 1.0498 - classification_loss: 0.2083
 127/1500 [=>............................] - ETA: 18:48 - loss: 1.2536 - regression_loss: 1.0463 - classification_loss: 0.2073
 128/1500 [=>............................] - ETA: 18:43 - loss: 1.2534 - regression_loss: 1.0458 - classification_loss: 0.2076
 129/1500 [=>............................] - ETA: 18:37 - loss: 1.2579 - regression_loss: 1.0500 - classification_loss: 0.2078
 130/1500 [=>............................] - ETA: 18:34 - loss: 1.2620 - regression_loss: 1.0540 - classification_loss: 0.2080
 131/1500 [=>............................] - ETA: 18:31 - loss: 1.2679 - regression_loss: 1.0599 - classification_loss: 0.2079
 132/1500 [=>............................] - ETA: 18:29 - loss: 1.2607 - regression_loss: 1.0538 - classification_loss: 0.2069
 133/1500 [=>............................] - ETA: 18:26 - loss: 1.2592 - regression_loss: 1.0527 - classification_loss: 0.2066
 134/1500 [=>............................] - ETA: 18:21 - loss: 1.2603 - regression_loss: 1.0540 - classification_loss: 0.2064
 135/1500 [=>............................] - ETA: 18:25 - loss: 1.2640 - regression_loss: 1.0573 - classification_loss: 0.2067
 136/1500 [=>............................] - ETA: 18:26 - loss: 1.2716 - regression_loss: 1.0597 - classification_loss: 0.2119
 137/1500 [=>............................] - ETA: 18:29 - loss: 1.2735 - regression_loss: 1.0614 - classification_loss: 0.2121
 138/1500 [=>............................] - ETA: 18:28 - loss: 1.2713 - regression_loss: 1.0594 - classification_loss: 0.2119
 139/1500 [=>............................] - ETA: 18:23 - loss: 1.2683 - regression_loss: 1.0575 - classification_loss: 0.2108
 140/1500 [=>............................] - ETA: 18:18 - loss: 1.2698 - regression_loss: 1.0590 - classification_loss: 0.2109
 141/1500 [=>............................] - ETA: 18:14 - loss: 1.2704 - regression_loss: 1.0588 - classification_loss: 0.2116
 142/1500 [=>............................] - ETA: 18:23 - loss: 1.2681 - regression_loss: 1.0566 - classification_loss: 0.2115
 143/1500 [=>............................] - ETA: 18:24 - loss: 1.2643 - regression_loss: 1.0537 - classification_loss: 0.2106
 144/1500 [=>............................] - ETA: 18:34 - loss: 1.2627 - regression_loss: 1.0529 - classification_loss: 0.2098
 145/1500 [=>............................] - ETA: 18:29 - loss: 1.2630 - regression_loss: 1.0530 - classification_loss: 0.2100
 146/1500 [=>............................] - ETA: 18:32 - loss: 1.2674 - regression_loss: 1.0563 - classification_loss: 0.2111
 147/1500 [=>............................] - ETA: 18:30 - loss: 1.2700 - regression_loss: 1.0593 - classification_loss: 0.2107
 148/1500 [=>............................] - ETA: 18:32 - loss: 1.2744 - regression_loss: 1.0640 - classification_loss: 0.2103
 149/1500 [=>............................] - ETA: 18:31 - loss: 1.2707 - regression_loss: 1.0608 - classification_loss: 0.2100
 150/1500 [==>...........................] - ETA: 18:37 - loss: 1.2708 - regression_loss: 1.0613 - classification_loss: 0.2095
 151/1500 [==>...........................] - ETA: 18:38 - loss: 1.2685 - regression_loss: 1.0593 - classification_loss: 0.2092
 152/1500 [==>...........................] - ETA: 18:36 - loss: 1.2624 - regression_loss: 1.0541 - classification_loss: 0.2084
 153/1500 [==>...........................] - ETA: 18:34 - loss: 1.2637 - regression_loss: 1.0563 - classification_loss: 0.2074
 154/1500 [==>...........................] - ETA: 18:47 - loss: 1.2682 - regression_loss: 1.0597 - classification_loss: 0.2085
 155/1500 [==>...........................] - ETA: 18:48 - loss: 1.2662 - regression_loss: 1.0575 - classification_loss: 0.2087
 156/1500 [==>...........................] - ETA: 18:52 - loss: 1.2749 - regression_loss: 1.0647 - classification_loss: 0.2102
 157/1500 [==>...........................] - ETA: 18:55 - loss: 1.2722 - regression_loss: 1.0630 - classification_loss: 0.2092
 158/1500 [==>...........................] - ETA: 18:51 - loss: 1.2731 - regression_loss: 1.0642 - classification_loss: 0.2089
 159/1500 [==>...........................] - ETA: 18:46 - loss: 1.2750 - regression_loss: 1.0658 - classification_loss: 0.2092
 160/1500 [==>...........................] - ETA: 18:50 - loss: 1.2735 - regression_loss: 1.0644 - classification_loss: 0.2091
 161/1500 [==>...........................] - ETA: 18:47 - loss: 1.2770 - regression_loss: 1.0672 - classification_loss: 0.2098
 162/1500 [==>...........................] - ETA: 18:43 - loss: 1.2731 - regression_loss: 1.0636 - classification_loss: 0.2095
 163/1500 [==>...........................] - ETA: 18:38 - loss: 1.2721 - regression_loss: 1.0631 - classification_loss: 0.2090
 164/1500 [==>...........................] - ETA: 18:34 - loss: 1.2702 - regression_loss: 1.0621 - classification_loss: 0.2081
 165/1500 [==>...........................] - ETA: 18:30 - loss: 1.2808 - regression_loss: 1.0696 - classification_loss: 0.2112
 166/1500 [==>...........................] - ETA: 18:25 - loss: 1.2773 - regression_loss: 1.0667 - classification_loss: 0.2107
 167/1500 [==>...........................] - ETA: 18:26 - loss: 1.2781 - regression_loss: 1.0672 - classification_loss: 0.2109
 168/1500 [==>...........................] - ETA: 18:21 - loss: 1.2802 - regression_loss: 1.0694 - classification_loss: 0.2108
 169/1500 [==>...........................] - ETA: 18:17 - loss: 1.2804 - regression_loss: 1.0702 - classification_loss: 0.2102
 170/1500 [==>...........................] - ETA: 18:12 - loss: 1.2814 - regression_loss: 1.0713 - classification_loss: 0.2101
 171/1500 [==>...........................] - ETA: 18:09 - loss: 1.2792 - regression_loss: 1.0699 - classification_loss: 0.2092
 172/1500 [==>...........................] - ETA: 18:05 - loss: 1.2773 - regression_loss: 1.0686 - classification_loss: 0.2087
 173/1500 [==>...........................] - ETA: 18:02 - loss: 1.2845 - regression_loss: 1.0748 - classification_loss: 0.2097
 174/1500 [==>...........................] - ETA: 17:59 - loss: 1.2826 - regression_loss: 1.0733 - classification_loss: 0.2093
 175/1500 [==>...........................] - ETA: 17:55 - loss: 1.2804 - regression_loss: 1.0719 - classification_loss: 0.2085
 176/1500 [==>...........................] - ETA: 17:52 - loss: 1.2800 - regression_loss: 1.0715 - classification_loss: 0.2085
 177/1500 [==>...........................] - ETA: 17:48 - loss: 1.2786 - regression_loss: 1.0709 - classification_loss: 0.2077
 178/1500 [==>...........................] - ETA: 17:48 - loss: 1.2761 - regression_loss: 1.0693 - classification_loss: 0.2068
 179/1500 [==>...........................] - ETA: 17:51 - loss: 1.2752 - regression_loss: 1.0688 - classification_loss: 0.2064
 180/1500 [==>...........................] - ETA: 17:47 - loss: 1.2751 - regression_loss: 1.0691 - classification_loss: 0.2060
 181/1500 [==>...........................] - ETA: 17:47 - loss: 1.2716 - regression_loss: 1.0659 - classification_loss: 0.2056
 182/1500 [==>...........................] - ETA: 17:53 - loss: 1.2770 - regression_loss: 1.0706 - classification_loss: 0.2064
 183/1500 [==>...........................] - ETA: 17:49 - loss: 1.2757 - regression_loss: 1.0698 - classification_loss: 0.2059
 184/1500 [==>...........................] - ETA: 17:45 - loss: 1.2731 - regression_loss: 1.0679 - classification_loss: 0.2052
 185/1500 [==>...........................] - ETA: 17:45 - loss: 1.2743 - regression_loss: 1.0693 - classification_loss: 0.2050
 186/1500 [==>...........................] - ETA: 17:41 - loss: 1.2740 - regression_loss: 1.0693 - classification_loss: 0.2047
 187/1500 [==>...........................] - ETA: 17:55 - loss: 1.2751 - regression_loss: 1.0699 - classification_loss: 0.2052
 188/1500 [==>...........................] - ETA: 17:56 - loss: 1.2790 - regression_loss: 1.0730 - classification_loss: 0.2060
 189/1500 [==>...........................] - ETA: 18:10 - loss: 1.2757 - regression_loss: 1.0703 - classification_loss: 0.2054
 190/1500 [==>...........................] - ETA: 18:07 - loss: 1.2767 - regression_loss: 1.0711 - classification_loss: 0.2056
 191/1500 [==>...........................] - ETA: 18:05 - loss: 1.2759 - regression_loss: 1.0703 - classification_loss: 0.2056
 192/1500 [==>...........................] - ETA: 18:00 - loss: 1.2741 - regression_loss: 1.0690 - classification_loss: 0.2052
 193/1500 [==>...........................] - ETA: 17:59 - loss: 1.2758 - regression_loss: 1.0678 - classification_loss: 0.2080
 194/1500 [==>...........................] - ETA: 17:56 - loss: 1.2776 - regression_loss: 1.0692 - classification_loss: 0.2084
 195/1500 [==>...........................] - ETA: 17:55 - loss: 1.2750 - regression_loss: 1.0672 - classification_loss: 0.2078
 196/1500 [==>...........................] - ETA: 18:00 - loss: 1.2724 - regression_loss: 1.0652 - classification_loss: 0.2072
 197/1500 [==>...........................] - ETA: 17:56 - loss: 1.2693 - regression_loss: 1.0628 - classification_loss: 0.2065
 198/1500 [==>...........................] - ETA: 17:52 - loss: 1.2674 - regression_loss: 1.0617 - classification_loss: 0.2057
 199/1500 [==>...........................] - ETA: 17:49 - loss: 1.2694 - regression_loss: 1.0635 - classification_loss: 0.2059
 200/1500 [===>..........................] - ETA: 17:47 - loss: 1.2689 - regression_loss: 1.0636 - classification_loss: 0.2052
 201/1500 [===>..........................] - ETA: 17:47 - loss: 1.2667 - regression_loss: 1.0621 - classification_loss: 0.2046
 202/1500 [===>..........................] - ETA: 17:44 - loss: 1.2652 - regression_loss: 1.0608 - classification_loss: 0.2044
 203/1500 [===>..........................] - ETA: 18:00 - loss: 1.2657 - regression_loss: 1.0611 - classification_loss: 0.2046
 204/1500 [===>..........................] - ETA: 17:56 - loss: 1.2650 - regression_loss: 1.0609 - classification_loss: 0.2041
 205/1500 [===>..........................] - ETA: 17:57 - loss: 1.2652 - regression_loss: 1.0607 - classification_loss: 0.2045
 206/1500 [===>..........................] - ETA: 17:57 - loss: 1.2631 - regression_loss: 1.0589 - classification_loss: 0.2041
 207/1500 [===>..........................] - ETA: 17:58 - loss: 1.2618 - regression_loss: 1.0580 - classification_loss: 0.2038
 208/1500 [===>..........................] - ETA: 17:57 - loss: 1.2610 - regression_loss: 1.0577 - classification_loss: 0.2033
 209/1500 [===>..........................] - ETA: 17:53 - loss: 1.2668 - regression_loss: 1.0622 - classification_loss: 0.2046
 210/1500 [===>..........................] - ETA: 17:52 - loss: 1.2676 - regression_loss: 1.0624 - classification_loss: 0.2051
 211/1500 [===>..........................] - ETA: 17:48 - loss: 1.2675 - regression_loss: 1.0629 - classification_loss: 0.2046
 212/1500 [===>..........................] - ETA: 17:45 - loss: 1.2665 - regression_loss: 1.0624 - classification_loss: 0.2041
 213/1500 [===>..........................] - ETA: 17:42 - loss: 1.2675 - regression_loss: 1.0636 - classification_loss: 0.2039
 214/1500 [===>..........................] - ETA: 17:39 - loss: 1.2639 - regression_loss: 1.0605 - classification_loss: 0.2034
 215/1500 [===>..........................] - ETA: 17:39 - loss: 1.2620 - regression_loss: 1.0588 - classification_loss: 0.2032
 216/1500 [===>..........................] - ETA: 17:38 - loss: 1.2629 - regression_loss: 1.0597 - classification_loss: 0.2032
 217/1500 [===>..........................] - ETA: 17:35 - loss: 1.2594 - regression_loss: 1.0565 - classification_loss: 0.2028
 218/1500 [===>..........................] - ETA: 17:32 - loss: 1.2618 - regression_loss: 1.0583 - classification_loss: 0.2035
 219/1500 [===>..........................] - ETA: 17:29 - loss: 1.2676 - regression_loss: 1.0635 - classification_loss: 0.2041
 220/1500 [===>..........................] - ETA: 17:26 - loss: 1.2706 - regression_loss: 1.0660 - classification_loss: 0.2046
 221/1500 [===>..........................] - ETA: 17:37 - loss: 1.2779 - regression_loss: 1.0711 - classification_loss: 0.2067
 222/1500 [===>..........................] - ETA: 17:36 - loss: 1.2781 - regression_loss: 1.0716 - classification_loss: 0.2065
 223/1500 [===>..........................] - ETA: 17:36 - loss: 1.2777 - regression_loss: 1.0716 - classification_loss: 0.2061
 224/1500 [===>..........................] - ETA: 17:45 - loss: 1.2821 - regression_loss: 1.0756 - classification_loss: 0.2066
 225/1500 [===>..........................] - ETA: 17:58 - loss: 1.2832 - regression_loss: 1.0767 - classification_loss: 0.2065
 226/1500 [===>..........................] - ETA: 17:55 - loss: 1.2829 - regression_loss: 1.0770 - classification_loss: 0.2060
 227/1500 [===>..........................] - ETA: 18:05 - loss: 1.2873 - regression_loss: 1.0804 - classification_loss: 0.2069
 228/1500 [===>..........................] - ETA: 18:02 - loss: 1.2831 - regression_loss: 1.0769 - classification_loss: 0.2063
 229/1500 [===>..........................] - ETA: 18:06 - loss: 1.2823 - regression_loss: 1.0765 - classification_loss: 0.2058
 230/1500 [===>..........................] - ETA: 18:03 - loss: 1.2863 - regression_loss: 1.0800 - classification_loss: 0.2063
 231/1500 [===>..........................] - ETA: 18:02 - loss: 1.2890 - regression_loss: 1.0820 - classification_loss: 0.2069
 232/1500 [===>..........................] - ETA: 18:01 - loss: 1.2893 - regression_loss: 1.0827 - classification_loss: 0.2066
 233/1500 [===>..........................] - ETA: 17:58 - loss: 1.2866 - regression_loss: 1.0804 - classification_loss: 0.2062
 234/1500 [===>..........................] - ETA: 17:55 - loss: 1.2851 - regression_loss: 1.0792 - classification_loss: 0.2059
 235/1500 [===>..........................] - ETA: 17:54 - loss: 1.2850 - regression_loss: 1.0796 - classification_loss: 0.2054
 236/1500 [===>..........................] - ETA: 17:50 - loss: 1.2827 - regression_loss: 1.0778 - classification_loss: 0.2049
 237/1500 [===>..........................] - ETA: 17:50 - loss: 1.2838 - regression_loss: 1.0782 - classification_loss: 0.2055
 238/1500 [===>..........................] - ETA: 17:55 - loss: 1.2815 - regression_loss: 1.0764 - classification_loss: 0.2051
 239/1500 [===>..........................] - ETA: 17:56 - loss: 1.2779 - regression_loss: 1.0735 - classification_loss: 0.2045
 240/1500 [===>..........................] - ETA: 17:55 - loss: 1.2753 - regression_loss: 1.0713 - classification_loss: 0.2040
 241/1500 [===>..........................] - ETA: 17:53 - loss: 1.2726 - regression_loss: 1.0692 - classification_loss: 0.2033
 242/1500 [===>..........................] - ETA: 17:56 - loss: 1.2711 - regression_loss: 1.0683 - classification_loss: 0.2028
 243/1500 [===>..........................] - ETA: 17:53 - loss: 1.2750 - regression_loss: 1.0718 - classification_loss: 0.2032
 244/1500 [===>..........................] - ETA: 17:58 - loss: 1.2733 - regression_loss: 1.0707 - classification_loss: 0.2026
 245/1500 [===>..........................] - ETA: 17:55 - loss: 1.2720 - regression_loss: 1.0699 - classification_loss: 0.2021
 246/1500 [===>..........................] - ETA: 17:54 - loss: 1.2698 - regression_loss: 1.0683 - classification_loss: 0.2016
 247/1500 [===>..........................] - ETA: 17:51 - loss: 1.2735 - regression_loss: 1.0711 - classification_loss: 0.2024
 248/1500 [===>..........................] - ETA: 17:51 - loss: 1.2720 - regression_loss: 1.0701 - classification_loss: 0.2019
 249/1500 [===>..........................] - ETA: 17:51 - loss: 1.2706 - regression_loss: 1.0691 - classification_loss: 0.2015
 250/1500 [====>.........................] - ETA: 17:55 - loss: 1.2698 - regression_loss: 1.0685 - classification_loss: 0.2013
 251/1500 [====>.........................] - ETA: 17:52 - loss: 1.2702 - regression_loss: 1.0685 - classification_loss: 0.2017
 252/1500 [====>.........................] - ETA: 17:51 - loss: 1.2705 - regression_loss: 1.0692 - classification_loss: 0.2013
 253/1500 [====>.........................] - ETA: 17:48 - loss: 1.2742 - regression_loss: 1.0721 - classification_loss: 0.2022
 254/1500 [====>.........................] - ETA: 17:47 - loss: 1.2782 - regression_loss: 1.0752 - classification_loss: 0.2031
 255/1500 [====>.........................] - ETA: 17:43 - loss: 1.2783 - regression_loss: 1.0755 - classification_loss: 0.2028
 256/1500 [====>.........................] - ETA: 17:43 - loss: 1.2773 - regression_loss: 1.0748 - classification_loss: 0.2025
 257/1500 [====>.........................] - ETA: 17:40 - loss: 1.2757 - regression_loss: 1.0735 - classification_loss: 0.2022
 258/1500 [====>.........................] - ETA: 17:40 - loss: 1.2787 - regression_loss: 1.0759 - classification_loss: 0.2028
 259/1500 [====>.........................] - ETA: 17:37 - loss: 1.2792 - regression_loss: 1.0766 - classification_loss: 0.2026
 260/1500 [====>.........................] - ETA: 17:35 - loss: 1.2813 - regression_loss: 1.0783 - classification_loss: 0.2030
 261/1500 [====>.........................] - ETA: 17:33 - loss: 1.2838 - regression_loss: 1.0806 - classification_loss: 0.2032
 262/1500 [====>.........................] - ETA: 17:31 - loss: 1.2828 - regression_loss: 1.0797 - classification_loss: 0.2030
 263/1500 [====>.........................] - ETA: 17:37 - loss: 1.2847 - regression_loss: 1.0813 - classification_loss: 0.2034
 264/1500 [====>.........................] - ETA: 17:36 - loss: 1.2822 - regression_loss: 1.0793 - classification_loss: 0.2029
 265/1500 [====>.........................] - ETA: 17:34 - loss: 1.2840 - regression_loss: 1.0812 - classification_loss: 0.2028
 266/1500 [====>.........................] - ETA: 17:31 - loss: 1.2821 - regression_loss: 1.0799 - classification_loss: 0.2023
 267/1500 [====>.........................] - ETA: 17:28 - loss: 1.2838 - regression_loss: 1.0812 - classification_loss: 0.2026
 268/1500 [====>.........................] - ETA: 17:25 - loss: 1.2843 - regression_loss: 1.0819 - classification_loss: 0.2024
 269/1500 [====>.........................] - ETA: 17:23 - loss: 1.2888 - regression_loss: 1.0863 - classification_loss: 0.2025
 270/1500 [====>.........................] - ETA: 17:25 - loss: 1.2885 - regression_loss: 1.0862 - classification_loss: 0.2023
 271/1500 [====>.........................] - ETA: 17:23 - loss: 1.2850 - regression_loss: 1.0833 - classification_loss: 0.2017
 272/1500 [====>.........................] - ETA: 17:20 - loss: 1.2840 - regression_loss: 1.0824 - classification_loss: 0.2017
 273/1500 [====>.........................] - ETA: 17:18 - loss: 1.2813 - regression_loss: 1.0802 - classification_loss: 0.2011
 274/1500 [====>.........................] - ETA: 17:16 - loss: 1.2844 - regression_loss: 1.0828 - classification_loss: 0.2016
 275/1500 [====>.........................] - ETA: 17:14 - loss: 1.2821 - regression_loss: 1.0810 - classification_loss: 0.2012
 276/1500 [====>.........................] - ETA: 17:11 - loss: 1.2796 - regression_loss: 1.0789 - classification_loss: 0.2006
 277/1500 [====>.........................] - ETA: 17:09 - loss: 1.2786 - regression_loss: 1.0782 - classification_loss: 0.2005
 278/1500 [====>.........................] - ETA: 17:07 - loss: 1.2778 - regression_loss: 1.0777 - classification_loss: 0.2001
 279/1500 [====>.........................] - ETA: 17:04 - loss: 1.2759 - regression_loss: 1.0760 - classification_loss: 0.1999
 280/1500 [====>.........................] - ETA: 17:04 - loss: 1.2738 - regression_loss: 1.0743 - classification_loss: 0.1995
 281/1500 [====>.........................] - ETA: 17:03 - loss: 1.2769 - regression_loss: 1.0773 - classification_loss: 0.1995
 282/1500 [====>.........................] - ETA: 17:00 - loss: 1.2764 - regression_loss: 1.0769 - classification_loss: 0.1995
 283/1500 [====>.........................] - ETA: 17:00 - loss: 1.2742 - regression_loss: 1.0750 - classification_loss: 0.1992
 284/1500 [====>.........................] - ETA: 16:57 - loss: 1.2730 - regression_loss: 1.0741 - classification_loss: 0.1989
 285/1500 [====>.........................] - ETA: 17:01 - loss: 1.2750 - regression_loss: 1.0759 - classification_loss: 0.1991
 286/1500 [====>.........................] - ETA: 17:01 - loss: 1.2736 - regression_loss: 1.0746 - classification_loss: 0.1989
 287/1500 [====>.........................] - ETA: 17:11 - loss: 1.2719 - regression_loss: 1.0734 - classification_loss: 0.1985
 288/1500 [====>.........................] - ETA: 17:10 - loss: 1.2702 - regression_loss: 1.0721 - classification_loss: 0.1980
 289/1500 [====>.........................] - ETA: 17:08 - loss: 1.2695 - regression_loss: 1.0714 - classification_loss: 0.1981
 290/1500 [====>.........................] - ETA: 17:08 - loss: 1.2681 - regression_loss: 1.0704 - classification_loss: 0.1977
 291/1500 [====>.........................] - ETA: 17:07 - loss: 1.2677 - regression_loss: 1.0701 - classification_loss: 0.1977
 292/1500 [====>.........................] - ETA: 17:04 - loss: 1.2678 - regression_loss: 1.0701 - classification_loss: 0.1977
 293/1500 [====>.........................] - ETA: 17:02 - loss: 1.2663 - regression_loss: 1.0686 - classification_loss: 0.1977
 294/1500 [====>.........................] - ETA: 17:03 - loss: 1.2632 - regression_loss: 1.0661 - classification_loss: 0.1972
 295/1500 [====>.........................] - ETA: 17:00 - loss: 1.2641 - regression_loss: 1.0670 - classification_loss: 0.1971
 296/1500 [====>.........................] - ETA: 17:01 - loss: 1.2619 - regression_loss: 1.0651 - classification_loss: 0.1968
 297/1500 [====>.........................] - ETA: 17:04 - loss: 1.2616 - regression_loss: 1.0650 - classification_loss: 0.1966
 298/1500 [====>.........................] - ETA: 17:04 - loss: 1.2601 - regression_loss: 1.0639 - classification_loss: 0.1962
 299/1500 [====>.........................] - ETA: 17:05 - loss: 1.2626 - regression_loss: 1.0658 - classification_loss: 0.1967
 300/1500 [=====>........................] - ETA: 17:03 - loss: 1.2595 - regression_loss: 1.0633 - classification_loss: 0.1962
 301/1500 [=====>........................] - ETA: 17:01 - loss: 1.2605 - regression_loss: 1.0641 - classification_loss: 0.1964
 302/1500 [=====>........................] - ETA: 16:59 - loss: 1.2642 - regression_loss: 1.0670 - classification_loss: 0.1972
 303/1500 [=====>........................] - ETA: 16:56 - loss: 1.2697 - regression_loss: 1.0710 - classification_loss: 0.1986
 304/1500 [=====>........................] - ETA: 16:53 - loss: 1.2673 - regression_loss: 1.0690 - classification_loss: 0.1983
 305/1500 [=====>........................] - ETA: 16:51 - loss: 1.2657 - regression_loss: 1.0673 - classification_loss: 0.1984
 306/1500 [=====>........................] - ETA: 16:52 - loss: 1.2681 - regression_loss: 1.0694 - classification_loss: 0.1987
 307/1500 [=====>........................] - ETA: 16:49 - loss: 1.2674 - regression_loss: 1.0689 - classification_loss: 0.1984
 308/1500 [=====>........................] - ETA: 16:47 - loss: 1.2652 - regression_loss: 1.0671 - classification_loss: 0.1980
 309/1500 [=====>........................] - ETA: 16:44 - loss: 1.2632 - regression_loss: 1.0657 - classification_loss: 0.1976
 310/1500 [=====>........................] - ETA: 16:41 - loss: 1.2620 - regression_loss: 1.0645 - classification_loss: 0.1975
 311/1500 [=====>........................] - ETA: 16:40 - loss: 1.2631 - regression_loss: 1.0652 - classification_loss: 0.1978
 312/1500 [=====>........................] - ETA: 16:38 - loss: 1.2616 - regression_loss: 1.0642 - classification_loss: 0.1974
 313/1500 [=====>........................] - ETA: 16:41 - loss: 1.2595 - regression_loss: 1.0625 - classification_loss: 0.1970
 314/1500 [=====>........................] - ETA: 16:42 - loss: 1.2580 - regression_loss: 1.0612 - classification_loss: 0.1968
 315/1500 [=====>........................] - ETA: 16:40 - loss: 1.2553 - regression_loss: 1.0588 - classification_loss: 0.1965
 316/1500 [=====>........................] - ETA: 16:38 - loss: 1.2548 - regression_loss: 1.0587 - classification_loss: 0.1962
 317/1500 [=====>........................] - ETA: 16:36 - loss: 1.2593 - regression_loss: 1.0621 - classification_loss: 0.1973
 318/1500 [=====>........................] - ETA: 16:33 - loss: 1.2581 - regression_loss: 1.0612 - classification_loss: 0.1969
 319/1500 [=====>........................] - ETA: 16:32 - loss: 1.2604 - regression_loss: 1.0611 - classification_loss: 0.1994
 320/1500 [=====>........................] - ETA: 16:30 - loss: 1.2583 - regression_loss: 1.0593 - classification_loss: 0.1990
 321/1500 [=====>........................] - ETA: 16:28 - loss: 1.2584 - regression_loss: 1.0595 - classification_loss: 0.1989
 322/1500 [=====>........................] - ETA: 16:29 - loss: 1.2574 - regression_loss: 1.0587 - classification_loss: 0.1987
 323/1500 [=====>........................] - ETA: 16:27 - loss: 1.2579 - regression_loss: 1.0594 - classification_loss: 0.1985
 324/1500 [=====>........................] - ETA: 16:25 - loss: 1.2584 - regression_loss: 1.0602 - classification_loss: 0.1982
 325/1500 [=====>........................] - ETA: 16:23 - loss: 1.2577 - regression_loss: 1.0597 - classification_loss: 0.1980
 326/1500 [=====>........................] - ETA: 16:20 - loss: 1.2553 - regression_loss: 1.0579 - classification_loss: 0.1975
 327/1500 [=====>........................] - ETA: 16:18 - loss: 1.2553 - regression_loss: 1.0581 - classification_loss: 0.1972
 328/1500 [=====>........................] - ETA: 16:17 - loss: 1.2540 - regression_loss: 1.0570 - classification_loss: 0.1970
 329/1500 [=====>........................] - ETA: 16:14 - loss: 1.2536 - regression_loss: 1.0568 - classification_loss: 0.1968
 330/1500 [=====>........................] - ETA: 16:15 - loss: 1.2554 - regression_loss: 1.0583 - classification_loss: 0.1971
 331/1500 [=====>........................] - ETA: 16:13 - loss: 1.2553 - regression_loss: 1.0584 - classification_loss: 0.1969
 332/1500 [=====>........................] - ETA: 16:13 - loss: 1.2544 - regression_loss: 1.0577 - classification_loss: 0.1967
 333/1500 [=====>........................] - ETA: 16:10 - loss: 1.2534 - regression_loss: 1.0568 - classification_loss: 0.1965
 334/1500 [=====>........................] - ETA: 16:08 - loss: 1.2529 - regression_loss: 1.0563 - classification_loss: 0.1966
 335/1500 [=====>........................] - ETA: 16:06 - loss: 1.2532 - regression_loss: 1.0567 - classification_loss: 0.1965
 336/1500 [=====>........................] - ETA: 16:06 - loss: 1.2537 - regression_loss: 1.0574 - classification_loss: 0.1962
 337/1500 [=====>........................] - ETA: 16:04 - loss: 1.2531 - regression_loss: 1.0570 - classification_loss: 0.1962
 338/1500 [=====>........................] - ETA: 16:02 - loss: 1.2517 - regression_loss: 1.0558 - classification_loss: 0.1959
 339/1500 [=====>........................] - ETA: 15:59 - loss: 1.2533 - regression_loss: 1.0571 - classification_loss: 0.1962
 340/1500 [=====>........................] - ETA: 15:57 - loss: 1.2528 - regression_loss: 1.0567 - classification_loss: 0.1961
 341/1500 [=====>........................] - ETA: 15:58 - loss: 1.2530 - regression_loss: 1.0571 - classification_loss: 0.1959
 342/1500 [=====>........................] - ETA: 15:55 - loss: 1.2530 - regression_loss: 1.0572 - classification_loss: 0.1957
 343/1500 [=====>........................] - ETA: 15:53 - loss: 1.2550 - regression_loss: 1.0589 - classification_loss: 0.1961
 344/1500 [=====>........................] - ETA: 15:56 - loss: 1.2552 - regression_loss: 1.0588 - classification_loss: 0.1964
 345/1500 [=====>........................] - ETA: 15:54 - loss: 1.2525 - regression_loss: 1.0566 - classification_loss: 0.1959
 346/1500 [=====>........................] - ETA: 15:53 - loss: 1.2505 - regression_loss: 1.0549 - classification_loss: 0.1955
 347/1500 [=====>........................] - ETA: 15:53 - loss: 1.2508 - regression_loss: 1.0553 - classification_loss: 0.1955
 348/1500 [=====>........................] - ETA: 15:54 - loss: 1.2506 - regression_loss: 1.0553 - classification_loss: 0.1953
 349/1500 [=====>........................] - ETA: 15:56 - loss: 1.2539 - regression_loss: 1.0580 - classification_loss: 0.1958
 350/1500 [======>.......................] - ETA: 15:54 - loss: 1.2535 - regression_loss: 1.0580 - classification_loss: 0.1955
 351/1500 [======>.......................] - ETA: 15:52 - loss: 1.2538 - regression_loss: 1.0586 - classification_loss: 0.1952
 352/1500 [======>.......................] - ETA: 15:51 - loss: 1.2545 - regression_loss: 1.0593 - classification_loss: 0.1952
 353/1500 [======>.......................] - ETA: 15:49 - loss: 1.2540 - regression_loss: 1.0590 - classification_loss: 0.1949
 354/1500 [======>.......................] - ETA: 15:49 - loss: 1.2530 - regression_loss: 1.0577 - classification_loss: 0.1953
 355/1500 [======>.......................] - ETA: 15:49 - loss: 1.2518 - regression_loss: 1.0568 - classification_loss: 0.1949
 356/1500 [======>.......................] - ETA: 15:50 - loss: 1.2517 - regression_loss: 1.0569 - classification_loss: 0.1948
 357/1500 [======>.......................] - ETA: 15:52 - loss: 1.2552 - regression_loss: 1.0594 - classification_loss: 0.1958
 358/1500 [======>.......................] - ETA: 15:50 - loss: 1.2556 - regression_loss: 1.0599 - classification_loss: 0.1957
 359/1500 [======>.......................] - ETA: 15:47 - loss: 1.2545 - regression_loss: 1.0591 - classification_loss: 0.1954
 360/1500 [======>.......................] - ETA: 15:45 - loss: 1.2532 - regression_loss: 1.0578 - classification_loss: 0.1954
 361/1500 [======>.......................] - ETA: 15:44 - loss: 1.2520 - regression_loss: 1.0569 - classification_loss: 0.1951
 362/1500 [======>.......................] - ETA: 15:41 - loss: 1.2510 - regression_loss: 1.0561 - classification_loss: 0.1949
 363/1500 [======>.......................] - ETA: 15:39 - loss: 1.2509 - regression_loss: 1.0559 - classification_loss: 0.1950
 364/1500 [======>.......................] - ETA: 15:44 - loss: 1.2529 - regression_loss: 1.0579 - classification_loss: 0.1950
 365/1500 [======>.......................] - ETA: 15:46 - loss: 1.2515 - regression_loss: 1.0565 - classification_loss: 0.1950
 366/1500 [======>.......................] - ETA: 15:46 - loss: 1.2497 - regression_loss: 1.0550 - classification_loss: 0.1947
 367/1500 [======>.......................] - ETA: 15:44 - loss: 1.2480 - regression_loss: 1.0536 - classification_loss: 0.1944
 368/1500 [======>.......................] - ETA: 15:46 - loss: 1.2514 - regression_loss: 1.0567 - classification_loss: 0.1948
 369/1500 [======>.......................] - ETA: 15:47 - loss: 1.2500 - regression_loss: 1.0553 - classification_loss: 0.1947
 370/1500 [======>.......................] - ETA: 15:50 - loss: 1.2520 - regression_loss: 1.0567 - classification_loss: 0.1953
 371/1500 [======>.......................] - ETA: 15:48 - loss: 1.2528 - regression_loss: 1.0576 - classification_loss: 0.1953
 372/1500 [======>.......................] - ETA: 15:46 - loss: 1.2519 - regression_loss: 1.0569 - classification_loss: 0.1950
 373/1500 [======>.......................] - ETA: 15:45 - loss: 1.2525 - regression_loss: 1.0575 - classification_loss: 0.1950
 374/1500 [======>.......................] - ETA: 15:45 - loss: 1.2506 - regression_loss: 1.0559 - classification_loss: 0.1947
 375/1500 [======>.......................] - ETA: 15:43 - loss: 1.2513 - regression_loss: 1.0567 - classification_loss: 0.1946
 376/1500 [======>.......................] - ETA: 15:43 - loss: 1.2494 - regression_loss: 1.0552 - classification_loss: 0.1942
 377/1500 [======>.......................] - ETA: 15:41 - loss: 1.2527 - regression_loss: 1.0575 - classification_loss: 0.1952
 378/1500 [======>.......................] - ETA: 15:45 - loss: 1.2528 - regression_loss: 1.0578 - classification_loss: 0.1950
 379/1500 [======>.......................] - ETA: 15:43 - loss: 1.2519 - regression_loss: 1.0571 - classification_loss: 0.1948
 380/1500 [======>.......................] - ETA: 15:41 - loss: 1.2504 - regression_loss: 1.0558 - classification_loss: 0.1946
 381/1500 [======>.......................] - ETA: 15:39 - loss: 1.2493 - regression_loss: 1.0550 - classification_loss: 0.1943
 382/1500 [======>.......................] - ETA: 15:39 - loss: 1.2481 - regression_loss: 1.0541 - classification_loss: 0.1939
 383/1500 [======>.......................] - ETA: 15:39 - loss: 1.2512 - regression_loss: 1.0560 - classification_loss: 0.1952
 384/1500 [======>.......................] - ETA: 15:38 - loss: 1.2500 - regression_loss: 1.0551 - classification_loss: 0.1949
 385/1500 [======>.......................] - ETA: 15:36 - loss: 1.2490 - regression_loss: 1.0544 - classification_loss: 0.1946
 386/1500 [======>.......................] - ETA: 15:36 - loss: 1.2481 - regression_loss: 1.0535 - classification_loss: 0.1946
 387/1500 [======>.......................] - ETA: 15:34 - loss: 1.2484 - regression_loss: 1.0540 - classification_loss: 0.1944
 388/1500 [======>.......................] - ETA: 15:33 - loss: 1.2478 - regression_loss: 1.0535 - classification_loss: 0.1943
 389/1500 [======>.......................] - ETA: 15:33 - loss: 1.2485 - regression_loss: 1.0541 - classification_loss: 0.1944
 390/1500 [======>.......................] - ETA: 15:32 - loss: 1.2473 - regression_loss: 1.0531 - classification_loss: 0.1942
 391/1500 [======>.......................] - ETA: 15:32 - loss: 1.2463 - regression_loss: 1.0523 - classification_loss: 0.1940
 392/1500 [======>.......................] - ETA: 15:30 - loss: 1.2461 - regression_loss: 1.0522 - classification_loss: 0.1938
 393/1500 [======>.......................] - ETA: 15:30 - loss: 1.2463 - regression_loss: 1.0526 - classification_loss: 0.1937
 394/1500 [======>.......................] - ETA: 15:28 - loss: 1.2450 - regression_loss: 1.0517 - classification_loss: 0.1934
 395/1500 [======>.......................] - ETA: 15:26 - loss: 1.2448 - regression_loss: 1.0518 - classification_loss: 0.1930
 396/1500 [======>.......................] - ETA: 15:25 - loss: 1.2444 - regression_loss: 1.0515 - classification_loss: 0.1930
 397/1500 [======>.......................] - ETA: 15:23 - loss: 1.2421 - regression_loss: 1.0493 - classification_loss: 0.1927
 398/1500 [======>.......................] - ETA: 15:21 - loss: 1.2420 - regression_loss: 1.0492 - classification_loss: 0.1927
 399/1500 [======>.......................] - ETA: 15:19 - loss: 1.2406 - regression_loss: 1.0480 - classification_loss: 0.1926
 400/1500 [=======>......................] - ETA: 15:17 - loss: 1.2403 - regression_loss: 1.0480 - classification_loss: 0.1924
 401/1500 [=======>......................] - ETA: 15:15 - loss: 1.2399 - regression_loss: 1.0478 - classification_loss: 0.1921
 402/1500 [=======>......................] - ETA: 15:13 - loss: 1.2401 - regression_loss: 1.0482 - classification_loss: 0.1919
 403/1500 [=======>......................] - ETA: 15:11 - loss: 1.2407 - regression_loss: 1.0490 - classification_loss: 0.1917
 404/1500 [=======>......................] - ETA: 15:09 - loss: 1.2388 - regression_loss: 1.0474 - classification_loss: 0.1914
 405/1500 [=======>......................] - ETA: 15:08 - loss: 1.2397 - regression_loss: 1.0482 - classification_loss: 0.1915
 406/1500 [=======>......................] - ETA: 15:07 - loss: 1.2403 - regression_loss: 1.0489 - classification_loss: 0.1914
 407/1500 [=======>......................] - ETA: 15:05 - loss: 1.2391 - regression_loss: 1.0481 - classification_loss: 0.1911
 408/1500 [=======>......................] - ETA: 15:05 - loss: 1.2422 - regression_loss: 1.0501 - classification_loss: 0.1921
 409/1500 [=======>......................] - ETA: 15:05 - loss: 1.2414 - regression_loss: 1.0491 - classification_loss: 0.1923
 410/1500 [=======>......................] - ETA: 15:04 - loss: 1.2409 - regression_loss: 1.0488 - classification_loss: 0.1921
 411/1500 [=======>......................] - ETA: 15:02 - loss: 1.2420 - regression_loss: 1.0498 - classification_loss: 0.1921
 412/1500 [=======>......................] - ETA: 15:00 - loss: 1.2408 - regression_loss: 1.0489 - classification_loss: 0.1919
 413/1500 [=======>......................] - ETA: 15:00 - loss: 1.2425 - regression_loss: 1.0506 - classification_loss: 0.1919
 414/1500 [=======>......................] - ETA: 14:59 - loss: 1.2412 - regression_loss: 1.0496 - classification_loss: 0.1916
 415/1500 [=======>......................] - ETA: 14:57 - loss: 1.2413 - regression_loss: 1.0498 - classification_loss: 0.1915
 416/1500 [=======>......................] - ETA: 14:55 - loss: 1.2432 - regression_loss: 1.0510 - classification_loss: 0.1922
 417/1500 [=======>......................] - ETA: 14:54 - loss: 1.2425 - regression_loss: 1.0504 - classification_loss: 0.1921
 418/1500 [=======>......................] - ETA: 14:52 - loss: 1.2421 - regression_loss: 1.0502 - classification_loss: 0.1919
 419/1500 [=======>......................] - ETA: 14:50 - loss: 1.2417 - regression_loss: 1.0500 - classification_loss: 0.1917
 420/1500 [=======>......................] - ETA: 14:50 - loss: 1.2396 - regression_loss: 1.0483 - classification_loss: 0.1913
 421/1500 [=======>......................] - ETA: 14:48 - loss: 1.2425 - regression_loss: 1.0504 - classification_loss: 0.1921
 422/1500 [=======>......................] - ETA: 14:49 - loss: 1.2426 - regression_loss: 1.0505 - classification_loss: 0.1921
 423/1500 [=======>......................] - ETA: 14:49 - loss: 1.2406 - regression_loss: 1.0489 - classification_loss: 0.1917
 424/1500 [=======>......................] - ETA: 14:47 - loss: 1.2394 - regression_loss: 1.0479 - classification_loss: 0.1915
 425/1500 [=======>......................] - ETA: 14:45 - loss: 1.2387 - regression_loss: 1.0474 - classification_loss: 0.1913
 426/1500 [=======>......................] - ETA: 14:45 - loss: 1.2407 - regression_loss: 1.0490 - classification_loss: 0.1917
 427/1500 [=======>......................] - ETA: 14:44 - loss: 1.2407 - regression_loss: 1.0491 - classification_loss: 0.1915
 428/1500 [=======>......................] - ETA: 14:43 - loss: 1.2417 - regression_loss: 1.0498 - classification_loss: 0.1919
 429/1500 [=======>......................] - ETA: 14:41 - loss: 1.2410 - regression_loss: 1.0491 - classification_loss: 0.1919
 430/1500 [=======>......................] - ETA: 14:41 - loss: 1.2406 - regression_loss: 1.0489 - classification_loss: 0.1916
 431/1500 [=======>......................] - ETA: 14:39 - loss: 1.2391 - regression_loss: 1.0474 - classification_loss: 0.1917
 432/1500 [=======>......................] - ETA: 14:38 - loss: 1.2389 - regression_loss: 1.0473 - classification_loss: 0.1917
 433/1500 [=======>......................] - ETA: 14:36 - loss: 1.2377 - regression_loss: 1.0463 - classification_loss: 0.1914
 434/1500 [=======>......................] - ETA: 14:35 - loss: 1.2407 - regression_loss: 1.0488 - classification_loss: 0.1919
 435/1500 [=======>......................] - ETA: 14:33 - loss: 1.2403 - regression_loss: 1.0485 - classification_loss: 0.1918
 436/1500 [=======>......................] - ETA: 14:31 - loss: 1.2388 - regression_loss: 1.0470 - classification_loss: 0.1918
 437/1500 [=======>......................] - ETA: 14:29 - loss: 1.2389 - regression_loss: 1.0470 - classification_loss: 0.1919
 438/1500 [=======>......................] - ETA: 14:28 - loss: 1.2393 - regression_loss: 1.0473 - classification_loss: 0.1920
 439/1500 [=======>......................] - ETA: 14:27 - loss: 1.2402 - regression_loss: 1.0481 - classification_loss: 0.1921
 440/1500 [=======>......................] - ETA: 14:25 - loss: 1.2396 - regression_loss: 1.0475 - classification_loss: 0.1921
 441/1500 [=======>......................] - ETA: 14:23 - loss: 1.2390 - regression_loss: 1.0467 - classification_loss: 0.1923
 442/1500 [=======>......................] - ETA: 14:24 - loss: 1.2399 - regression_loss: 1.0470 - classification_loss: 0.1929
 443/1500 [=======>......................] - ETA: 14:23 - loss: 1.2394 - regression_loss: 1.0463 - classification_loss: 0.1931
 444/1500 [=======>......................] - ETA: 14:22 - loss: 1.2386 - regression_loss: 1.0457 - classification_loss: 0.1928
 445/1500 [=======>......................] - ETA: 14:21 - loss: 1.2388 - regression_loss: 1.0459 - classification_loss: 0.1930
 446/1500 [=======>......................] - ETA: 14:21 - loss: 1.2401 - regression_loss: 1.0471 - classification_loss: 0.1931
 447/1500 [=======>......................] - ETA: 14:19 - loss: 1.2416 - regression_loss: 1.0486 - classification_loss: 0.1930
 448/1500 [=======>......................] - ETA: 14:17 - loss: 1.2414 - regression_loss: 1.0483 - classification_loss: 0.1931
 449/1500 [=======>......................] - ETA: 14:17 - loss: 1.2405 - regression_loss: 1.0476 - classification_loss: 0.1929
 450/1500 [========>.....................] - ETA: 14:15 - loss: 1.2402 - regression_loss: 1.0475 - classification_loss: 0.1927
 451/1500 [========>.....................] - ETA: 14:13 - loss: 1.2394 - regression_loss: 1.0470 - classification_loss: 0.1924
 452/1500 [========>.....................] - ETA: 14:11 - loss: 1.2383 - regression_loss: 1.0461 - classification_loss: 0.1922
 453/1500 [========>.....................] - ETA: 14:10 - loss: 1.2368 - regression_loss: 1.0448 - classification_loss: 0.1920
 454/1500 [========>.....................] - ETA: 14:08 - loss: 1.2359 - regression_loss: 1.0443 - classification_loss: 0.1917
 455/1500 [========>.....................] - ETA: 14:08 - loss: 1.2363 - regression_loss: 1.0445 - classification_loss: 0.1918
 456/1500 [========>.....................] - ETA: 14:06 - loss: 1.2343 - regression_loss: 1.0429 - classification_loss: 0.1915
 457/1500 [========>.....................] - ETA: 14:05 - loss: 1.2340 - regression_loss: 1.0428 - classification_loss: 0.1912
 458/1500 [========>.....................] - ETA: 14:03 - loss: 1.2332 - regression_loss: 1.0420 - classification_loss: 0.1913
 459/1500 [========>.....................] - ETA: 14:03 - loss: 1.2327 - regression_loss: 1.0417 - classification_loss: 0.1911
 460/1500 [========>.....................] - ETA: 14:02 - loss: 1.2324 - regression_loss: 1.0412 - classification_loss: 0.1912
 461/1500 [========>.....................] - ETA: 14:00 - loss: 1.2306 - regression_loss: 1.0397 - classification_loss: 0.1908
 462/1500 [========>.....................] - ETA: 13:58 - loss: 1.2293 - regression_loss: 1.0387 - classification_loss: 0.1906
 463/1500 [========>.....................] - ETA: 13:58 - loss: 1.2280 - regression_loss: 1.0377 - classification_loss: 0.1903
 464/1500 [========>.....................] - ETA: 13:56 - loss: 1.2285 - regression_loss: 1.0380 - classification_loss: 0.1905
 465/1500 [========>.....................] - ETA: 13:55 - loss: 1.2277 - regression_loss: 1.0372 - classification_loss: 0.1905
 466/1500 [========>.....................] - ETA: 13:54 - loss: 1.2271 - regression_loss: 1.0368 - classification_loss: 0.1903
 467/1500 [========>.....................] - ETA: 13:53 - loss: 1.2270 - regression_loss: 1.0368 - classification_loss: 0.1902
 468/1500 [========>.....................] - ETA: 13:52 - loss: 1.2267 - regression_loss: 1.0367 - classification_loss: 0.1900
 469/1500 [========>.....................] - ETA: 13:53 - loss: 1.2261 - regression_loss: 1.0363 - classification_loss: 0.1898
 470/1500 [========>.....................] - ETA: 13:52 - loss: 1.2245 - regression_loss: 1.0351 - classification_loss: 0.1894
 471/1500 [========>.....................] - ETA: 13:52 - loss: 1.2230 - regression_loss: 1.0337 - classification_loss: 0.1893
 472/1500 [========>.....................] - ETA: 13:51 - loss: 1.2236 - regression_loss: 1.0340 - classification_loss: 0.1897
 473/1500 [========>.....................] - ETA: 13:52 - loss: 1.2225 - regression_loss: 1.0331 - classification_loss: 0.1894
 474/1500 [========>.....................] - ETA: 13:50 - loss: 1.2217 - regression_loss: 1.0326 - classification_loss: 0.1891
 475/1500 [========>.....................] - ETA: 13:51 - loss: 1.2215 - regression_loss: 1.0323 - classification_loss: 0.1891
 476/1500 [========>.....................] - ETA: 13:50 - loss: 1.2211 - regression_loss: 1.0322 - classification_loss: 0.1890
 477/1500 [========>.....................] - ETA: 13:52 - loss: 1.2202 - regression_loss: 1.0314 - classification_loss: 0.1888
 478/1500 [========>.....................] - ETA: 13:50 - loss: 1.2194 - regression_loss: 1.0306 - classification_loss: 0.1888
 479/1500 [========>.....................] - ETA: 13:48 - loss: 1.2203 - regression_loss: 1.0311 - classification_loss: 0.1892
 480/1500 [========>.....................] - ETA: 13:49 - loss: 1.2216 - regression_loss: 1.0319 - classification_loss: 0.1897
 481/1500 [========>.....................] - ETA: 13:48 - loss: 1.2212 - regression_loss: 1.0316 - classification_loss: 0.1896
 482/1500 [========>.....................] - ETA: 13:46 - loss: 1.2219 - regression_loss: 1.0321 - classification_loss: 0.1898
 483/1500 [========>.....................] - ETA: 13:44 - loss: 1.2204 - regression_loss: 1.0308 - classification_loss: 0.1896
 484/1500 [========>.....................] - ETA: 13:43 - loss: 1.2189 - regression_loss: 1.0296 - classification_loss: 0.1893
 485/1500 [========>.....................] - ETA: 13:41 - loss: 1.2182 - regression_loss: 1.0291 - classification_loss: 0.1891
 486/1500 [========>.....................] - ETA: 13:40 - loss: 1.2166 - regression_loss: 1.0278 - classification_loss: 0.1888
 487/1500 [========>.....................] - ETA: 13:38 - loss: 1.2161 - regression_loss: 1.0275 - classification_loss: 0.1886
 488/1500 [========>.....................] - ETA: 13:37 - loss: 1.2156 - regression_loss: 1.0272 - classification_loss: 0.1884
 489/1500 [========>.....................] - ETA: 13:35 - loss: 1.2155 - regression_loss: 1.0273 - classification_loss: 0.1883
 490/1500 [========>.....................] - ETA: 13:34 - loss: 1.2163 - regression_loss: 1.0280 - classification_loss: 0.1884
 491/1500 [========>.....................] - ETA: 13:34 - loss: 1.2182 - regression_loss: 1.0293 - classification_loss: 0.1889
 492/1500 [========>.....................] - ETA: 13:32 - loss: 1.2172 - regression_loss: 1.0285 - classification_loss: 0.1886
 493/1500 [========>.....................] - ETA: 13:31 - loss: 1.2168 - regression_loss: 1.0283 - classification_loss: 0.1885
 494/1500 [========>.....................] - ETA: 13:31 - loss: 1.2189 - regression_loss: 1.0305 - classification_loss: 0.1885
 495/1500 [========>.....................] - ETA: 13:30 - loss: 1.2190 - regression_loss: 1.0306 - classification_loss: 0.1884
 496/1500 [========>.....................] - ETA: 13:29 - loss: 1.2184 - regression_loss: 1.0302 - classification_loss: 0.1882
 497/1500 [========>.....................] - ETA: 13:27 - loss: 1.2190 - regression_loss: 1.0308 - classification_loss: 0.1882
 498/1500 [========>.....................] - ETA: 13:27 - loss: 1.2198 - regression_loss: 1.0312 - classification_loss: 0.1885
 499/1500 [========>.....................] - ETA: 13:26 - loss: 1.2189 - regression_loss: 1.0306 - classification_loss: 0.1883
 500/1500 [=========>....................] - ETA: 13:25 - loss: 1.2194 - regression_loss: 1.0310 - classification_loss: 0.1885
 501/1500 [=========>....................] - ETA: 13:24 - loss: 1.2189 - regression_loss: 1.0305 - classification_loss: 0.1884
 502/1500 [=========>....................] - ETA: 13:23 - loss: 1.2175 - regression_loss: 1.0291 - classification_loss: 0.1884
 503/1500 [=========>....................] - ETA: 13:23 - loss: 1.2179 - regression_loss: 1.0294 - classification_loss: 0.1884
 504/1500 [=========>....................] - ETA: 13:21 - loss: 1.2178 - regression_loss: 1.0295 - classification_loss: 0.1883
 505/1500 [=========>....................] - ETA: 13:20 - loss: 1.2176 - regression_loss: 1.0295 - classification_loss: 0.1881
 506/1500 [=========>....................] - ETA: 13:19 - loss: 1.2181 - regression_loss: 1.0299 - classification_loss: 0.1882
 507/1500 [=========>....................] - ETA: 13:17 - loss: 1.2202 - regression_loss: 1.0303 - classification_loss: 0.1899
 508/1500 [=========>....................] - ETA: 13:16 - loss: 1.2198 - regression_loss: 1.0301 - classification_loss: 0.1898
 509/1500 [=========>....................] - ETA: 13:14 - loss: 1.2213 - regression_loss: 1.0314 - classification_loss: 0.1899
 510/1500 [=========>....................] - ETA: 13:13 - loss: 1.2219 - regression_loss: 1.0319 - classification_loss: 0.1900
 511/1500 [=========>....................] - ETA: 13:11 - loss: 1.2236 - regression_loss: 1.0331 - classification_loss: 0.1906
 512/1500 [=========>....................] - ETA: 13:10 - loss: 1.2231 - regression_loss: 1.0328 - classification_loss: 0.1904
 513/1500 [=========>....................] - ETA: 13:09 - loss: 1.2215 - regression_loss: 1.0315 - classification_loss: 0.1901
 514/1500 [=========>....................] - ETA: 13:07 - loss: 1.2216 - regression_loss: 1.0313 - classification_loss: 0.1903
 515/1500 [=========>....................] - ETA: 13:06 - loss: 1.2203 - regression_loss: 1.0302 - classification_loss: 0.1901
 516/1500 [=========>....................] - ETA: 13:04 - loss: 1.2190 - regression_loss: 1.0291 - classification_loss: 0.1899
 517/1500 [=========>....................] - ETA: 13:03 - loss: 1.2197 - regression_loss: 1.0294 - classification_loss: 0.1902
 518/1500 [=========>....................] - ETA: 13:02 - loss: 1.2195 - regression_loss: 1.0294 - classification_loss: 0.1901
 519/1500 [=========>....................] - ETA: 13:01 - loss: 1.2221 - regression_loss: 1.0314 - classification_loss: 0.1907
 520/1500 [=========>....................] - ETA: 13:01 - loss: 1.2217 - regression_loss: 1.0310 - classification_loss: 0.1907
 521/1500 [=========>....................] - ETA: 12:59 - loss: 1.2236 - regression_loss: 1.0323 - classification_loss: 0.1912
 522/1500 [=========>....................] - ETA: 12:58 - loss: 1.2223 - regression_loss: 1.0313 - classification_loss: 0.1910
 523/1500 [=========>....................] - ETA: 12:57 - loss: 1.2219 - regression_loss: 1.0310 - classification_loss: 0.1908
 524/1500 [=========>....................] - ETA: 12:56 - loss: 1.2213 - regression_loss: 1.0306 - classification_loss: 0.1907
 525/1500 [=========>....................] - ETA: 12:56 - loss: 1.2207 - regression_loss: 1.0301 - classification_loss: 0.1906
 526/1500 [=========>....................] - ETA: 12:55 - loss: 1.2212 - regression_loss: 1.0304 - classification_loss: 0.1909
 527/1500 [=========>....................] - ETA: 12:54 - loss: 1.2206 - regression_loss: 1.0299 - classification_loss: 0.1907
 528/1500 [=========>....................] - ETA: 12:54 - loss: 1.2194 - regression_loss: 1.0290 - classification_loss: 0.1904
 529/1500 [=========>....................] - ETA: 12:52 - loss: 1.2205 - regression_loss: 1.0298 - classification_loss: 0.1908
 530/1500 [=========>....................] - ETA: 12:51 - loss: 1.2197 - regression_loss: 1.0292 - classification_loss: 0.1905
 531/1500 [=========>....................] - ETA: 12:50 - loss: 1.2202 - regression_loss: 1.0297 - classification_loss: 0.1905
 532/1500 [=========>....................] - ETA: 12:49 - loss: 1.2200 - regression_loss: 1.0296 - classification_loss: 0.1905
 533/1500 [=========>....................] - ETA: 12:49 - loss: 1.2215 - regression_loss: 1.0307 - classification_loss: 0.1908
 534/1500 [=========>....................] - ETA: 12:47 - loss: 1.2219 - regression_loss: 1.0309 - classification_loss: 0.1909
 535/1500 [=========>....................] - ETA: 12:48 - loss: 1.2215 - regression_loss: 1.0308 - classification_loss: 0.1908
 536/1500 [=========>....................] - ETA: 12:49 - loss: 1.2202 - regression_loss: 1.0297 - classification_loss: 0.1905
 537/1500 [=========>....................] - ETA: 12:48 - loss: 1.2196 - regression_loss: 1.0292 - classification_loss: 0.1904
 538/1500 [=========>....................] - ETA: 12:47 - loss: 1.2213 - regression_loss: 1.0308 - classification_loss: 0.1905
 539/1500 [=========>....................] - ETA: 12:46 - loss: 1.2226 - regression_loss: 1.0317 - classification_loss: 0.1909
 540/1500 [=========>....................] - ETA: 12:47 - loss: 1.2222 - regression_loss: 1.0315 - classification_loss: 0.1907
 541/1500 [=========>....................] - ETA: 12:47 - loss: 1.2212 - regression_loss: 1.0307 - classification_loss: 0.1906
 542/1500 [=========>....................] - ETA: 12:46 - loss: 1.2202 - regression_loss: 1.0298 - classification_loss: 0.1903
 543/1500 [=========>....................] - ETA: 12:45 - loss: 1.2202 - regression_loss: 1.0298 - classification_loss: 0.1904
 544/1500 [=========>....................] - ETA: 12:44 - loss: 1.2200 - regression_loss: 1.0297 - classification_loss: 0.1903
 545/1500 [=========>....................] - ETA: 12:44 - loss: 1.2213 - regression_loss: 1.0308 - classification_loss: 0.1906
 546/1500 [=========>....................] - ETA: 12:44 - loss: 1.2204 - regression_loss: 1.0300 - classification_loss: 0.1904
 547/1500 [=========>....................] - ETA: 12:44 - loss: 1.2217 - regression_loss: 1.0312 - classification_loss: 0.1905
 548/1500 [=========>....................] - ETA: 12:43 - loss: 1.2218 - regression_loss: 1.0314 - classification_loss: 0.1904
 549/1500 [=========>....................] - ETA: 12:42 - loss: 1.2214 - regression_loss: 1.0311 - classification_loss: 0.1903
 550/1500 [==========>...................] - ETA: 12:40 - loss: 1.2212 - regression_loss: 1.0310 - classification_loss: 0.1901
 551/1500 [==========>...................] - ETA: 12:39 - loss: 1.2207 - regression_loss: 1.0308 - classification_loss: 0.1899
 552/1500 [==========>...................] - ETA: 12:38 - loss: 1.2218 - regression_loss: 1.0316 - classification_loss: 0.1902
 553/1500 [==========>...................] - ETA: 12:37 - loss: 1.2225 - regression_loss: 1.0321 - classification_loss: 0.1903
 554/1500 [==========>...................] - ETA: 12:36 - loss: 1.2241 - regression_loss: 1.0335 - classification_loss: 0.1905
 555/1500 [==========>...................] - ETA: 12:35 - loss: 1.2249 - regression_loss: 1.0340 - classification_loss: 0.1909
 556/1500 [==========>...................] - ETA: 12:33 - loss: 1.2250 - regression_loss: 1.0342 - classification_loss: 0.1908
 557/1500 [==========>...................] - ETA: 12:32 - loss: 1.2252 - regression_loss: 1.0344 - classification_loss: 0.1907
 558/1500 [==========>...................] - ETA: 12:31 - loss: 1.2242 - regression_loss: 1.0337 - classification_loss: 0.1905
 559/1500 [==========>...................] - ETA: 12:30 - loss: 1.2244 - regression_loss: 1.0339 - classification_loss: 0.1904
 560/1500 [==========>...................] - ETA: 12:29 - loss: 1.2241 - regression_loss: 1.0336 - classification_loss: 0.1904
 561/1500 [==========>...................] - ETA: 12:27 - loss: 1.2232 - regression_loss: 1.0329 - classification_loss: 0.1903
 562/1500 [==========>...................] - ETA: 12:26 - loss: 1.2227 - regression_loss: 1.0325 - classification_loss: 0.1902
 563/1500 [==========>...................] - ETA: 12:24 - loss: 1.2228 - regression_loss: 1.0322 - classification_loss: 0.1905
 564/1500 [==========>...................] - ETA: 12:23 - loss: 1.2226 - regression_loss: 1.0323 - classification_loss: 0.1904
 565/1500 [==========>...................] - ETA: 12:23 - loss: 1.2235 - regression_loss: 1.0329 - classification_loss: 0.1906
 566/1500 [==========>...................] - ETA: 12:23 - loss: 1.2222 - regression_loss: 1.0318 - classification_loss: 0.1904
 567/1500 [==========>...................] - ETA: 12:21 - loss: 1.2214 - regression_loss: 1.0312 - classification_loss: 0.1901
 568/1500 [==========>...................] - ETA: 12:20 - loss: 1.2220 - regression_loss: 1.0310 - classification_loss: 0.1910
 569/1500 [==========>...................] - ETA: 12:19 - loss: 1.2211 - regression_loss: 1.0304 - classification_loss: 0.1908
 570/1500 [==========>...................] - ETA: 12:18 - loss: 1.2228 - regression_loss: 1.0316 - classification_loss: 0.1912
 571/1500 [==========>...................] - ETA: 12:17 - loss: 1.2219 - regression_loss: 1.0309 - classification_loss: 0.1911
 572/1500 [==========>...................] - ETA: 12:16 - loss: 1.2222 - regression_loss: 1.0312 - classification_loss: 0.1910
 573/1500 [==========>...................] - ETA: 12:15 - loss: 1.2228 - regression_loss: 1.0315 - classification_loss: 0.1913
 574/1500 [==========>...................] - ETA: 12:13 - loss: 1.2222 - regression_loss: 1.0310 - classification_loss: 0.1912
 575/1500 [==========>...................] - ETA: 12:12 - loss: 1.2220 - regression_loss: 1.0309 - classification_loss: 0.1911
 576/1500 [==========>...................] - ETA: 12:11 - loss: 1.2219 - regression_loss: 1.0311 - classification_loss: 0.1909
 577/1500 [==========>...................] - ETA: 12:11 - loss: 1.2211 - regression_loss: 1.0304 - classification_loss: 0.1907
 578/1500 [==========>...................] - ETA: 12:10 - loss: 1.2208 - regression_loss: 1.0300 - classification_loss: 0.1908
 579/1500 [==========>...................] - ETA: 12:08 - loss: 1.2208 - regression_loss: 1.0300 - classification_loss: 0.1908
 580/1500 [==========>...................] - ETA: 12:07 - loss: 1.2225 - regression_loss: 1.0316 - classification_loss: 0.1909
 581/1500 [==========>...................] - ETA: 12:06 - loss: 1.2246 - regression_loss: 1.0335 - classification_loss: 0.1911
 582/1500 [==========>...................] - ETA: 12:06 - loss: 1.2271 - regression_loss: 1.0354 - classification_loss: 0.1917
 583/1500 [==========>...................] - ETA: 12:07 - loss: 1.2259 - regression_loss: 1.0345 - classification_loss: 0.1915
 584/1500 [==========>...................] - ETA: 12:05 - loss: 1.2272 - regression_loss: 1.0354 - classification_loss: 0.1918
 585/1500 [==========>...................] - ETA: 12:05 - loss: 1.2275 - regression_loss: 1.0357 - classification_loss: 0.1919
 586/1500 [==========>...................] - ETA: 12:05 - loss: 1.2278 - regression_loss: 1.0357 - classification_loss: 0.1920
 587/1500 [==========>...................] - ETA: 12:06 - loss: 1.2274 - regression_loss: 1.0356 - classification_loss: 0.1919
 588/1500 [==========>...................] - ETA: 12:04 - loss: 1.2269 - regression_loss: 1.0351 - classification_loss: 0.1918
 589/1500 [==========>...................] - ETA: 12:03 - loss: 1.2262 - regression_loss: 1.0346 - classification_loss: 0.1916
 590/1500 [==========>...................] - ETA: 12:02 - loss: 1.2261 - regression_loss: 1.0346 - classification_loss: 0.1915
 591/1500 [==========>...................] - ETA: 12:01 - loss: 1.2261 - regression_loss: 1.0344 - classification_loss: 0.1918
 592/1500 [==========>...................] - ETA: 12:00 - loss: 1.2272 - regression_loss: 1.0351 - classification_loss: 0.1921
 593/1500 [==========>...................] - ETA: 12:01 - loss: 1.2269 - regression_loss: 1.0349 - classification_loss: 0.1919
 594/1500 [==========>...................] - ETA: 12:00 - loss: 1.2265 - regression_loss: 1.0345 - classification_loss: 0.1919
 595/1500 [==========>...................] - ETA: 11:59 - loss: 1.2269 - regression_loss: 1.0348 - classification_loss: 0.1921
 596/1500 [==========>...................] - ETA: 11:57 - loss: 1.2264 - regression_loss: 1.0345 - classification_loss: 0.1919
 597/1500 [==========>...................] - ETA: 11:56 - loss: 1.2289 - regression_loss: 1.0366 - classification_loss: 0.1923
 598/1500 [==========>...................] - ETA: 11:55 - loss: 1.2281 - regression_loss: 1.0359 - classification_loss: 0.1921
 599/1500 [==========>...................] - ETA: 11:54 - loss: 1.2274 - regression_loss: 1.0354 - classification_loss: 0.1920
 600/1500 [===========>..................] - ETA: 11:53 - loss: 1.2285 - regression_loss: 1.0361 - classification_loss: 0.1924
 601/1500 [===========>..................] - ETA: 11:52 - loss: 1.2296 - regression_loss: 1.0370 - classification_loss: 0.1926
 602/1500 [===========>..................] - ETA: 11:51 - loss: 1.2294 - regression_loss: 1.0369 - classification_loss: 0.1926
 603/1500 [===========>..................] - ETA: 11:50 - loss: 1.2289 - regression_loss: 1.0363 - classification_loss: 0.1925
 604/1500 [===========>..................] - ETA: 11:49 - loss: 1.2302 - regression_loss: 1.0374 - classification_loss: 0.1929
 605/1500 [===========>..................] - ETA: 11:48 - loss: 1.2325 - regression_loss: 1.0392 - classification_loss: 0.1933
 606/1500 [===========>..................] - ETA: 11:48 - loss: 1.2325 - regression_loss: 1.0393 - classification_loss: 0.1932
 607/1500 [===========>..................] - ETA: 11:48 - loss: 1.2320 - regression_loss: 1.0390 - classification_loss: 0.1930
 608/1500 [===========>..................] - ETA: 11:48 - loss: 1.2317 - regression_loss: 1.0387 - classification_loss: 0.1930
 609/1500 [===========>..................] - ETA: 11:46 - loss: 1.2309 - regression_loss: 1.0381 - classification_loss: 0.1928
 610/1500 [===========>..................] - ETA: 11:45 - loss: 1.2309 - regression_loss: 1.0381 - classification_loss: 0.1928
 611/1500 [===========>..................] - ETA: 11:44 - loss: 1.2303 - regression_loss: 1.0377 - classification_loss: 0.1927
 612/1500 [===========>..................] - ETA: 11:42 - loss: 1.2296 - regression_loss: 1.0371 - classification_loss: 0.1925
 613/1500 [===========>..................] - ETA: 11:41 - loss: 1.2299 - regression_loss: 1.0374 - classification_loss: 0.1925
 614/1500 [===========>..................] - ETA: 11:40 - loss: 1.2290 - regression_loss: 1.0366 - classification_loss: 0.1923
 615/1500 [===========>..................] - ETA: 11:39 - loss: 1.2288 - regression_loss: 1.0365 - classification_loss: 0.1923
 616/1500 [===========>..................] - ETA: 11:39 - loss: 1.2289 - regression_loss: 1.0366 - classification_loss: 0.1923
 617/1500 [===========>..................] - ETA: 11:39 - loss: 1.2290 - regression_loss: 1.0366 - classification_loss: 0.1924
 618/1500 [===========>..................] - ETA: 11:38 - loss: 1.2293 - regression_loss: 1.0370 - classification_loss: 0.1924
 619/1500 [===========>..................] - ETA: 11:37 - loss: 1.2286 - regression_loss: 1.0364 - classification_loss: 0.1922
 620/1500 [===========>..................] - ETA: 11:36 - loss: 1.2274 - regression_loss: 1.0354 - classification_loss: 0.1920
 621/1500 [===========>..................] - ETA: 11:35 - loss: 1.2286 - regression_loss: 1.0364 - classification_loss: 0.1922
 622/1500 [===========>..................] - ETA: 11:34 - loss: 1.2294 - regression_loss: 1.0372 - classification_loss: 0.1922
 623/1500 [===========>..................] - ETA: 11:33 - loss: 1.2288 - regression_loss: 1.0367 - classification_loss: 0.1921
 624/1500 [===========>..................] - ETA: 11:32 - loss: 1.2292 - regression_loss: 1.0371 - classification_loss: 0.1920
 625/1500 [===========>..................] - ETA: 11:31 - loss: 1.2306 - regression_loss: 1.0384 - classification_loss: 0.1922
 626/1500 [===========>..................] - ETA: 11:31 - loss: 1.2314 - regression_loss: 1.0392 - classification_loss: 0.1922
 627/1500 [===========>..................] - ETA: 11:34 - loss: 1.2325 - regression_loss: 1.0402 - classification_loss: 0.1923
 628/1500 [===========>..................] - ETA: 11:34 - loss: 1.2318 - regression_loss: 1.0397 - classification_loss: 0.1921
 629/1500 [===========>..................] - ETA: 11:32 - loss: 1.2305 - regression_loss: 1.0386 - classification_loss: 0.1918
 630/1500 [===========>..................] - ETA: 11:32 - loss: 1.2330 - regression_loss: 1.0405 - classification_loss: 0.1925
 631/1500 [===========>..................] - ETA: 11:31 - loss: 1.2326 - regression_loss: 1.0402 - classification_loss: 0.1924
 632/1500 [===========>..................] - ETA: 11:30 - loss: 1.2326 - regression_loss: 1.0401 - classification_loss: 0.1925
 633/1500 [===========>..................] - ETA: 11:29 - loss: 1.2320 - regression_loss: 1.0397 - classification_loss: 0.1923
 634/1500 [===========>..................] - ETA: 11:28 - loss: 1.2328 - regression_loss: 1.0403 - classification_loss: 0.1925
 635/1500 [===========>..................] - ETA: 11:26 - loss: 1.2334 - regression_loss: 1.0407 - classification_loss: 0.1927
 636/1500 [===========>..................] - ETA: 11:25 - loss: 1.2353 - regression_loss: 1.0420 - classification_loss: 0.1933
 637/1500 [===========>..................] - ETA: 11:24 - loss: 1.2347 - regression_loss: 1.0414 - classification_loss: 0.1933
 638/1500 [===========>..................] - ETA: 11:25 - loss: 1.2341 - regression_loss: 1.0410 - classification_loss: 0.1931
 639/1500 [===========>..................] - ETA: 11:23 - loss: 1.2337 - regression_loss: 1.0408 - classification_loss: 0.1929
 640/1500 [===========>..................] - ETA: 11:22 - loss: 1.2346 - regression_loss: 1.0418 - classification_loss: 0.1928
 641/1500 [===========>..................] - ETA: 11:21 - loss: 1.2350 - regression_loss: 1.0421 - classification_loss: 0.1930
 642/1500 [===========>..................] - ETA: 11:20 - loss: 1.2360 - regression_loss: 1.0430 - classification_loss: 0.1930
 643/1500 [===========>..................] - ETA: 11:19 - loss: 1.2351 - regression_loss: 1.0422 - classification_loss: 0.1929
 644/1500 [===========>..................] - ETA: 11:18 - loss: 1.2358 - regression_loss: 1.0429 - classification_loss: 0.1930
 645/1500 [===========>..................] - ETA: 11:17 - loss: 1.2362 - regression_loss: 1.0431 - classification_loss: 0.1931
 646/1500 [===========>..................] - ETA: 11:16 - loss: 1.2368 - regression_loss: 1.0436 - classification_loss: 0.1932
 647/1500 [===========>..................] - ETA: 11:15 - loss: 1.2371 - regression_loss: 1.0438 - classification_loss: 0.1934
 648/1500 [===========>..................] - ETA: 11:13 - loss: 1.2361 - regression_loss: 1.0429 - classification_loss: 0.1931
 649/1500 [===========>..................] - ETA: 11:13 - loss: 1.2350 - regression_loss: 1.0420 - classification_loss: 0.1930
 650/1500 [============>.................] - ETA: 11:12 - loss: 1.2371 - regression_loss: 1.0436 - classification_loss: 0.1935
 651/1500 [============>.................] - ETA: 11:12 - loss: 1.2376 - regression_loss: 1.0439 - classification_loss: 0.1936
 652/1500 [============>.................] - ETA: 11:11 - loss: 1.2391 - regression_loss: 1.0451 - classification_loss: 0.1939
 653/1500 [============>.................] - ETA: 11:11 - loss: 1.2410 - regression_loss: 1.0466 - classification_loss: 0.1944
 654/1500 [============>.................] - ETA: 11:10 - loss: 1.2403 - regression_loss: 1.0460 - classification_loss: 0.1944
 655/1500 [============>.................] - ETA: 11:09 - loss: 1.2401 - regression_loss: 1.0457 - classification_loss: 0.1943
 656/1500 [============>.................] - ETA: 11:08 - loss: 1.2419 - regression_loss: 1.0474 - classification_loss: 0.1945
 657/1500 [============>.................] - ETA: 11:08 - loss: 1.2419 - regression_loss: 1.0474 - classification_loss: 0.1945
 658/1500 [============>.................] - ETA: 11:06 - loss: 1.2428 - regression_loss: 1.0481 - classification_loss: 0.1947
 659/1500 [============>.................] - ETA: 11:05 - loss: 1.2426 - regression_loss: 1.0480 - classification_loss: 0.1946
 660/1500 [============>.................] - ETA: 11:05 - loss: 1.2414 - regression_loss: 1.0470 - classification_loss: 0.1944
 661/1500 [============>.................] - ETA: 11:05 - loss: 1.2431 - regression_loss: 1.0486 - classification_loss: 0.1945
 662/1500 [============>.................] - ETA: 11:04 - loss: 1.2432 - regression_loss: 1.0486 - classification_loss: 0.1946
 663/1500 [============>.................] - ETA: 11:05 - loss: 1.2422 - regression_loss: 1.0479 - classification_loss: 0.1943
 664/1500 [============>.................] - ETA: 11:04 - loss: 1.2414 - regression_loss: 1.0473 - classification_loss: 0.1941
 665/1500 [============>.................] - ETA: 11:03 - loss: 1.2412 - regression_loss: 1.0471 - classification_loss: 0.1941
 666/1500 [============>.................] - ETA: 11:02 - loss: 1.2415 - regression_loss: 1.0471 - classification_loss: 0.1943
 667/1500 [============>.................] - ETA: 11:01 - loss: 1.2410 - regression_loss: 1.0468 - classification_loss: 0.1942
 668/1500 [============>.................] - ETA: 11:00 - loss: 1.2413 - regression_loss: 1.0472 - classification_loss: 0.1941
 669/1500 [============>.................] - ETA: 10:59 - loss: 1.2421 - regression_loss: 1.0479 - classification_loss: 0.1942
 670/1500 [============>.................] - ETA: 10:58 - loss: 1.2415 - regression_loss: 1.0474 - classification_loss: 0.1941
 671/1500 [============>.................] - ETA: 10:57 - loss: 1.2441 - regression_loss: 1.0496 - classification_loss: 0.1945
 672/1500 [============>.................] - ETA: 10:56 - loss: 1.2446 - regression_loss: 1.0498 - classification_loss: 0.1948
 673/1500 [============>.................] - ETA: 10:56 - loss: 1.2442 - regression_loss: 1.0495 - classification_loss: 0.1947
 674/1500 [============>.................] - ETA: 10:55 - loss: 1.2432 - regression_loss: 1.0486 - classification_loss: 0.1946
 675/1500 [============>.................] - ETA: 10:54 - loss: 1.2432 - regression_loss: 1.0487 - classification_loss: 0.1944
 676/1500 [============>.................] - ETA: 10:53 - loss: 1.2430 - regression_loss: 1.0486 - classification_loss: 0.1944
 677/1500 [============>.................] - ETA: 10:52 - loss: 1.2421 - regression_loss: 1.0479 - classification_loss: 0.1942
 678/1500 [============>.................] - ETA: 10:51 - loss: 1.2426 - regression_loss: 1.0482 - classification_loss: 0.1943
 679/1500 [============>.................] - ETA: 10:50 - loss: 1.2422 - regression_loss: 1.0481 - classification_loss: 0.1941
 680/1500 [============>.................] - ETA: 10:49 - loss: 1.2425 - regression_loss: 1.0483 - classification_loss: 0.1942
 681/1500 [============>.................] - ETA: 10:47 - loss: 1.2430 - regression_loss: 1.0489 - classification_loss: 0.1941
 682/1500 [============>.................] - ETA: 10:46 - loss: 1.2437 - regression_loss: 1.0497 - classification_loss: 0.1940
 683/1500 [============>.................] - ETA: 10:45 - loss: 1.2437 - regression_loss: 1.0497 - classification_loss: 0.1939
 684/1500 [============>.................] - ETA: 10:44 - loss: 1.2432 - regression_loss: 1.0493 - classification_loss: 0.1939
 685/1500 [============>.................] - ETA: 10:43 - loss: 1.2424 - regression_loss: 1.0486 - classification_loss: 0.1938
 686/1500 [============>.................] - ETA: 10:43 - loss: 1.2432 - regression_loss: 1.0490 - classification_loss: 0.1941
 687/1500 [============>.................] - ETA: 10:42 - loss: 1.2445 - regression_loss: 1.0495 - classification_loss: 0.1951
 688/1500 [============>.................] - ETA: 10:41 - loss: 1.2441 - regression_loss: 1.0491 - classification_loss: 0.1950
 689/1500 [============>.................] - ETA: 10:40 - loss: 1.2454 - regression_loss: 1.0503 - classification_loss: 0.1951
 690/1500 [============>.................] - ETA: 10:39 - loss: 1.2447 - regression_loss: 1.0497 - classification_loss: 0.1951
 691/1500 [============>.................] - ETA: 10:38 - loss: 1.2464 - regression_loss: 1.0509 - classification_loss: 0.1954
 692/1500 [============>.................] - ETA: 10:37 - loss: 1.2456 - regression_loss: 1.0503 - classification_loss: 0.1953
 693/1500 [============>.................] - ETA: 10:36 - loss: 1.2455 - regression_loss: 1.0502 - classification_loss: 0.1953
 694/1500 [============>.................] - ETA: 10:35 - loss: 1.2464 - regression_loss: 1.0509 - classification_loss: 0.1955
 695/1500 [============>.................] - ETA: 10:34 - loss: 1.2479 - regression_loss: 1.0522 - classification_loss: 0.1957
 696/1500 [============>.................] - ETA: 10:33 - loss: 1.2474 - regression_loss: 1.0518 - classification_loss: 0.1956
 697/1500 [============>.................] - ETA: 10:32 - loss: 1.2465 - regression_loss: 1.0511 - classification_loss: 0.1954
 698/1500 [============>.................] - ETA: 10:32 - loss: 1.2472 - regression_loss: 1.0516 - classification_loss: 0.1957
 699/1500 [============>.................] - ETA: 10:31 - loss: 1.2468 - regression_loss: 1.0514 - classification_loss: 0.1955
 700/1500 [=============>................] - ETA: 10:31 - loss: 1.2476 - regression_loss: 1.0521 - classification_loss: 0.1955
 701/1500 [=============>................] - ETA: 10:30 - loss: 1.2472 - regression_loss: 1.0518 - classification_loss: 0.1954
 702/1500 [=============>................] - ETA: 10:29 - loss: 1.2463 - regression_loss: 1.0511 - classification_loss: 0.1951
 703/1500 [=============>................] - ETA: 10:28 - loss: 1.2452 - regression_loss: 1.0502 - classification_loss: 0.1949
 704/1500 [=============>................] - ETA: 10:27 - loss: 1.2448 - regression_loss: 1.0499 - classification_loss: 0.1949
 705/1500 [=============>................] - ETA: 10:26 - loss: 1.2443 - regression_loss: 1.0495 - classification_loss: 0.1948
 706/1500 [=============>................] - ETA: 10:26 - loss: 1.2452 - regression_loss: 1.0502 - classification_loss: 0.1949
 707/1500 [=============>................] - ETA: 10:25 - loss: 1.2452 - regression_loss: 1.0503 - classification_loss: 0.1949
 708/1500 [=============>................] - ETA: 10:24 - loss: 1.2441 - regression_loss: 1.0495 - classification_loss: 0.1946
 709/1500 [=============>................] - ETA: 10:23 - loss: 1.2433 - regression_loss: 1.0487 - classification_loss: 0.1945
 710/1500 [=============>................] - ETA: 10:22 - loss: 1.2441 - regression_loss: 1.0493 - classification_loss: 0.1948
 711/1500 [=============>................] - ETA: 10:21 - loss: 1.2462 - regression_loss: 1.0505 - classification_loss: 0.1957
 712/1500 [=============>................] - ETA: 10:20 - loss: 1.2461 - regression_loss: 1.0505 - classification_loss: 0.1956
 713/1500 [=============>................] - ETA: 10:19 - loss: 1.2457 - regression_loss: 1.0503 - classification_loss: 0.1954
 714/1500 [=============>................] - ETA: 10:18 - loss: 1.2454 - regression_loss: 1.0500 - classification_loss: 0.1954
 715/1500 [=============>................] - ETA: 10:18 - loss: 1.2463 - regression_loss: 1.0508 - classification_loss: 0.1955
 716/1500 [=============>................] - ETA: 10:17 - loss: 1.2454 - regression_loss: 1.0500 - classification_loss: 0.1954
 717/1500 [=============>................] - ETA: 10:16 - loss: 1.2461 - regression_loss: 1.0505 - classification_loss: 0.1956
 718/1500 [=============>................] - ETA: 10:15 - loss: 1.2465 - regression_loss: 1.0509 - classification_loss: 0.1956
 719/1500 [=============>................] - ETA: 10:14 - loss: 1.2460 - regression_loss: 1.0506 - classification_loss: 0.1954
 720/1500 [=============>................] - ETA: 10:13 - loss: 1.2470 - regression_loss: 1.0514 - classification_loss: 0.1956
 721/1500 [=============>................] - ETA: 10:11 - loss: 1.2478 - regression_loss: 1.0521 - classification_loss: 0.1957
 722/1500 [=============>................] - ETA: 10:10 - loss: 1.2479 - regression_loss: 1.0524 - classification_loss: 0.1955
 723/1500 [=============>................] - ETA: 10:09 - loss: 1.2467 - regression_loss: 1.0513 - classification_loss: 0.1954
 724/1500 [=============>................] - ETA: 10:08 - loss: 1.2460 - regression_loss: 1.0506 - classification_loss: 0.1953
 725/1500 [=============>................] - ETA: 10:07 - loss: 1.2459 - regression_loss: 1.0505 - classification_loss: 0.1953
 726/1500 [=============>................] - ETA: 10:06 - loss: 1.2474 - regression_loss: 1.0517 - classification_loss: 0.1957
 727/1500 [=============>................] - ETA: 10:05 - loss: 1.2468 - regression_loss: 1.0512 - classification_loss: 0.1955
 728/1500 [=============>................] - ETA: 10:04 - loss: 1.2471 - regression_loss: 1.0512 - classification_loss: 0.1958
 729/1500 [=============>................] - ETA: 10:03 - loss: 1.2473 - regression_loss: 1.0513 - classification_loss: 0.1960
 730/1500 [=============>................] - ETA: 10:02 - loss: 1.2468 - regression_loss: 1.0509 - classification_loss: 0.1958
 731/1500 [=============>................] - ETA: 10:02 - loss: 1.2462 - regression_loss: 1.0505 - classification_loss: 0.1957
 732/1500 [=============>................] - ETA: 10:00 - loss: 1.2461 - regression_loss: 1.0504 - classification_loss: 0.1957
 733/1500 [=============>................] - ETA: 10:00 - loss: 1.2468 - regression_loss: 1.0510 - classification_loss: 0.1958
 734/1500 [=============>................] - ETA: 9:59 - loss: 1.2460 - regression_loss: 1.0504 - classification_loss: 0.1957 
 735/1500 [=============>................] - ETA: 9:58 - loss: 1.2460 - regression_loss: 1.0504 - classification_loss: 0.1956
 736/1500 [=============>................] - ETA: 9:57 - loss: 1.2471 - regression_loss: 1.0514 - classification_loss: 0.1957
 737/1500 [=============>................] - ETA: 9:56 - loss: 1.2479 - regression_loss: 1.0523 - classification_loss: 0.1956
 738/1500 [=============>................] - ETA: 9:56 - loss: 1.2476 - regression_loss: 1.0519 - classification_loss: 0.1957
 739/1500 [=============>................] - ETA: 9:55 - loss: 1.2479 - regression_loss: 1.0521 - classification_loss: 0.1958
 740/1500 [=============>................] - ETA: 9:53 - loss: 1.2481 - regression_loss: 1.0522 - classification_loss: 0.1959
 741/1500 [=============>................] - ETA: 9:53 - loss: 1.2481 - regression_loss: 1.0522 - classification_loss: 0.1960
 742/1500 [=============>................] - ETA: 9:52 - loss: 1.2483 - regression_loss: 1.0524 - classification_loss: 0.1959
 743/1500 [=============>................] - ETA: 9:51 - loss: 1.2474 - regression_loss: 1.0516 - classification_loss: 0.1958
 744/1500 [=============>................] - ETA: 9:51 - loss: 1.2462 - regression_loss: 1.0506 - classification_loss: 0.1956
 745/1500 [=============>................] - ETA: 9:50 - loss: 1.2481 - regression_loss: 1.0521 - classification_loss: 0.1960
 746/1500 [=============>................] - ETA: 9:49 - loss: 1.2477 - regression_loss: 1.0518 - classification_loss: 0.1959
 747/1500 [=============>................] - ETA: 9:49 - loss: 1.2468 - regression_loss: 1.0511 - classification_loss: 0.1957
 748/1500 [=============>................] - ETA: 9:48 - loss: 1.2467 - regression_loss: 1.0510 - classification_loss: 0.1956
 749/1500 [=============>................] - ETA: 9:47 - loss: 1.2468 - regression_loss: 1.0512 - classification_loss: 0.1957
 750/1500 [==============>...............] - ETA: 9:46 - loss: 1.2469 - regression_loss: 1.0513 - classification_loss: 0.1956
 751/1500 [==============>...............] - ETA: 9:45 - loss: 1.2472 - regression_loss: 1.0516 - classification_loss: 0.1957
 752/1500 [==============>...............] - ETA: 9:43 - loss: 1.2469 - regression_loss: 1.0514 - classification_loss: 0.1955
 753/1500 [==============>...............] - ETA: 9:42 - loss: 1.2474 - regression_loss: 1.0515 - classification_loss: 0.1958
 754/1500 [==============>...............] - ETA: 9:41 - loss: 1.2466 - regression_loss: 1.0509 - classification_loss: 0.1957
 755/1500 [==============>...............] - ETA: 9:41 - loss: 1.2476 - regression_loss: 1.0518 - classification_loss: 0.1959
 756/1500 [==============>...............] - ETA: 9:40 - loss: 1.2475 - regression_loss: 1.0518 - classification_loss: 0.1958
 757/1500 [==============>...............] - ETA: 9:39 - loss: 1.2474 - regression_loss: 1.0517 - classification_loss: 0.1957
 758/1500 [==============>...............] - ETA: 9:37 - loss: 1.2464 - regression_loss: 1.0510 - classification_loss: 0.1955
 759/1500 [==============>...............] - ETA: 9:36 - loss: 1.2461 - regression_loss: 1.0508 - classification_loss: 0.1953
 760/1500 [==============>...............] - ETA: 9:35 - loss: 1.2455 - regression_loss: 1.0502 - classification_loss: 0.1952
 761/1500 [==============>...............] - ETA: 9:36 - loss: 1.2461 - regression_loss: 1.0506 - classification_loss: 0.1954
 762/1500 [==============>...............] - ETA: 9:35 - loss: 1.2472 - regression_loss: 1.0514 - classification_loss: 0.1957
 763/1500 [==============>...............] - ETA: 9:33 - loss: 1.2473 - regression_loss: 1.0513 - classification_loss: 0.1960
 764/1500 [==============>...............] - ETA: 9:33 - loss: 1.2470 - regression_loss: 1.0512 - classification_loss: 0.1959
 765/1500 [==============>...............] - ETA: 9:34 - loss: 1.2465 - regression_loss: 1.0508 - classification_loss: 0.1957
 766/1500 [==============>...............] - ETA: 9:36 - loss: 1.2454 - regression_loss: 1.0499 - classification_loss: 0.1955
 767/1500 [==============>...............] - ETA: 9:36 - loss: 1.2444 - regression_loss: 1.0491 - classification_loss: 0.1953
 768/1500 [==============>...............] - ETA: 9:35 - loss: 1.2458 - regression_loss: 1.0501 - classification_loss: 0.1956
 769/1500 [==============>...............] - ETA: 9:34 - loss: 1.2449 - regression_loss: 1.0495 - classification_loss: 0.1954
 770/1500 [==============>...............] - ETA: 9:33 - loss: 1.2461 - regression_loss: 1.0503 - classification_loss: 0.1958
 771/1500 [==============>...............] - ETA: 9:32 - loss: 1.2469 - regression_loss: 1.0509 - classification_loss: 0.1960
 772/1500 [==============>...............] - ETA: 9:31 - loss: 1.2480 - regression_loss: 1.0519 - classification_loss: 0.1961
 773/1500 [==============>...............] - ETA: 9:30 - loss: 1.2483 - regression_loss: 1.0522 - classification_loss: 0.1962
 774/1500 [==============>...............] - ETA: 9:29 - loss: 1.2477 - regression_loss: 1.0517 - classification_loss: 0.1961
 775/1500 [==============>...............] - ETA: 9:29 - loss: 1.2466 - regression_loss: 1.0507 - classification_loss: 0.1959
 776/1500 [==============>...............] - ETA: 9:28 - loss: 1.2467 - regression_loss: 1.0509 - classification_loss: 0.1959
 777/1500 [==============>...............] - ETA: 9:27 - loss: 1.2471 - regression_loss: 1.0511 - classification_loss: 0.1961
 778/1500 [==============>...............] - ETA: 9:27 - loss: 1.2475 - regression_loss: 1.0514 - classification_loss: 0.1961
 779/1500 [==============>...............] - ETA: 9:26 - loss: 1.2475 - regression_loss: 1.0515 - classification_loss: 0.1960
 780/1500 [==============>...............] - ETA: 9:25 - loss: 1.2470 - regression_loss: 1.0511 - classification_loss: 0.1959
 781/1500 [==============>...............] - ETA: 9:25 - loss: 1.2460 - regression_loss: 1.0503 - classification_loss: 0.1957
 782/1500 [==============>...............] - ETA: 9:24 - loss: 1.2461 - regression_loss: 1.0505 - classification_loss: 0.1956
 783/1500 [==============>...............] - ETA: 9:23 - loss: 1.2452 - regression_loss: 1.0498 - classification_loss: 0.1954
 784/1500 [==============>...............] - ETA: 9:22 - loss: 1.2453 - regression_loss: 1.0498 - classification_loss: 0.1955
 785/1500 [==============>...............] - ETA: 9:21 - loss: 1.2457 - regression_loss: 1.0502 - classification_loss: 0.1955
 786/1500 [==============>...............] - ETA: 9:20 - loss: 1.2449 - regression_loss: 1.0496 - classification_loss: 0.1953
 787/1500 [==============>...............] - ETA: 9:19 - loss: 1.2447 - regression_loss: 1.0495 - classification_loss: 0.1952
 788/1500 [==============>...............] - ETA: 9:19 - loss: 1.2450 - regression_loss: 1.0497 - classification_loss: 0.1952
 789/1500 [==============>...............] - ETA: 9:19 - loss: 1.2454 - regression_loss: 1.0501 - classification_loss: 0.1953
 790/1500 [==============>...............] - ETA: 9:19 - loss: 1.2461 - regression_loss: 1.0507 - classification_loss: 0.1954
 791/1500 [==============>...............] - ETA: 9:18 - loss: 1.2458 - regression_loss: 1.0505 - classification_loss: 0.1953
 792/1500 [==============>...............] - ETA: 9:17 - loss: 1.2454 - regression_loss: 1.0502 - classification_loss: 0.1952
 793/1500 [==============>...............] - ETA: 9:16 - loss: 1.2450 - regression_loss: 1.0495 - classification_loss: 0.1954
 794/1500 [==============>...............] - ETA: 9:15 - loss: 1.2441 - regression_loss: 1.0488 - classification_loss: 0.1953
 795/1500 [==============>...............] - ETA: 9:14 - loss: 1.2434 - regression_loss: 1.0480 - classification_loss: 0.1954
 796/1500 [==============>...............] - ETA: 9:14 - loss: 1.2448 - regression_loss: 1.0487 - classification_loss: 0.1961
 797/1500 [==============>...............] - ETA: 9:14 - loss: 1.2443 - regression_loss: 1.0483 - classification_loss: 0.1960
 798/1500 [==============>...............] - ETA: 9:13 - loss: 1.2454 - regression_loss: 1.0490 - classification_loss: 0.1964
 799/1500 [==============>...............] - ETA: 9:13 - loss: 1.2457 - regression_loss: 1.0492 - classification_loss: 0.1965
 800/1500 [===============>..............] - ETA: 9:12 - loss: 1.2457 - regression_loss: 1.0492 - classification_loss: 0.1965
 801/1500 [===============>..............] - ETA: 9:11 - loss: 1.2464 - regression_loss: 1.0499 - classification_loss: 0.1965
 802/1500 [===============>..............] - ETA: 9:10 - loss: 1.2456 - regression_loss: 1.0493 - classification_loss: 0.1963
 803/1500 [===============>..............] - ETA: 9:09 - loss: 1.2456 - regression_loss: 1.0493 - classification_loss: 0.1963
 804/1500 [===============>..............] - ETA: 9:08 - loss: 1.2446 - regression_loss: 1.0485 - classification_loss: 0.1961
 805/1500 [===============>..............] - ETA: 9:07 - loss: 1.2439 - regression_loss: 1.0479 - classification_loss: 0.1960
 806/1500 [===============>..............] - ETA: 9:07 - loss: 1.2442 - regression_loss: 1.0479 - classification_loss: 0.1963
 807/1500 [===============>..............] - ETA: 9:06 - loss: 1.2445 - regression_loss: 1.0481 - classification_loss: 0.1964
 808/1500 [===============>..............] - ETA: 9:06 - loss: 1.2441 - regression_loss: 1.0479 - classification_loss: 0.1962
 809/1500 [===============>..............] - ETA: 9:05 - loss: 1.2447 - regression_loss: 1.0484 - classification_loss: 0.1963
 810/1500 [===============>..............] - ETA: 9:04 - loss: 1.2443 - regression_loss: 1.0480 - classification_loss: 0.1963
 811/1500 [===============>..............] - ETA: 9:03 - loss: 1.2444 - regression_loss: 1.0481 - classification_loss: 0.1963
 812/1500 [===============>..............] - ETA: 9:04 - loss: 1.2460 - regression_loss: 1.0493 - classification_loss: 0.1967
 813/1500 [===============>..............] - ETA: 9:03 - loss: 1.2453 - regression_loss: 1.0488 - classification_loss: 0.1965
 814/1500 [===============>..............] - ETA: 9:02 - loss: 1.2452 - regression_loss: 1.0487 - classification_loss: 0.1965
 815/1500 [===============>..............] - ETA: 9:01 - loss: 1.2465 - regression_loss: 1.0499 - classification_loss: 0.1966
 816/1500 [===============>..............] - ETA: 8:59 - loss: 1.2462 - regression_loss: 1.0497 - classification_loss: 0.1965
 817/1500 [===============>..............] - ETA: 8:59 - loss: 1.2477 - regression_loss: 1.0508 - classification_loss: 0.1968
 818/1500 [===============>..............] - ETA: 8:58 - loss: 1.2471 - regression_loss: 1.0504 - classification_loss: 0.1967
 819/1500 [===============>..............] - ETA: 8:56 - loss: 1.2462 - regression_loss: 1.0496 - classification_loss: 0.1966
 820/1500 [===============>..............] - ETA: 8:56 - loss: 1.2468 - regression_loss: 1.0501 - classification_loss: 0.1967
 821/1500 [===============>..............] - ETA: 8:55 - loss: 1.2461 - regression_loss: 1.0496 - classification_loss: 0.1965
 822/1500 [===============>..............] - ETA: 8:54 - loss: 1.2467 - regression_loss: 1.0499 - classification_loss: 0.1968
 823/1500 [===============>..............] - ETA: 8:54 - loss: 1.2467 - regression_loss: 1.0500 - classification_loss: 0.1967
 824/1500 [===============>..............] - ETA: 8:53 - loss: 1.2465 - regression_loss: 1.0499 - classification_loss: 0.1966
 825/1500 [===============>..............] - ETA: 8:52 - loss: 1.2455 - regression_loss: 1.0491 - classification_loss: 0.1964
 826/1500 [===============>..............] - ETA: 8:51 - loss: 1.2455 - regression_loss: 1.0491 - classification_loss: 0.1964
 827/1500 [===============>..............] - ETA: 8:51 - loss: 1.2451 - regression_loss: 1.0487 - classification_loss: 0.1964
 828/1500 [===============>..............] - ETA: 8:49 - loss: 1.2454 - regression_loss: 1.0486 - classification_loss: 0.1968
 829/1500 [===============>..............] - ETA: 8:49 - loss: 1.2451 - regression_loss: 1.0481 - classification_loss: 0.1970
 830/1500 [===============>..............] - ETA: 8:48 - loss: 1.2446 - regression_loss: 1.0477 - classification_loss: 0.1969
 831/1500 [===============>..............] - ETA: 8:47 - loss: 1.2440 - regression_loss: 1.0472 - classification_loss: 0.1968
 832/1500 [===============>..............] - ETA: 8:46 - loss: 1.2451 - regression_loss: 1.0482 - classification_loss: 0.1969
 833/1500 [===============>..............] - ETA: 8:45 - loss: 1.2457 - regression_loss: 1.0486 - classification_loss: 0.1970
 834/1500 [===============>..............] - ETA: 8:44 - loss: 1.2456 - regression_loss: 1.0487 - classification_loss: 0.1969
 835/1500 [===============>..............] - ETA: 8:43 - loss: 1.2459 - regression_loss: 1.0490 - classification_loss: 0.1968
 836/1500 [===============>..............] - ETA: 8:43 - loss: 1.2457 - regression_loss: 1.0488 - classification_loss: 0.1969
 837/1500 [===============>..............] - ETA: 8:42 - loss: 1.2459 - regression_loss: 1.0490 - classification_loss: 0.1968
 838/1500 [===============>..............] - ETA: 8:41 - loss: 1.2455 - regression_loss: 1.0488 - classification_loss: 0.1967
 839/1500 [===============>..............] - ETA: 8:40 - loss: 1.2454 - regression_loss: 1.0486 - classification_loss: 0.1968
 840/1500 [===============>..............] - ETA: 8:39 - loss: 1.2454 - regression_loss: 1.0487 - classification_loss: 0.1968
 841/1500 [===============>..............] - ETA: 8:38 - loss: 1.2444 - regression_loss: 1.0478 - classification_loss: 0.1966
 842/1500 [===============>..............] - ETA: 8:37 - loss: 1.2438 - regression_loss: 1.0472 - classification_loss: 0.1966
 843/1500 [===============>..............] - ETA: 8:36 - loss: 1.2435 - regression_loss: 1.0470 - classification_loss: 0.1965
 844/1500 [===============>..............] - ETA: 8:36 - loss: 1.2437 - regression_loss: 1.0473 - classification_loss: 0.1964
 845/1500 [===============>..............] - ETA: 8:35 - loss: 1.2432 - regression_loss: 1.0469 - classification_loss: 0.1963
 846/1500 [===============>..............] - ETA: 8:33 - loss: 1.2431 - regression_loss: 1.0468 - classification_loss: 0.1963
 847/1500 [===============>..............] - ETA: 8:34 - loss: 1.2428 - regression_loss: 1.0466 - classification_loss: 0.1962
 848/1500 [===============>..............] - ETA: 8:33 - loss: 1.2421 - regression_loss: 1.0460 - classification_loss: 0.1961
 849/1500 [===============>..............] - ETA: 8:33 - loss: 1.2414 - regression_loss: 1.0454 - classification_loss: 0.1960
 850/1500 [================>.............] - ETA: 8:32 - loss: 1.2408 - regression_loss: 1.0450 - classification_loss: 0.1958
 851/1500 [================>.............] - ETA: 8:31 - loss: 1.2411 - regression_loss: 1.0453 - classification_loss: 0.1958
 852/1500 [================>.............] - ETA: 8:30 - loss: 1.2426 - regression_loss: 1.0465 - classification_loss: 0.1961
 853/1500 [================>.............] - ETA: 8:29 - loss: 1.2437 - regression_loss: 1.0471 - classification_loss: 0.1966
 854/1500 [================>.............] - ETA: 8:28 - loss: 1.2440 - regression_loss: 1.0469 - classification_loss: 0.1970
 855/1500 [================>.............] - ETA: 8:27 - loss: 1.2435 - regression_loss: 1.0466 - classification_loss: 0.1969
 856/1500 [================>.............] - ETA: 8:27 - loss: 1.2435 - regression_loss: 1.0465 - classification_loss: 0.1970
 857/1500 [================>.............] - ETA: 8:26 - loss: 1.2431 - regression_loss: 1.0461 - classification_loss: 0.1970
 858/1500 [================>.............] - ETA: 8:25 - loss: 1.2431 - regression_loss: 1.0462 - classification_loss: 0.1968
 859/1500 [================>.............] - ETA: 8:24 - loss: 1.2440 - regression_loss: 1.0469 - classification_loss: 0.1971
 860/1500 [================>.............] - ETA: 8:24 - loss: 1.2431 - regression_loss: 1.0462 - classification_loss: 0.1970
 861/1500 [================>.............] - ETA: 8:23 - loss: 1.2427 - regression_loss: 1.0459 - classification_loss: 0.1968
 862/1500 [================>.............] - ETA: 8:21 - loss: 1.2432 - regression_loss: 1.0465 - classification_loss: 0.1967
 863/1500 [================>.............] - ETA: 8:20 - loss: 1.2425 - regression_loss: 1.0460 - classification_loss: 0.1965
 864/1500 [================>.............] - ETA: 8:19 - loss: 1.2417 - regression_loss: 1.0454 - classification_loss: 0.1963
 865/1500 [================>.............] - ETA: 8:18 - loss: 1.2420 - regression_loss: 1.0456 - classification_loss: 0.1964
 866/1500 [================>.............] - ETA: 8:17 - loss: 1.2418 - regression_loss: 1.0455 - classification_loss: 0.1963
 867/1500 [================>.............] - ETA: 8:16 - loss: 1.2412 - regression_loss: 1.0450 - classification_loss: 0.1962
 868/1500 [================>.............] - ETA: 8:15 - loss: 1.2403 - regression_loss: 1.0443 - classification_loss: 0.1961
 869/1500 [================>.............] - ETA: 8:14 - loss: 1.2398 - regression_loss: 1.0439 - classification_loss: 0.1959
 870/1500 [================>.............] - ETA: 8:14 - loss: 1.2405 - regression_loss: 1.0445 - classification_loss: 0.1960
 871/1500 [================>.............] - ETA: 8:13 - loss: 1.2408 - regression_loss: 1.0448 - classification_loss: 0.1960
 872/1500 [================>.............] - ETA: 8:12 - loss: 1.2415 - regression_loss: 1.0455 - classification_loss: 0.1960
 873/1500 [================>.............] - ETA: 8:12 - loss: 1.2411 - regression_loss: 1.0451 - classification_loss: 0.1959
 874/1500 [================>.............] - ETA: 8:12 - loss: 1.2408 - regression_loss: 1.0449 - classification_loss: 0.1959
 875/1500 [================>.............] - ETA: 8:11 - loss: 1.2419 - regression_loss: 1.0458 - classification_loss: 0.1961
 876/1500 [================>.............] - ETA: 8:10 - loss: 1.2418 - regression_loss: 1.0456 - classification_loss: 0.1962
 877/1500 [================>.............] - ETA: 8:10 - loss: 1.2421 - regression_loss: 1.0458 - classification_loss: 0.1963
 878/1500 [================>.............] - ETA: 8:09 - loss: 1.2422 - regression_loss: 1.0459 - classification_loss: 0.1963
 879/1500 [================>.............] - ETA: 8:09 - loss: 1.2444 - regression_loss: 1.0465 - classification_loss: 0.1980
 880/1500 [================>.............] - ETA: 8:08 - loss: 1.2445 - regression_loss: 1.0464 - classification_loss: 0.1980
 881/1500 [================>.............] - ETA: 8:08 - loss: 1.2446 - regression_loss: 1.0463 - classification_loss: 0.1983
 882/1500 [================>.............] - ETA: 8:08 - loss: 1.2442 - regression_loss: 1.0461 - classification_loss: 0.1981
 883/1500 [================>.............] - ETA: 8:07 - loss: 1.2448 - regression_loss: 1.0465 - classification_loss: 0.1983
 884/1500 [================>.............] - ETA: 8:06 - loss: 1.2451 - regression_loss: 1.0469 - classification_loss: 0.1983
 885/1500 [================>.............] - ETA: 8:05 - loss: 1.2450 - regression_loss: 1.0469 - classification_loss: 0.1982
 886/1500 [================>.............] - ETA: 8:04 - loss: 1.2448 - regression_loss: 1.0468 - classification_loss: 0.1981
 887/1500 [================>.............] - ETA: 8:03 - loss: 1.2457 - regression_loss: 1.0474 - classification_loss: 0.1983
 888/1500 [================>.............] - ETA: 8:02 - loss: 1.2460 - regression_loss: 1.0477 - classification_loss: 0.1983
 889/1500 [================>.............] - ETA: 8:02 - loss: 1.2461 - regression_loss: 1.0478 - classification_loss: 0.1983
 890/1500 [================>.............] - ETA: 8:01 - loss: 1.2461 - regression_loss: 1.0479 - classification_loss: 0.1983
 891/1500 [================>.............] - ETA: 8:00 - loss: 1.2463 - regression_loss: 1.0481 - classification_loss: 0.1982
 892/1500 [================>.............] - ETA: 7:59 - loss: 1.2474 - regression_loss: 1.0489 - classification_loss: 0.1984
 893/1500 [================>.............] - ETA: 7:58 - loss: 1.2470 - regression_loss: 1.0486 - classification_loss: 0.1984
 894/1500 [================>.............] - ETA: 7:59 - loss: 1.2471 - regression_loss: 1.0487 - classification_loss: 0.1985
 895/1500 [================>.............] - ETA: 7:59 - loss: 1.2464 - regression_loss: 1.0481 - classification_loss: 0.1983
 896/1500 [================>.............] - ETA: 7:57 - loss: 1.2456 - regression_loss: 1.0475 - classification_loss: 0.1981
 897/1500 [================>.............] - ETA: 7:56 - loss: 1.2458 - regression_loss: 1.0478 - classification_loss: 0.1981
 898/1500 [================>.............] - ETA: 7:56 - loss: 1.2457 - regression_loss: 1.0478 - classification_loss: 0.1980
 899/1500 [================>.............] - ETA: 7:55 - loss: 1.2461 - regression_loss: 1.0482 - classification_loss: 0.1980
 900/1500 [=================>............] - ETA: 7:55 - loss: 1.2454 - regression_loss: 1.0476 - classification_loss: 0.1978
 901/1500 [=================>............] - ETA: 7:53 - loss: 1.2458 - regression_loss: 1.0479 - classification_loss: 0.1978
 902/1500 [=================>............] - ETA: 7:52 - loss: 1.2454 - regression_loss: 1.0475 - classification_loss: 0.1979
 903/1500 [=================>............] - ETA: 7:52 - loss: 1.2453 - regression_loss: 1.0474 - classification_loss: 0.1979
 904/1500 [=================>............] - ETA: 7:51 - loss: 1.2450 - regression_loss: 1.0472 - classification_loss: 0.1977
 905/1500 [=================>............] - ETA: 7:50 - loss: 1.2451 - regression_loss: 1.0474 - classification_loss: 0.1977
 906/1500 [=================>............] - ETA: 7:49 - loss: 1.2446 - regression_loss: 1.0470 - classification_loss: 0.1976
 907/1500 [=================>............] - ETA: 7:48 - loss: 1.2439 - regression_loss: 1.0465 - classification_loss: 0.1974
 908/1500 [=================>............] - ETA: 7:47 - loss: 1.2449 - regression_loss: 1.0470 - classification_loss: 0.1979
 909/1500 [=================>............] - ETA: 7:46 - loss: 1.2456 - regression_loss: 1.0478 - classification_loss: 0.1978
 910/1500 [=================>............] - ETA: 7:46 - loss: 1.2449 - regression_loss: 1.0472 - classification_loss: 0.1977
 911/1500 [=================>............] - ETA: 7:45 - loss: 1.2441 - regression_loss: 1.0466 - classification_loss: 0.1976
 912/1500 [=================>............] - ETA: 7:44 - loss: 1.2441 - regression_loss: 1.0465 - classification_loss: 0.1977
 913/1500 [=================>............] - ETA: 7:43 - loss: 1.2440 - regression_loss: 1.0462 - classification_loss: 0.1977
 914/1500 [=================>............] - ETA: 7:42 - loss: 1.2435 - regression_loss: 1.0458 - classification_loss: 0.1976
 915/1500 [=================>............] - ETA: 7:41 - loss: 1.2437 - regression_loss: 1.0461 - classification_loss: 0.1977
 916/1500 [=================>............] - ETA: 7:41 - loss: 1.2433 - regression_loss: 1.0457 - classification_loss: 0.1976
 917/1500 [=================>............] - ETA: 7:40 - loss: 1.2436 - regression_loss: 1.0459 - classification_loss: 0.1976
 918/1500 [=================>............] - ETA: 7:39 - loss: 1.2438 - regression_loss: 1.0461 - classification_loss: 0.1977
 919/1500 [=================>............] - ETA: 7:39 - loss: 1.2435 - regression_loss: 1.0459 - classification_loss: 0.1976
 920/1500 [=================>............] - ETA: 7:38 - loss: 1.2434 - regression_loss: 1.0458 - classification_loss: 0.1976
 921/1500 [=================>............] - ETA: 7:37 - loss: 1.2434 - regression_loss: 1.0459 - classification_loss: 0.1975
 922/1500 [=================>............] - ETA: 7:35 - loss: 1.2434 - regression_loss: 1.0460 - classification_loss: 0.1975
 923/1500 [=================>............] - ETA: 7:35 - loss: 1.2451 - regression_loss: 1.0473 - classification_loss: 0.1978
 924/1500 [=================>............] - ETA: 7:34 - loss: 1.2448 - regression_loss: 1.0471 - classification_loss: 0.1978
 925/1500 [=================>............] - ETA: 7:33 - loss: 1.2443 - regression_loss: 1.0466 - classification_loss: 0.1977
 926/1500 [=================>............] - ETA: 7:32 - loss: 1.2441 - regression_loss: 1.0464 - classification_loss: 0.1976
 927/1500 [=================>............] - ETA: 7:32 - loss: 1.2447 - regression_loss: 1.0471 - classification_loss: 0.1976
 928/1500 [=================>............] - ETA: 7:31 - loss: 1.2446 - regression_loss: 1.0470 - classification_loss: 0.1975
 929/1500 [=================>............] - ETA: 7:30 - loss: 1.2439 - regression_loss: 1.0465 - classification_loss: 0.1974
 930/1500 [=================>............] - ETA: 7:29 - loss: 1.2432 - regression_loss: 1.0459 - classification_loss: 0.1973
 931/1500 [=================>............] - ETA: 7:28 - loss: 1.2427 - regression_loss: 1.0456 - classification_loss: 0.1972
 932/1500 [=================>............] - ETA: 7:27 - loss: 1.2422 - regression_loss: 1.0451 - classification_loss: 0.1970
 933/1500 [=================>............] - ETA: 7:26 - loss: 1.2414 - regression_loss: 1.0445 - classification_loss: 0.1969
 934/1500 [=================>............] - ETA: 7:25 - loss: 1.2412 - regression_loss: 1.0445 - classification_loss: 0.1967
 935/1500 [=================>............] - ETA: 7:25 - loss: 1.2414 - regression_loss: 1.0448 - classification_loss: 0.1966
 936/1500 [=================>............] - ETA: 7:24 - loss: 1.2423 - regression_loss: 1.0455 - classification_loss: 0.1968
 937/1500 [=================>............] - ETA: 7:23 - loss: 1.2426 - regression_loss: 1.0458 - classification_loss: 0.1968
 938/1500 [=================>............] - ETA: 7:23 - loss: 1.2435 - regression_loss: 1.0465 - classification_loss: 0.1970
 939/1500 [=================>............] - ETA: 7:22 - loss: 1.2432 - regression_loss: 1.0462 - classification_loss: 0.1970
 940/1500 [=================>............] - ETA: 7:21 - loss: 1.2429 - regression_loss: 1.0460 - classification_loss: 0.1969
 941/1500 [=================>............] - ETA: 7:20 - loss: 1.2443 - regression_loss: 1.0472 - classification_loss: 0.1971
 942/1500 [=================>............] - ETA: 7:19 - loss: 1.2444 - regression_loss: 1.0473 - classification_loss: 0.1971
 943/1500 [=================>............] - ETA: 7:19 - loss: 1.2452 - regression_loss: 1.0480 - classification_loss: 0.1972
 944/1500 [=================>............] - ETA: 7:18 - loss: 1.2446 - regression_loss: 1.0473 - classification_loss: 0.1973
 945/1500 [=================>............] - ETA: 7:18 - loss: 1.2460 - regression_loss: 1.0485 - classification_loss: 0.1975
 946/1500 [=================>............] - ETA: 7:17 - loss: 1.2462 - regression_loss: 1.0485 - classification_loss: 0.1977
 947/1500 [=================>............] - ETA: 7:17 - loss: 1.2471 - regression_loss: 1.0491 - classification_loss: 0.1981
 948/1500 [=================>............] - ETA: 7:16 - loss: 1.2463 - regression_loss: 1.0483 - classification_loss: 0.1980
 949/1500 [=================>............] - ETA: 7:15 - loss: 1.2465 - regression_loss: 1.0485 - classification_loss: 0.1980
 950/1500 [==================>...........] - ETA: 7:14 - loss: 1.2464 - regression_loss: 1.0485 - classification_loss: 0.1979
 951/1500 [==================>...........] - ETA: 7:13 - loss: 1.2473 - regression_loss: 1.0492 - classification_loss: 0.1980
 952/1500 [==================>...........] - ETA: 7:13 - loss: 1.2469 - regression_loss: 1.0490 - classification_loss: 0.1979
 953/1500 [==================>...........] - ETA: 7:13 - loss: 1.2467 - regression_loss: 1.0487 - classification_loss: 0.1979
 954/1500 [==================>...........] - ETA: 7:13 - loss: 1.2472 - regression_loss: 1.0491 - classification_loss: 0.1982
 955/1500 [==================>...........] - ETA: 7:12 - loss: 1.2486 - regression_loss: 1.0503 - classification_loss: 0.1983
 956/1500 [==================>...........] - ETA: 7:11 - loss: 1.2482 - regression_loss: 1.0501 - classification_loss: 0.1982
 957/1500 [==================>...........] - ETA: 7:10 - loss: 1.2495 - regression_loss: 1.0511 - classification_loss: 0.1985
 958/1500 [==================>...........] - ETA: 7:09 - loss: 1.2497 - regression_loss: 1.0513 - classification_loss: 0.1984
 959/1500 [==================>...........] - ETA: 7:09 - loss: 1.2492 - regression_loss: 1.0509 - classification_loss: 0.1983
 960/1500 [==================>...........] - ETA: 7:08 - loss: 1.2498 - regression_loss: 1.0516 - classification_loss: 0.1982
 961/1500 [==================>...........] - ETA: 7:07 - loss: 1.2493 - regression_loss: 1.0513 - classification_loss: 0.1980
 962/1500 [==================>...........] - ETA: 7:06 - loss: 1.2486 - regression_loss: 1.0507 - classification_loss: 0.1979
 963/1500 [==================>...........] - ETA: 7:05 - loss: 1.2482 - regression_loss: 1.0504 - classification_loss: 0.1978
 964/1500 [==================>...........] - ETA: 7:04 - loss: 1.2487 - regression_loss: 1.0508 - classification_loss: 0.1979
 965/1500 [==================>...........] - ETA: 7:03 - loss: 1.2483 - regression_loss: 1.0505 - classification_loss: 0.1978
 966/1500 [==================>...........] - ETA: 7:02 - loss: 1.2502 - regression_loss: 1.0519 - classification_loss: 0.1983
 967/1500 [==================>...........] - ETA: 7:01 - loss: 1.2504 - regression_loss: 1.0521 - classification_loss: 0.1983
 968/1500 [==================>...........] - ETA: 7:00 - loss: 1.2511 - regression_loss: 1.0527 - classification_loss: 0.1984
 969/1500 [==================>...........] - ETA: 6:59 - loss: 1.2508 - regression_loss: 1.0521 - classification_loss: 0.1987
 970/1500 [==================>...........] - ETA: 6:59 - loss: 1.2510 - regression_loss: 1.0522 - classification_loss: 0.1988
 971/1500 [==================>...........] - ETA: 6:58 - loss: 1.2504 - regression_loss: 1.0517 - classification_loss: 0.1987
 972/1500 [==================>...........] - ETA: 6:57 - loss: 1.2503 - regression_loss: 1.0516 - classification_loss: 0.1987
 973/1500 [==================>...........] - ETA: 6:56 - loss: 1.2514 - regression_loss: 1.0527 - classification_loss: 0.1988
 974/1500 [==================>...........] - ETA: 6:55 - loss: 1.2528 - regression_loss: 1.0538 - classification_loss: 0.1991
 975/1500 [==================>...........] - ETA: 6:54 - loss: 1.2523 - regression_loss: 1.0534 - classification_loss: 0.1989
 976/1500 [==================>...........] - ETA: 6:54 - loss: 1.2522 - regression_loss: 1.0533 - classification_loss: 0.1989
 977/1500 [==================>...........] - ETA: 6:53 - loss: 1.2519 - regression_loss: 1.0531 - classification_loss: 0.1988
 978/1500 [==================>...........] - ETA: 6:52 - loss: 1.2520 - regression_loss: 1.0531 - classification_loss: 0.1989
 979/1500 [==================>...........] - ETA: 6:51 - loss: 1.2533 - regression_loss: 1.0542 - classification_loss: 0.1991
 980/1500 [==================>...........] - ETA: 6:50 - loss: 1.2533 - regression_loss: 1.0542 - classification_loss: 0.1991
 981/1500 [==================>...........] - ETA: 6:50 - loss: 1.2534 - regression_loss: 1.0543 - classification_loss: 0.1991
 982/1500 [==================>...........] - ETA: 6:49 - loss: 1.2534 - regression_loss: 1.0541 - classification_loss: 0.1993
 983/1500 [==================>...........] - ETA: 6:48 - loss: 1.2528 - regression_loss: 1.0536 - classification_loss: 0.1992
 984/1500 [==================>...........] - ETA: 6:47 - loss: 1.2518 - regression_loss: 1.0528 - classification_loss: 0.1990
 985/1500 [==================>...........] - ETA: 6:47 - loss: 1.2521 - regression_loss: 1.0529 - classification_loss: 0.1992
 986/1500 [==================>...........] - ETA: 6:46 - loss: 1.2515 - regression_loss: 1.0524 - classification_loss: 0.1991
 987/1500 [==================>...........] - ETA: 6:45 - loss: 1.2507 - regression_loss: 1.0517 - classification_loss: 0.1990
 988/1500 [==================>...........] - ETA: 6:44 - loss: 1.2516 - regression_loss: 1.0525 - classification_loss: 0.1991
 989/1500 [==================>...........] - ETA: 6:43 - loss: 1.2515 - regression_loss: 1.0525 - classification_loss: 0.1990
 990/1500 [==================>...........] - ETA: 6:42 - loss: 1.2520 - regression_loss: 1.0526 - classification_loss: 0.1994
 991/1500 [==================>...........] - ETA: 6:41 - loss: 1.2515 - regression_loss: 1.0522 - classification_loss: 0.1992
 992/1500 [==================>...........] - ETA: 6:41 - loss: 1.2522 - regression_loss: 1.0528 - classification_loss: 0.1994
 993/1500 [==================>...........] - ETA: 6:40 - loss: 1.2515 - regression_loss: 1.0522 - classification_loss: 0.1993
 994/1500 [==================>...........] - ETA: 6:39 - loss: 1.2512 - regression_loss: 1.0519 - classification_loss: 0.1993
 995/1500 [==================>...........] - ETA: 6:38 - loss: 1.2506 - regression_loss: 1.0514 - classification_loss: 0.1992
 996/1500 [==================>...........] - ETA: 6:37 - loss: 1.2511 - regression_loss: 1.0517 - classification_loss: 0.1994
 997/1500 [==================>...........] - ETA: 6:36 - loss: 1.2510 - regression_loss: 1.0516 - classification_loss: 0.1994
 998/1500 [==================>...........] - ETA: 6:35 - loss: 1.2513 - regression_loss: 1.0518 - classification_loss: 0.1994
 999/1500 [==================>...........] - ETA: 6:34 - loss: 1.2507 - regression_loss: 1.0513 - classification_loss: 0.1994
1000/1500 [===================>..........] - ETA: 6:33 - loss: 1.2505 - regression_loss: 1.0512 - classification_loss: 0.1993
1001/1500 [===================>..........] - ETA: 6:33 - loss: 1.2499 - regression_loss: 1.0507 - classification_loss: 0.1992
1002/1500 [===================>..........] - ETA: 6:32 - loss: 1.2509 - regression_loss: 1.0515 - classification_loss: 0.1994
1003/1500 [===================>..........] - ETA: 6:32 - loss: 1.2510 - regression_loss: 1.0517 - classification_loss: 0.1994
1004/1500 [===================>..........] - ETA: 6:31 - loss: 1.2507 - regression_loss: 1.0515 - classification_loss: 0.1993
1005/1500 [===================>..........] - ETA: 6:30 - loss: 1.2516 - regression_loss: 1.0522 - classification_loss: 0.1994
1006/1500 [===================>..........] - ETA: 6:29 - loss: 1.2511 - regression_loss: 1.0518 - classification_loss: 0.1993
1007/1500 [===================>..........] - ETA: 6:29 - loss: 1.2507 - regression_loss: 1.0515 - classification_loss: 0.1992
1008/1500 [===================>..........] - ETA: 6:28 - loss: 1.2511 - regression_loss: 1.0519 - classification_loss: 0.1992
1009/1500 [===================>..........] - ETA: 6:27 - loss: 1.2506 - regression_loss: 1.0515 - classification_loss: 0.1991
1010/1500 [===================>..........] - ETA: 6:26 - loss: 1.2503 - regression_loss: 1.0513 - classification_loss: 0.1991
1011/1500 [===================>..........] - ETA: 6:25 - loss: 1.2503 - regression_loss: 1.0513 - classification_loss: 0.1990
1012/1500 [===================>..........] - ETA: 6:24 - loss: 1.2496 - regression_loss: 1.0508 - classification_loss: 0.1988
1013/1500 [===================>..........] - ETA: 6:23 - loss: 1.2506 - regression_loss: 1.0516 - classification_loss: 0.1990
1014/1500 [===================>..........] - ETA: 6:23 - loss: 1.2520 - regression_loss: 1.0522 - classification_loss: 0.1998
1015/1500 [===================>..........] - ETA: 6:22 - loss: 1.2516 - regression_loss: 1.0519 - classification_loss: 0.1998
1016/1500 [===================>..........] - ETA: 6:22 - loss: 1.2524 - regression_loss: 1.0525 - classification_loss: 0.1999
1017/1500 [===================>..........] - ETA: 6:21 - loss: 1.2521 - regression_loss: 1.0523 - classification_loss: 0.1998
1018/1500 [===================>..........] - ETA: 6:20 - loss: 1.2518 - regression_loss: 1.0521 - classification_loss: 0.1998
1019/1500 [===================>..........] - ETA: 6:19 - loss: 1.2519 - regression_loss: 1.0521 - classification_loss: 0.1998
1020/1500 [===================>..........] - ETA: 6:18 - loss: 1.2523 - regression_loss: 1.0525 - classification_loss: 0.1998
1021/1500 [===================>..........] - ETA: 6:17 - loss: 1.2518 - regression_loss: 1.0522 - classification_loss: 0.1997
1022/1500 [===================>..........] - ETA: 6:16 - loss: 1.2517 - regression_loss: 1.0521 - classification_loss: 0.1996
1023/1500 [===================>..........] - ETA: 6:15 - loss: 1.2517 - regression_loss: 1.0521 - classification_loss: 0.1996
1024/1500 [===================>..........] - ETA: 6:14 - loss: 1.2529 - regression_loss: 1.0530 - classification_loss: 0.2000
1025/1500 [===================>..........] - ETA: 6:14 - loss: 1.2522 - regression_loss: 1.0524 - classification_loss: 0.1998
1026/1500 [===================>..........] - ETA: 6:13 - loss: 1.2521 - regression_loss: 1.0523 - classification_loss: 0.1997
1027/1500 [===================>..........] - ETA: 6:12 - loss: 1.2514 - regression_loss: 1.0518 - classification_loss: 0.1996
1028/1500 [===================>..........] - ETA: 6:11 - loss: 1.2511 - regression_loss: 1.0517 - classification_loss: 0.1995
1029/1500 [===================>..........] - ETA: 6:10 - loss: 1.2508 - regression_loss: 1.0515 - classification_loss: 0.1993
1030/1500 [===================>..........] - ETA: 6:10 - loss: 1.2518 - regression_loss: 1.0522 - classification_loss: 0.1996
1031/1500 [===================>..........] - ETA: 6:09 - loss: 1.2515 - regression_loss: 1.0520 - classification_loss: 0.1995
1032/1500 [===================>..........] - ETA: 6:08 - loss: 1.2511 - regression_loss: 1.0516 - classification_loss: 0.1994
1033/1500 [===================>..........] - ETA: 6:07 - loss: 1.2508 - regression_loss: 1.0514 - classification_loss: 0.1994
1034/1500 [===================>..........] - ETA: 6:06 - loss: 1.2507 - regression_loss: 1.0513 - classification_loss: 0.1994
1035/1500 [===================>..........] - ETA: 6:06 - loss: 1.2507 - regression_loss: 1.0511 - classification_loss: 0.1997
1036/1500 [===================>..........] - ETA: 6:05 - loss: 1.2508 - regression_loss: 1.0511 - classification_loss: 0.1997
1037/1500 [===================>..........] - ETA: 6:04 - loss: 1.2503 - regression_loss: 1.0507 - classification_loss: 0.1996
1038/1500 [===================>..........] - ETA: 6:03 - loss: 1.2508 - regression_loss: 1.0509 - classification_loss: 0.1999
1039/1500 [===================>..........] - ETA: 6:02 - loss: 1.2507 - regression_loss: 1.0507 - classification_loss: 0.1999
1040/1500 [===================>..........] - ETA: 6:01 - loss: 1.2504 - regression_loss: 1.0505 - classification_loss: 0.1999
1041/1500 [===================>..........] - ETA: 6:00 - loss: 1.2506 - regression_loss: 1.0507 - classification_loss: 0.1999
1042/1500 [===================>..........] - ETA: 5:59 - loss: 1.2510 - regression_loss: 1.0509 - classification_loss: 0.2001
1043/1500 [===================>..........] - ETA: 5:58 - loss: 1.2518 - regression_loss: 1.0516 - classification_loss: 0.2002
1044/1500 [===================>..........] - ETA: 5:57 - loss: 1.2517 - regression_loss: 1.0516 - classification_loss: 0.2001
1045/1500 [===================>..........] - ETA: 5:57 - loss: 1.2511 - regression_loss: 1.0510 - classification_loss: 0.2000
1046/1500 [===================>..........] - ETA: 5:56 - loss: 1.2506 - regression_loss: 1.0508 - classification_loss: 0.1999
1047/1500 [===================>..........] - ETA: 5:55 - loss: 1.2504 - regression_loss: 1.0506 - classification_loss: 0.1998
1048/1500 [===================>..........] - ETA: 5:55 - loss: 1.2513 - regression_loss: 1.0514 - classification_loss: 0.1999
1049/1500 [===================>..........] - ETA: 5:54 - loss: 1.2509 - regression_loss: 1.0512 - classification_loss: 0.1997
1050/1500 [====================>.........] - ETA: 5:53 - loss: 1.2518 - regression_loss: 1.0518 - classification_loss: 0.2000
1051/1500 [====================>.........] - ETA: 5:53 - loss: 1.2512 - regression_loss: 1.0513 - classification_loss: 0.1999
1052/1500 [====================>.........] - ETA: 5:52 - loss: 1.2516 - regression_loss: 1.0513 - classification_loss: 0.2002
1053/1500 [====================>.........] - ETA: 5:51 - loss: 1.2520 - regression_loss: 1.0517 - classification_loss: 0.2003
1054/1500 [====================>.........] - ETA: 5:51 - loss: 1.2526 - regression_loss: 1.0523 - classification_loss: 0.2003
1055/1500 [====================>.........] - ETA: 5:50 - loss: 1.2538 - regression_loss: 1.0532 - classification_loss: 0.2006
1056/1500 [====================>.........] - ETA: 5:49 - loss: 1.2535 - regression_loss: 1.0530 - classification_loss: 0.2006
1057/1500 [====================>.........] - ETA: 5:48 - loss: 1.2540 - regression_loss: 1.0533 - classification_loss: 0.2007
1058/1500 [====================>.........] - ETA: 5:47 - loss: 1.2537 - regression_loss: 1.0531 - classification_loss: 0.2006
1059/1500 [====================>.........] - ETA: 5:46 - loss: 1.2534 - regression_loss: 1.0529 - classification_loss: 0.2005
1060/1500 [====================>.........] - ETA: 5:46 - loss: 1.2528 - regression_loss: 1.0524 - classification_loss: 0.2004
1061/1500 [====================>.........] - ETA: 5:45 - loss: 1.2521 - regression_loss: 1.0519 - classification_loss: 0.2003
1062/1500 [====================>.........] - ETA: 5:44 - loss: 1.2521 - regression_loss: 1.0518 - classification_loss: 0.2003
1063/1500 [====================>.........] - ETA: 5:43 - loss: 1.2516 - regression_loss: 1.0514 - classification_loss: 0.2002
1064/1500 [====================>.........] - ETA: 5:42 - loss: 1.2521 - regression_loss: 1.0519 - classification_loss: 0.2002
1065/1500 [====================>.........] - ETA: 5:42 - loss: 1.2526 - regression_loss: 1.0523 - classification_loss: 0.2003
1066/1500 [====================>.........] - ETA: 5:41 - loss: 1.2528 - regression_loss: 1.0526 - classification_loss: 0.2003
1067/1500 [====================>.........] - ETA: 5:40 - loss: 1.2521 - regression_loss: 1.0520 - classification_loss: 0.2001
1068/1500 [====================>.........] - ETA: 5:39 - loss: 1.2525 - regression_loss: 1.0524 - classification_loss: 0.2001
1069/1500 [====================>.........] - ETA: 5:39 - loss: 1.2528 - regression_loss: 1.0528 - classification_loss: 0.2000
1070/1500 [====================>.........] - ETA: 5:38 - loss: 1.2527 - regression_loss: 1.0528 - classification_loss: 0.1999
1071/1500 [====================>.........] - ETA: 5:37 - loss: 1.2526 - regression_loss: 1.0527 - classification_loss: 0.1998
1072/1500 [====================>.........] - ETA: 5:36 - loss: 1.2522 - regression_loss: 1.0524 - classification_loss: 0.1998
1073/1500 [====================>.........] - ETA: 5:36 - loss: 1.2517 - regression_loss: 1.0520 - classification_loss: 0.1997
1074/1500 [====================>.........] - ETA: 5:35 - loss: 1.2516 - regression_loss: 1.0520 - classification_loss: 0.1996
1075/1500 [====================>.........] - ETA: 5:34 - loss: 1.2510 - regression_loss: 1.0515 - classification_loss: 0.1995
1076/1500 [====================>.........] - ETA: 5:33 - loss: 1.2503 - regression_loss: 1.0509 - classification_loss: 0.1994
1077/1500 [====================>.........] - ETA: 5:32 - loss: 1.2508 - regression_loss: 1.0514 - classification_loss: 0.1994
1078/1500 [====================>.........] - ETA: 5:31 - loss: 1.2508 - regression_loss: 1.0514 - classification_loss: 0.1994
1079/1500 [====================>.........] - ETA: 5:30 - loss: 1.2506 - regression_loss: 1.0513 - classification_loss: 0.1993
1080/1500 [====================>.........] - ETA: 5:30 - loss: 1.2503 - regression_loss: 1.0510 - classification_loss: 0.1993
1081/1500 [====================>.........] - ETA: 5:29 - loss: 1.2504 - regression_loss: 1.0512 - classification_loss: 0.1992
1082/1500 [====================>.........] - ETA: 5:28 - loss: 1.2504 - regression_loss: 1.0512 - classification_loss: 0.1992
1083/1500 [====================>.........] - ETA: 5:27 - loss: 1.2499 - regression_loss: 1.0508 - classification_loss: 0.1991
1084/1500 [====================>.........] - ETA: 5:26 - loss: 1.2493 - regression_loss: 1.0503 - classification_loss: 0.1990
1085/1500 [====================>.........] - ETA: 5:26 - loss: 1.2489 - regression_loss: 1.0499 - classification_loss: 0.1990
1086/1500 [====================>.........] - ETA: 5:25 - loss: 1.2491 - regression_loss: 1.0502 - classification_loss: 0.1989
1087/1500 [====================>.........] - ETA: 5:24 - loss: 1.2493 - regression_loss: 1.0504 - classification_loss: 0.1989
1088/1500 [====================>.........] - ETA: 5:23 - loss: 1.2492 - regression_loss: 1.0504 - classification_loss: 0.1988
1089/1500 [====================>.........] - ETA: 5:22 - loss: 1.2499 - regression_loss: 1.0504 - classification_loss: 0.1995
1090/1500 [====================>.........] - ETA: 5:21 - loss: 1.2498 - regression_loss: 1.0503 - classification_loss: 0.1995
1091/1500 [====================>.........] - ETA: 5:20 - loss: 1.2494 - regression_loss: 1.0500 - classification_loss: 0.1994
1092/1500 [====================>.........] - ETA: 5:20 - loss: 1.2487 - regression_loss: 1.0495 - classification_loss: 0.1993
1093/1500 [====================>.........] - ETA: 5:19 - loss: 1.2489 - regression_loss: 1.0497 - classification_loss: 0.1992
1094/1500 [====================>.........] - ETA: 5:19 - loss: 1.2485 - regression_loss: 1.0493 - classification_loss: 0.1991
1095/1500 [====================>.........] - ETA: 5:18 - loss: 1.2488 - regression_loss: 1.0496 - classification_loss: 0.1992
1096/1500 [====================>.........] - ETA: 5:17 - loss: 1.2484 - regression_loss: 1.0492 - classification_loss: 0.1992
1097/1500 [====================>.........] - ETA: 5:16 - loss: 1.2482 - regression_loss: 1.0491 - classification_loss: 0.1991
1098/1500 [====================>.........] - ETA: 5:15 - loss: 1.2477 - regression_loss: 1.0487 - classification_loss: 0.1990
1099/1500 [====================>.........] - ETA: 5:14 - loss: 1.2477 - regression_loss: 1.0487 - classification_loss: 0.1990
1100/1500 [=====================>........] - ETA: 5:13 - loss: 1.2489 - regression_loss: 1.0497 - classification_loss: 0.1992
1101/1500 [=====================>........] - ETA: 5:12 - loss: 1.2491 - regression_loss: 1.0498 - classification_loss: 0.1993
1102/1500 [=====================>........] - ETA: 5:12 - loss: 1.2489 - regression_loss: 1.0497 - classification_loss: 0.1992
1103/1500 [=====================>........] - ETA: 5:11 - loss: 1.2485 - regression_loss: 1.0494 - classification_loss: 0.1991
1104/1500 [=====================>........] - ETA: 5:10 - loss: 1.2480 - regression_loss: 1.0490 - classification_loss: 0.1990
1105/1500 [=====================>........] - ETA: 5:09 - loss: 1.2476 - regression_loss: 1.0487 - classification_loss: 0.1989
1106/1500 [=====================>........] - ETA: 5:08 - loss: 1.2478 - regression_loss: 1.0489 - classification_loss: 0.1989
1107/1500 [=====================>........] - ETA: 5:07 - loss: 1.2475 - regression_loss: 1.0487 - classification_loss: 0.1988
1108/1500 [=====================>........] - ETA: 5:07 - loss: 1.2472 - regression_loss: 1.0485 - classification_loss: 0.1988
1109/1500 [=====================>........] - ETA: 5:06 - loss: 1.2476 - regression_loss: 1.0488 - classification_loss: 0.1988
1110/1500 [=====================>........] - ETA: 5:05 - loss: 1.2472 - regression_loss: 1.0485 - classification_loss: 0.1987
1111/1500 [=====================>........] - ETA: 5:04 - loss: 1.2470 - regression_loss: 1.0482 - classification_loss: 0.1987
1112/1500 [=====================>........] - ETA: 5:03 - loss: 1.2464 - regression_loss: 1.0478 - classification_loss: 0.1986
1113/1500 [=====================>........] - ETA: 5:02 - loss: 1.2461 - regression_loss: 1.0476 - classification_loss: 0.1985
1114/1500 [=====================>........] - ETA: 5:01 - loss: 1.2465 - regression_loss: 1.0480 - classification_loss: 0.1985
1115/1500 [=====================>........] - ETA: 5:01 - loss: 1.2463 - regression_loss: 1.0477 - classification_loss: 0.1986
1116/1500 [=====================>........] - ETA: 5:00 - loss: 1.2457 - regression_loss: 1.0473 - classification_loss: 0.1984
1117/1500 [=====================>........] - ETA: 4:59 - loss: 1.2465 - regression_loss: 1.0479 - classification_loss: 0.1986
1118/1500 [=====================>........] - ETA: 4:59 - loss: 1.2464 - regression_loss: 1.0478 - classification_loss: 0.1986
1119/1500 [=====================>........] - ETA: 4:58 - loss: 1.2461 - regression_loss: 1.0475 - classification_loss: 0.1985
1120/1500 [=====================>........] - ETA: 4:58 - loss: 1.2453 - regression_loss: 1.0468 - classification_loss: 0.1984
1121/1500 [=====================>........] - ETA: 4:57 - loss: 1.2455 - regression_loss: 1.0471 - classification_loss: 0.1984
1122/1500 [=====================>........] - ETA: 4:56 - loss: 1.2459 - regression_loss: 1.0474 - classification_loss: 0.1985
1123/1500 [=====================>........] - ETA: 4:55 - loss: 1.2466 - regression_loss: 1.0477 - classification_loss: 0.1988
1124/1500 [=====================>........] - ETA: 4:54 - loss: 1.2460 - regression_loss: 1.0473 - classification_loss: 0.1988
1125/1500 [=====================>........] - ETA: 4:54 - loss: 1.2457 - regression_loss: 1.0470 - classification_loss: 0.1987
1126/1500 [=====================>........] - ETA: 4:53 - loss: 1.2455 - regression_loss: 1.0466 - classification_loss: 0.1990
1127/1500 [=====================>........] - ETA: 4:52 - loss: 1.2456 - regression_loss: 1.0467 - classification_loss: 0.1989
1128/1500 [=====================>........] - ETA: 4:51 - loss: 1.2452 - regression_loss: 1.0464 - classification_loss: 0.1988
1129/1500 [=====================>........] - ETA: 4:50 - loss: 1.2448 - regression_loss: 1.0462 - classification_loss: 0.1986
1130/1500 [=====================>........] - ETA: 4:49 - loss: 1.2449 - regression_loss: 1.0463 - classification_loss: 0.1986
1131/1500 [=====================>........] - ETA: 4:49 - loss: 1.2453 - regression_loss: 1.0466 - classification_loss: 0.1987
1132/1500 [=====================>........] - ETA: 4:48 - loss: 1.2457 - regression_loss: 1.0469 - classification_loss: 0.1988
1133/1500 [=====================>........] - ETA: 4:48 - loss: 1.2456 - regression_loss: 1.0468 - classification_loss: 0.1987
1134/1500 [=====================>........] - ETA: 4:47 - loss: 1.2455 - regression_loss: 1.0467 - classification_loss: 0.1988
1135/1500 [=====================>........] - ETA: 4:46 - loss: 1.2454 - regression_loss: 1.0467 - classification_loss: 0.1987
1136/1500 [=====================>........] - ETA: 4:46 - loss: 1.2451 - regression_loss: 1.0465 - classification_loss: 0.1987
1137/1500 [=====================>........] - ETA: 4:45 - loss: 1.2451 - regression_loss: 1.0465 - classification_loss: 0.1986
1138/1500 [=====================>........] - ETA: 4:44 - loss: 1.2449 - regression_loss: 1.0464 - classification_loss: 0.1985
1139/1500 [=====================>........] - ETA: 4:43 - loss: 1.2444 - regression_loss: 1.0460 - classification_loss: 0.1984
1140/1500 [=====================>........] - ETA: 4:42 - loss: 1.2447 - regression_loss: 1.0462 - classification_loss: 0.1985
1141/1500 [=====================>........] - ETA: 4:41 - loss: 1.2455 - regression_loss: 1.0469 - classification_loss: 0.1986
1142/1500 [=====================>........] - ETA: 4:41 - loss: 1.2462 - regression_loss: 1.0475 - classification_loss: 0.1987
1143/1500 [=====================>........] - ETA: 4:40 - loss: 1.2469 - regression_loss: 1.0481 - classification_loss: 0.1988
1144/1500 [=====================>........] - ETA: 4:39 - loss: 1.2469 - regression_loss: 1.0481 - classification_loss: 0.1988
1145/1500 [=====================>........] - ETA: 4:38 - loss: 1.2469 - regression_loss: 1.0481 - classification_loss: 0.1988
1146/1500 [=====================>........] - ETA: 4:37 - loss: 1.2465 - regression_loss: 1.0479 - classification_loss: 0.1986
1147/1500 [=====================>........] - ETA: 4:36 - loss: 1.2459 - regression_loss: 1.0474 - classification_loss: 0.1985
1148/1500 [=====================>........] - ETA: 4:36 - loss: 1.2463 - regression_loss: 1.0477 - classification_loss: 0.1986
1149/1500 [=====================>........] - ETA: 4:35 - loss: 1.2462 - regression_loss: 1.0477 - classification_loss: 0.1985
1150/1500 [======================>.......] - ETA: 4:34 - loss: 1.2459 - regression_loss: 1.0476 - classification_loss: 0.1983
1151/1500 [======================>.......] - ETA: 4:33 - loss: 1.2458 - regression_loss: 1.0475 - classification_loss: 0.1983
1152/1500 [======================>.......] - ETA: 4:32 - loss: 1.2454 - regression_loss: 1.0470 - classification_loss: 0.1983
1153/1500 [======================>.......] - ETA: 4:31 - loss: 1.2451 - regression_loss: 1.0467 - classification_loss: 0.1983
1154/1500 [======================>.......] - ETA: 4:31 - loss: 1.2444 - regression_loss: 1.0462 - classification_loss: 0.1982
1155/1500 [======================>.......] - ETA: 4:30 - loss: 1.2454 - regression_loss: 1.0472 - classification_loss: 0.1982
1156/1500 [======================>.......] - ETA: 4:29 - loss: 1.2466 - regression_loss: 1.0474 - classification_loss: 0.1991
1157/1500 [======================>.......] - ETA: 4:29 - loss: 1.2472 - regression_loss: 1.0479 - classification_loss: 0.1993
1158/1500 [======================>.......] - ETA: 4:28 - loss: 1.2473 - regression_loss: 1.0480 - classification_loss: 0.1993
1159/1500 [======================>.......] - ETA: 4:27 - loss: 1.2474 - regression_loss: 1.0481 - classification_loss: 0.1992
1160/1500 [======================>.......] - ETA: 4:26 - loss: 1.2477 - regression_loss: 1.0485 - classification_loss: 0.1993
1161/1500 [======================>.......] - ETA: 4:25 - loss: 1.2475 - regression_loss: 1.0484 - classification_loss: 0.1991
1162/1500 [======================>.......] - ETA: 4:25 - loss: 1.2472 - regression_loss: 1.0481 - classification_loss: 0.1991
1163/1500 [======================>.......] - ETA: 4:24 - loss: 1.2475 - regression_loss: 1.0482 - classification_loss: 0.1992
1164/1500 [======================>.......] - ETA: 4:23 - loss: 1.2469 - regression_loss: 1.0477 - classification_loss: 0.1991
1165/1500 [======================>.......] - ETA: 4:22 - loss: 1.2470 - regression_loss: 1.0478 - classification_loss: 0.1992
1166/1500 [======================>.......] - ETA: 4:21 - loss: 1.2470 - regression_loss: 1.0477 - classification_loss: 0.1992
1167/1500 [======================>.......] - ETA: 4:21 - loss: 1.2463 - regression_loss: 1.0471 - classification_loss: 0.1991
1168/1500 [======================>.......] - ETA: 4:20 - loss: 1.2462 - regression_loss: 1.0471 - classification_loss: 0.1991
1169/1500 [======================>.......] - ETA: 4:19 - loss: 1.2462 - regression_loss: 1.0469 - classification_loss: 0.1992
1170/1500 [======================>.......] - ETA: 4:18 - loss: 1.2463 - regression_loss: 1.0470 - classification_loss: 0.1993
1171/1500 [======================>.......] - ETA: 4:17 - loss: 1.2456 - regression_loss: 1.0464 - classification_loss: 0.1992
1172/1500 [======================>.......] - ETA: 4:16 - loss: 1.2452 - regression_loss: 1.0461 - classification_loss: 0.1991
1173/1500 [======================>.......] - ETA: 4:16 - loss: 1.2453 - regression_loss: 1.0462 - classification_loss: 0.1991
1174/1500 [======================>.......] - ETA: 4:15 - loss: 1.2451 - regression_loss: 1.0461 - classification_loss: 0.1990
1175/1500 [======================>.......] - ETA: 4:14 - loss: 1.2447 - regression_loss: 1.0458 - classification_loss: 0.1989
1176/1500 [======================>.......] - ETA: 4:13 - loss: 1.2444 - regression_loss: 1.0456 - classification_loss: 0.1988
1177/1500 [======================>.......] - ETA: 4:12 - loss: 1.2442 - regression_loss: 1.0455 - classification_loss: 0.1987
1178/1500 [======================>.......] - ETA: 4:11 - loss: 1.2440 - regression_loss: 1.0453 - classification_loss: 0.1987
1179/1500 [======================>.......] - ETA: 4:11 - loss: 1.2450 - regression_loss: 1.0461 - classification_loss: 0.1989
1180/1500 [======================>.......] - ETA: 4:10 - loss: 1.2445 - regression_loss: 1.0456 - classification_loss: 0.1988
1181/1500 [======================>.......] - ETA: 4:09 - loss: 1.2442 - regression_loss: 1.0453 - classification_loss: 0.1989
1182/1500 [======================>.......] - ETA: 4:08 - loss: 1.2455 - regression_loss: 1.0464 - classification_loss: 0.1991
1183/1500 [======================>.......] - ETA: 4:07 - loss: 1.2455 - regression_loss: 1.0464 - classification_loss: 0.1991
1184/1500 [======================>.......] - ETA: 4:06 - loss: 1.2456 - regression_loss: 1.0466 - classification_loss: 0.1990
1185/1500 [======================>.......] - ETA: 4:06 - loss: 1.2459 - regression_loss: 1.0467 - classification_loss: 0.1992
1186/1500 [======================>.......] - ETA: 4:05 - loss: 1.2463 - regression_loss: 1.0470 - classification_loss: 0.1993
1187/1500 [======================>.......] - ETA: 4:04 - loss: 1.2462 - regression_loss: 1.0469 - classification_loss: 0.1993
1188/1500 [======================>.......] - ETA: 4:03 - loss: 1.2463 - regression_loss: 1.0471 - classification_loss: 0.1992
1189/1500 [======================>.......] - ETA: 4:02 - loss: 1.2462 - regression_loss: 1.0470 - classification_loss: 0.1992
1190/1500 [======================>.......] - ETA: 4:02 - loss: 1.2466 - regression_loss: 1.0474 - classification_loss: 0.1992
1191/1500 [======================>.......] - ETA: 4:01 - loss: 1.2469 - regression_loss: 1.0476 - classification_loss: 0.1993
1192/1500 [======================>.......] - ETA: 4:00 - loss: 1.2475 - regression_loss: 1.0481 - classification_loss: 0.1994
1193/1500 [======================>.......] - ETA: 3:59 - loss: 1.2469 - regression_loss: 1.0476 - classification_loss: 0.1992
1194/1500 [======================>.......] - ETA: 3:58 - loss: 1.2477 - regression_loss: 1.0484 - classification_loss: 0.1993
1195/1500 [======================>.......] - ETA: 3:58 - loss: 1.2475 - regression_loss: 1.0483 - classification_loss: 0.1992
1196/1500 [======================>.......] - ETA: 3:57 - loss: 1.2477 - regression_loss: 1.0485 - classification_loss: 0.1993
1197/1500 [======================>.......] - ETA: 3:56 - loss: 1.2480 - regression_loss: 1.0487 - classification_loss: 0.1992
1198/1500 [======================>.......] - ETA: 3:55 - loss: 1.2472 - regression_loss: 1.0481 - classification_loss: 0.1991
1199/1500 [======================>.......] - ETA: 3:54 - loss: 1.2467 - regression_loss: 1.0477 - classification_loss: 0.1990
1200/1500 [=======================>......] - ETA: 3:54 - loss: 1.2470 - regression_loss: 1.0480 - classification_loss: 0.1990
1201/1500 [=======================>......] - ETA: 3:53 - loss: 1.2477 - regression_loss: 1.0486 - classification_loss: 0.1991
1202/1500 [=======================>......] - ETA: 3:52 - loss: 1.2485 - regression_loss: 1.0483 - classification_loss: 0.2001
1203/1500 [=======================>......] - ETA: 3:51 - loss: 1.2483 - regression_loss: 1.0482 - classification_loss: 0.2000
1204/1500 [=======================>......] - ETA: 3:50 - loss: 1.2480 - regression_loss: 1.0481 - classification_loss: 0.1999
1205/1500 [=======================>......] - ETA: 3:50 - loss: 1.2479 - regression_loss: 1.0479 - classification_loss: 0.2000
1206/1500 [=======================>......] - ETA: 3:49 - loss: 1.2475 - regression_loss: 1.0476 - classification_loss: 0.1999
1207/1500 [=======================>......] - ETA: 3:48 - loss: 1.2470 - regression_loss: 1.0472 - classification_loss: 0.1997
1208/1500 [=======================>......] - ETA: 3:47 - loss: 1.2472 - regression_loss: 1.0475 - classification_loss: 0.1998
1209/1500 [=======================>......] - ETA: 3:46 - loss: 1.2472 - regression_loss: 1.0474 - classification_loss: 0.1998
1210/1500 [=======================>......] - ETA: 3:46 - loss: 1.2476 - regression_loss: 1.0475 - classification_loss: 0.2000
1211/1500 [=======================>......] - ETA: 3:45 - loss: 1.2473 - regression_loss: 1.0474 - classification_loss: 0.2000
1212/1500 [=======================>......] - ETA: 3:44 - loss: 1.2475 - regression_loss: 1.0475 - classification_loss: 0.1999
1213/1500 [=======================>......] - ETA: 3:43 - loss: 1.2480 - regression_loss: 1.0481 - classification_loss: 0.1999
1214/1500 [=======================>......] - ETA: 3:43 - loss: 1.2475 - regression_loss: 1.0477 - classification_loss: 0.1998
1215/1500 [=======================>......] - ETA: 3:42 - loss: 1.2475 - regression_loss: 1.0477 - classification_loss: 0.1998
1216/1500 [=======================>......] - ETA: 3:41 - loss: 1.2477 - regression_loss: 1.0479 - classification_loss: 0.1998
1217/1500 [=======================>......] - ETA: 3:41 - loss: 1.2477 - regression_loss: 1.0480 - classification_loss: 0.1998
1218/1500 [=======================>......] - ETA: 3:40 - loss: 1.2477 - regression_loss: 1.0480 - classification_loss: 0.1997
1219/1500 [=======================>......] - ETA: 3:39 - loss: 1.2472 - regression_loss: 1.0477 - classification_loss: 0.1995
1220/1500 [=======================>......] - ETA: 3:38 - loss: 1.2476 - regression_loss: 1.0480 - classification_loss: 0.1997
1221/1500 [=======================>......] - ETA: 3:37 - loss: 1.2483 - regression_loss: 1.0484 - classification_loss: 0.1999
1222/1500 [=======================>......] - ETA: 3:37 - loss: 1.2477 - regression_loss: 1.0479 - classification_loss: 0.1998
1223/1500 [=======================>......] - ETA: 3:36 - loss: 1.2481 - regression_loss: 1.0481 - classification_loss: 0.1999
1224/1500 [=======================>......] - ETA: 3:35 - loss: 1.2481 - regression_loss: 1.0482 - classification_loss: 0.2000
1225/1500 [=======================>......] - ETA: 3:34 - loss: 1.2481 - regression_loss: 1.0482 - classification_loss: 0.1999
1226/1500 [=======================>......] - ETA: 3:33 - loss: 1.2478 - regression_loss: 1.0480 - classification_loss: 0.1998
1227/1500 [=======================>......] - ETA: 3:33 - loss: 1.2478 - regression_loss: 1.0479 - classification_loss: 0.1998
1228/1500 [=======================>......] - ETA: 3:32 - loss: 1.2476 - regression_loss: 1.0478 - classification_loss: 0.1998
1229/1500 [=======================>......] - ETA: 3:31 - loss: 1.2476 - regression_loss: 1.0477 - classification_loss: 0.1998
1230/1500 [=======================>......] - ETA: 3:30 - loss: 1.2476 - regression_loss: 1.0478 - classification_loss: 0.1997
1231/1500 [=======================>......] - ETA: 3:29 - loss: 1.2469 - regression_loss: 1.0473 - classification_loss: 0.1996
1232/1500 [=======================>......] - ETA: 3:29 - loss: 1.2481 - regression_loss: 1.0483 - classification_loss: 0.1998
1233/1500 [=======================>......] - ETA: 3:28 - loss: 1.2476 - regression_loss: 1.0479 - classification_loss: 0.1997
1234/1500 [=======================>......] - ETA: 3:27 - loss: 1.2474 - regression_loss: 1.0477 - classification_loss: 0.1997
1235/1500 [=======================>......] - ETA: 3:26 - loss: 1.2471 - regression_loss: 1.0474 - classification_loss: 0.1997
1236/1500 [=======================>......] - ETA: 3:26 - loss: 1.2468 - regression_loss: 1.0470 - classification_loss: 0.1998
1237/1500 [=======================>......] - ETA: 3:25 - loss: 1.2465 - regression_loss: 1.0467 - classification_loss: 0.1997
1238/1500 [=======================>......] - ETA: 3:24 - loss: 1.2468 - regression_loss: 1.0470 - classification_loss: 0.1998
1239/1500 [=======================>......] - ETA: 3:23 - loss: 1.2465 - regression_loss: 1.0468 - classification_loss: 0.1997
1240/1500 [=======================>......] - ETA: 3:22 - loss: 1.2463 - regression_loss: 1.0466 - classification_loss: 0.1996
1241/1500 [=======================>......] - ETA: 3:21 - loss: 1.2456 - regression_loss: 1.0461 - classification_loss: 0.1995
1242/1500 [=======================>......] - ETA: 3:21 - loss: 1.2451 - regression_loss: 1.0457 - classification_loss: 0.1994
1243/1500 [=======================>......] - ETA: 3:20 - loss: 1.2449 - regression_loss: 1.0456 - classification_loss: 0.1993
1244/1500 [=======================>......] - ETA: 3:19 - loss: 1.2444 - regression_loss: 1.0452 - classification_loss: 0.1992
1245/1500 [=======================>......] - ETA: 3:18 - loss: 1.2439 - regression_loss: 1.0448 - classification_loss: 0.1991
1246/1500 [=======================>......] - ETA: 3:17 - loss: 1.2434 - regression_loss: 1.0444 - classification_loss: 0.1990
1247/1500 [=======================>......] - ETA: 3:17 - loss: 1.2432 - regression_loss: 1.0443 - classification_loss: 0.1989
1248/1500 [=======================>......] - ETA: 3:16 - loss: 1.2425 - regression_loss: 1.0438 - classification_loss: 0.1988
1249/1500 [=======================>......] - ETA: 3:15 - loss: 1.2422 - regression_loss: 1.0435 - classification_loss: 0.1987
1250/1500 [========================>.....] - ETA: 3:14 - loss: 1.2429 - regression_loss: 1.0442 - classification_loss: 0.1987
1251/1500 [========================>.....] - ETA: 3:14 - loss: 1.2432 - regression_loss: 1.0445 - classification_loss: 0.1988
1252/1500 [========================>.....] - ETA: 3:13 - loss: 1.2427 - regression_loss: 1.0440 - classification_loss: 0.1986
1253/1500 [========================>.....] - ETA: 3:12 - loss: 1.2428 - regression_loss: 1.0441 - classification_loss: 0.1987
1254/1500 [========================>.....] - ETA: 3:11 - loss: 1.2422 - regression_loss: 1.0436 - classification_loss: 0.1986
1255/1500 [========================>.....] - ETA: 3:10 - loss: 1.2419 - regression_loss: 1.0434 - classification_loss: 0.1985
1256/1500 [========================>.....] - ETA: 3:09 - loss: 1.2416 - regression_loss: 1.0431 - classification_loss: 0.1985
1257/1500 [========================>.....] - ETA: 3:09 - loss: 1.2411 - regression_loss: 1.0427 - classification_loss: 0.1984
1258/1500 [========================>.....] - ETA: 3:08 - loss: 1.2408 - regression_loss: 1.0425 - classification_loss: 0.1984
1259/1500 [========================>.....] - ETA: 3:07 - loss: 1.2412 - regression_loss: 1.0428 - classification_loss: 0.1984
1260/1500 [========================>.....] - ETA: 3:06 - loss: 1.2417 - regression_loss: 1.0432 - classification_loss: 0.1985
1261/1500 [========================>.....] - ETA: 3:05 - loss: 1.2416 - regression_loss: 1.0432 - classification_loss: 0.1984
1262/1500 [========================>.....] - ETA: 3:05 - loss: 1.2417 - regression_loss: 1.0433 - classification_loss: 0.1984
1263/1500 [========================>.....] - ETA: 3:04 - loss: 1.2413 - regression_loss: 1.0430 - classification_loss: 0.1983
1264/1500 [========================>.....] - ETA: 3:03 - loss: 1.2414 - regression_loss: 1.0430 - classification_loss: 0.1984
1265/1500 [========================>.....] - ETA: 3:02 - loss: 1.2417 - regression_loss: 1.0433 - classification_loss: 0.1984
1266/1500 [========================>.....] - ETA: 3:01 - loss: 1.2413 - regression_loss: 1.0430 - classification_loss: 0.1983
1267/1500 [========================>.....] - ETA: 3:00 - loss: 1.2416 - regression_loss: 1.0433 - classification_loss: 0.1983
1268/1500 [========================>.....] - ETA: 2:59 - loss: 1.2412 - regression_loss: 1.0430 - classification_loss: 0.1982
1269/1500 [========================>.....] - ETA: 2:59 - loss: 1.2412 - regression_loss: 1.0430 - classification_loss: 0.1982
1270/1500 [========================>.....] - ETA: 2:58 - loss: 1.2407 - regression_loss: 1.0426 - classification_loss: 0.1981
1271/1500 [========================>.....] - ETA: 2:57 - loss: 1.2403 - regression_loss: 1.0423 - classification_loss: 0.1981
1272/1500 [========================>.....] - ETA: 2:56 - loss: 1.2403 - regression_loss: 1.0423 - classification_loss: 0.1980
1273/1500 [========================>.....] - ETA: 2:56 - loss: 1.2403 - regression_loss: 1.0423 - classification_loss: 0.1980
1274/1500 [========================>.....] - ETA: 2:55 - loss: 1.2405 - regression_loss: 1.0425 - classification_loss: 0.1980
1275/1500 [========================>.....] - ETA: 2:54 - loss: 1.2398 - regression_loss: 1.0420 - classification_loss: 0.1979
1276/1500 [========================>.....] - ETA: 2:54 - loss: 1.2393 - regression_loss: 1.0415 - classification_loss: 0.1978
1277/1500 [========================>.....] - ETA: 2:53 - loss: 1.2393 - regression_loss: 1.0415 - classification_loss: 0.1978
1278/1500 [========================>.....] - ETA: 2:52 - loss: 1.2393 - regression_loss: 1.0415 - classification_loss: 0.1978
1279/1500 [========================>.....] - ETA: 2:52 - loss: 1.2392 - regression_loss: 1.0413 - classification_loss: 0.1979
1280/1500 [========================>.....] - ETA: 2:51 - loss: 1.2399 - regression_loss: 1.0419 - classification_loss: 0.1980
1281/1500 [========================>.....] - ETA: 2:50 - loss: 1.2394 - regression_loss: 1.0415 - classification_loss: 0.1979
1282/1500 [========================>.....] - ETA: 2:49 - loss: 1.2390 - regression_loss: 1.0412 - classification_loss: 0.1978
1283/1500 [========================>.....] - ETA: 2:49 - loss: 1.2387 - regression_loss: 1.0410 - classification_loss: 0.1977
1284/1500 [========================>.....] - ETA: 2:48 - loss: 1.2382 - regression_loss: 1.0406 - classification_loss: 0.1976
1285/1500 [========================>.....] - ETA: 2:47 - loss: 1.2388 - regression_loss: 1.0411 - classification_loss: 0.1977
1286/1500 [========================>.....] - ETA: 2:46 - loss: 1.2390 - regression_loss: 1.0411 - classification_loss: 0.1979
1287/1500 [========================>.....] - ETA: 2:45 - loss: 1.2384 - regression_loss: 1.0406 - classification_loss: 0.1978
1288/1500 [========================>.....] - ETA: 2:45 - loss: 1.2386 - regression_loss: 1.0408 - classification_loss: 0.1978
1289/1500 [========================>.....] - ETA: 2:44 - loss: 1.2381 - regression_loss: 1.0403 - classification_loss: 0.1977
1290/1500 [========================>.....] - ETA: 2:43 - loss: 1.2383 - regression_loss: 1.0405 - classification_loss: 0.1978
1291/1500 [========================>.....] - ETA: 2:42 - loss: 1.2387 - regression_loss: 1.0408 - classification_loss: 0.1979
1292/1500 [========================>.....] - ETA: 2:41 - loss: 1.2392 - regression_loss: 1.0413 - classification_loss: 0.1979
1293/1500 [========================>.....] - ETA: 2:41 - loss: 1.2389 - regression_loss: 1.0410 - classification_loss: 0.1978
1294/1500 [========================>.....] - ETA: 2:40 - loss: 1.2396 - regression_loss: 1.0416 - classification_loss: 0.1980
1295/1500 [========================>.....] - ETA: 2:39 - loss: 1.2392 - regression_loss: 1.0413 - classification_loss: 0.1980
1296/1500 [========================>.....] - ETA: 2:39 - loss: 1.2391 - regression_loss: 1.0411 - classification_loss: 0.1979
1297/1500 [========================>.....] - ETA: 2:38 - loss: 1.2398 - regression_loss: 1.0418 - classification_loss: 0.1980
1298/1500 [========================>.....] - ETA: 2:37 - loss: 1.2408 - regression_loss: 1.0425 - classification_loss: 0.1983
1299/1500 [========================>.....] - ETA: 2:36 - loss: 1.2403 - regression_loss: 1.0421 - classification_loss: 0.1982
1300/1500 [=========================>....] - ETA: 2:35 - loss: 1.2397 - regression_loss: 1.0416 - classification_loss: 0.1981
1301/1500 [=========================>....] - ETA: 2:35 - loss: 1.2397 - regression_loss: 1.0416 - classification_loss: 0.1980
1302/1500 [=========================>....] - ETA: 2:34 - loss: 1.2394 - regression_loss: 1.0415 - classification_loss: 0.1979
1303/1500 [=========================>....] - ETA: 2:33 - loss: 1.2389 - regression_loss: 1.0410 - classification_loss: 0.1978
1304/1500 [=========================>....] - ETA: 2:32 - loss: 1.2387 - regression_loss: 1.0410 - classification_loss: 0.1977
1305/1500 [=========================>....] - ETA: 2:31 - loss: 1.2386 - regression_loss: 1.0409 - classification_loss: 0.1978
1306/1500 [=========================>....] - ETA: 2:31 - loss: 1.2390 - regression_loss: 1.0413 - classification_loss: 0.1977
1307/1500 [=========================>....] - ETA: 2:30 - loss: 1.2386 - regression_loss: 1.0410 - classification_loss: 0.1976
1308/1500 [=========================>....] - ETA: 2:29 - loss: 1.2388 - regression_loss: 1.0412 - classification_loss: 0.1976
1309/1500 [=========================>....] - ETA: 2:29 - loss: 1.2384 - regression_loss: 1.0409 - classification_loss: 0.1975
1310/1500 [=========================>....] - ETA: 2:28 - loss: 1.2382 - regression_loss: 1.0408 - classification_loss: 0.1974
1311/1500 [=========================>....] - ETA: 2:27 - loss: 1.2383 - regression_loss: 1.0409 - classification_loss: 0.1974
1312/1500 [=========================>....] - ETA: 2:26 - loss: 1.2379 - regression_loss: 1.0406 - classification_loss: 0.1973
1313/1500 [=========================>....] - ETA: 2:25 - loss: 1.2376 - regression_loss: 1.0403 - classification_loss: 0.1972
1314/1500 [=========================>....] - ETA: 2:25 - loss: 1.2372 - regression_loss: 1.0401 - classification_loss: 0.1971
1315/1500 [=========================>....] - ETA: 2:24 - loss: 1.2379 - regression_loss: 1.0406 - classification_loss: 0.1973
1316/1500 [=========================>....] - ETA: 2:23 - loss: 1.2376 - regression_loss: 1.0404 - classification_loss: 0.1972
1317/1500 [=========================>....] - ETA: 2:22 - loss: 1.2378 - regression_loss: 1.0406 - classification_loss: 0.1971
1318/1500 [=========================>....] - ETA: 2:21 - loss: 1.2372 - regression_loss: 1.0401 - classification_loss: 0.1971
1319/1500 [=========================>....] - ETA: 2:21 - loss: 1.2373 - regression_loss: 1.0400 - classification_loss: 0.1973
1320/1500 [=========================>....] - ETA: 2:20 - loss: 1.2377 - regression_loss: 1.0404 - classification_loss: 0.1974
1321/1500 [=========================>....] - ETA: 2:19 - loss: 1.2373 - regression_loss: 1.0400 - classification_loss: 0.1973
1322/1500 [=========================>....] - ETA: 2:18 - loss: 1.2373 - regression_loss: 1.0400 - classification_loss: 0.1973
1323/1500 [=========================>....] - ETA: 2:17 - loss: 1.2370 - regression_loss: 1.0398 - classification_loss: 0.1972
1324/1500 [=========================>....] - ETA: 2:17 - loss: 1.2364 - regression_loss: 1.0394 - classification_loss: 0.1971
1325/1500 [=========================>....] - ETA: 2:16 - loss: 1.2365 - regression_loss: 1.0395 - classification_loss: 0.1970
1326/1500 [=========================>....] - ETA: 2:15 - loss: 1.2362 - regression_loss: 1.0393 - classification_loss: 0.1969
1327/1500 [=========================>....] - ETA: 2:14 - loss: 1.2357 - regression_loss: 1.0389 - classification_loss: 0.1968
1328/1500 [=========================>....] - ETA: 2:13 - loss: 1.2367 - regression_loss: 1.0398 - classification_loss: 0.1969
1329/1500 [=========================>....] - ETA: 2:13 - loss: 1.2371 - regression_loss: 1.0402 - classification_loss: 0.1969
1330/1500 [=========================>....] - ETA: 2:12 - loss: 1.2366 - regression_loss: 1.0398 - classification_loss: 0.1968
1331/1500 [=========================>....] - ETA: 2:11 - loss: 1.2362 - regression_loss: 1.0394 - classification_loss: 0.1968
1332/1500 [=========================>....] - ETA: 2:10 - loss: 1.2357 - regression_loss: 1.0390 - classification_loss: 0.1967
1333/1500 [=========================>....] - ETA: 2:09 - loss: 1.2351 - regression_loss: 1.0386 - classification_loss: 0.1966
1334/1500 [=========================>....] - ETA: 2:09 - loss: 1.2355 - regression_loss: 1.0390 - classification_loss: 0.1966
1335/1500 [=========================>....] - ETA: 2:08 - loss: 1.2352 - regression_loss: 1.0387 - classification_loss: 0.1965
1336/1500 [=========================>....] - ETA: 2:07 - loss: 1.2348 - regression_loss: 1.0384 - classification_loss: 0.1964
1337/1500 [=========================>....] - ETA: 2:06 - loss: 1.2346 - regression_loss: 1.0382 - classification_loss: 0.1965
1338/1500 [=========================>....] - ETA: 2:05 - loss: 1.2344 - regression_loss: 1.0380 - classification_loss: 0.1964
1339/1500 [=========================>....] - ETA: 2:05 - loss: 1.2352 - regression_loss: 1.0386 - classification_loss: 0.1966
1340/1500 [=========================>....] - ETA: 2:04 - loss: 1.2358 - regression_loss: 1.0391 - classification_loss: 0.1967
1341/1500 [=========================>....] - ETA: 2:03 - loss: 1.2367 - regression_loss: 1.0399 - classification_loss: 0.1968
1342/1500 [=========================>....] - ETA: 2:02 - loss: 1.2370 - regression_loss: 1.0403 - classification_loss: 0.1967
1343/1500 [=========================>....] - ETA: 2:01 - loss: 1.2370 - regression_loss: 1.0403 - classification_loss: 0.1967
1344/1500 [=========================>....] - ETA: 2:01 - loss: 1.2370 - regression_loss: 1.0403 - classification_loss: 0.1966
1345/1500 [=========================>....] - ETA: 2:00 - loss: 1.2370 - regression_loss: 1.0404 - classification_loss: 0.1965
1346/1500 [=========================>....] - ETA: 1:59 - loss: 1.2369 - regression_loss: 1.0405 - classification_loss: 0.1965
1347/1500 [=========================>....] - ETA: 1:58 - loss: 1.2368 - regression_loss: 1.0403 - classification_loss: 0.1964
1348/1500 [=========================>....] - ETA: 1:58 - loss: 1.2367 - regression_loss: 1.0403 - classification_loss: 0.1963
1349/1500 [=========================>....] - ETA: 1:57 - loss: 1.2360 - regression_loss: 1.0398 - classification_loss: 0.1962
1350/1500 [==========================>...] - ETA: 1:56 - loss: 1.2367 - regression_loss: 1.0403 - classification_loss: 0.1964
1351/1500 [==========================>...] - ETA: 1:55 - loss: 1.2378 - regression_loss: 1.0411 - classification_loss: 0.1966
1352/1500 [==========================>...] - ETA: 1:55 - loss: 1.2386 - regression_loss: 1.0418 - classification_loss: 0.1967
1353/1500 [==========================>...] - ETA: 1:54 - loss: 1.2386 - regression_loss: 1.0419 - classification_loss: 0.1968
1354/1500 [==========================>...] - ETA: 1:53 - loss: 1.2393 - regression_loss: 1.0425 - classification_loss: 0.1968
1355/1500 [==========================>...] - ETA: 1:52 - loss: 1.2386 - regression_loss: 1.0420 - classification_loss: 0.1966
1356/1500 [==========================>...] - ETA: 1:51 - loss: 1.2381 - regression_loss: 1.0415 - classification_loss: 0.1966
1357/1500 [==========================>...] - ETA: 1:51 - loss: 1.2386 - regression_loss: 1.0420 - classification_loss: 0.1966
1358/1500 [==========================>...] - ETA: 1:50 - loss: 1.2386 - regression_loss: 1.0420 - classification_loss: 0.1965
1359/1500 [==========================>...] - ETA: 1:49 - loss: 1.2391 - regression_loss: 1.0425 - classification_loss: 0.1967
1360/1500 [==========================>...] - ETA: 1:48 - loss: 1.2390 - regression_loss: 1.0424 - classification_loss: 0.1966
1361/1500 [==========================>...] - ETA: 1:48 - loss: 1.2384 - regression_loss: 1.0419 - classification_loss: 0.1965
1362/1500 [==========================>...] - ETA: 1:47 - loss: 1.2397 - regression_loss: 1.0429 - classification_loss: 0.1968
1363/1500 [==========================>...] - ETA: 1:46 - loss: 1.2393 - regression_loss: 1.0426 - classification_loss: 0.1967
1364/1500 [==========================>...] - ETA: 1:45 - loss: 1.2399 - regression_loss: 1.0431 - classification_loss: 0.1968
1365/1500 [==========================>...] - ETA: 1:44 - loss: 1.2398 - regression_loss: 1.0430 - classification_loss: 0.1968
1366/1500 [==========================>...] - ETA: 1:44 - loss: 1.2396 - regression_loss: 1.0428 - classification_loss: 0.1968
1367/1500 [==========================>...] - ETA: 1:43 - loss: 1.2401 - regression_loss: 1.0432 - classification_loss: 0.1969
1368/1500 [==========================>...] - ETA: 1:42 - loss: 1.2396 - regression_loss: 1.0428 - classification_loss: 0.1968
1369/1500 [==========================>...] - ETA: 1:41 - loss: 1.2394 - regression_loss: 1.0427 - classification_loss: 0.1967
1370/1500 [==========================>...] - ETA: 1:41 - loss: 1.2402 - regression_loss: 1.0434 - classification_loss: 0.1969
1371/1500 [==========================>...] - ETA: 1:40 - loss: 1.2404 - regression_loss: 1.0435 - classification_loss: 0.1968
1372/1500 [==========================>...] - ETA: 1:39 - loss: 1.2405 - regression_loss: 1.0437 - classification_loss: 0.1968
1373/1500 [==========================>...] - ETA: 1:38 - loss: 1.2401 - regression_loss: 1.0434 - classification_loss: 0.1967
1374/1500 [==========================>...] - ETA: 1:38 - loss: 1.2399 - regression_loss: 1.0432 - classification_loss: 0.1967
1375/1500 [==========================>...] - ETA: 1:37 - loss: 1.2395 - regression_loss: 1.0428 - classification_loss: 0.1967
1376/1500 [==========================>...] - ETA: 1:36 - loss: 1.2400 - regression_loss: 1.0432 - classification_loss: 0.1968
1377/1500 [==========================>...] - ETA: 1:35 - loss: 1.2399 - regression_loss: 1.0432 - classification_loss: 0.1967
1378/1500 [==========================>...] - ETA: 1:34 - loss: 1.2394 - regression_loss: 1.0428 - classification_loss: 0.1966
1379/1500 [==========================>...] - ETA: 1:34 - loss: 1.2393 - regression_loss: 1.0428 - classification_loss: 0.1965
1380/1500 [==========================>...] - ETA: 1:33 - loss: 1.2395 - regression_loss: 1.0429 - classification_loss: 0.1966
1381/1500 [==========================>...] - ETA: 1:32 - loss: 1.2396 - regression_loss: 1.0430 - classification_loss: 0.1966
1382/1500 [==========================>...] - ETA: 1:31 - loss: 1.2393 - regression_loss: 1.0428 - classification_loss: 0.1965
1383/1500 [==========================>...] - ETA: 1:30 - loss: 1.2392 - regression_loss: 1.0427 - classification_loss: 0.1965
1384/1500 [==========================>...] - ETA: 1:30 - loss: 1.2387 - regression_loss: 1.0423 - classification_loss: 0.1964
1385/1500 [==========================>...] - ETA: 1:29 - loss: 1.2390 - regression_loss: 1.0425 - classification_loss: 0.1965
1386/1500 [==========================>...] - ETA: 1:28 - loss: 1.2388 - regression_loss: 1.0424 - classification_loss: 0.1964
1387/1500 [==========================>...] - ETA: 1:27 - loss: 1.2389 - regression_loss: 1.0422 - classification_loss: 0.1967
1388/1500 [==========================>...] - ETA: 1:26 - loss: 1.2396 - regression_loss: 1.0422 - classification_loss: 0.1974
1389/1500 [==========================>...] - ETA: 1:26 - loss: 1.2398 - regression_loss: 1.0424 - classification_loss: 0.1974
1390/1500 [==========================>...] - ETA: 1:25 - loss: 1.2400 - regression_loss: 1.0426 - classification_loss: 0.1974
1391/1500 [==========================>...] - ETA: 1:24 - loss: 1.2405 - regression_loss: 1.0429 - classification_loss: 0.1976
1392/1500 [==========================>...] - ETA: 1:23 - loss: 1.2399 - regression_loss: 1.0424 - classification_loss: 0.1975
1393/1500 [==========================>...] - ETA: 1:22 - loss: 1.2398 - regression_loss: 1.0423 - classification_loss: 0.1975
1394/1500 [==========================>...] - ETA: 1:22 - loss: 1.2400 - regression_loss: 1.0424 - classification_loss: 0.1976
1395/1500 [==========================>...] - ETA: 1:21 - loss: 1.2403 - regression_loss: 1.0426 - classification_loss: 0.1977
1396/1500 [==========================>...] - ETA: 1:20 - loss: 1.2400 - regression_loss: 1.0424 - classification_loss: 0.1976
1397/1500 [==========================>...] - ETA: 1:19 - loss: 1.2400 - regression_loss: 1.0424 - classification_loss: 0.1976
1398/1500 [==========================>...] - ETA: 1:19 - loss: 1.2401 - regression_loss: 1.0424 - classification_loss: 0.1977
1399/1500 [==========================>...] - ETA: 1:18 - loss: 1.2401 - regression_loss: 1.0424 - classification_loss: 0.1978
1400/1500 [===========================>..] - ETA: 1:17 - loss: 1.2401 - regression_loss: 1.0424 - classification_loss: 0.1977
1401/1500 [===========================>..] - ETA: 1:16 - loss: 1.2411 - regression_loss: 1.0431 - classification_loss: 0.1980
1402/1500 [===========================>..] - ETA: 1:15 - loss: 1.2412 - regression_loss: 1.0433 - classification_loss: 0.1979
1403/1500 [===========================>..] - ETA: 1:15 - loss: 1.2410 - regression_loss: 1.0431 - classification_loss: 0.1979
1404/1500 [===========================>..] - ETA: 1:14 - loss: 1.2415 - regression_loss: 1.0435 - classification_loss: 0.1980
1405/1500 [===========================>..] - ETA: 1:13 - loss: 1.2413 - regression_loss: 1.0433 - classification_loss: 0.1979
1406/1500 [===========================>..] - ETA: 1:12 - loss: 1.2426 - regression_loss: 1.0443 - classification_loss: 0.1983
1407/1500 [===========================>..] - ETA: 1:12 - loss: 1.2423 - regression_loss: 1.0441 - classification_loss: 0.1982
1408/1500 [===========================>..] - ETA: 1:11 - loss: 1.2424 - regression_loss: 1.0442 - classification_loss: 0.1982
1409/1500 [===========================>..] - ETA: 1:10 - loss: 1.2428 - regression_loss: 1.0446 - classification_loss: 0.1982
1410/1500 [===========================>..] - ETA: 1:09 - loss: 1.2426 - regression_loss: 1.0445 - classification_loss: 0.1981
1411/1500 [===========================>..] - ETA: 1:08 - loss: 1.2424 - regression_loss: 1.0443 - classification_loss: 0.1981
1412/1500 [===========================>..] - ETA: 1:08 - loss: 1.2423 - regression_loss: 1.0442 - classification_loss: 0.1980
1413/1500 [===========================>..] - ETA: 1:07 - loss: 1.2421 - regression_loss: 1.0442 - classification_loss: 0.1979
1414/1500 [===========================>..] - ETA: 1:06 - loss: 1.2419 - regression_loss: 1.0441 - classification_loss: 0.1979
1415/1500 [===========================>..] - ETA: 1:05 - loss: 1.2416 - regression_loss: 1.0437 - classification_loss: 0.1978
1416/1500 [===========================>..] - ETA: 1:04 - loss: 1.2416 - regression_loss: 1.0437 - classification_loss: 0.1979
1417/1500 [===========================>..] - ETA: 1:04 - loss: 1.2414 - regression_loss: 1.0436 - classification_loss: 0.1978
1418/1500 [===========================>..] - ETA: 1:03 - loss: 1.2415 - regression_loss: 1.0437 - classification_loss: 0.1977
1419/1500 [===========================>..] - ETA: 1:02 - loss: 1.2411 - regression_loss: 1.0434 - classification_loss: 0.1977
1420/1500 [===========================>..] - ETA: 1:01 - loss: 1.2416 - regression_loss: 1.0439 - classification_loss: 0.1977
1421/1500 [===========================>..] - ETA: 1:01 - loss: 1.2411 - regression_loss: 1.0435 - classification_loss: 0.1976
1422/1500 [===========================>..] - ETA: 1:00 - loss: 1.2415 - regression_loss: 1.0438 - classification_loss: 0.1977
1423/1500 [===========================>..] - ETA: 59s - loss: 1.2413 - regression_loss: 1.0436 - classification_loss: 0.1976 
1424/1500 [===========================>..] - ETA: 58s - loss: 1.2411 - regression_loss: 1.0436 - classification_loss: 0.1976
1425/1500 [===========================>..] - ETA: 58s - loss: 1.2412 - regression_loss: 1.0437 - classification_loss: 0.1976
1426/1500 [===========================>..] - ETA: 57s - loss: 1.2409 - regression_loss: 1.0434 - classification_loss: 0.1975
1427/1500 [===========================>..] - ETA: 56s - loss: 1.2409 - regression_loss: 1.0434 - classification_loss: 0.1974
1428/1500 [===========================>..] - ETA: 55s - loss: 1.2405 - regression_loss: 1.0431 - classification_loss: 0.1973
1429/1500 [===========================>..] - ETA: 54s - loss: 1.2403 - regression_loss: 1.0431 - classification_loss: 0.1972
1430/1500 [===========================>..] - ETA: 54s - loss: 1.2399 - regression_loss: 1.0428 - classification_loss: 0.1972
1431/1500 [===========================>..] - ETA: 53s - loss: 1.2396 - regression_loss: 1.0425 - classification_loss: 0.1971
1432/1500 [===========================>..] - ETA: 52s - loss: 1.2393 - regression_loss: 1.0423 - classification_loss: 0.1970
1433/1500 [===========================>..] - ETA: 51s - loss: 1.2399 - regression_loss: 1.0428 - classification_loss: 0.1971
1434/1500 [===========================>..] - ETA: 51s - loss: 1.2392 - regression_loss: 1.0422 - classification_loss: 0.1970
1435/1500 [===========================>..] - ETA: 50s - loss: 1.2394 - regression_loss: 1.0424 - classification_loss: 0.1970
1436/1500 [===========================>..] - ETA: 49s - loss: 1.2395 - regression_loss: 1.0425 - classification_loss: 0.1971
1437/1500 [===========================>..] - ETA: 48s - loss: 1.2399 - regression_loss: 1.0428 - classification_loss: 0.1971
1438/1500 [===========================>..] - ETA: 47s - loss: 1.2404 - regression_loss: 1.0432 - classification_loss: 0.1972
1439/1500 [===========================>..] - ETA: 47s - loss: 1.2400 - regression_loss: 1.0429 - classification_loss: 0.1971
1440/1500 [===========================>..] - ETA: 46s - loss: 1.2402 - regression_loss: 1.0430 - classification_loss: 0.1971
1441/1500 [===========================>..] - ETA: 45s - loss: 1.2403 - regression_loss: 1.0431 - classification_loss: 0.1972
1442/1500 [===========================>..] - ETA: 44s - loss: 1.2405 - regression_loss: 1.0428 - classification_loss: 0.1976
1443/1500 [===========================>..] - ETA: 44s - loss: 1.2402 - regression_loss: 1.0425 - classification_loss: 0.1976
1444/1500 [===========================>..] - ETA: 43s - loss: 1.2402 - regression_loss: 1.0425 - classification_loss: 0.1976
1445/1500 [===========================>..] - ETA: 42s - loss: 1.2399 - regression_loss: 1.0424 - classification_loss: 0.1975
1446/1500 [===========================>..] - ETA: 41s - loss: 1.2397 - regression_loss: 1.0422 - classification_loss: 0.1975
1447/1500 [===========================>..] - ETA: 40s - loss: 1.2394 - regression_loss: 1.0420 - classification_loss: 0.1974
1448/1500 [===========================>..] - ETA: 40s - loss: 1.2395 - regression_loss: 1.0421 - classification_loss: 0.1974
1449/1500 [===========================>..] - ETA: 39s - loss: 1.2396 - regression_loss: 1.0421 - classification_loss: 0.1975
1450/1500 [============================>.] - ETA: 38s - loss: 1.2406 - regression_loss: 1.0428 - classification_loss: 0.1977
1451/1500 [============================>.] - ETA: 37s - loss: 1.2405 - regression_loss: 1.0428 - classification_loss: 0.1976
1452/1500 [============================>.] - ETA: 37s - loss: 1.2403 - regression_loss: 1.0427 - classification_loss: 0.1976
1453/1500 [============================>.] - ETA: 36s - loss: 1.2400 - regression_loss: 1.0425 - classification_loss: 0.1975
1454/1500 [============================>.] - ETA: 35s - loss: 1.2397 - regression_loss: 1.0423 - classification_loss: 0.1974
1455/1500 [============================>.] - ETA: 34s - loss: 1.2397 - regression_loss: 1.0424 - classification_loss: 0.1973
1456/1500 [============================>.] - ETA: 34s - loss: 1.2395 - regression_loss: 1.0423 - classification_loss: 0.1972
1457/1500 [============================>.] - ETA: 33s - loss: 1.2400 - regression_loss: 1.0427 - classification_loss: 0.1972
1458/1500 [============================>.] - ETA: 32s - loss: 1.2402 - regression_loss: 1.0429 - classification_loss: 0.1973
1459/1500 [============================>.] - ETA: 31s - loss: 1.2400 - regression_loss: 1.0426 - classification_loss: 0.1973
1460/1500 [============================>.] - ETA: 31s - loss: 1.2403 - regression_loss: 1.0428 - classification_loss: 0.1974
1461/1500 [============================>.] - ETA: 30s - loss: 1.2400 - regression_loss: 1.0426 - classification_loss: 0.1974
1462/1500 [============================>.] - ETA: 29s - loss: 1.2402 - regression_loss: 1.0427 - classification_loss: 0.1975
1463/1500 [============================>.] - ETA: 28s - loss: 1.2402 - regression_loss: 1.0427 - classification_loss: 0.1975
1464/1500 [============================>.] - ETA: 27s - loss: 1.2401 - regression_loss: 1.0427 - classification_loss: 0.1974
1465/1500 [============================>.] - ETA: 27s - loss: 1.2399 - regression_loss: 1.0425 - classification_loss: 0.1974
1466/1500 [============================>.] - ETA: 26s - loss: 1.2405 - regression_loss: 1.0430 - classification_loss: 0.1975
1467/1500 [============================>.] - ETA: 25s - loss: 1.2404 - regression_loss: 1.0430 - classification_loss: 0.1974
1468/1500 [============================>.] - ETA: 24s - loss: 1.2402 - regression_loss: 1.0427 - classification_loss: 0.1975
1469/1500 [============================>.] - ETA: 24s - loss: 1.2407 - regression_loss: 1.0431 - classification_loss: 0.1976
1470/1500 [============================>.] - ETA: 23s - loss: 1.2410 - regression_loss: 1.0434 - classification_loss: 0.1976
1471/1500 [============================>.] - ETA: 22s - loss: 1.2408 - regression_loss: 1.0433 - classification_loss: 0.1975
1472/1500 [============================>.] - ETA: 21s - loss: 1.2406 - regression_loss: 1.0431 - classification_loss: 0.1975
1473/1500 [============================>.] - ETA: 20s - loss: 1.2413 - regression_loss: 1.0436 - classification_loss: 0.1977
1474/1500 [============================>.] - ETA: 20s - loss: 1.2417 - regression_loss: 1.0440 - classification_loss: 0.1977
1475/1500 [============================>.] - ETA: 19s - loss: 1.2416 - regression_loss: 1.0439 - classification_loss: 0.1977
1476/1500 [============================>.] - ETA: 18s - loss: 1.2413 - regression_loss: 1.0437 - classification_loss: 0.1976
1477/1500 [============================>.] - ETA: 17s - loss: 1.2418 - regression_loss: 1.0441 - classification_loss: 0.1977
1478/1500 [============================>.] - ETA: 17s - loss: 1.2414 - regression_loss: 1.0438 - classification_loss: 0.1976
1479/1500 [============================>.] - ETA: 16s - loss: 1.2418 - regression_loss: 1.0442 - classification_loss: 0.1976
1480/1500 [============================>.] - ETA: 15s - loss: 1.2413 - regression_loss: 1.0438 - classification_loss: 0.1975
1481/1500 [============================>.] - ETA: 14s - loss: 1.2417 - regression_loss: 1.0442 - classification_loss: 0.1975
1482/1500 [============================>.] - ETA: 13s - loss: 1.2414 - regression_loss: 1.0440 - classification_loss: 0.1974
1483/1500 [============================>.] - ETA: 13s - loss: 1.2418 - regression_loss: 1.0443 - classification_loss: 0.1975
1484/1500 [============================>.] - ETA: 12s - loss: 1.2428 - regression_loss: 1.0451 - classification_loss: 0.1976
1485/1500 [============================>.] - ETA: 11s - loss: 1.2434 - regression_loss: 1.0455 - classification_loss: 0.1978
1486/1500 [============================>.] - ETA: 10s - loss: 1.2438 - regression_loss: 1.0460 - classification_loss: 0.1978
1487/1500 [============================>.] - ETA: 10s - loss: 1.2444 - regression_loss: 1.0464 - classification_loss: 0.1980
1488/1500 [============================>.] - ETA: 9s - loss: 1.2442 - regression_loss: 1.0462 - classification_loss: 0.1979 
1489/1500 [============================>.] - ETA: 8s - loss: 1.2439 - regression_loss: 1.0461 - classification_loss: 0.1978
1490/1500 [============================>.] - ETA: 7s - loss: 1.2435 - regression_loss: 1.0458 - classification_loss: 0.1978
1491/1500 [============================>.] - ETA: 6s - loss: 1.2442 - regression_loss: 1.0463 - classification_loss: 0.1978
1492/1500 [============================>.] - ETA: 6s - loss: 1.2441 - regression_loss: 1.0463 - classification_loss: 0.1978
1493/1500 [============================>.] - ETA: 5s - loss: 1.2436 - regression_loss: 1.0460 - classification_loss: 0.1977
1494/1500 [============================>.] - ETA: 4s - loss: 1.2446 - regression_loss: 1.0468 - classification_loss: 0.1978
1495/1500 [============================>.] - ETA: 3s - loss: 1.2443 - regression_loss: 1.0465 - classification_loss: 0.1978
1496/1500 [============================>.] - ETA: 3s - loss: 1.2445 - regression_loss: 1.0467 - classification_loss: 0.1978
1497/1500 [============================>.] - ETA: 2s - loss: 1.2441 - regression_loss: 1.0464 - classification_loss: 0.1977
1498/1500 [============================>.] - ETA: 1s - loss: 1.2441 - regression_loss: 1.0463 - classification_loss: 0.1977
1499/1500 [============================>.] - ETA: 0s - loss: 1.2447 - regression_loss: 1.0468 - classification_loss: 0.1978
1500/1500 [==============================] - 1162s 774ms/step - loss: 1.2442 - regression_loss: 1.0465 - classification_loss: 0.1978

Epoch 00008: saving model to ./snapshots/resnet50_csv_08.h5
Epoch 9/10

   1/1500 [..............................] - ETA: 10:27 - loss: 0.6513 - regression_loss: 0.5192 - classification_loss: 0.1321
   2/1500 [..............................] - ETA: 10:46 - loss: 1.2838 - regression_loss: 0.8755 - classification_loss: 0.4082
   3/1500 [..............................] - ETA: 10:38 - loss: 0.9768 - regression_loss: 0.6849 - classification_loss: 0.2919
   4/1500 [..............................] - ETA: 10:23 - loss: 0.8945 - regression_loss: 0.6502 - classification_loss: 0.2444
   5/1500 [..............................] - ETA: 10:22 - loss: 0.8909 - regression_loss: 0.6714 - classification_loss: 0.2195
   6/1500 [..............................] - ETA: 14:35 - loss: 1.0404 - regression_loss: 0.7730 - classification_loss: 0.2674
   7/1500 [..............................] - ETA: 13:53 - loss: 1.0662 - regression_loss: 0.8128 - classification_loss: 0.2534
   8/1500 [..............................] - ETA: 13:21 - loss: 1.1816 - regression_loss: 0.9299 - classification_loss: 0.2517
   9/1500 [..............................] - ETA: 14:18 - loss: 1.1702 - regression_loss: 0.9258 - classification_loss: 0.2444
  10/1500 [..............................] - ETA: 13:50 - loss: 1.1536 - regression_loss: 0.9244 - classification_loss: 0.2291
  11/1500 [..............................] - ETA: 15:00 - loss: 1.1038 - regression_loss: 0.8874 - classification_loss: 0.2164
  12/1500 [..............................] - ETA: 14:36 - loss: 1.1311 - regression_loss: 0.9225 - classification_loss: 0.2086
  13/1500 [..............................] - ETA: 14:18 - loss: 1.1396 - regression_loss: 0.9394 - classification_loss: 0.2002
  14/1500 [..............................] - ETA: 14:37 - loss: 1.1392 - regression_loss: 0.9394 - classification_loss: 0.1998
  15/1500 [..............................] - ETA: 14:18 - loss: 1.1424 - regression_loss: 0.9491 - classification_loss: 0.1933
  16/1500 [..............................] - ETA: 14:05 - loss: 1.1139 - regression_loss: 0.9262 - classification_loss: 0.1877
  17/1500 [..............................] - ETA: 13:49 - loss: 1.0807 - regression_loss: 0.9017 - classification_loss: 0.1790
  18/1500 [..............................] - ETA: 13:58 - loss: 1.0528 - regression_loss: 0.8790 - classification_loss: 0.1738
  19/1500 [..............................] - ETA: 14:17 - loss: 1.0439 - regression_loss: 0.8739 - classification_loss: 0.1700
  20/1500 [..............................] - ETA: 14:05 - loss: 1.1317 - regression_loss: 0.9578 - classification_loss: 0.1738
  21/1500 [..............................] - ETA: 13:50 - loss: 1.1284 - regression_loss: 0.9567 - classification_loss: 0.1717
  22/1500 [..............................] - ETA: 13:38 - loss: 1.1596 - regression_loss: 0.9889 - classification_loss: 0.1707
  23/1500 [..............................] - ETA: 13:30 - loss: 1.1676 - regression_loss: 0.9977 - classification_loss: 0.1699
  24/1500 [..............................] - ETA: 13:22 - loss: 1.1699 - regression_loss: 0.9990 - classification_loss: 0.1709
  25/1500 [..............................] - ETA: 13:15 - loss: 1.1751 - regression_loss: 1.0014 - classification_loss: 0.1737
  26/1500 [..............................] - ETA: 13:08 - loss: 1.2038 - regression_loss: 1.0280 - classification_loss: 0.1758
  27/1500 [..............................] - ETA: 13:16 - loss: 1.2196 - regression_loss: 1.0434 - classification_loss: 0.1762
  28/1500 [..............................] - ETA: 13:40 - loss: 1.2175 - regression_loss: 1.0376 - classification_loss: 0.1799
  29/1500 [..............................] - ETA: 14:23 - loss: 1.1961 - regression_loss: 1.0194 - classification_loss: 0.1767
  30/1500 [..............................] - ETA: 15:02 - loss: 1.2249 - regression_loss: 1.0412 - classification_loss: 0.1837
  31/1500 [..............................] - ETA: 15:09 - loss: 1.2195 - regression_loss: 1.0365 - classification_loss: 0.1829
  32/1500 [..............................] - ETA: 15:00 - loss: 1.2280 - regression_loss: 1.0422 - classification_loss: 0.1857
  33/1500 [..............................] - ETA: 16:21 - loss: 1.2287 - regression_loss: 1.0415 - classification_loss: 0.1872
  34/1500 [..............................] - ETA: 16:17 - loss: 1.2176 - regression_loss: 1.0343 - classification_loss: 0.1833
  35/1500 [..............................] - ETA: 16:10 - loss: 1.2097 - regression_loss: 1.0284 - classification_loss: 0.1813
  36/1500 [..............................] - ETA: 17:06 - loss: 1.1969 - regression_loss: 1.0174 - classification_loss: 0.1795
  37/1500 [..............................] - ETA: 16:51 - loss: 1.1953 - regression_loss: 1.0184 - classification_loss: 0.1769
  38/1500 [..............................] - ETA: 17:34 - loss: 1.1932 - regression_loss: 1.0184 - classification_loss: 0.1748
  39/1500 [..............................] - ETA: 17:44 - loss: 1.1916 - regression_loss: 1.0158 - classification_loss: 0.1758
  40/1500 [..............................] - ETA: 17:36 - loss: 1.1778 - regression_loss: 1.0054 - classification_loss: 0.1724
  41/1500 [..............................] - ETA: 17:40 - loss: 1.1669 - regression_loss: 0.9962 - classification_loss: 0.1706
  42/1500 [..............................] - ETA: 17:58 - loss: 1.1657 - regression_loss: 0.9944 - classification_loss: 0.1713
  43/1500 [..............................] - ETA: 18:19 - loss: 1.1576 - regression_loss: 0.9882 - classification_loss: 0.1693
  44/1500 [..............................] - ETA: 19:41 - loss: 1.1828 - regression_loss: 1.0106 - classification_loss: 0.1723
  45/1500 [..............................] - ETA: 19:45 - loss: 1.1901 - regression_loss: 1.0185 - classification_loss: 0.1716
  46/1500 [..............................] - ETA: 19:48 - loss: 1.2101 - regression_loss: 1.0333 - classification_loss: 0.1767
  47/1500 [..............................] - ETA: 19:45 - loss: 1.2162 - regression_loss: 1.0382 - classification_loss: 0.1780
  48/1500 [..............................] - ETA: 20:05 - loss: 1.2118 - regression_loss: 1.0340 - classification_loss: 0.1778
  49/1500 [..............................] - ETA: 20:06 - loss: 1.2099 - regression_loss: 1.0337 - classification_loss: 0.1763
  50/1500 [>.............................] - ETA: 19:53 - loss: 1.2243 - regression_loss: 1.0454 - classification_loss: 0.1789
  51/1500 [>.............................] - ETA: 19:40 - loss: 1.2102 - regression_loss: 1.0338 - classification_loss: 0.1763
  52/1500 [>.............................] - ETA: 19:55 - loss: 1.1988 - regression_loss: 1.0245 - classification_loss: 0.1742
  53/1500 [>.............................] - ETA: 19:58 - loss: 1.1839 - regression_loss: 1.0118 - classification_loss: 0.1721
  54/1500 [>.............................] - ETA: 19:58 - loss: 1.1692 - regression_loss: 0.9994 - classification_loss: 0.1697
  55/1500 [>.............................] - ETA: 19:46 - loss: 1.1594 - regression_loss: 0.9909 - classification_loss: 0.1685
  56/1500 [>.............................] - ETA: 19:38 - loss: 1.1786 - regression_loss: 1.0067 - classification_loss: 0.1719
  57/1500 [>.............................] - ETA: 19:32 - loss: 1.1807 - regression_loss: 1.0082 - classification_loss: 0.1725
  58/1500 [>.............................] - ETA: 19:42 - loss: 1.1696 - regression_loss: 0.9991 - classification_loss: 0.1705
  59/1500 [>.............................] - ETA: 19:31 - loss: 1.1882 - regression_loss: 1.0132 - classification_loss: 0.1750
  60/1500 [>.............................] - ETA: 19:34 - loss: 1.1832 - regression_loss: 1.0099 - classification_loss: 0.1733
  61/1500 [>.............................] - ETA: 19:32 - loss: 1.1830 - regression_loss: 1.0104 - classification_loss: 0.1726
  62/1500 [>.............................] - ETA: 19:31 - loss: 1.1820 - regression_loss: 1.0082 - classification_loss: 0.1739
  63/1500 [>.............................] - ETA: 19:28 - loss: 1.1814 - regression_loss: 1.0086 - classification_loss: 0.1728
  64/1500 [>.............................] - ETA: 19:27 - loss: 1.1732 - regression_loss: 1.0019 - classification_loss: 0.1713
  65/1500 [>.............................] - ETA: 19:17 - loss: 1.1645 - regression_loss: 0.9951 - classification_loss: 0.1694
  66/1500 [>.............................] - ETA: 19:54 - loss: 1.1835 - regression_loss: 1.0099 - classification_loss: 0.1736
  67/1500 [>.............................] - ETA: 19:46 - loss: 1.1776 - regression_loss: 1.0052 - classification_loss: 0.1724
  68/1500 [>.............................] - ETA: 19:47 - loss: 1.1762 - regression_loss: 1.0033 - classification_loss: 0.1728
  69/1500 [>.............................] - ETA: 19:38 - loss: 1.1970 - regression_loss: 1.0230 - classification_loss: 0.1740
  70/1500 [>.............................] - ETA: 19:47 - loss: 1.1904 - regression_loss: 1.0180 - classification_loss: 0.1724
  71/1500 [>.............................] - ETA: 19:50 - loss: 1.1898 - regression_loss: 1.0180 - classification_loss: 0.1718
  72/1500 [>.............................] - ETA: 19:50 - loss: 1.1979 - regression_loss: 1.0258 - classification_loss: 0.1721
  73/1500 [>.............................] - ETA: 20:09 - loss: 1.2135 - regression_loss: 1.0332 - classification_loss: 0.1803
  74/1500 [>.............................] - ETA: 20:00 - loss: 1.2252 - regression_loss: 1.0418 - classification_loss: 0.1834
  75/1500 [>.............................] - ETA: 19:50 - loss: 1.2257 - regression_loss: 1.0418 - classification_loss: 0.1840
  76/1500 [>.............................] - ETA: 19:41 - loss: 1.2262 - regression_loss: 1.0423 - classification_loss: 0.1839
  77/1500 [>.............................] - ETA: 19:41 - loss: 1.2249 - regression_loss: 1.0421 - classification_loss: 0.1829
  78/1500 [>.............................] - ETA: 19:33 - loss: 1.2283 - regression_loss: 1.0463 - classification_loss: 0.1820
  79/1500 [>.............................] - ETA: 19:39 - loss: 1.2222 - regression_loss: 1.0403 - classification_loss: 0.1819
  80/1500 [>.............................] - ETA: 19:55 - loss: 1.2217 - regression_loss: 1.0407 - classification_loss: 0.1810
  81/1500 [>.............................] - ETA: 19:45 - loss: 1.2398 - regression_loss: 1.0539 - classification_loss: 0.1859
  82/1500 [>.............................] - ETA: 19:36 - loss: 1.2360 - regression_loss: 1.0512 - classification_loss: 0.1848
  83/1500 [>.............................] - ETA: 19:35 - loss: 1.2330 - regression_loss: 1.0481 - classification_loss: 0.1849
  84/1500 [>.............................] - ETA: 19:30 - loss: 1.2427 - regression_loss: 1.0555 - classification_loss: 0.1872
  85/1500 [>.............................] - ETA: 19:22 - loss: 1.2406 - regression_loss: 1.0540 - classification_loss: 0.1866
  86/1500 [>.............................] - ETA: 19:15 - loss: 1.2448 - regression_loss: 1.0581 - classification_loss: 0.1867
  87/1500 [>.............................] - ETA: 19:36 - loss: 1.2468 - regression_loss: 1.0602 - classification_loss: 0.1866
  88/1500 [>.............................] - ETA: 19:31 - loss: 1.2552 - regression_loss: 1.0673 - classification_loss: 0.1879
  89/1500 [>.............................] - ETA: 19:27 - loss: 1.2515 - regression_loss: 1.0646 - classification_loss: 0.1869
  90/1500 [>.............................] - ETA: 19:27 - loss: 1.2539 - regression_loss: 1.0677 - classification_loss: 0.1862
  91/1500 [>.............................] - ETA: 19:30 - loss: 1.2482 - regression_loss: 1.0634 - classification_loss: 0.1848
  92/1500 [>.............................] - ETA: 19:24 - loss: 1.2465 - regression_loss: 1.0625 - classification_loss: 0.1840
  93/1500 [>.............................] - ETA: 19:20 - loss: 1.2402 - regression_loss: 1.0569 - classification_loss: 0.1833
  94/1500 [>.............................] - ETA: 19:22 - loss: 1.2544 - regression_loss: 1.0673 - classification_loss: 0.1871
  95/1500 [>.............................] - ETA: 19:14 - loss: 1.2481 - regression_loss: 1.0623 - classification_loss: 0.1857
  96/1500 [>.............................] - ETA: 19:07 - loss: 1.2394 - regression_loss: 1.0550 - classification_loss: 0.1844
  97/1500 [>.............................] - ETA: 19:09 - loss: 1.2485 - regression_loss: 1.0616 - classification_loss: 0.1870
  98/1500 [>.............................] - ETA: 19:14 - loss: 1.2427 - regression_loss: 1.0565 - classification_loss: 0.1861
  99/1500 [>.............................] - ETA: 19:07 - loss: 1.2505 - regression_loss: 1.0639 - classification_loss: 0.1866
 100/1500 [=>............................] - ETA: 19:01 - loss: 1.2446 - regression_loss: 1.0591 - classification_loss: 0.1855
 101/1500 [=>............................] - ETA: 18:57 - loss: 1.2440 - regression_loss: 1.0583 - classification_loss: 0.1856
 102/1500 [=>............................] - ETA: 18:51 - loss: 1.2527 - regression_loss: 1.0662 - classification_loss: 0.1864
 103/1500 [=>............................] - ETA: 18:57 - loss: 1.2517 - regression_loss: 1.0646 - classification_loss: 0.1871
 104/1500 [=>............................] - ETA: 18:51 - loss: 1.2543 - regression_loss: 1.0676 - classification_loss: 0.1867
 105/1500 [=>............................] - ETA: 18:46 - loss: 1.2591 - regression_loss: 1.0727 - classification_loss: 0.1864
 106/1500 [=>............................] - ETA: 18:39 - loss: 1.2586 - regression_loss: 1.0719 - classification_loss: 0.1866
 107/1500 [=>............................] - ETA: 18:35 - loss: 1.2521 - regression_loss: 1.0666 - classification_loss: 0.1855
 108/1500 [=>............................] - ETA: 18:29 - loss: 1.2623 - regression_loss: 1.0751 - classification_loss: 0.1873
 109/1500 [=>............................] - ETA: 18:24 - loss: 1.2649 - regression_loss: 1.0776 - classification_loss: 0.1873
 110/1500 [=>............................] - ETA: 18:18 - loss: 1.2606 - regression_loss: 1.0744 - classification_loss: 0.1862
 111/1500 [=>............................] - ETA: 18:25 - loss: 1.2588 - regression_loss: 1.0698 - classification_loss: 0.1891
 112/1500 [=>............................] - ETA: 18:36 - loss: 1.2576 - regression_loss: 1.0691 - classification_loss: 0.1884
 113/1500 [=>............................] - ETA: 18:31 - loss: 1.2665 - regression_loss: 1.0770 - classification_loss: 0.1895
 114/1500 [=>............................] - ETA: 18:40 - loss: 1.2634 - regression_loss: 1.0737 - classification_loss: 0.1897
 115/1500 [=>............................] - ETA: 18:45 - loss: 1.2601 - regression_loss: 1.0703 - classification_loss: 0.1898
 116/1500 [=>............................] - ETA: 18:44 - loss: 1.2600 - regression_loss: 1.0707 - classification_loss: 0.1894
 117/1500 [=>............................] - ETA: 18:51 - loss: 1.2658 - regression_loss: 1.0760 - classification_loss: 0.1897
 118/1500 [=>............................] - ETA: 18:45 - loss: 1.2805 - regression_loss: 1.0860 - classification_loss: 0.1945
 119/1500 [=>............................] - ETA: 18:47 - loss: 1.2865 - regression_loss: 1.0909 - classification_loss: 0.1956
 120/1500 [=>............................] - ETA: 18:41 - loss: 1.2951 - regression_loss: 1.0976 - classification_loss: 0.1975
 121/1500 [=>............................] - ETA: 18:51 - loss: 1.3014 - regression_loss: 1.1020 - classification_loss: 0.1995
 122/1500 [=>............................] - ETA: 18:50 - loss: 1.2979 - regression_loss: 1.0993 - classification_loss: 0.1986
 123/1500 [=>............................] - ETA: 18:50 - loss: 1.2889 - regression_loss: 1.0904 - classification_loss: 0.1986
 124/1500 [=>............................] - ETA: 18:46 - loss: 1.2812 - regression_loss: 1.0840 - classification_loss: 0.1972
 125/1500 [=>............................] - ETA: 18:41 - loss: 1.2851 - regression_loss: 1.0878 - classification_loss: 0.1974
 126/1500 [=>............................] - ETA: 18:36 - loss: 1.2893 - regression_loss: 1.0919 - classification_loss: 0.1974
 127/1500 [=>............................] - ETA: 18:33 - loss: 1.2860 - regression_loss: 1.0894 - classification_loss: 0.1966
 128/1500 [=>............................] - ETA: 18:37 - loss: 1.2848 - regression_loss: 1.0886 - classification_loss: 0.1962
 129/1500 [=>............................] - ETA: 18:38 - loss: 1.2801 - regression_loss: 1.0849 - classification_loss: 0.1952
 130/1500 [=>............................] - ETA: 18:32 - loss: 1.2742 - regression_loss: 1.0799 - classification_loss: 0.1943
 131/1500 [=>............................] - ETA: 18:38 - loss: 1.2839 - regression_loss: 1.0822 - classification_loss: 0.2017
 132/1500 [=>............................] - ETA: 18:49 - loss: 1.2834 - regression_loss: 1.0814 - classification_loss: 0.2020
 133/1500 [=>............................] - ETA: 18:43 - loss: 1.2836 - regression_loss: 1.0814 - classification_loss: 0.2022
 134/1500 [=>............................] - ETA: 18:38 - loss: 1.2865 - regression_loss: 1.0827 - classification_loss: 0.2038
 135/1500 [=>............................] - ETA: 18:37 - loss: 1.2849 - regression_loss: 1.0810 - classification_loss: 0.2039
 136/1500 [=>............................] - ETA: 18:49 - loss: 1.2945 - regression_loss: 1.0883 - classification_loss: 0.2061
 137/1500 [=>............................] - ETA: 18:44 - loss: 1.2919 - regression_loss: 1.0867 - classification_loss: 0.2052
 138/1500 [=>............................] - ETA: 18:43 - loss: 1.2918 - regression_loss: 1.0872 - classification_loss: 0.2046
 139/1500 [=>............................] - ETA: 18:41 - loss: 1.2898 - regression_loss: 1.0862 - classification_loss: 0.2037
 140/1500 [=>............................] - ETA: 18:44 - loss: 1.2875 - regression_loss: 1.0841 - classification_loss: 0.2034
 141/1500 [=>............................] - ETA: 18:42 - loss: 1.2862 - regression_loss: 1.0831 - classification_loss: 0.2031
 142/1500 [=>............................] - ETA: 18:43 - loss: 1.2854 - regression_loss: 1.0823 - classification_loss: 0.2030
 143/1500 [=>............................] - ETA: 18:40 - loss: 1.2879 - regression_loss: 1.0846 - classification_loss: 0.2032
 144/1500 [=>............................] - ETA: 18:35 - loss: 1.2850 - regression_loss: 1.0822 - classification_loss: 0.2028
 145/1500 [=>............................] - ETA: 18:34 - loss: 1.2803 - regression_loss: 1.0782 - classification_loss: 0.2020
 146/1500 [=>............................] - ETA: 18:38 - loss: 1.2757 - regression_loss: 1.0736 - classification_loss: 0.2021
 147/1500 [=>............................] - ETA: 18:35 - loss: 1.2778 - regression_loss: 1.0762 - classification_loss: 0.2016
 148/1500 [=>............................] - ETA: 18:32 - loss: 1.2732 - regression_loss: 1.0723 - classification_loss: 0.2009
 149/1500 [=>............................] - ETA: 18:36 - loss: 1.2710 - regression_loss: 1.0708 - classification_loss: 0.2002
 150/1500 [==>...........................] - ETA: 18:31 - loss: 1.2674 - regression_loss: 1.0676 - classification_loss: 0.1998
 151/1500 [==>...........................] - ETA: 18:31 - loss: 1.2750 - regression_loss: 1.0745 - classification_loss: 0.2005
 152/1500 [==>...........................] - ETA: 18:30 - loss: 1.2764 - regression_loss: 1.0754 - classification_loss: 0.2010
 153/1500 [==>...........................] - ETA: 18:26 - loss: 1.2760 - regression_loss: 1.0754 - classification_loss: 0.2005
 154/1500 [==>...........................] - ETA: 18:26 - loss: 1.2735 - regression_loss: 1.0737 - classification_loss: 0.1997
 155/1500 [==>...........................] - ETA: 18:22 - loss: 1.2716 - regression_loss: 1.0722 - classification_loss: 0.1994
 156/1500 [==>...........................] - ETA: 18:18 - loss: 1.2686 - regression_loss: 1.0699 - classification_loss: 0.1987
 157/1500 [==>...........................] - ETA: 18:14 - loss: 1.2671 - regression_loss: 1.0679 - classification_loss: 0.1992
 158/1500 [==>...........................] - ETA: 18:15 - loss: 1.2680 - regression_loss: 1.0683 - classification_loss: 0.1997
 159/1500 [==>...........................] - ETA: 18:21 - loss: 1.2730 - regression_loss: 1.0714 - classification_loss: 0.2015
 160/1500 [==>...........................] - ETA: 18:19 - loss: 1.2729 - regression_loss: 1.0713 - classification_loss: 0.2016
 161/1500 [==>...........................] - ETA: 18:15 - loss: 1.2705 - regression_loss: 1.0694 - classification_loss: 0.2011
 162/1500 [==>...........................] - ETA: 18:10 - loss: 1.2676 - regression_loss: 1.0669 - classification_loss: 0.2007
 163/1500 [==>...........................] - ETA: 18:07 - loss: 1.2676 - regression_loss: 1.0668 - classification_loss: 0.2008
 164/1500 [==>...........................] - ETA: 18:03 - loss: 1.2763 - regression_loss: 1.0730 - classification_loss: 0.2033
 165/1500 [==>...........................] - ETA: 18:01 - loss: 1.2840 - regression_loss: 1.0791 - classification_loss: 0.2049
 166/1500 [==>...........................] - ETA: 17:58 - loss: 1.2792 - regression_loss: 1.0754 - classification_loss: 0.2038
 167/1500 [==>...........................] - ETA: 17:55 - loss: 1.2741 - regression_loss: 1.0712 - classification_loss: 0.2029
 168/1500 [==>...........................] - ETA: 18:04 - loss: 1.2709 - regression_loss: 1.0684 - classification_loss: 0.2025
 169/1500 [==>...........................] - ETA: 18:10 - loss: 1.2697 - regression_loss: 1.0678 - classification_loss: 0.2019
 170/1500 [==>...........................] - ETA: 18:11 - loss: 1.2698 - regression_loss: 1.0683 - classification_loss: 0.2016
 171/1500 [==>...........................] - ETA: 18:07 - loss: 1.2686 - regression_loss: 1.0673 - classification_loss: 0.2012
 172/1500 [==>...........................] - ETA: 18:04 - loss: 1.2704 - regression_loss: 1.0692 - classification_loss: 0.2012
 173/1500 [==>...........................] - ETA: 18:03 - loss: 1.2683 - regression_loss: 1.0670 - classification_loss: 0.2014
 174/1500 [==>...........................] - ETA: 18:04 - loss: 1.2724 - regression_loss: 1.0698 - classification_loss: 0.2026
 175/1500 [==>...........................] - ETA: 18:00 - loss: 1.2689 - regression_loss: 1.0670 - classification_loss: 0.2018
 176/1500 [==>...........................] - ETA: 17:56 - loss: 1.2744 - regression_loss: 1.0711 - classification_loss: 0.2033
 177/1500 [==>...........................] - ETA: 17:52 - loss: 1.2715 - regression_loss: 1.0689 - classification_loss: 0.2027
 178/1500 [==>...........................] - ETA: 17:49 - loss: 1.2711 - regression_loss: 1.0677 - classification_loss: 0.2034
 179/1500 [==>...........................] - ETA: 17:51 - loss: 1.2791 - regression_loss: 1.0750 - classification_loss: 0.2041
 180/1500 [==>...........................] - ETA: 17:54 - loss: 1.2773 - regression_loss: 1.0735 - classification_loss: 0.2038
 181/1500 [==>...........................] - ETA: 17:50 - loss: 1.2773 - regression_loss: 1.0738 - classification_loss: 0.2034
 182/1500 [==>...........................] - ETA: 17:47 - loss: 1.2833 - regression_loss: 1.0785 - classification_loss: 0.2048
 183/1500 [==>...........................] - ETA: 17:43 - loss: 1.2797 - regression_loss: 1.0758 - classification_loss: 0.2039
 184/1500 [==>...........................] - ETA: 17:41 - loss: 1.2773 - regression_loss: 1.0733 - classification_loss: 0.2040
 185/1500 [==>...........................] - ETA: 17:38 - loss: 1.2762 - regression_loss: 1.0725 - classification_loss: 0.2036
 186/1500 [==>...........................] - ETA: 17:36 - loss: 1.2756 - regression_loss: 1.0725 - classification_loss: 0.2030
 187/1500 [==>...........................] - ETA: 17:41 - loss: 1.2777 - regression_loss: 1.0743 - classification_loss: 0.2035
 188/1500 [==>...........................] - ETA: 17:46 - loss: 1.2769 - regression_loss: 1.0731 - classification_loss: 0.2038
 189/1500 [==>...........................] - ETA: 17:51 - loss: 1.2806 - regression_loss: 1.0762 - classification_loss: 0.2044
 190/1500 [==>...........................] - ETA: 17:53 - loss: 1.2780 - regression_loss: 1.0741 - classification_loss: 0.2039
 191/1500 [==>...........................] - ETA: 17:50 - loss: 1.2759 - regression_loss: 1.0722 - classification_loss: 0.2037
 192/1500 [==>...........................] - ETA: 17:50 - loss: 1.2749 - regression_loss: 1.0714 - classification_loss: 0.2035
 193/1500 [==>...........................] - ETA: 18:03 - loss: 1.2732 - regression_loss: 1.0701 - classification_loss: 0.2032
 194/1500 [==>...........................] - ETA: 18:03 - loss: 1.2754 - regression_loss: 1.0718 - classification_loss: 0.2036
 195/1500 [==>...........................] - ETA: 18:00 - loss: 1.2770 - regression_loss: 1.0733 - classification_loss: 0.2037
 196/1500 [==>...........................] - ETA: 17:58 - loss: 1.2812 - regression_loss: 1.0775 - classification_loss: 0.2037
 197/1500 [==>...........................] - ETA: 17:53 - loss: 1.2804 - regression_loss: 1.0764 - classification_loss: 0.2041
 198/1500 [==>...........................] - ETA: 17:50 - loss: 1.2821 - regression_loss: 1.0786 - classification_loss: 0.2036
 199/1500 [==>...........................] - ETA: 17:51 - loss: 1.2877 - regression_loss: 1.0799 - classification_loss: 0.2078
 200/1500 [===>..........................] - ETA: 17:47 - loss: 1.2889 - regression_loss: 1.0811 - classification_loss: 0.2078
 201/1500 [===>..........................] - ETA: 17:50 - loss: 1.2854 - regression_loss: 1.0784 - classification_loss: 0.2069
 202/1500 [===>..........................] - ETA: 17:52 - loss: 1.2842 - regression_loss: 1.0776 - classification_loss: 0.2067
 203/1500 [===>..........................] - ETA: 17:50 - loss: 1.2834 - regression_loss: 1.0773 - classification_loss: 0.2061
 204/1500 [===>..........................] - ETA: 17:59 - loss: 1.2888 - regression_loss: 1.0817 - classification_loss: 0.2070
 205/1500 [===>..........................] - ETA: 17:56 - loss: 1.2874 - regression_loss: 1.0802 - classification_loss: 0.2072
 206/1500 [===>..........................] - ETA: 17:55 - loss: 1.2943 - regression_loss: 1.0850 - classification_loss: 0.2093
 207/1500 [===>..........................] - ETA: 17:53 - loss: 1.2913 - regression_loss: 1.0828 - classification_loss: 0.2085
 208/1500 [===>..........................] - ETA: 17:50 - loss: 1.2906 - regression_loss: 1.0822 - classification_loss: 0.2084
 209/1500 [===>..........................] - ETA: 17:47 - loss: 1.2898 - regression_loss: 1.0816 - classification_loss: 0.2081
 210/1500 [===>..........................] - ETA: 17:50 - loss: 1.2939 - regression_loss: 1.0851 - classification_loss: 0.2087
 211/1500 [===>..........................] - ETA: 17:46 - loss: 1.2911 - regression_loss: 1.0829 - classification_loss: 0.2082
 212/1500 [===>..........................] - ETA: 17:43 - loss: 1.2933 - regression_loss: 1.0851 - classification_loss: 0.2082
 213/1500 [===>..........................] - ETA: 17:45 - loss: 1.2926 - regression_loss: 1.0847 - classification_loss: 0.2079
 214/1500 [===>..........................] - ETA: 17:48 - loss: 1.2955 - regression_loss: 1.0875 - classification_loss: 0.2080
 215/1500 [===>..........................] - ETA: 17:46 - loss: 1.2917 - regression_loss: 1.0845 - classification_loss: 0.2072
 216/1500 [===>..........................] - ETA: 17:42 - loss: 1.2872 - regression_loss: 1.0807 - classification_loss: 0.2065
 217/1500 [===>..........................] - ETA: 17:39 - loss: 1.2835 - regression_loss: 1.0778 - classification_loss: 0.2057
 218/1500 [===>..........................] - ETA: 17:35 - loss: 1.2807 - regression_loss: 1.0757 - classification_loss: 0.2050
 219/1500 [===>..........................] - ETA: 17:32 - loss: 1.2787 - regression_loss: 1.0739 - classification_loss: 0.2048
 220/1500 [===>..........................] - ETA: 17:29 - loss: 1.2777 - regression_loss: 1.0730 - classification_loss: 0.2047
 221/1500 [===>..........................] - ETA: 17:26 - loss: 1.2761 - regression_loss: 1.0719 - classification_loss: 0.2041
 222/1500 [===>..........................] - ETA: 17:22 - loss: 1.2741 - regression_loss: 1.0704 - classification_loss: 0.2037
 223/1500 [===>..........................] - ETA: 17:23 - loss: 1.2808 - regression_loss: 1.0755 - classification_loss: 0.2053
 224/1500 [===>..........................] - ETA: 17:20 - loss: 1.2814 - regression_loss: 1.0762 - classification_loss: 0.2052
 225/1500 [===>..........................] - ETA: 17:23 - loss: 1.2781 - regression_loss: 1.0737 - classification_loss: 0.2045
 226/1500 [===>..........................] - ETA: 17:26 - loss: 1.2803 - regression_loss: 1.0753 - classification_loss: 0.2051
 227/1500 [===>..........................] - ETA: 17:23 - loss: 1.2770 - regression_loss: 1.0726 - classification_loss: 0.2044
 228/1500 [===>..........................] - ETA: 17:20 - loss: 1.2744 - regression_loss: 1.0704 - classification_loss: 0.2040
 229/1500 [===>..........................] - ETA: 17:18 - loss: 1.2734 - regression_loss: 1.0698 - classification_loss: 0.2037
 230/1500 [===>..........................] - ETA: 17:15 - loss: 1.2707 - regression_loss: 1.0676 - classification_loss: 0.2031
 231/1500 [===>..........................] - ETA: 17:12 - loss: 1.2737 - regression_loss: 1.0708 - classification_loss: 0.2029
 232/1500 [===>..........................] - ETA: 17:12 - loss: 1.2695 - regression_loss: 1.0674 - classification_loss: 0.2022
 233/1500 [===>..........................] - ETA: 17:14 - loss: 1.2737 - regression_loss: 1.0710 - classification_loss: 0.2027
 234/1500 [===>..........................] - ETA: 17:11 - loss: 1.2709 - regression_loss: 1.0685 - classification_loss: 0.2024
 235/1500 [===>..........................] - ETA: 17:12 - loss: 1.2687 - regression_loss: 1.0669 - classification_loss: 0.2018
 236/1500 [===>..........................] - ETA: 17:10 - loss: 1.2699 - regression_loss: 1.0683 - classification_loss: 0.2016
 237/1500 [===>..........................] - ETA: 17:07 - loss: 1.2672 - regression_loss: 1.0658 - classification_loss: 0.2015
 238/1500 [===>..........................] - ETA: 17:09 - loss: 1.2653 - regression_loss: 1.0644 - classification_loss: 0.2009
 239/1500 [===>..........................] - ETA: 17:11 - loss: 1.2704 - regression_loss: 1.0683 - classification_loss: 0.2021
 240/1500 [===>..........................] - ETA: 17:11 - loss: 1.2697 - regression_loss: 1.0681 - classification_loss: 0.2016
 241/1500 [===>..........................] - ETA: 17:12 - loss: 1.2675 - regression_loss: 1.0660 - classification_loss: 0.2015
 242/1500 [===>..........................] - ETA: 17:09 - loss: 1.2729 - regression_loss: 1.0700 - classification_loss: 0.2029
 243/1500 [===>..........................] - ETA: 17:07 - loss: 1.2708 - regression_loss: 1.0680 - classification_loss: 0.2028
 244/1500 [===>..........................] - ETA: 17:04 - loss: 1.2723 - regression_loss: 1.0691 - classification_loss: 0.2032
 245/1500 [===>..........................] - ETA: 17:01 - loss: 1.2742 - regression_loss: 1.0706 - classification_loss: 0.2036
 246/1500 [===>..........................] - ETA: 17:04 - loss: 1.2711 - regression_loss: 1.0680 - classification_loss: 0.2030
 247/1500 [===>..........................] - ETA: 17:03 - loss: 1.2711 - regression_loss: 1.0677 - classification_loss: 0.2034
 248/1500 [===>..........................] - ETA: 17:01 - loss: 1.2720 - regression_loss: 1.0683 - classification_loss: 0.2037
 249/1500 [===>..........................] - ETA: 16:58 - loss: 1.2707 - regression_loss: 1.0673 - classification_loss: 0.2033
 250/1500 [====>.........................] - ETA: 16:55 - loss: 1.2698 - regression_loss: 1.0663 - classification_loss: 0.2035
 251/1500 [====>.........................] - ETA: 16:54 - loss: 1.2664 - regression_loss: 1.0635 - classification_loss: 0.2029
 252/1500 [====>.........................] - ETA: 16:51 - loss: 1.2700 - regression_loss: 1.0667 - classification_loss: 0.2033
 253/1500 [====>.........................] - ETA: 16:50 - loss: 1.2712 - regression_loss: 1.0681 - classification_loss: 0.2031
 254/1500 [====>.........................] - ETA: 16:47 - loss: 1.2713 - regression_loss: 1.0681 - classification_loss: 0.2032
 255/1500 [====>.........................] - ETA: 16:51 - loss: 1.2696 - regression_loss: 1.0669 - classification_loss: 0.2027
 256/1500 [====>.........................] - ETA: 16:49 - loss: 1.2725 - regression_loss: 1.0697 - classification_loss: 0.2028
 257/1500 [====>.........................] - ETA: 16:48 - loss: 1.2730 - regression_loss: 1.0703 - classification_loss: 0.2027
 258/1500 [====>.........................] - ETA: 16:49 - loss: 1.2710 - regression_loss: 1.0686 - classification_loss: 0.2024
 259/1500 [====>.........................] - ETA: 16:45 - loss: 1.2688 - regression_loss: 1.0665 - classification_loss: 0.2022
 260/1500 [====>.........................] - ETA: 16:45 - loss: 1.2682 - regression_loss: 1.0662 - classification_loss: 0.2020
 261/1500 [====>.........................] - ETA: 16:43 - loss: 1.2660 - regression_loss: 1.0645 - classification_loss: 0.2015
 262/1500 [====>.........................] - ETA: 16:40 - loss: 1.2649 - regression_loss: 1.0636 - classification_loss: 0.2013
 263/1500 [====>.........................] - ETA: 16:37 - loss: 1.2661 - regression_loss: 1.0639 - classification_loss: 0.2022
 264/1500 [====>.........................] - ETA: 16:37 - loss: 1.2704 - regression_loss: 1.0674 - classification_loss: 0.2030
 265/1500 [====>.........................] - ETA: 16:35 - loss: 1.2739 - regression_loss: 1.0705 - classification_loss: 0.2034
 266/1500 [====>.........................] - ETA: 16:33 - loss: 1.2737 - regression_loss: 1.0703 - classification_loss: 0.2034
 267/1500 [====>.........................] - ETA: 16:33 - loss: 1.2728 - regression_loss: 1.0697 - classification_loss: 0.2030
 268/1500 [====>.........................] - ETA: 16:32 - loss: 1.2758 - regression_loss: 1.0723 - classification_loss: 0.2035
 269/1500 [====>.........................] - ETA: 16:30 - loss: 1.2760 - regression_loss: 1.0707 - classification_loss: 0.2053
 270/1500 [====>.........................] - ETA: 16:29 - loss: 1.2732 - regression_loss: 1.0685 - classification_loss: 0.2046
 271/1500 [====>.........................] - ETA: 16:39 - loss: 1.2710 - regression_loss: 1.0667 - classification_loss: 0.2043
 272/1500 [====>.........................] - ETA: 16:36 - loss: 1.2700 - regression_loss: 1.0660 - classification_loss: 0.2040
 273/1500 [====>.........................] - ETA: 16:35 - loss: 1.2702 - regression_loss: 1.0663 - classification_loss: 0.2039
 274/1500 [====>.........................] - ETA: 16:34 - loss: 1.2682 - regression_loss: 1.0645 - classification_loss: 0.2037
 275/1500 [====>.........................] - ETA: 16:33 - loss: 1.2696 - regression_loss: 1.0657 - classification_loss: 0.2039
 276/1500 [====>.........................] - ETA: 16:37 - loss: 1.2680 - regression_loss: 1.0644 - classification_loss: 0.2036
 277/1500 [====>.........................] - ETA: 16:35 - loss: 1.2708 - regression_loss: 1.0660 - classification_loss: 0.2048
 278/1500 [====>.........................] - ETA: 16:39 - loss: 1.2686 - regression_loss: 1.0644 - classification_loss: 0.2042
 279/1500 [====>.........................] - ETA: 16:39 - loss: 1.2680 - regression_loss: 1.0641 - classification_loss: 0.2039
 280/1500 [====>.........................] - ETA: 16:37 - loss: 1.2742 - regression_loss: 1.0693 - classification_loss: 0.2049
 281/1500 [====>.........................] - ETA: 16:34 - loss: 1.2714 - regression_loss: 1.0671 - classification_loss: 0.2043
 282/1500 [====>.........................] - ETA: 16:36 - loss: 1.2710 - regression_loss: 1.0668 - classification_loss: 0.2042
 283/1500 [====>.........................] - ETA: 16:33 - loss: 1.2742 - regression_loss: 1.0701 - classification_loss: 0.2041
 284/1500 [====>.........................] - ETA: 16:32 - loss: 1.2730 - regression_loss: 1.0691 - classification_loss: 0.2039
 285/1500 [====>.........................] - ETA: 16:32 - loss: 1.2716 - regression_loss: 1.0677 - classification_loss: 0.2039
 286/1500 [====>.........................] - ETA: 16:30 - loss: 1.2708 - regression_loss: 1.0671 - classification_loss: 0.2038
 287/1500 [====>.........................] - ETA: 16:32 - loss: 1.2737 - regression_loss: 1.0696 - classification_loss: 0.2041
 288/1500 [====>.........................] - ETA: 16:30 - loss: 1.2716 - regression_loss: 1.0680 - classification_loss: 0.2036
 289/1500 [====>.........................] - ETA: 16:33 - loss: 1.2694 - regression_loss: 1.0662 - classification_loss: 0.2032
 290/1500 [====>.........................] - ETA: 16:31 - loss: 1.2672 - regression_loss: 1.0645 - classification_loss: 0.2027
 291/1500 [====>.........................] - ETA: 16:31 - loss: 1.2671 - regression_loss: 1.0643 - classification_loss: 0.2028
 292/1500 [====>.........................] - ETA: 16:31 - loss: 1.2661 - regression_loss: 1.0631 - classification_loss: 0.2030
 293/1500 [====>.........................] - ETA: 16:28 - loss: 1.2654 - regression_loss: 1.0627 - classification_loss: 0.2028
 294/1500 [====>.........................] - ETA: 16:27 - loss: 1.2668 - regression_loss: 1.0640 - classification_loss: 0.2028
 295/1500 [====>.........................] - ETA: 16:24 - loss: 1.2677 - regression_loss: 1.0651 - classification_loss: 0.2026
 296/1500 [====>.........................] - ETA: 16:22 - loss: 1.2651 - regression_loss: 1.0629 - classification_loss: 0.2022
 297/1500 [====>.........................] - ETA: 16:20 - loss: 1.2670 - regression_loss: 1.0645 - classification_loss: 0.2025
 298/1500 [====>.........................] - ETA: 16:23 - loss: 1.2679 - regression_loss: 1.0654 - classification_loss: 0.2025
 299/1500 [====>.........................] - ETA: 16:20 - loss: 1.2654 - regression_loss: 1.0634 - classification_loss: 0.2020
 300/1500 [=====>........................] - ETA: 16:18 - loss: 1.2674 - regression_loss: 1.0654 - classification_loss: 0.2020
 301/1500 [=====>........................] - ETA: 16:16 - loss: 1.2666 - regression_loss: 1.0650 - classification_loss: 0.2016
 302/1500 [=====>........................] - ETA: 16:15 - loss: 1.2642 - regression_loss: 1.0631 - classification_loss: 0.2011
 303/1500 [=====>........................] - ETA: 16:13 - loss: 1.2648 - regression_loss: 1.0637 - classification_loss: 0.2012
 304/1500 [=====>........................] - ETA: 16:11 - loss: 1.2651 - regression_loss: 1.0642 - classification_loss: 0.2009
 305/1500 [=====>........................] - ETA: 16:09 - loss: 1.2684 - regression_loss: 1.0664 - classification_loss: 0.2020
 306/1500 [=====>........................] - ETA: 16:10 - loss: 1.2681 - regression_loss: 1.0665 - classification_loss: 0.2016
 307/1500 [=====>........................] - ETA: 16:10 - loss: 1.2658 - regression_loss: 1.0647 - classification_loss: 0.2012
 308/1500 [=====>........................] - ETA: 16:10 - loss: 1.2653 - regression_loss: 1.0640 - classification_loss: 0.2012
 309/1500 [=====>........................] - ETA: 16:07 - loss: 1.2643 - regression_loss: 1.0632 - classification_loss: 0.2011
 310/1500 [=====>........................] - ETA: 16:07 - loss: 1.2657 - regression_loss: 1.0644 - classification_loss: 0.2013
 311/1500 [=====>........................] - ETA: 16:06 - loss: 1.2680 - regression_loss: 1.0659 - classification_loss: 0.2022
 312/1500 [=====>........................] - ETA: 16:07 - loss: 1.2691 - regression_loss: 1.0668 - classification_loss: 0.2022
 313/1500 [=====>........................] - ETA: 16:05 - loss: 1.2689 - regression_loss: 1.0670 - classification_loss: 0.2019
 314/1500 [=====>........................] - ETA: 16:06 - loss: 1.2675 - regression_loss: 1.0659 - classification_loss: 0.2016
 315/1500 [=====>........................] - ETA: 16:04 - loss: 1.2653 - regression_loss: 1.0642 - classification_loss: 0.2011
 316/1500 [=====>........................] - ETA: 16:01 - loss: 1.2640 - regression_loss: 1.0632 - classification_loss: 0.2008
 317/1500 [=====>........................] - ETA: 15:59 - loss: 1.2636 - regression_loss: 1.0628 - classification_loss: 0.2008
 318/1500 [=====>........................] - ETA: 15:59 - loss: 1.2655 - regression_loss: 1.0623 - classification_loss: 0.2032
 319/1500 [=====>........................] - ETA: 15:57 - loss: 1.2651 - regression_loss: 1.0623 - classification_loss: 0.2028
 320/1500 [=====>........................] - ETA: 15:56 - loss: 1.2634 - regression_loss: 1.0609 - classification_loss: 0.2026
 321/1500 [=====>........................] - ETA: 15:53 - loss: 1.2683 - regression_loss: 1.0645 - classification_loss: 0.2038
 322/1500 [=====>........................] - ETA: 15:51 - loss: 1.2654 - regression_loss: 1.0620 - classification_loss: 0.2033
 323/1500 [=====>........................] - ETA: 15:51 - loss: 1.2635 - regression_loss: 1.0606 - classification_loss: 0.2029
 324/1500 [=====>........................] - ETA: 15:49 - loss: 1.2611 - regression_loss: 1.0584 - classification_loss: 0.2028
 325/1500 [=====>........................] - ETA: 15:47 - loss: 1.2588 - regression_loss: 1.0560 - classification_loss: 0.2028
 326/1500 [=====>........................] - ETA: 15:46 - loss: 1.2578 - regression_loss: 1.0554 - classification_loss: 0.2024
 327/1500 [=====>........................] - ETA: 15:44 - loss: 1.2577 - regression_loss: 1.0557 - classification_loss: 0.2020
 328/1500 [=====>........................] - ETA: 15:48 - loss: 1.2583 - regression_loss: 1.0561 - classification_loss: 0.2022
 329/1500 [=====>........................] - ETA: 15:46 - loss: 1.2589 - regression_loss: 1.0568 - classification_loss: 0.2020
 330/1500 [=====>........................] - ETA: 15:49 - loss: 1.2606 - regression_loss: 1.0579 - classification_loss: 0.2027
 331/1500 [=====>........................] - ETA: 15:49 - loss: 1.2605 - regression_loss: 1.0581 - classification_loss: 0.2024
 332/1500 [=====>........................] - ETA: 15:47 - loss: 1.2592 - regression_loss: 1.0572 - classification_loss: 0.2020
 333/1500 [=====>........................] - ETA: 15:45 - loss: 1.2584 - regression_loss: 1.0566 - classification_loss: 0.2019
 334/1500 [=====>........................] - ETA: 15:44 - loss: 1.2604 - regression_loss: 1.0578 - classification_loss: 0.2025
 335/1500 [=====>........................] - ETA: 15:43 - loss: 1.2591 - regression_loss: 1.0567 - classification_loss: 0.2024
 336/1500 [=====>........................] - ETA: 15:41 - loss: 1.2568 - regression_loss: 1.0549 - classification_loss: 0.2020
 337/1500 [=====>........................] - ETA: 15:43 - loss: 1.2582 - regression_loss: 1.0560 - classification_loss: 0.2022
 338/1500 [=====>........................] - ETA: 15:45 - loss: 1.2558 - regression_loss: 1.0539 - classification_loss: 0.2019
 339/1500 [=====>........................] - ETA: 15:43 - loss: 1.2575 - regression_loss: 1.0553 - classification_loss: 0.2022
 340/1500 [=====>........................] - ETA: 15:41 - loss: 1.2555 - regression_loss: 1.0535 - classification_loss: 0.2020
 341/1500 [=====>........................] - ETA: 15:40 - loss: 1.2550 - regression_loss: 1.0531 - classification_loss: 0.2019
 342/1500 [=====>........................] - ETA: 15:38 - loss: 1.2528 - regression_loss: 1.0514 - classification_loss: 0.2014
 343/1500 [=====>........................] - ETA: 15:37 - loss: 1.2531 - regression_loss: 1.0519 - classification_loss: 0.2012
 344/1500 [=====>........................] - ETA: 15:36 - loss: 1.2521 - regression_loss: 1.0512 - classification_loss: 0.2009
 345/1500 [=====>........................] - ETA: 15:35 - loss: 1.2510 - regression_loss: 1.0504 - classification_loss: 0.2006
 346/1500 [=====>........................] - ETA: 15:40 - loss: 1.2524 - regression_loss: 1.0505 - classification_loss: 0.2020
 347/1500 [=====>........................] - ETA: 15:37 - loss: 1.2574 - regression_loss: 1.0540 - classification_loss: 0.2033
 348/1500 [=====>........................] - ETA: 15:36 - loss: 1.2581 - regression_loss: 1.0549 - classification_loss: 0.2032
 349/1500 [=====>........................] - ETA: 15:34 - loss: 1.2554 - regression_loss: 1.0526 - classification_loss: 0.2028
 350/1500 [======>.......................] - ETA: 15:33 - loss: 1.2531 - regression_loss: 1.0507 - classification_loss: 0.2024
 351/1500 [======>.......................] - ETA: 15:32 - loss: 1.2519 - regression_loss: 1.0498 - classification_loss: 0.2020
 352/1500 [======>.......................] - ETA: 15:31 - loss: 1.2516 - regression_loss: 1.0498 - classification_loss: 0.2017
 353/1500 [======>.......................] - ETA: 15:29 - loss: 1.2506 - regression_loss: 1.0491 - classification_loss: 0.2015
 354/1500 [======>.......................] - ETA: 15:30 - loss: 1.2485 - regression_loss: 1.0474 - classification_loss: 0.2011
 355/1500 [======>.......................] - ETA: 15:28 - loss: 1.2475 - regression_loss: 1.0465 - classification_loss: 0.2009
 356/1500 [======>.......................] - ETA: 15:26 - loss: 1.2450 - regression_loss: 1.0446 - classification_loss: 0.2005
 357/1500 [======>.......................] - ETA: 15:25 - loss: 1.2447 - regression_loss: 1.0444 - classification_loss: 0.2003
 358/1500 [======>.......................] - ETA: 15:23 - loss: 1.2433 - regression_loss: 1.0432 - classification_loss: 0.2001
 359/1500 [======>.......................] - ETA: 15:26 - loss: 1.2414 - regression_loss: 1.0417 - classification_loss: 0.1997
 360/1500 [======>.......................] - ETA: 15:26 - loss: 1.2406 - regression_loss: 1.0412 - classification_loss: 0.1995
 361/1500 [======>.......................] - ETA: 15:26 - loss: 1.2398 - regression_loss: 1.0407 - classification_loss: 0.1991
 362/1500 [======>.......................] - ETA: 15:24 - loss: 1.2390 - regression_loss: 1.0403 - classification_loss: 0.1987
 363/1500 [======>.......................] - ETA: 15:25 - loss: 1.2397 - regression_loss: 1.0410 - classification_loss: 0.1987
 364/1500 [======>.......................] - ETA: 15:24 - loss: 1.2377 - regression_loss: 1.0393 - classification_loss: 0.1984
 365/1500 [======>.......................] - ETA: 15:23 - loss: 1.2387 - regression_loss: 1.0405 - classification_loss: 0.1982
 366/1500 [======>.......................] - ETA: 15:21 - loss: 1.2373 - regression_loss: 1.0394 - classification_loss: 0.1979
 367/1500 [======>.......................] - ETA: 15:20 - loss: 1.2349 - regression_loss: 1.0375 - classification_loss: 0.1974
 368/1500 [======>.......................] - ETA: 15:18 - loss: 1.2343 - regression_loss: 1.0371 - classification_loss: 0.1972
 369/1500 [======>.......................] - ETA: 15:25 - loss: 1.2362 - regression_loss: 1.0387 - classification_loss: 0.1975
 370/1500 [======>.......................] - ETA: 15:24 - loss: 1.2346 - regression_loss: 1.0374 - classification_loss: 0.1972
 371/1500 [======>.......................] - ETA: 15:22 - loss: 1.2326 - regression_loss: 1.0358 - classification_loss: 0.1968
 372/1500 [======>.......................] - ETA: 15:21 - loss: 1.2347 - regression_loss: 1.0378 - classification_loss: 0.1968
 373/1500 [======>.......................] - ETA: 15:20 - loss: 1.2335 - regression_loss: 1.0371 - classification_loss: 0.1965
 374/1500 [======>.......................] - ETA: 15:19 - loss: 1.2326 - regression_loss: 1.0364 - classification_loss: 0.1962
 375/1500 [======>.......................] - ETA: 15:17 - loss: 1.2323 - regression_loss: 1.0361 - classification_loss: 0.1961
 376/1500 [======>.......................] - ETA: 15:18 - loss: 1.2313 - regression_loss: 1.0354 - classification_loss: 0.1959
 377/1500 [======>.......................] - ETA: 15:18 - loss: 1.2317 - regression_loss: 1.0361 - classification_loss: 0.1956
 378/1500 [======>.......................] - ETA: 15:16 - loss: 1.2325 - regression_loss: 1.0366 - classification_loss: 0.1960
 379/1500 [======>.......................] - ETA: 15:15 - loss: 1.2333 - regression_loss: 1.0375 - classification_loss: 0.1958
 380/1500 [======>.......................] - ETA: 15:14 - loss: 1.2322 - regression_loss: 1.0367 - classification_loss: 0.1955
 381/1500 [======>.......................] - ETA: 15:12 - loss: 1.2308 - regression_loss: 1.0356 - classification_loss: 0.1952
 382/1500 [======>.......................] - ETA: 15:10 - loss: 1.2296 - regression_loss: 1.0347 - classification_loss: 0.1949
 383/1500 [======>.......................] - ETA: 15:10 - loss: 1.2304 - regression_loss: 1.0350 - classification_loss: 0.1954
 384/1500 [======>.......................] - ETA: 15:12 - loss: 1.2308 - regression_loss: 1.0356 - classification_loss: 0.1953
 385/1500 [======>.......................] - ETA: 15:11 - loss: 1.2291 - regression_loss: 1.0341 - classification_loss: 0.1950
 386/1500 [======>.......................] - ETA: 15:11 - loss: 1.2278 - regression_loss: 1.0332 - classification_loss: 0.1946
 387/1500 [======>.......................] - ETA: 15:10 - loss: 1.2268 - regression_loss: 1.0326 - classification_loss: 0.1942
 388/1500 [======>.......................] - ETA: 15:08 - loss: 1.2263 - regression_loss: 1.0322 - classification_loss: 0.1941
 389/1500 [======>.......................] - ETA: 15:07 - loss: 1.2252 - regression_loss: 1.0313 - classification_loss: 0.1940
 390/1500 [======>.......................] - ETA: 15:09 - loss: 1.2273 - regression_loss: 1.0327 - classification_loss: 0.1946
 391/1500 [======>.......................] - ETA: 15:08 - loss: 1.2283 - regression_loss: 1.0336 - classification_loss: 0.1948
 392/1500 [======>.......................] - ETA: 15:07 - loss: 1.2294 - regression_loss: 1.0344 - classification_loss: 0.1950
 393/1500 [======>.......................] - ETA: 15:05 - loss: 1.2287 - regression_loss: 1.0338 - classification_loss: 0.1949
 394/1500 [======>.......................] - ETA: 15:03 - loss: 1.2269 - regression_loss: 1.0324 - classification_loss: 0.1945
 395/1500 [======>.......................] - ETA: 15:01 - loss: 1.2259 - regression_loss: 1.0318 - classification_loss: 0.1942
 396/1500 [======>.......................] - ETA: 14:59 - loss: 1.2248 - regression_loss: 1.0308 - classification_loss: 0.1940
 397/1500 [======>.......................] - ETA: 14:57 - loss: 1.2239 - regression_loss: 1.0302 - classification_loss: 0.1937
 398/1500 [======>.......................] - ETA: 14:57 - loss: 1.2237 - regression_loss: 1.0302 - classification_loss: 0.1935
 399/1500 [======>.......................] - ETA: 14:56 - loss: 1.2237 - regression_loss: 1.0302 - classification_loss: 0.1935
 400/1500 [=======>......................] - ETA: 14:55 - loss: 1.2232 - regression_loss: 1.0299 - classification_loss: 0.1933
 401/1500 [=======>......................] - ETA: 14:53 - loss: 1.2225 - regression_loss: 1.0294 - classification_loss: 0.1931
 402/1500 [=======>......................] - ETA: 14:51 - loss: 1.2233 - regression_loss: 1.0299 - classification_loss: 0.1933
 403/1500 [=======>......................] - ETA: 14:53 - loss: 1.2215 - regression_loss: 1.0285 - classification_loss: 0.1931
 404/1500 [=======>......................] - ETA: 14:52 - loss: 1.2201 - regression_loss: 1.0273 - classification_loss: 0.1928
 405/1500 [=======>......................] - ETA: 14:50 - loss: 1.2201 - regression_loss: 1.0274 - classification_loss: 0.1927
 406/1500 [=======>......................] - ETA: 14:48 - loss: 1.2195 - regression_loss: 1.0266 - classification_loss: 0.1929
 407/1500 [=======>......................] - ETA: 14:46 - loss: 1.2184 - regression_loss: 1.0255 - classification_loss: 0.1929
 408/1500 [=======>......................] - ETA: 14:46 - loss: 1.2178 - regression_loss: 1.0250 - classification_loss: 0.1928
 409/1500 [=======>......................] - ETA: 14:46 - loss: 1.2167 - regression_loss: 1.0242 - classification_loss: 0.1925
 410/1500 [=======>......................] - ETA: 14:45 - loss: 1.2165 - regression_loss: 1.0239 - classification_loss: 0.1926
 411/1500 [=======>......................] - ETA: 14:44 - loss: 1.2192 - regression_loss: 1.0256 - classification_loss: 0.1935
 412/1500 [=======>......................] - ETA: 14:44 - loss: 1.2203 - regression_loss: 1.0266 - classification_loss: 0.1937
 413/1500 [=======>......................] - ETA: 14:43 - loss: 1.2195 - regression_loss: 1.0261 - classification_loss: 0.1934
 414/1500 [=======>......................] - ETA: 14:41 - loss: 1.2192 - regression_loss: 1.0260 - classification_loss: 0.1932
 415/1500 [=======>......................] - ETA: 14:42 - loss: 1.2207 - regression_loss: 1.0274 - classification_loss: 0.1932
 416/1500 [=======>......................] - ETA: 14:40 - loss: 1.2207 - regression_loss: 1.0276 - classification_loss: 0.1930
 417/1500 [=======>......................] - ETA: 14:38 - loss: 1.2189 - regression_loss: 1.0262 - classification_loss: 0.1927
 418/1500 [=======>......................] - ETA: 14:38 - loss: 1.2185 - regression_loss: 1.0259 - classification_loss: 0.1926
 419/1500 [=======>......................] - ETA: 14:38 - loss: 1.2173 - regression_loss: 1.0247 - classification_loss: 0.1926
 420/1500 [=======>......................] - ETA: 14:36 - loss: 1.2156 - regression_loss: 1.0233 - classification_loss: 0.1922
 421/1500 [=======>......................] - ETA: 14:35 - loss: 1.2144 - regression_loss: 1.0225 - classification_loss: 0.1919
 422/1500 [=======>......................] - ETA: 14:33 - loss: 1.2126 - regression_loss: 1.0209 - classification_loss: 0.1917
 423/1500 [=======>......................] - ETA: 14:31 - loss: 1.2119 - regression_loss: 1.0204 - classification_loss: 0.1915
 424/1500 [=======>......................] - ETA: 14:33 - loss: 1.2112 - regression_loss: 1.0200 - classification_loss: 0.1912
 425/1500 [=======>......................] - ETA: 14:31 - loss: 1.2124 - regression_loss: 1.0212 - classification_loss: 0.1913
 426/1500 [=======>......................] - ETA: 14:29 - loss: 1.2119 - regression_loss: 1.0207 - classification_loss: 0.1912
 427/1500 [=======>......................] - ETA: 14:29 - loss: 1.2119 - regression_loss: 1.0209 - classification_loss: 0.1910
 428/1500 [=======>......................] - ETA: 14:27 - loss: 1.2112 - regression_loss: 1.0203 - classification_loss: 0.1909
 429/1500 [=======>......................] - ETA: 14:26 - loss: 1.2121 - regression_loss: 1.0210 - classification_loss: 0.1912
 430/1500 [=======>......................] - ETA: 14:24 - loss: 1.2123 - regression_loss: 1.0212 - classification_loss: 0.1911
 431/1500 [=======>......................] - ETA: 14:25 - loss: 1.2118 - regression_loss: 1.0207 - classification_loss: 0.1910
 432/1500 [=======>......................] - ETA: 14:24 - loss: 1.2128 - regression_loss: 1.0214 - classification_loss: 0.1915
 433/1500 [=======>......................] - ETA: 14:25 - loss: 1.2133 - regression_loss: 1.0219 - classification_loss: 0.1914
 434/1500 [=======>......................] - ETA: 14:29 - loss: 1.2123 - regression_loss: 1.0210 - classification_loss: 0.1913
 435/1500 [=======>......................] - ETA: 14:27 - loss: 1.2114 - regression_loss: 1.0201 - classification_loss: 0.1912
 436/1500 [=======>......................] - ETA: 14:28 - loss: 1.2093 - regression_loss: 1.0184 - classification_loss: 0.1909
 437/1500 [=======>......................] - ETA: 14:26 - loss: 1.2088 - regression_loss: 1.0178 - classification_loss: 0.1910
 438/1500 [=======>......................] - ETA: 14:24 - loss: 1.2082 - regression_loss: 1.0174 - classification_loss: 0.1908
 439/1500 [=======>......................] - ETA: 14:23 - loss: 1.2083 - regression_loss: 1.0176 - classification_loss: 0.1907
 440/1500 [=======>......................] - ETA: 14:21 - loss: 1.2076 - regression_loss: 1.0170 - classification_loss: 0.1906
 441/1500 [=======>......................] - ETA: 14:20 - loss: 1.2103 - regression_loss: 1.0175 - classification_loss: 0.1928
 442/1500 [=======>......................] - ETA: 14:18 - loss: 1.2092 - regression_loss: 1.0166 - classification_loss: 0.1927
 443/1500 [=======>......................] - ETA: 14:18 - loss: 1.2092 - regression_loss: 1.0166 - classification_loss: 0.1926
 444/1500 [=======>......................] - ETA: 14:17 - loss: 1.2093 - regression_loss: 1.0166 - classification_loss: 0.1927
 445/1500 [=======>......................] - ETA: 14:15 - loss: 1.2107 - regression_loss: 1.0179 - classification_loss: 0.1928
 446/1500 [=======>......................] - ETA: 14:17 - loss: 1.2120 - regression_loss: 1.0191 - classification_loss: 0.1930
 447/1500 [=======>......................] - ETA: 14:19 - loss: 1.2140 - regression_loss: 1.0205 - classification_loss: 0.1935
 448/1500 [=======>......................] - ETA: 14:17 - loss: 1.2148 - regression_loss: 1.0214 - classification_loss: 0.1934
 449/1500 [=======>......................] - ETA: 14:15 - loss: 1.2140 - regression_loss: 1.0207 - classification_loss: 0.1933
 450/1500 [========>.....................] - ETA: 14:14 - loss: 1.2136 - regression_loss: 1.0202 - classification_loss: 0.1934
 451/1500 [========>.....................] - ETA: 14:12 - loss: 1.2138 - regression_loss: 1.0206 - classification_loss: 0.1932
 452/1500 [========>.....................] - ETA: 14:11 - loss: 1.2129 - regression_loss: 1.0200 - classification_loss: 0.1929
 453/1500 [========>.....................] - ETA: 14:09 - loss: 1.2113 - regression_loss: 1.0186 - classification_loss: 0.1927
 454/1500 [========>.....................] - ETA: 14:07 - loss: 1.2104 - regression_loss: 1.0178 - classification_loss: 0.1925
 455/1500 [========>.....................] - ETA: 14:06 - loss: 1.2104 - regression_loss: 1.0177 - classification_loss: 0.1927
 456/1500 [========>.....................] - ETA: 14:04 - loss: 1.2091 - regression_loss: 1.0167 - classification_loss: 0.1924
 457/1500 [========>.....................] - ETA: 14:06 - loss: 1.2079 - regression_loss: 1.0158 - classification_loss: 0.1921
 458/1500 [========>.....................] - ETA: 14:04 - loss: 1.2082 - regression_loss: 1.0162 - classification_loss: 0.1920
 459/1500 [========>.....................] - ETA: 14:03 - loss: 1.2068 - regression_loss: 1.0152 - classification_loss: 0.1917
 460/1500 [========>.....................] - ETA: 14:03 - loss: 1.2064 - regression_loss: 1.0148 - classification_loss: 0.1916
 461/1500 [========>.....................] - ETA: 14:04 - loss: 1.2062 - regression_loss: 1.0145 - classification_loss: 0.1916
 462/1500 [========>.....................] - ETA: 14:03 - loss: 1.2049 - regression_loss: 1.0134 - classification_loss: 0.1914
 463/1500 [========>.....................] - ETA: 14:01 - loss: 1.2057 - regression_loss: 1.0144 - classification_loss: 0.1913
 464/1500 [========>.....................] - ETA: 13:59 - loss: 1.2053 - regression_loss: 1.0140 - classification_loss: 0.1913
 465/1500 [========>.....................] - ETA: 13:57 - loss: 1.2054 - regression_loss: 1.0142 - classification_loss: 0.1913
 466/1500 [========>.....................] - ETA: 13:56 - loss: 1.2049 - regression_loss: 1.0138 - classification_loss: 0.1911
 467/1500 [========>.....................] - ETA: 13:54 - loss: 1.2042 - regression_loss: 1.0134 - classification_loss: 0.1908
 468/1500 [========>.....................] - ETA: 13:53 - loss: 1.2043 - regression_loss: 1.0133 - classification_loss: 0.1909
 469/1500 [========>.....................] - ETA: 13:53 - loss: 1.2047 - regression_loss: 1.0138 - classification_loss: 0.1909
 470/1500 [========>.....................] - ETA: 13:55 - loss: 1.2048 - regression_loss: 1.0139 - classification_loss: 0.1909
 471/1500 [========>.....................] - ETA: 13:53 - loss: 1.2069 - regression_loss: 1.0156 - classification_loss: 0.1913
 472/1500 [========>.....................] - ETA: 13:52 - loss: 1.2065 - regression_loss: 1.0152 - classification_loss: 0.1913
 473/1500 [========>.....................] - ETA: 13:52 - loss: 1.2048 - regression_loss: 1.0138 - classification_loss: 0.1910
 474/1500 [========>.....................] - ETA: 13:51 - loss: 1.2037 - regression_loss: 1.0129 - classification_loss: 0.1907
 475/1500 [========>.....................] - ETA: 13:50 - loss: 1.2026 - regression_loss: 1.0122 - classification_loss: 0.1905
 476/1500 [========>.....................] - ETA: 13:49 - loss: 1.2023 - regression_loss: 1.0121 - classification_loss: 0.1903
 477/1500 [========>.....................] - ETA: 13:47 - loss: 1.2018 - regression_loss: 1.0117 - classification_loss: 0.1902
 478/1500 [========>.....................] - ETA: 13:46 - loss: 1.2007 - regression_loss: 1.0108 - classification_loss: 0.1899
 479/1500 [========>.....................] - ETA: 13:45 - loss: 1.1995 - regression_loss: 1.0098 - classification_loss: 0.1897
 480/1500 [========>.....................] - ETA: 13:47 - loss: 1.2007 - regression_loss: 1.0112 - classification_loss: 0.1895
 481/1500 [========>.....................] - ETA: 13:46 - loss: 1.2000 - regression_loss: 1.0108 - classification_loss: 0.1893
 482/1500 [========>.....................] - ETA: 13:47 - loss: 1.2004 - regression_loss: 1.0110 - classification_loss: 0.1894
 483/1500 [========>.....................] - ETA: 13:46 - loss: 1.2006 - regression_loss: 1.0111 - classification_loss: 0.1894
 484/1500 [========>.....................] - ETA: 13:44 - loss: 1.2023 - regression_loss: 1.0127 - classification_loss: 0.1896
 485/1500 [========>.....................] - ETA: 13:43 - loss: 1.2018 - regression_loss: 1.0122 - classification_loss: 0.1895
 486/1500 [========>.....................] - ETA: 13:42 - loss: 1.2023 - regression_loss: 1.0128 - classification_loss: 0.1895
 487/1500 [========>.....................] - ETA: 13:41 - loss: 1.2017 - regression_loss: 1.0123 - classification_loss: 0.1894
 488/1500 [========>.....................] - ETA: 13:39 - loss: 1.2011 - regression_loss: 1.0119 - classification_loss: 0.1892
 489/1500 [========>.....................] - ETA: 13:40 - loss: 1.1999 - regression_loss: 1.0108 - classification_loss: 0.1890
 490/1500 [========>.....................] - ETA: 13:39 - loss: 1.2017 - regression_loss: 1.0114 - classification_loss: 0.1903
 491/1500 [========>.....................] - ETA: 13:37 - loss: 1.2030 - regression_loss: 1.0126 - classification_loss: 0.1904
 492/1500 [========>.....................] - ETA: 13:36 - loss: 1.2034 - regression_loss: 1.0129 - classification_loss: 0.1904
 493/1500 [========>.....................] - ETA: 13:36 - loss: 1.2027 - regression_loss: 1.0125 - classification_loss: 0.1902
 494/1500 [========>.....................] - ETA: 13:35 - loss: 1.2044 - regression_loss: 1.0139 - classification_loss: 0.1904
 495/1500 [========>.....................] - ETA: 13:34 - loss: 1.2029 - regression_loss: 1.0128 - classification_loss: 0.1901
 496/1500 [========>.....................] - ETA: 13:33 - loss: 1.2015 - regression_loss: 1.0116 - classification_loss: 0.1898
 497/1500 [========>.....................] - ETA: 13:32 - loss: 1.2017 - regression_loss: 1.0118 - classification_loss: 0.1899
 498/1500 [========>.....................] - ETA: 13:32 - loss: 1.2051 - regression_loss: 1.0144 - classification_loss: 0.1908
 499/1500 [========>.....................] - ETA: 13:31 - loss: 1.2050 - regression_loss: 1.0143 - classification_loss: 0.1906
 500/1500 [=========>....................] - ETA: 13:30 - loss: 1.2050 - regression_loss: 1.0144 - classification_loss: 0.1906
 501/1500 [=========>....................] - ETA: 13:29 - loss: 1.2053 - regression_loss: 1.0146 - classification_loss: 0.1907
 502/1500 [=========>....................] - ETA: 13:31 - loss: 1.2060 - regression_loss: 1.0152 - classification_loss: 0.1908
 503/1500 [=========>....................] - ETA: 13:29 - loss: 1.2044 - regression_loss: 1.0140 - classification_loss: 0.1904
 504/1500 [=========>....................] - ETA: 13:28 - loss: 1.2046 - regression_loss: 1.0142 - classification_loss: 0.1903
 505/1500 [=========>....................] - ETA: 13:29 - loss: 1.2039 - regression_loss: 1.0138 - classification_loss: 0.1901
 506/1500 [=========>....................] - ETA: 13:27 - loss: 1.2051 - regression_loss: 1.0147 - classification_loss: 0.1904
 507/1500 [=========>....................] - ETA: 13:26 - loss: 1.2043 - regression_loss: 1.0140 - classification_loss: 0.1903
 508/1500 [=========>....................] - ETA: 13:24 - loss: 1.2039 - regression_loss: 1.0139 - classification_loss: 0.1901
 509/1500 [=========>....................] - ETA: 13:23 - loss: 1.2036 - regression_loss: 1.0133 - classification_loss: 0.1902
 510/1500 [=========>....................] - ETA: 13:24 - loss: 1.2030 - regression_loss: 1.0129 - classification_loss: 0.1901
 511/1500 [=========>....................] - ETA: 13:22 - loss: 1.2055 - regression_loss: 1.0149 - classification_loss: 0.1906
 512/1500 [=========>....................] - ETA: 13:21 - loss: 1.2046 - regression_loss: 1.0141 - classification_loss: 0.1905
 513/1500 [=========>....................] - ETA: 13:21 - loss: 1.2046 - regression_loss: 1.0143 - classification_loss: 0.1903
 514/1500 [=========>....................] - ETA: 13:20 - loss: 1.2036 - regression_loss: 1.0135 - classification_loss: 0.1901
 515/1500 [=========>....................] - ETA: 13:19 - loss: 1.2052 - regression_loss: 1.0147 - classification_loss: 0.1905
 516/1500 [=========>....................] - ETA: 13:18 - loss: 1.2050 - regression_loss: 1.0146 - classification_loss: 0.1904
 517/1500 [=========>....................] - ETA: 13:16 - loss: 1.2047 - regression_loss: 1.0145 - classification_loss: 0.1902
 518/1500 [=========>....................] - ETA: 13:17 - loss: 1.2056 - regression_loss: 1.0152 - classification_loss: 0.1904
 519/1500 [=========>....................] - ETA: 13:15 - loss: 1.2043 - regression_loss: 1.0141 - classification_loss: 0.1901
 520/1500 [=========>....................] - ETA: 13:15 - loss: 1.2042 - regression_loss: 1.0141 - classification_loss: 0.1900
 521/1500 [=========>....................] - ETA: 13:13 - loss: 1.2027 - regression_loss: 1.0129 - classification_loss: 0.1898
 522/1500 [=========>....................] - ETA: 13:12 - loss: 1.2019 - regression_loss: 1.0124 - classification_loss: 0.1895
 523/1500 [=========>....................] - ETA: 13:10 - loss: 1.2031 - regression_loss: 1.0136 - classification_loss: 0.1895
 524/1500 [=========>....................] - ETA: 13:09 - loss: 1.2047 - regression_loss: 1.0149 - classification_loss: 0.1899
 525/1500 [=========>....................] - ETA: 13:07 - loss: 1.2044 - regression_loss: 1.0147 - classification_loss: 0.1897
 526/1500 [=========>....................] - ETA: 13:06 - loss: 1.2028 - regression_loss: 1.0134 - classification_loss: 0.1895
 527/1500 [=========>....................] - ETA: 13:04 - loss: 1.2025 - regression_loss: 1.0132 - classification_loss: 0.1893
 528/1500 [=========>....................] - ETA: 13:03 - loss: 1.2033 - regression_loss: 1.0140 - classification_loss: 0.1893
 529/1500 [=========>....................] - ETA: 13:02 - loss: 1.2024 - regression_loss: 1.0133 - classification_loss: 0.1892
 530/1500 [=========>....................] - ETA: 13:02 - loss: 1.2023 - regression_loss: 1.0130 - classification_loss: 0.1893
 531/1500 [=========>....................] - ETA: 13:00 - loss: 1.2025 - regression_loss: 1.0132 - classification_loss: 0.1893
 532/1500 [=========>....................] - ETA: 13:00 - loss: 1.2011 - regression_loss: 1.0121 - classification_loss: 0.1890
 533/1500 [=========>....................] - ETA: 12:59 - loss: 1.1999 - regression_loss: 1.0111 - classification_loss: 0.1888
 534/1500 [=========>....................] - ETA: 12:58 - loss: 1.1989 - regression_loss: 1.0103 - classification_loss: 0.1886
 535/1500 [=========>....................] - ETA: 12:57 - loss: 1.1994 - regression_loss: 1.0107 - classification_loss: 0.1887
 536/1500 [=========>....................] - ETA: 12:57 - loss: 1.1986 - regression_loss: 1.0101 - classification_loss: 0.1886
 537/1500 [=========>....................] - ETA: 12:56 - loss: 1.1981 - regression_loss: 1.0096 - classification_loss: 0.1885
 538/1500 [=========>....................] - ETA: 12:54 - loss: 1.1983 - regression_loss: 1.0098 - classification_loss: 0.1885
 539/1500 [=========>....................] - ETA: 12:54 - loss: 1.1981 - regression_loss: 1.0097 - classification_loss: 0.1884
 540/1500 [=========>....................] - ETA: 12:57 - loss: 1.2005 - regression_loss: 1.0114 - classification_loss: 0.1891
 541/1500 [=========>....................] - ETA: 12:55 - loss: 1.2005 - regression_loss: 1.0115 - classification_loss: 0.1890
 542/1500 [=========>....................] - ETA: 12:54 - loss: 1.2014 - regression_loss: 1.0124 - classification_loss: 0.1890
 543/1500 [=========>....................] - ETA: 12:52 - loss: 1.2006 - regression_loss: 1.0117 - classification_loss: 0.1889
 544/1500 [=========>....................] - ETA: 12:51 - loss: 1.1997 - regression_loss: 1.0111 - classification_loss: 0.1887
 545/1500 [=========>....................] - ETA: 12:49 - loss: 1.1997 - regression_loss: 1.0110 - classification_loss: 0.1887
 546/1500 [=========>....................] - ETA: 12:48 - loss: 1.2022 - regression_loss: 1.0129 - classification_loss: 0.1893
 547/1500 [=========>....................] - ETA: 12:47 - loss: 1.2023 - regression_loss: 1.0131 - classification_loss: 0.1892
 548/1500 [=========>....................] - ETA: 12:45 - loss: 1.2015 - regression_loss: 1.0125 - classification_loss: 0.1890
 549/1500 [=========>....................] - ETA: 12:45 - loss: 1.2013 - regression_loss: 1.0123 - classification_loss: 0.1890
 550/1500 [==========>...................] - ETA: 12:46 - loss: 1.2011 - regression_loss: 1.0122 - classification_loss: 0.1889
 551/1500 [==========>...................] - ETA: 12:44 - loss: 1.2012 - regression_loss: 1.0122 - classification_loss: 0.1889
 552/1500 [==========>...................] - ETA: 12:45 - loss: 1.2008 - regression_loss: 1.0121 - classification_loss: 0.1887
 553/1500 [==========>...................] - ETA: 12:45 - loss: 1.2032 - regression_loss: 1.0139 - classification_loss: 0.1893
 554/1500 [==========>...................] - ETA: 12:44 - loss: 1.2021 - regression_loss: 1.0130 - classification_loss: 0.1891
 555/1500 [==========>...................] - ETA: 12:42 - loss: 1.2020 - regression_loss: 1.0130 - classification_loss: 0.1890
 556/1500 [==========>...................] - ETA: 12:41 - loss: 1.2028 - regression_loss: 1.0137 - classification_loss: 0.1891
 557/1500 [==========>...................] - ETA: 12:39 - loss: 1.2036 - regression_loss: 1.0145 - classification_loss: 0.1891
 558/1500 [==========>...................] - ETA: 12:38 - loss: 1.2031 - regression_loss: 1.0140 - classification_loss: 0.1891
 559/1500 [==========>...................] - ETA: 12:37 - loss: 1.2023 - regression_loss: 1.0135 - classification_loss: 0.1889
 560/1500 [==========>...................] - ETA: 12:35 - loss: 1.2044 - regression_loss: 1.0150 - classification_loss: 0.1893
 561/1500 [==========>...................] - ETA: 12:34 - loss: 1.2030 - regression_loss: 1.0140 - classification_loss: 0.1891
 562/1500 [==========>...................] - ETA: 12:34 - loss: 1.2031 - regression_loss: 1.0141 - classification_loss: 0.1890
 563/1500 [==========>...................] - ETA: 12:32 - loss: 1.2028 - regression_loss: 1.0139 - classification_loss: 0.1889
 564/1500 [==========>...................] - ETA: 12:31 - loss: 1.2054 - regression_loss: 1.0160 - classification_loss: 0.1894
 565/1500 [==========>...................] - ETA: 12:31 - loss: 1.2046 - regression_loss: 1.0153 - classification_loss: 0.1893
 566/1500 [==========>...................] - ETA: 12:30 - loss: 1.2053 - regression_loss: 1.0159 - classification_loss: 0.1894
 567/1500 [==========>...................] - ETA: 12:29 - loss: 1.2045 - regression_loss: 1.0153 - classification_loss: 0.1892
 568/1500 [==========>...................] - ETA: 12:27 - loss: 1.2056 - regression_loss: 1.0162 - classification_loss: 0.1894
 569/1500 [==========>...................] - ETA: 12:26 - loss: 1.2055 - regression_loss: 1.0163 - classification_loss: 0.1893
 570/1500 [==========>...................] - ETA: 12:24 - loss: 1.2060 - regression_loss: 1.0168 - classification_loss: 0.1892
 571/1500 [==========>...................] - ETA: 12:24 - loss: 1.2061 - regression_loss: 1.0168 - classification_loss: 0.1892
 572/1500 [==========>...................] - ETA: 12:24 - loss: 1.2055 - regression_loss: 1.0165 - classification_loss: 0.1890
 573/1500 [==========>...................] - ETA: 12:23 - loss: 1.2041 - regression_loss: 1.0154 - classification_loss: 0.1888
 574/1500 [==========>...................] - ETA: 12:21 - loss: 1.2034 - regression_loss: 1.0148 - classification_loss: 0.1886
 575/1500 [==========>...................] - ETA: 12:20 - loss: 1.2023 - regression_loss: 1.0139 - classification_loss: 0.1884
 576/1500 [==========>...................] - ETA: 12:20 - loss: 1.2013 - regression_loss: 1.0130 - classification_loss: 0.1883
 577/1500 [==========>...................] - ETA: 12:18 - loss: 1.2006 - regression_loss: 1.0124 - classification_loss: 0.1882
 578/1500 [==========>...................] - ETA: 12:17 - loss: 1.2011 - regression_loss: 1.0129 - classification_loss: 0.1882
 579/1500 [==========>...................] - ETA: 12:17 - loss: 1.2041 - regression_loss: 1.0143 - classification_loss: 0.1898
 580/1500 [==========>...................] - ETA: 12:15 - loss: 1.2038 - regression_loss: 1.0142 - classification_loss: 0.1896
 581/1500 [==========>...................] - ETA: 12:14 - loss: 1.2036 - regression_loss: 1.0140 - classification_loss: 0.1896
 582/1500 [==========>...................] - ETA: 12:13 - loss: 1.2035 - regression_loss: 1.0142 - classification_loss: 0.1893
 583/1500 [==========>...................] - ETA: 12:12 - loss: 1.2024 - regression_loss: 1.0133 - classification_loss: 0.1891
 584/1500 [==========>...................] - ETA: 12:11 - loss: 1.2020 - regression_loss: 1.0131 - classification_loss: 0.1889
 585/1500 [==========>...................] - ETA: 12:10 - loss: 1.2008 - regression_loss: 1.0121 - classification_loss: 0.1887
 586/1500 [==========>...................] - ETA: 12:09 - loss: 1.1995 - regression_loss: 1.0110 - classification_loss: 0.1885
 587/1500 [==========>...................] - ETA: 12:09 - loss: 1.2001 - regression_loss: 1.0115 - classification_loss: 0.1887
 588/1500 [==========>...................] - ETA: 12:08 - loss: 1.2021 - regression_loss: 1.0131 - classification_loss: 0.1890
 589/1500 [==========>...................] - ETA: 12:07 - loss: 1.2024 - regression_loss: 1.0133 - classification_loss: 0.1891
 590/1500 [==========>...................] - ETA: 12:06 - loss: 1.2015 - regression_loss: 1.0127 - classification_loss: 0.1888
 591/1500 [==========>...................] - ETA: 12:06 - loss: 1.2016 - regression_loss: 1.0128 - classification_loss: 0.1888
 592/1500 [==========>...................] - ETA: 12:06 - loss: 1.2006 - regression_loss: 1.0120 - classification_loss: 0.1885
 593/1500 [==========>...................] - ETA: 12:05 - loss: 1.2020 - regression_loss: 1.0130 - classification_loss: 0.1890
 594/1500 [==========>...................] - ETA: 12:05 - loss: 1.2016 - regression_loss: 1.0127 - classification_loss: 0.1889
 595/1500 [==========>...................] - ETA: 12:04 - loss: 1.2001 - regression_loss: 1.0115 - classification_loss: 0.1886
 596/1500 [==========>...................] - ETA: 12:03 - loss: 1.2019 - regression_loss: 1.0129 - classification_loss: 0.1890
 597/1500 [==========>...................] - ETA: 12:04 - loss: 1.2015 - regression_loss: 1.0127 - classification_loss: 0.1888
 598/1500 [==========>...................] - ETA: 12:07 - loss: 1.2034 - regression_loss: 1.0145 - classification_loss: 0.1889
 599/1500 [==========>...................] - ETA: 12:05 - loss: 1.2035 - regression_loss: 1.0147 - classification_loss: 0.1889
 600/1500 [===========>..................] - ETA: 12:04 - loss: 1.2026 - regression_loss: 1.0140 - classification_loss: 0.1887
 601/1500 [===========>..................] - ETA: 12:03 - loss: 1.2018 - regression_loss: 1.0133 - classification_loss: 0.1885
 602/1500 [===========>..................] - ETA: 12:02 - loss: 1.2012 - regression_loss: 1.0129 - classification_loss: 0.1883
 603/1500 [===========>..................] - ETA: 12:01 - loss: 1.2026 - regression_loss: 1.0139 - classification_loss: 0.1886
 604/1500 [===========>..................] - ETA: 11:59 - loss: 1.2045 - regression_loss: 1.0156 - classification_loss: 0.1889
 605/1500 [===========>..................] - ETA: 11:59 - loss: 1.2051 - regression_loss: 1.0160 - classification_loss: 0.1890
 606/1500 [===========>..................] - ETA: 11:57 - loss: 1.2055 - regression_loss: 1.0163 - classification_loss: 0.1892
 607/1500 [===========>..................] - ETA: 11:57 - loss: 1.2070 - regression_loss: 1.0175 - classification_loss: 0.1896
 608/1500 [===========>..................] - ETA: 11:57 - loss: 1.2069 - regression_loss: 1.0173 - classification_loss: 0.1896
 609/1500 [===========>..................] - ETA: 11:56 - loss: 1.2061 - regression_loss: 1.0166 - classification_loss: 0.1895
 610/1500 [===========>..................] - ETA: 11:55 - loss: 1.2057 - regression_loss: 1.0163 - classification_loss: 0.1894
 611/1500 [===========>..................] - ETA: 11:54 - loss: 1.2052 - regression_loss: 1.0159 - classification_loss: 0.1893
 612/1500 [===========>..................] - ETA: 11:52 - loss: 1.2041 - regression_loss: 1.0148 - classification_loss: 0.1892
 613/1500 [===========>..................] - ETA: 11:51 - loss: 1.2049 - regression_loss: 1.0157 - classification_loss: 0.1893
 614/1500 [===========>..................] - ETA: 11:50 - loss: 1.2039 - regression_loss: 1.0149 - classification_loss: 0.1891
 615/1500 [===========>..................] - ETA: 11:50 - loss: 1.2048 - regression_loss: 1.0150 - classification_loss: 0.1898
 616/1500 [===========>..................] - ETA: 11:49 - loss: 1.2041 - regression_loss: 1.0145 - classification_loss: 0.1897
 617/1500 [===========>..................] - ETA: 11:48 - loss: 1.2063 - regression_loss: 1.0165 - classification_loss: 0.1899
 618/1500 [===========>..................] - ETA: 11:48 - loss: 1.2051 - regression_loss: 1.0154 - classification_loss: 0.1897
 619/1500 [===========>..................] - ETA: 11:48 - loss: 1.2045 - regression_loss: 1.0150 - classification_loss: 0.1895
 620/1500 [===========>..................] - ETA: 11:46 - loss: 1.2053 - regression_loss: 1.0156 - classification_loss: 0.1897
 621/1500 [===========>..................] - ETA: 11:45 - loss: 1.2053 - regression_loss: 1.0157 - classification_loss: 0.1896
 622/1500 [===========>..................] - ETA: 11:44 - loss: 1.2041 - regression_loss: 1.0147 - classification_loss: 0.1894
 623/1500 [===========>..................] - ETA: 11:42 - loss: 1.2048 - regression_loss: 1.0154 - classification_loss: 0.1894
 624/1500 [===========>..................] - ETA: 11:41 - loss: 1.2037 - regression_loss: 1.0145 - classification_loss: 0.1892
 625/1500 [===========>..................] - ETA: 11:40 - loss: 1.2031 - regression_loss: 1.0138 - classification_loss: 0.1893
 626/1500 [===========>..................] - ETA: 11:39 - loss: 1.2022 - regression_loss: 1.0130 - classification_loss: 0.1891
 627/1500 [===========>..................] - ETA: 11:38 - loss: 1.2024 - regression_loss: 1.0133 - classification_loss: 0.1891
 628/1500 [===========>..................] - ETA: 11:38 - loss: 1.2023 - regression_loss: 1.0133 - classification_loss: 0.1891
 629/1500 [===========>..................] - ETA: 11:37 - loss: 1.2034 - regression_loss: 1.0142 - classification_loss: 0.1892
 630/1500 [===========>..................] - ETA: 11:38 - loss: 1.2033 - regression_loss: 1.0140 - classification_loss: 0.1893
 631/1500 [===========>..................] - ETA: 11:36 - loss: 1.2039 - regression_loss: 1.0144 - classification_loss: 0.1895
 632/1500 [===========>..................] - ETA: 11:35 - loss: 1.2029 - regression_loss: 1.0137 - classification_loss: 0.1893
 633/1500 [===========>..................] - ETA: 11:35 - loss: 1.2024 - regression_loss: 1.0132 - classification_loss: 0.1891
 634/1500 [===========>..................] - ETA: 11:34 - loss: 1.2014 - regression_loss: 1.0124 - classification_loss: 0.1890
 635/1500 [===========>..................] - ETA: 11:34 - loss: 1.2019 - regression_loss: 1.0130 - classification_loss: 0.1889
 636/1500 [===========>..................] - ETA: 11:33 - loss: 1.2011 - regression_loss: 1.0124 - classification_loss: 0.1887
 637/1500 [===========>..................] - ETA: 11:31 - loss: 1.2018 - regression_loss: 1.0128 - classification_loss: 0.1890
 638/1500 [===========>..................] - ETA: 11:30 - loss: 1.2014 - regression_loss: 1.0124 - classification_loss: 0.1889
 639/1500 [===========>..................] - ETA: 11:29 - loss: 1.2013 - regression_loss: 1.0126 - classification_loss: 0.1888
 640/1500 [===========>..................] - ETA: 11:27 - loss: 1.2010 - regression_loss: 1.0124 - classification_loss: 0.1886
 641/1500 [===========>..................] - ETA: 11:26 - loss: 1.2017 - regression_loss: 1.0131 - classification_loss: 0.1887
 642/1500 [===========>..................] - ETA: 11:26 - loss: 1.2012 - regression_loss: 1.0127 - classification_loss: 0.1885
 643/1500 [===========>..................] - ETA: 11:25 - loss: 1.2010 - regression_loss: 1.0126 - classification_loss: 0.1884
 644/1500 [===========>..................] - ETA: 11:25 - loss: 1.2000 - regression_loss: 1.0118 - classification_loss: 0.1882
 645/1500 [===========>..................] - ETA: 11:23 - loss: 1.2000 - regression_loss: 1.0119 - classification_loss: 0.1882
 646/1500 [===========>..................] - ETA: 11:22 - loss: 1.1994 - regression_loss: 1.0112 - classification_loss: 0.1881
 647/1500 [===========>..................] - ETA: 11:21 - loss: 1.2004 - regression_loss: 1.0119 - classification_loss: 0.1885
 648/1500 [===========>..................] - ETA: 11:20 - loss: 1.2008 - regression_loss: 1.0122 - classification_loss: 0.1886
 649/1500 [===========>..................] - ETA: 11:19 - loss: 1.2003 - regression_loss: 1.0117 - classification_loss: 0.1886
 650/1500 [============>.................] - ETA: 11:18 - loss: 1.2026 - regression_loss: 1.0136 - classification_loss: 0.1890
 651/1500 [============>.................] - ETA: 11:17 - loss: 1.2028 - regression_loss: 1.0138 - classification_loss: 0.1890
 652/1500 [============>.................] - ETA: 11:16 - loss: 1.2022 - regression_loss: 1.0133 - classification_loss: 0.1889
 653/1500 [============>.................] - ETA: 11:15 - loss: 1.2017 - regression_loss: 1.0129 - classification_loss: 0.1888
 654/1500 [============>.................] - ETA: 11:14 - loss: 1.2013 - regression_loss: 1.0127 - classification_loss: 0.1886
 655/1500 [============>.................] - ETA: 11:13 - loss: 1.2005 - regression_loss: 1.0121 - classification_loss: 0.1884
 656/1500 [============>.................] - ETA: 11:12 - loss: 1.2011 - regression_loss: 1.0126 - classification_loss: 0.1884
 657/1500 [============>.................] - ETA: 11:11 - loss: 1.2007 - regression_loss: 1.0123 - classification_loss: 0.1884
 658/1500 [============>.................] - ETA: 11:11 - loss: 1.2003 - regression_loss: 1.0120 - classification_loss: 0.1883
 659/1500 [============>.................] - ETA: 11:10 - loss: 1.1997 - regression_loss: 1.0114 - classification_loss: 0.1883
 660/1500 [============>.................] - ETA: 11:08 - loss: 1.1991 - regression_loss: 1.0109 - classification_loss: 0.1882
 661/1500 [============>.................] - ETA: 11:08 - loss: 1.1986 - regression_loss: 1.0106 - classification_loss: 0.1881
 662/1500 [============>.................] - ETA: 11:08 - loss: 1.1980 - regression_loss: 1.0100 - classification_loss: 0.1880
 663/1500 [============>.................] - ETA: 11:09 - loss: 1.1989 - regression_loss: 1.0106 - classification_loss: 0.1883
 664/1500 [============>.................] - ETA: 11:08 - loss: 1.2002 - regression_loss: 1.0116 - classification_loss: 0.1886
 665/1500 [============>.................] - ETA: 11:07 - loss: 1.2004 - regression_loss: 1.0118 - classification_loss: 0.1886
 666/1500 [============>.................] - ETA: 11:06 - loss: 1.2014 - regression_loss: 1.0126 - classification_loss: 0.1888
 667/1500 [============>.................] - ETA: 11:04 - loss: 1.2015 - regression_loss: 1.0128 - classification_loss: 0.1887
 668/1500 [============>.................] - ETA: 11:04 - loss: 1.2017 - regression_loss: 1.0130 - classification_loss: 0.1887
 669/1500 [============>.................] - ETA: 11:03 - loss: 1.2015 - regression_loss: 1.0129 - classification_loss: 0.1886
 670/1500 [============>.................] - ETA: 11:02 - loss: 1.2010 - regression_loss: 1.0125 - classification_loss: 0.1885
 671/1500 [============>.................] - ETA: 11:01 - loss: 1.2014 - regression_loss: 1.0129 - classification_loss: 0.1885
 672/1500 [============>.................] - ETA: 11:01 - loss: 1.2027 - regression_loss: 1.0141 - classification_loss: 0.1885
 673/1500 [============>.................] - ETA: 11:00 - loss: 1.2021 - regression_loss: 1.0138 - classification_loss: 0.1883
 674/1500 [============>.................] - ETA: 11:00 - loss: 1.2011 - regression_loss: 1.0130 - classification_loss: 0.1881
 675/1500 [============>.................] - ETA: 10:59 - loss: 1.2016 - regression_loss: 1.0135 - classification_loss: 0.1881
 676/1500 [============>.................] - ETA: 10:59 - loss: 1.2027 - regression_loss: 1.0143 - classification_loss: 0.1884
 677/1500 [============>.................] - ETA: 10:58 - loss: 1.2016 - regression_loss: 1.0134 - classification_loss: 0.1882
 678/1500 [============>.................] - ETA: 10:58 - loss: 1.2014 - regression_loss: 1.0133 - classification_loss: 0.1881
 679/1500 [============>.................] - ETA: 10:56 - loss: 1.2013 - regression_loss: 1.0133 - classification_loss: 0.1879
 680/1500 [============>.................] - ETA: 10:57 - loss: 1.2012 - regression_loss: 1.0133 - classification_loss: 0.1878
 681/1500 [============>.................] - ETA: 10:56 - loss: 1.2002 - regression_loss: 1.0126 - classification_loss: 0.1876
 682/1500 [============>.................] - ETA: 10:55 - loss: 1.1999 - regression_loss: 1.0122 - classification_loss: 0.1877
 683/1500 [============>.................] - ETA: 10:54 - loss: 1.1996 - regression_loss: 1.0119 - classification_loss: 0.1877
 684/1500 [============>.................] - ETA: 10:54 - loss: 1.1991 - regression_loss: 1.0114 - classification_loss: 0.1877
 685/1500 [============>.................] - ETA: 10:53 - loss: 1.1989 - regression_loss: 1.0113 - classification_loss: 0.1875
 686/1500 [============>.................] - ETA: 10:52 - loss: 1.1979 - regression_loss: 1.0106 - classification_loss: 0.1873
 687/1500 [============>.................] - ETA: 10:52 - loss: 1.1972 - regression_loss: 1.0100 - classification_loss: 0.1873
 688/1500 [============>.................] - ETA: 10:51 - loss: 1.1970 - regression_loss: 1.0099 - classification_loss: 0.1871
 689/1500 [============>.................] - ETA: 10:50 - loss: 1.1964 - regression_loss: 1.0094 - classification_loss: 0.1870
 690/1500 [============>.................] - ETA: 10:49 - loss: 1.1963 - regression_loss: 1.0091 - classification_loss: 0.1873
 691/1500 [============>.................] - ETA: 10:50 - loss: 1.1976 - regression_loss: 1.0101 - classification_loss: 0.1875
 692/1500 [============>.................] - ETA: 10:49 - loss: 1.1972 - regression_loss: 1.0099 - classification_loss: 0.1873
 693/1500 [============>.................] - ETA: 10:47 - loss: 1.1978 - regression_loss: 1.0104 - classification_loss: 0.1875
 694/1500 [============>.................] - ETA: 10:47 - loss: 1.1980 - regression_loss: 1.0106 - classification_loss: 0.1874
 695/1500 [============>.................] - ETA: 10:46 - loss: 1.1988 - regression_loss: 1.0114 - classification_loss: 0.1874
 696/1500 [============>.................] - ETA: 10:45 - loss: 1.1980 - regression_loss: 1.0106 - classification_loss: 0.1874
 697/1500 [============>.................] - ETA: 10:45 - loss: 1.1999 - regression_loss: 1.0122 - classification_loss: 0.1877
 698/1500 [============>.................] - ETA: 10:44 - loss: 1.2003 - regression_loss: 1.0126 - classification_loss: 0.1876
 699/1500 [============>.................] - ETA: 10:43 - loss: 1.2001 - regression_loss: 1.0125 - classification_loss: 0.1876
 700/1500 [=============>................] - ETA: 10:41 - loss: 1.1996 - regression_loss: 1.0122 - classification_loss: 0.1874
 701/1500 [=============>................] - ETA: 10:40 - loss: 1.2006 - regression_loss: 1.0132 - classification_loss: 0.1875
 702/1500 [=============>................] - ETA: 10:40 - loss: 1.2017 - regression_loss: 1.0140 - classification_loss: 0.1877
 703/1500 [=============>................] - ETA: 10:39 - loss: 1.2028 - regression_loss: 1.0149 - classification_loss: 0.1879
 704/1500 [=============>................] - ETA: 10:39 - loss: 1.2040 - regression_loss: 1.0159 - classification_loss: 0.1881
 705/1500 [=============>................] - ETA: 10:37 - loss: 1.2040 - regression_loss: 1.0161 - classification_loss: 0.1879
 706/1500 [=============>................] - ETA: 10:37 - loss: 1.2055 - regression_loss: 1.0173 - classification_loss: 0.1883
 707/1500 [=============>................] - ETA: 10:36 - loss: 1.2051 - regression_loss: 1.0169 - classification_loss: 0.1882
 708/1500 [=============>................] - ETA: 10:35 - loss: 1.2057 - regression_loss: 1.0176 - classification_loss: 0.1881
 709/1500 [=============>................] - ETA: 10:34 - loss: 1.2056 - regression_loss: 1.0175 - classification_loss: 0.1881
 710/1500 [=============>................] - ETA: 10:32 - loss: 1.2048 - regression_loss: 1.0169 - classification_loss: 0.1879
 711/1500 [=============>................] - ETA: 10:31 - loss: 1.2051 - regression_loss: 1.0173 - classification_loss: 0.1878
 712/1500 [=============>................] - ETA: 10:30 - loss: 1.2054 - regression_loss: 1.0171 - classification_loss: 0.1883
 713/1500 [=============>................] - ETA: 10:29 - loss: 1.2050 - regression_loss: 1.0166 - classification_loss: 0.1884
 714/1500 [=============>................] - ETA: 10:28 - loss: 1.2043 - regression_loss: 1.0161 - classification_loss: 0.1882
 715/1500 [=============>................] - ETA: 10:27 - loss: 1.2035 - regression_loss: 1.0154 - classification_loss: 0.1881
 716/1500 [=============>................] - ETA: 10:27 - loss: 1.2045 - regression_loss: 1.0162 - classification_loss: 0.1882
 717/1500 [=============>................] - ETA: 10:26 - loss: 1.2060 - regression_loss: 1.0173 - classification_loss: 0.1886
 718/1500 [=============>................] - ETA: 10:25 - loss: 1.2063 - regression_loss: 1.0177 - classification_loss: 0.1886
 719/1500 [=============>................] - ETA: 10:24 - loss: 1.2061 - regression_loss: 1.0177 - classification_loss: 0.1884
 720/1500 [=============>................] - ETA: 10:23 - loss: 1.2055 - regression_loss: 1.0172 - classification_loss: 0.1883
 721/1500 [=============>................] - ETA: 10:22 - loss: 1.2056 - regression_loss: 1.0173 - classification_loss: 0.1883
 722/1500 [=============>................] - ETA: 10:21 - loss: 1.2070 - regression_loss: 1.0185 - classification_loss: 0.1885
 723/1500 [=============>................] - ETA: 10:21 - loss: 1.2069 - regression_loss: 1.0185 - classification_loss: 0.1884
 724/1500 [=============>................] - ETA: 10:22 - loss: 1.2086 - regression_loss: 1.0199 - classification_loss: 0.1887
 725/1500 [=============>................] - ETA: 10:21 - loss: 1.2089 - regression_loss: 1.0202 - classification_loss: 0.1887
 726/1500 [=============>................] - ETA: 10:20 - loss: 1.2088 - regression_loss: 1.0202 - classification_loss: 0.1886
 727/1500 [=============>................] - ETA: 10:19 - loss: 1.2092 - regression_loss: 1.0204 - classification_loss: 0.1889
 728/1500 [=============>................] - ETA: 10:19 - loss: 1.2089 - regression_loss: 1.0200 - classification_loss: 0.1889
 729/1500 [=============>................] - ETA: 10:18 - loss: 1.2081 - regression_loss: 1.0194 - classification_loss: 0.1887
 730/1500 [=============>................] - ETA: 10:18 - loss: 1.2075 - regression_loss: 1.0190 - classification_loss: 0.1885
 731/1500 [=============>................] - ETA: 10:16 - loss: 1.2068 - regression_loss: 1.0185 - classification_loss: 0.1883
 732/1500 [=============>................] - ETA: 10:16 - loss: 1.2067 - regression_loss: 1.0186 - classification_loss: 0.1881
 733/1500 [=============>................] - ETA: 10:16 - loss: 1.2082 - regression_loss: 1.0198 - classification_loss: 0.1884
 734/1500 [=============>................] - ETA: 10:16 - loss: 1.2076 - regression_loss: 1.0192 - classification_loss: 0.1883
 735/1500 [=============>................] - ETA: 10:15 - loss: 1.2079 - regression_loss: 1.0196 - classification_loss: 0.1883
 736/1500 [=============>................] - ETA: 10:14 - loss: 1.2075 - regression_loss: 1.0193 - classification_loss: 0.1883
 737/1500 [=============>................] - ETA: 10:12 - loss: 1.2074 - regression_loss: 1.0192 - classification_loss: 0.1882
 738/1500 [=============>................] - ETA: 10:11 - loss: 1.2081 - regression_loss: 1.0198 - classification_loss: 0.1883
 739/1500 [=============>................] - ETA: 10:12 - loss: 1.2088 - regression_loss: 1.0205 - classification_loss: 0.1883
 740/1500 [=============>................] - ETA: 10:11 - loss: 1.2097 - regression_loss: 1.0213 - classification_loss: 0.1884
 741/1500 [=============>................] - ETA: 10:10 - loss: 1.2101 - regression_loss: 1.0217 - classification_loss: 0.1884
 742/1500 [=============>................] - ETA: 10:09 - loss: 1.2108 - regression_loss: 1.0221 - classification_loss: 0.1887
 743/1500 [=============>................] - ETA: 10:08 - loss: 1.2114 - regression_loss: 1.0223 - classification_loss: 0.1891
 744/1500 [=============>................] - ETA: 10:07 - loss: 1.2113 - regression_loss: 1.0223 - classification_loss: 0.1890
 745/1500 [=============>................] - ETA: 10:06 - loss: 1.2120 - regression_loss: 1.0231 - classification_loss: 0.1889
 746/1500 [=============>................] - ETA: 10:04 - loss: 1.2142 - regression_loss: 1.0248 - classification_loss: 0.1894
 747/1500 [=============>................] - ETA: 10:03 - loss: 1.2135 - regression_loss: 1.0242 - classification_loss: 0.1893
 748/1500 [=============>................] - ETA: 10:02 - loss: 1.2125 - regression_loss: 1.0234 - classification_loss: 0.1891
 749/1500 [=============>................] - ETA: 10:02 - loss: 1.2118 - regression_loss: 1.0228 - classification_loss: 0.1890
 750/1500 [==============>...............] - ETA: 10:01 - loss: 1.2117 - regression_loss: 1.0228 - classification_loss: 0.1889
 751/1500 [==============>...............] - ETA: 10:00 - loss: 1.2111 - regression_loss: 1.0223 - classification_loss: 0.1887
 752/1500 [==============>...............] - ETA: 10:00 - loss: 1.2105 - regression_loss: 1.0219 - classification_loss: 0.1886
 753/1500 [==============>...............] - ETA: 9:59 - loss: 1.2108 - regression_loss: 1.0222 - classification_loss: 0.1886 
 754/1500 [==============>...............] - ETA: 9:58 - loss: 1.2113 - regression_loss: 1.0228 - classification_loss: 0.1885
 755/1500 [==============>...............] - ETA: 9:58 - loss: 1.2112 - regression_loss: 1.0226 - classification_loss: 0.1887
 756/1500 [==============>...............] - ETA: 9:57 - loss: 1.2108 - regression_loss: 1.0221 - classification_loss: 0.1886
 757/1500 [==============>...............] - ETA: 9:56 - loss: 1.2128 - regression_loss: 1.0225 - classification_loss: 0.1903
 758/1500 [==============>...............] - ETA: 9:55 - loss: 1.2120 - regression_loss: 1.0219 - classification_loss: 0.1901
 759/1500 [==============>...............] - ETA: 9:53 - loss: 1.2116 - regression_loss: 1.0216 - classification_loss: 0.1900
 760/1500 [==============>...............] - ETA: 9:53 - loss: 1.2124 - regression_loss: 1.0222 - classification_loss: 0.1902
 761/1500 [==============>...............] - ETA: 9:52 - loss: 1.2119 - regression_loss: 1.0218 - classification_loss: 0.1901
 762/1500 [==============>...............] - ETA: 9:51 - loss: 1.2112 - regression_loss: 1.0212 - classification_loss: 0.1900
 763/1500 [==============>...............] - ETA: 9:50 - loss: 1.2132 - regression_loss: 1.0228 - classification_loss: 0.1904
 764/1500 [==============>...............] - ETA: 9:49 - loss: 1.2129 - regression_loss: 1.0226 - classification_loss: 0.1903
 765/1500 [==============>...............] - ETA: 9:48 - loss: 1.2138 - regression_loss: 1.0234 - classification_loss: 0.1904
 766/1500 [==============>...............] - ETA: 9:47 - loss: 1.2136 - regression_loss: 1.0234 - classification_loss: 0.1903
 767/1500 [==============>...............] - ETA: 9:46 - loss: 1.2136 - regression_loss: 1.0235 - classification_loss: 0.1902
 768/1500 [==============>...............] - ETA: 9:45 - loss: 1.2143 - regression_loss: 1.0242 - classification_loss: 0.1901
 769/1500 [==============>...............] - ETA: 9:44 - loss: 1.2134 - regression_loss: 1.0235 - classification_loss: 0.1899
 770/1500 [==============>...............] - ETA: 9:44 - loss: 1.2132 - regression_loss: 1.0232 - classification_loss: 0.1900
 771/1500 [==============>...............] - ETA: 9:43 - loss: 1.2131 - regression_loss: 1.0232 - classification_loss: 0.1898
 772/1500 [==============>...............] - ETA: 9:42 - loss: 1.2136 - regression_loss: 1.0235 - classification_loss: 0.1901
 773/1500 [==============>...............] - ETA: 9:41 - loss: 1.2129 - regression_loss: 1.0230 - classification_loss: 0.1899
 774/1500 [==============>...............] - ETA: 9:40 - loss: 1.2118 - regression_loss: 1.0220 - classification_loss: 0.1898
 775/1500 [==============>...............] - ETA: 9:39 - loss: 1.2117 - regression_loss: 1.0218 - classification_loss: 0.1899
 776/1500 [==============>...............] - ETA: 9:39 - loss: 1.2117 - regression_loss: 1.0219 - classification_loss: 0.1898
 777/1500 [==============>...............] - ETA: 9:38 - loss: 1.2105 - regression_loss: 1.0209 - classification_loss: 0.1896
 778/1500 [==============>...............] - ETA: 9:37 - loss: 1.2109 - regression_loss: 1.0212 - classification_loss: 0.1897
 779/1500 [==============>...............] - ETA: 9:36 - loss: 1.2097 - regression_loss: 1.0202 - classification_loss: 0.1895
 780/1500 [==============>...............] - ETA: 9:35 - loss: 1.2108 - regression_loss: 1.0210 - classification_loss: 0.1898
 781/1500 [==============>...............] - ETA: 9:34 - loss: 1.2107 - regression_loss: 1.0210 - classification_loss: 0.1896
 782/1500 [==============>...............] - ETA: 9:33 - loss: 1.2109 - regression_loss: 1.0212 - classification_loss: 0.1896
 783/1500 [==============>...............] - ETA: 9:32 - loss: 1.2126 - regression_loss: 1.0224 - classification_loss: 0.1902
 784/1500 [==============>...............] - ETA: 9:31 - loss: 1.2120 - regression_loss: 1.0219 - classification_loss: 0.1901
 785/1500 [==============>...............] - ETA: 9:30 - loss: 1.2113 - regression_loss: 1.0214 - classification_loss: 0.1899
 786/1500 [==============>...............] - ETA: 9:29 - loss: 1.2108 - regression_loss: 1.0210 - classification_loss: 0.1898
 787/1500 [==============>...............] - ETA: 9:28 - loss: 1.2100 - regression_loss: 1.0203 - classification_loss: 0.1897
 788/1500 [==============>...............] - ETA: 9:27 - loss: 1.2099 - regression_loss: 1.0203 - classification_loss: 0.1896
 789/1500 [==============>...............] - ETA: 9:27 - loss: 1.2102 - regression_loss: 1.0206 - classification_loss: 0.1896
 790/1500 [==============>...............] - ETA: 9:25 - loss: 1.2098 - regression_loss: 1.0201 - classification_loss: 0.1896
 791/1500 [==============>...............] - ETA: 9:25 - loss: 1.2092 - regression_loss: 1.0197 - classification_loss: 0.1895
 792/1500 [==============>...............] - ETA: 9:24 - loss: 1.2090 - regression_loss: 1.0197 - classification_loss: 0.1893
 793/1500 [==============>...............] - ETA: 9:23 - loss: 1.2092 - regression_loss: 1.0199 - classification_loss: 0.1893
 794/1500 [==============>...............] - ETA: 9:22 - loss: 1.2095 - regression_loss: 1.0202 - classification_loss: 0.1893
 795/1500 [==============>...............] - ETA: 9:21 - loss: 1.2098 - regression_loss: 1.0205 - classification_loss: 0.1893
 796/1500 [==============>...............] - ETA: 9:21 - loss: 1.2115 - regression_loss: 1.0218 - classification_loss: 0.1898
 797/1500 [==============>...............] - ETA: 9:19 - loss: 1.2112 - regression_loss: 1.0216 - classification_loss: 0.1896
 798/1500 [==============>...............] - ETA: 9:18 - loss: 1.2107 - regression_loss: 1.0212 - classification_loss: 0.1895
 799/1500 [==============>...............] - ETA: 9:17 - loss: 1.2101 - regression_loss: 1.0206 - classification_loss: 0.1895
 800/1500 [===============>..............] - ETA: 9:16 - loss: 1.2099 - regression_loss: 1.0204 - classification_loss: 0.1895
 801/1500 [===============>..............] - ETA: 9:15 - loss: 1.2089 - regression_loss: 1.0196 - classification_loss: 0.1893
 802/1500 [===============>..............] - ETA: 9:15 - loss: 1.2083 - regression_loss: 1.0191 - classification_loss: 0.1893
 803/1500 [===============>..............] - ETA: 9:14 - loss: 1.2088 - regression_loss: 1.0196 - classification_loss: 0.1892
 804/1500 [===============>..............] - ETA: 9:13 - loss: 1.2091 - regression_loss: 1.0198 - classification_loss: 0.1893
 805/1500 [===============>..............] - ETA: 9:12 - loss: 1.2084 - regression_loss: 1.0192 - classification_loss: 0.1891
 806/1500 [===============>..............] - ETA: 9:11 - loss: 1.2074 - regression_loss: 1.0184 - classification_loss: 0.1890
 807/1500 [===============>..............] - ETA: 9:10 - loss: 1.2069 - regression_loss: 1.0181 - classification_loss: 0.1888
 808/1500 [===============>..............] - ETA: 9:09 - loss: 1.2060 - regression_loss: 1.0174 - classification_loss: 0.1886
 809/1500 [===============>..............] - ETA: 9:08 - loss: 1.2058 - regression_loss: 1.0173 - classification_loss: 0.1885
 810/1500 [===============>..............] - ETA: 9:07 - loss: 1.2054 - regression_loss: 1.0170 - classification_loss: 0.1884
 811/1500 [===============>..............] - ETA: 9:06 - loss: 1.2045 - regression_loss: 1.0162 - classification_loss: 0.1883
 812/1500 [===============>..............] - ETA: 9:06 - loss: 1.2041 - regression_loss: 1.0159 - classification_loss: 0.1882
 813/1500 [===============>..............] - ETA: 9:06 - loss: 1.2032 - regression_loss: 1.0152 - classification_loss: 0.1880
 814/1500 [===============>..............] - ETA: 9:04 - loss: 1.2026 - regression_loss: 1.0146 - classification_loss: 0.1880
 815/1500 [===============>..............] - ETA: 9:04 - loss: 1.2021 - regression_loss: 1.0143 - classification_loss: 0.1878
 816/1500 [===============>..............] - ETA: 9:03 - loss: 1.2032 - regression_loss: 1.0152 - classification_loss: 0.1880
 817/1500 [===============>..............] - ETA: 9:02 - loss: 1.2023 - regression_loss: 1.0145 - classification_loss: 0.1878
 818/1500 [===============>..............] - ETA: 9:01 - loss: 1.2028 - regression_loss: 1.0150 - classification_loss: 0.1879
 819/1500 [===============>..............] - ETA: 9:00 - loss: 1.2038 - regression_loss: 1.0158 - classification_loss: 0.1880
 820/1500 [===============>..............] - ETA: 8:59 - loss: 1.2033 - regression_loss: 1.0154 - classification_loss: 0.1878
 821/1500 [===============>..............] - ETA: 8:58 - loss: 1.2027 - regression_loss: 1.0150 - classification_loss: 0.1877
 822/1500 [===============>..............] - ETA: 8:57 - loss: 1.2035 - regression_loss: 1.0156 - classification_loss: 0.1879
 823/1500 [===============>..............] - ETA: 8:56 - loss: 1.2031 - regression_loss: 1.0152 - classification_loss: 0.1879
 824/1500 [===============>..............] - ETA: 8:55 - loss: 1.2027 - regression_loss: 1.0149 - classification_loss: 0.1878
 825/1500 [===============>..............] - ETA: 8:55 - loss: 1.2022 - regression_loss: 1.0145 - classification_loss: 0.1877
 826/1500 [===============>..............] - ETA: 8:54 - loss: 1.2018 - regression_loss: 1.0143 - classification_loss: 0.1875
 827/1500 [===============>..............] - ETA: 8:53 - loss: 1.2042 - regression_loss: 1.0149 - classification_loss: 0.1893
 828/1500 [===============>..............] - ETA: 8:52 - loss: 1.2051 - regression_loss: 1.0157 - classification_loss: 0.1893
 829/1500 [===============>..............] - ETA: 8:51 - loss: 1.2044 - regression_loss: 1.0152 - classification_loss: 0.1892
 830/1500 [===============>..............] - ETA: 8:51 - loss: 1.2059 - regression_loss: 1.0161 - classification_loss: 0.1897
 831/1500 [===============>..............] - ETA: 8:50 - loss: 1.2051 - regression_loss: 1.0154 - classification_loss: 0.1896
 832/1500 [===============>..............] - ETA: 8:49 - loss: 1.2042 - regression_loss: 1.0147 - classification_loss: 0.1895
 833/1500 [===============>..............] - ETA: 8:48 - loss: 1.2035 - regression_loss: 1.0140 - classification_loss: 0.1896
 834/1500 [===============>..............] - ETA: 8:47 - loss: 1.2027 - regression_loss: 1.0132 - classification_loss: 0.1895
 835/1500 [===============>..............] - ETA: 8:46 - loss: 1.2021 - regression_loss: 1.0127 - classification_loss: 0.1894
 836/1500 [===============>..............] - ETA: 8:45 - loss: 1.2020 - regression_loss: 1.0127 - classification_loss: 0.1892
 837/1500 [===============>..............] - ETA: 8:44 - loss: 1.2028 - regression_loss: 1.0134 - classification_loss: 0.1894
 838/1500 [===============>..............] - ETA: 8:43 - loss: 1.2026 - regression_loss: 1.0133 - classification_loss: 0.1894
 839/1500 [===============>..............] - ETA: 8:43 - loss: 1.2039 - regression_loss: 1.0145 - classification_loss: 0.1894
 840/1500 [===============>..............] - ETA: 8:42 - loss: 1.2032 - regression_loss: 1.0137 - classification_loss: 0.1895
 841/1500 [===============>..............] - ETA: 8:41 - loss: 1.2036 - regression_loss: 1.0140 - classification_loss: 0.1896
 842/1500 [===============>..............] - ETA: 8:42 - loss: 1.2044 - regression_loss: 1.0146 - classification_loss: 0.1899
 843/1500 [===============>..............] - ETA: 8:41 - loss: 1.2052 - regression_loss: 1.0151 - classification_loss: 0.1901
 844/1500 [===============>..............] - ETA: 8:40 - loss: 1.2060 - regression_loss: 1.0159 - classification_loss: 0.1901
 845/1500 [===============>..............] - ETA: 8:39 - loss: 1.2056 - regression_loss: 1.0156 - classification_loss: 0.1900
 846/1500 [===============>..............] - ETA: 8:38 - loss: 1.2066 - regression_loss: 1.0166 - classification_loss: 0.1900
 847/1500 [===============>..............] - ETA: 8:39 - loss: 1.2076 - regression_loss: 1.0174 - classification_loss: 0.1902
 848/1500 [===============>..............] - ETA: 8:38 - loss: 1.2074 - regression_loss: 1.0173 - classification_loss: 0.1901
 849/1500 [===============>..............] - ETA: 8:37 - loss: 1.2069 - regression_loss: 1.0169 - classification_loss: 0.1900
 850/1500 [================>.............] - ETA: 8:36 - loss: 1.2062 - regression_loss: 1.0164 - classification_loss: 0.1898
 851/1500 [================>.............] - ETA: 8:35 - loss: 1.2057 - regression_loss: 1.0160 - classification_loss: 0.1897
 852/1500 [================>.............] - ETA: 8:33 - loss: 1.2056 - regression_loss: 1.0159 - classification_loss: 0.1897
 853/1500 [================>.............] - ETA: 8:32 - loss: 1.2052 - regression_loss: 1.0156 - classification_loss: 0.1895
 854/1500 [================>.............] - ETA: 8:31 - loss: 1.2048 - regression_loss: 1.0154 - classification_loss: 0.1894
 855/1500 [================>.............] - ETA: 8:30 - loss: 1.2057 - regression_loss: 1.0162 - classification_loss: 0.1895
 856/1500 [================>.............] - ETA: 8:29 - loss: 1.2057 - regression_loss: 1.0163 - classification_loss: 0.1894
 857/1500 [================>.............] - ETA: 8:28 - loss: 1.2052 - regression_loss: 1.0159 - classification_loss: 0.1893
 858/1500 [================>.............] - ETA: 8:27 - loss: 1.2057 - regression_loss: 1.0163 - classification_loss: 0.1894
 859/1500 [================>.............] - ETA: 8:26 - loss: 1.2053 - regression_loss: 1.0159 - classification_loss: 0.1894
 860/1500 [================>.............] - ETA: 8:25 - loss: 1.2051 - regression_loss: 1.0157 - classification_loss: 0.1893
 861/1500 [================>.............] - ETA: 8:24 - loss: 1.2052 - regression_loss: 1.0155 - classification_loss: 0.1897
 862/1500 [================>.............] - ETA: 8:23 - loss: 1.2050 - regression_loss: 1.0152 - classification_loss: 0.1898
 863/1500 [================>.............] - ETA: 8:23 - loss: 1.2050 - regression_loss: 1.0152 - classification_loss: 0.1898
 864/1500 [================>.............] - ETA: 8:22 - loss: 1.2050 - regression_loss: 1.0153 - classification_loss: 0.1897
 865/1500 [================>.............] - ETA: 8:22 - loss: 1.2048 - regression_loss: 1.0151 - classification_loss: 0.1897
 866/1500 [================>.............] - ETA: 8:22 - loss: 1.2050 - regression_loss: 1.0152 - classification_loss: 0.1898
 867/1500 [================>.............] - ETA: 8:21 - loss: 1.2045 - regression_loss: 1.0147 - classification_loss: 0.1898
 868/1500 [================>.............] - ETA: 8:20 - loss: 1.2036 - regression_loss: 1.0140 - classification_loss: 0.1896
 869/1500 [================>.............] - ETA: 8:19 - loss: 1.2036 - regression_loss: 1.0139 - classification_loss: 0.1897
 870/1500 [================>.............] - ETA: 8:18 - loss: 1.2031 - regression_loss: 1.0135 - classification_loss: 0.1896
 871/1500 [================>.............] - ETA: 8:17 - loss: 1.2030 - regression_loss: 1.0134 - classification_loss: 0.1896
 872/1500 [================>.............] - ETA: 8:16 - loss: 1.2031 - regression_loss: 1.0136 - classification_loss: 0.1895
 873/1500 [================>.............] - ETA: 8:15 - loss: 1.2022 - regression_loss: 1.0129 - classification_loss: 0.1893
 874/1500 [================>.............] - ETA: 8:14 - loss: 1.2019 - regression_loss: 1.0126 - classification_loss: 0.1893
 875/1500 [================>.............] - ETA: 8:13 - loss: 1.2013 - regression_loss: 1.0121 - classification_loss: 0.1892
 876/1500 [================>.............] - ETA: 8:12 - loss: 1.2009 - regression_loss: 1.0119 - classification_loss: 0.1890
 877/1500 [================>.............] - ETA: 8:11 - loss: 1.1999 - regression_loss: 1.0111 - classification_loss: 0.1889
 878/1500 [================>.............] - ETA: 8:11 - loss: 1.1992 - regression_loss: 1.0104 - classification_loss: 0.1887
 879/1500 [================>.............] - ETA: 8:10 - loss: 1.1994 - regression_loss: 1.0107 - classification_loss: 0.1887
 880/1500 [================>.............] - ETA: 8:09 - loss: 1.1990 - regression_loss: 1.0105 - classification_loss: 0.1886
 881/1500 [================>.............] - ETA: 8:08 - loss: 1.1985 - regression_loss: 1.0100 - classification_loss: 0.1884
 882/1500 [================>.............] - ETA: 8:08 - loss: 1.1979 - regression_loss: 1.0096 - classification_loss: 0.1883
 883/1500 [================>.............] - ETA: 8:07 - loss: 1.1982 - regression_loss: 1.0099 - classification_loss: 0.1883
 884/1500 [================>.............] - ETA: 8:06 - loss: 1.1982 - regression_loss: 1.0100 - classification_loss: 0.1883
 885/1500 [================>.............] - ETA: 8:05 - loss: 1.1982 - regression_loss: 1.0100 - classification_loss: 0.1882
 886/1500 [================>.............] - ETA: 8:04 - loss: 1.1993 - regression_loss: 1.0110 - classification_loss: 0.1883
 887/1500 [================>.............] - ETA: 8:03 - loss: 1.1997 - regression_loss: 1.0113 - classification_loss: 0.1883
 888/1500 [================>.............] - ETA: 8:03 - loss: 1.1994 - regression_loss: 1.0111 - classification_loss: 0.1883
 889/1500 [================>.............] - ETA: 8:02 - loss: 1.1992 - regression_loss: 1.0110 - classification_loss: 0.1882
 890/1500 [================>.............] - ETA: 8:01 - loss: 1.2009 - regression_loss: 1.0122 - classification_loss: 0.1886
 891/1500 [================>.............] - ETA: 8:00 - loss: 1.2016 - regression_loss: 1.0128 - classification_loss: 0.1888
 892/1500 [================>.............] - ETA: 7:59 - loss: 1.2012 - regression_loss: 1.0126 - classification_loss: 0.1887
 893/1500 [================>.............] - ETA: 7:58 - loss: 1.2014 - regression_loss: 1.0127 - classification_loss: 0.1887
 894/1500 [================>.............] - ETA: 7:57 - loss: 1.2006 - regression_loss: 1.0121 - classification_loss: 0.1885
 895/1500 [================>.............] - ETA: 7:56 - loss: 1.2002 - regression_loss: 1.0118 - classification_loss: 0.1884
 896/1500 [================>.............] - ETA: 7:56 - loss: 1.2009 - regression_loss: 1.0124 - classification_loss: 0.1885
 897/1500 [================>.............] - ETA: 7:55 - loss: 1.2007 - regression_loss: 1.0121 - classification_loss: 0.1886
 898/1500 [================>.............] - ETA: 7:54 - loss: 1.2007 - regression_loss: 1.0122 - classification_loss: 0.1886
 899/1500 [================>.............] - ETA: 7:53 - loss: 1.2003 - regression_loss: 1.0118 - classification_loss: 0.1884
 900/1500 [=================>............] - ETA: 7:52 - loss: 1.1996 - regression_loss: 1.0113 - classification_loss: 0.1883
 901/1500 [=================>............] - ETA: 7:51 - loss: 1.2001 - regression_loss: 1.0118 - classification_loss: 0.1883
 902/1500 [=================>............] - ETA: 7:50 - loss: 1.2010 - regression_loss: 1.0125 - classification_loss: 0.1885
 903/1500 [=================>............] - ETA: 7:49 - loss: 1.2019 - regression_loss: 1.0133 - classification_loss: 0.1886
 904/1500 [=================>............] - ETA: 7:48 - loss: 1.2014 - regression_loss: 1.0130 - classification_loss: 0.1884
 905/1500 [=================>............] - ETA: 7:48 - loss: 1.2023 - regression_loss: 1.0135 - classification_loss: 0.1887
 906/1500 [=================>............] - ETA: 7:47 - loss: 1.2021 - regression_loss: 1.0135 - classification_loss: 0.1887
 907/1500 [=================>............] - ETA: 7:45 - loss: 1.2012 - regression_loss: 1.0127 - classification_loss: 0.1885
 908/1500 [=================>............] - ETA: 7:45 - loss: 1.2004 - regression_loss: 1.0120 - classification_loss: 0.1884
 909/1500 [=================>............] - ETA: 7:44 - loss: 1.1999 - regression_loss: 1.0117 - classification_loss: 0.1882
 910/1500 [=================>............] - ETA: 7:43 - loss: 1.1998 - regression_loss: 1.0115 - classification_loss: 0.1883
 911/1500 [=================>............] - ETA: 7:42 - loss: 1.2009 - regression_loss: 1.0125 - classification_loss: 0.1884
 912/1500 [=================>............] - ETA: 7:41 - loss: 1.2003 - regression_loss: 1.0120 - classification_loss: 0.1883
 913/1500 [=================>............] - ETA: 7:40 - loss: 1.2000 - regression_loss: 1.0117 - classification_loss: 0.1882
 914/1500 [=================>............] - ETA: 7:40 - loss: 1.1995 - regression_loss: 1.0111 - classification_loss: 0.1884
 915/1500 [=================>............] - ETA: 7:40 - loss: 1.1989 - regression_loss: 1.0107 - classification_loss: 0.1882
 916/1500 [=================>............] - ETA: 7:39 - loss: 1.1986 - regression_loss: 1.0102 - classification_loss: 0.1883
 917/1500 [=================>............] - ETA: 7:38 - loss: 1.1999 - regression_loss: 1.0113 - classification_loss: 0.1886
 918/1500 [=================>............] - ETA: 7:37 - loss: 1.1991 - regression_loss: 1.0107 - classification_loss: 0.1885
 919/1500 [=================>............] - ETA: 7:37 - loss: 1.1984 - regression_loss: 1.0100 - classification_loss: 0.1883
 920/1500 [=================>............] - ETA: 7:36 - loss: 1.1986 - regression_loss: 1.0103 - classification_loss: 0.1884
 921/1500 [=================>............] - ETA: 7:37 - loss: 1.1978 - regression_loss: 1.0096 - classification_loss: 0.1882
 922/1500 [=================>............] - ETA: 7:36 - loss: 1.1980 - regression_loss: 1.0097 - classification_loss: 0.1883
 923/1500 [=================>............] - ETA: 7:35 - loss: 1.1983 - regression_loss: 1.0099 - classification_loss: 0.1884
 924/1500 [=================>............] - ETA: 7:34 - loss: 1.1974 - regression_loss: 1.0092 - classification_loss: 0.1882
 925/1500 [=================>............] - ETA: 7:33 - loss: 1.1986 - regression_loss: 1.0101 - classification_loss: 0.1885
 926/1500 [=================>............] - ETA: 7:32 - loss: 1.1983 - regression_loss: 1.0099 - classification_loss: 0.1884
 927/1500 [=================>............] - ETA: 7:31 - loss: 1.1986 - regression_loss: 1.0101 - classification_loss: 0.1885
 928/1500 [=================>............] - ETA: 7:31 - loss: 1.1998 - regression_loss: 1.0110 - classification_loss: 0.1887
 929/1500 [=================>............] - ETA: 7:30 - loss: 1.1997 - regression_loss: 1.0111 - classification_loss: 0.1886
 930/1500 [=================>............] - ETA: 7:29 - loss: 1.1992 - regression_loss: 1.0107 - classification_loss: 0.1885
 931/1500 [=================>............] - ETA: 7:28 - loss: 1.1998 - regression_loss: 1.0112 - classification_loss: 0.1886
 932/1500 [=================>............] - ETA: 7:28 - loss: 1.2011 - regression_loss: 1.0114 - classification_loss: 0.1897
 933/1500 [=================>............] - ETA: 7:27 - loss: 1.2007 - regression_loss: 1.0111 - classification_loss: 0.1896
 934/1500 [=================>............] - ETA: 7:26 - loss: 1.2003 - regression_loss: 1.0108 - classification_loss: 0.1896
 935/1500 [=================>............] - ETA: 7:25 - loss: 1.2005 - regression_loss: 1.0109 - classification_loss: 0.1896
 936/1500 [=================>............] - ETA: 7:24 - loss: 1.2008 - regression_loss: 1.0111 - classification_loss: 0.1896
 937/1500 [=================>............] - ETA: 7:24 - loss: 1.2006 - regression_loss: 1.0110 - classification_loss: 0.1896
 938/1500 [=================>............] - ETA: 7:23 - loss: 1.2002 - regression_loss: 1.0107 - classification_loss: 0.1895
 939/1500 [=================>............] - ETA: 7:22 - loss: 1.2001 - regression_loss: 1.0106 - classification_loss: 0.1895
 940/1500 [=================>............] - ETA: 7:21 - loss: 1.1999 - regression_loss: 1.0105 - classification_loss: 0.1894
 941/1500 [=================>............] - ETA: 7:20 - loss: 1.1991 - regression_loss: 1.0098 - classification_loss: 0.1893
 942/1500 [=================>............] - ETA: 7:19 - loss: 1.1992 - regression_loss: 1.0100 - classification_loss: 0.1893
 943/1500 [=================>............] - ETA: 7:18 - loss: 1.1987 - regression_loss: 1.0094 - classification_loss: 0.1893
 944/1500 [=================>............] - ETA: 7:17 - loss: 1.1983 - regression_loss: 1.0091 - classification_loss: 0.1892
 945/1500 [=================>............] - ETA: 7:16 - loss: 1.1979 - regression_loss: 1.0088 - classification_loss: 0.1891
 946/1500 [=================>............] - ETA: 7:15 - loss: 1.1972 - regression_loss: 1.0082 - classification_loss: 0.1890
 947/1500 [=================>............] - ETA: 7:14 - loss: 1.1980 - regression_loss: 1.0089 - classification_loss: 0.1892
 948/1500 [=================>............] - ETA: 7:13 - loss: 1.1975 - regression_loss: 1.0084 - classification_loss: 0.1890
 949/1500 [=================>............] - ETA: 7:13 - loss: 1.1970 - regression_loss: 1.0080 - classification_loss: 0.1890
 950/1500 [==================>...........] - ETA: 7:12 - loss: 1.1966 - regression_loss: 1.0078 - classification_loss: 0.1888
 951/1500 [==================>...........] - ETA: 7:11 - loss: 1.1963 - regression_loss: 1.0075 - classification_loss: 0.1888
 952/1500 [==================>...........] - ETA: 7:11 - loss: 1.1958 - regression_loss: 1.0072 - classification_loss: 0.1886
 953/1500 [==================>...........] - ETA: 7:10 - loss: 1.1954 - regression_loss: 1.0068 - classification_loss: 0.1886
 954/1500 [==================>...........] - ETA: 7:09 - loss: 1.1965 - regression_loss: 1.0077 - classification_loss: 0.1888
 955/1500 [==================>...........] - ETA: 7:09 - loss: 1.1971 - regression_loss: 1.0083 - classification_loss: 0.1888
 956/1500 [==================>...........] - ETA: 7:08 - loss: 1.1980 - regression_loss: 1.0090 - classification_loss: 0.1890
 957/1500 [==================>...........] - ETA: 7:07 - loss: 1.1992 - regression_loss: 1.0100 - classification_loss: 0.1892
 958/1500 [==================>...........] - ETA: 7:06 - loss: 1.1995 - regression_loss: 1.0104 - classification_loss: 0.1892
 959/1500 [==================>...........] - ETA: 7:06 - loss: 1.1990 - regression_loss: 1.0100 - classification_loss: 0.1890
 960/1500 [==================>...........] - ETA: 7:06 - loss: 1.1995 - regression_loss: 1.0104 - classification_loss: 0.1890
 961/1500 [==================>...........] - ETA: 7:04 - loss: 1.1989 - regression_loss: 1.0099 - classification_loss: 0.1889
 962/1500 [==================>...........] - ETA: 7:04 - loss: 1.1985 - regression_loss: 1.0097 - classification_loss: 0.1888
 963/1500 [==================>...........] - ETA: 7:03 - loss: 1.1976 - regression_loss: 1.0090 - classification_loss: 0.1886
 964/1500 [==================>...........] - ETA: 7:02 - loss: 1.1980 - regression_loss: 1.0094 - classification_loss: 0.1886
 965/1500 [==================>...........] - ETA: 7:02 - loss: 1.1976 - regression_loss: 1.0092 - classification_loss: 0.1884
 966/1500 [==================>...........] - ETA: 7:01 - loss: 1.1969 - regression_loss: 1.0085 - classification_loss: 0.1883
 967/1500 [==================>...........] - ETA: 7:00 - loss: 1.1971 - regression_loss: 1.0086 - classification_loss: 0.1885
 968/1500 [==================>...........] - ETA: 7:00 - loss: 1.1967 - regression_loss: 1.0082 - classification_loss: 0.1885
 969/1500 [==================>...........] - ETA: 6:59 - loss: 1.1973 - regression_loss: 1.0088 - classification_loss: 0.1885
 970/1500 [==================>...........] - ETA: 6:58 - loss: 1.1969 - regression_loss: 1.0083 - classification_loss: 0.1886
 971/1500 [==================>...........] - ETA: 6:57 - loss: 1.1972 - regression_loss: 1.0086 - classification_loss: 0.1886
 972/1500 [==================>...........] - ETA: 6:56 - loss: 1.1972 - regression_loss: 1.0087 - classification_loss: 0.1886
 973/1500 [==================>...........] - ETA: 6:55 - loss: 1.1969 - regression_loss: 1.0085 - classification_loss: 0.1884
 974/1500 [==================>...........] - ETA: 6:54 - loss: 1.1963 - regression_loss: 1.0079 - classification_loss: 0.1884
 975/1500 [==================>...........] - ETA: 6:53 - loss: 1.1959 - regression_loss: 1.0075 - classification_loss: 0.1884
 976/1500 [==================>...........] - ETA: 6:52 - loss: 1.1954 - regression_loss: 1.0071 - classification_loss: 0.1883
 977/1500 [==================>...........] - ETA: 6:51 - loss: 1.1953 - regression_loss: 1.0072 - classification_loss: 0.1882
 978/1500 [==================>...........] - ETA: 6:50 - loss: 1.1958 - regression_loss: 1.0076 - classification_loss: 0.1882
 979/1500 [==================>...........] - ETA: 6:50 - loss: 1.1959 - regression_loss: 1.0077 - classification_loss: 0.1882
 980/1500 [==================>...........] - ETA: 6:49 - loss: 1.1959 - regression_loss: 1.0078 - classification_loss: 0.1881
 981/1500 [==================>...........] - ETA: 6:48 - loss: 1.1957 - regression_loss: 1.0076 - classification_loss: 0.1881
 982/1500 [==================>...........] - ETA: 6:47 - loss: 1.1956 - regression_loss: 1.0076 - classification_loss: 0.1880
 983/1500 [==================>...........] - ETA: 6:46 - loss: 1.1957 - regression_loss: 1.0077 - classification_loss: 0.1880
 984/1500 [==================>...........] - ETA: 6:45 - loss: 1.1957 - regression_loss: 1.0077 - classification_loss: 0.1881
 985/1500 [==================>...........] - ETA: 6:44 - loss: 1.1951 - regression_loss: 1.0072 - classification_loss: 0.1879
 986/1500 [==================>...........] - ETA: 6:43 - loss: 1.1944 - regression_loss: 1.0065 - classification_loss: 0.1878
 987/1500 [==================>...........] - ETA: 6:42 - loss: 1.1941 - regression_loss: 1.0064 - classification_loss: 0.1877
 988/1500 [==================>...........] - ETA: 6:41 - loss: 1.1947 - regression_loss: 1.0068 - classification_loss: 0.1879
 989/1500 [==================>...........] - ETA: 6:40 - loss: 1.1947 - regression_loss: 1.0068 - classification_loss: 0.1879
 990/1500 [==================>...........] - ETA: 6:39 - loss: 1.1943 - regression_loss: 1.0065 - classification_loss: 0.1878
 991/1500 [==================>...........] - ETA: 6:38 - loss: 1.1953 - regression_loss: 1.0074 - classification_loss: 0.1879
 992/1500 [==================>...........] - ETA: 6:37 - loss: 1.1947 - regression_loss: 1.0069 - classification_loss: 0.1878
 993/1500 [==================>...........] - ETA: 6:38 - loss: 1.1949 - regression_loss: 1.0071 - classification_loss: 0.1878
 994/1500 [==================>...........] - ETA: 6:37 - loss: 1.1957 - regression_loss: 1.0078 - classification_loss: 0.1879
 995/1500 [==================>...........] - ETA: 6:36 - loss: 1.1956 - regression_loss: 1.0078 - classification_loss: 0.1878
 996/1500 [==================>...........] - ETA: 6:35 - loss: 1.1948 - regression_loss: 1.0072 - classification_loss: 0.1876
 997/1500 [==================>...........] - ETA: 6:35 - loss: 1.1943 - regression_loss: 1.0068 - classification_loss: 0.1875
 998/1500 [==================>...........] - ETA: 6:34 - loss: 1.1942 - regression_loss: 1.0068 - classification_loss: 0.1874
 999/1500 [==================>...........] - ETA: 6:33 - loss: 1.1935 - regression_loss: 1.0062 - classification_loss: 0.1873
1000/1500 [===================>..........] - ETA: 6:32 - loss: 1.1930 - regression_loss: 1.0059 - classification_loss: 0.1872
1001/1500 [===================>..........] - ETA: 6:32 - loss: 1.1926 - regression_loss: 1.0055 - classification_loss: 0.1871
1002/1500 [===================>..........] - ETA: 6:31 - loss: 1.1921 - regression_loss: 1.0052 - classification_loss: 0.1869
1003/1500 [===================>..........] - ETA: 6:30 - loss: 1.1919 - regression_loss: 1.0050 - classification_loss: 0.1870
1004/1500 [===================>..........] - ETA: 6:29 - loss: 1.1917 - regression_loss: 1.0049 - classification_loss: 0.1869
1005/1500 [===================>..........] - ETA: 6:28 - loss: 1.1920 - regression_loss: 1.0051 - classification_loss: 0.1869
1006/1500 [===================>..........] - ETA: 6:28 - loss: 1.1923 - regression_loss: 1.0054 - classification_loss: 0.1869
1007/1500 [===================>..........] - ETA: 6:27 - loss: 1.1922 - regression_loss: 1.0054 - classification_loss: 0.1868
1008/1500 [===================>..........] - ETA: 6:26 - loss: 1.1917 - regression_loss: 1.0050 - classification_loss: 0.1867
1009/1500 [===================>..........] - ETA: 6:25 - loss: 1.1925 - regression_loss: 1.0059 - classification_loss: 0.1866
1010/1500 [===================>..........] - ETA: 6:24 - loss: 1.1923 - regression_loss: 1.0058 - classification_loss: 0.1865
1011/1500 [===================>..........] - ETA: 6:23 - loss: 1.1918 - regression_loss: 1.0054 - classification_loss: 0.1864
1012/1500 [===================>..........] - ETA: 6:23 - loss: 1.1928 - regression_loss: 1.0063 - classification_loss: 0.1865
1013/1500 [===================>..........] - ETA: 6:22 - loss: 1.1921 - regression_loss: 1.0058 - classification_loss: 0.1863
1014/1500 [===================>..........] - ETA: 6:21 - loss: 1.1923 - regression_loss: 1.0059 - classification_loss: 0.1863
1015/1500 [===================>..........] - ETA: 6:21 - loss: 1.1927 - regression_loss: 1.0063 - classification_loss: 0.1864
1016/1500 [===================>..........] - ETA: 6:20 - loss: 1.1919 - regression_loss: 1.0056 - classification_loss: 0.1863
1017/1500 [===================>..........] - ETA: 6:19 - loss: 1.1918 - regression_loss: 1.0056 - classification_loss: 0.1862
1018/1500 [===================>..........] - ETA: 6:19 - loss: 1.1912 - regression_loss: 1.0051 - classification_loss: 0.1861
1019/1500 [===================>..........] - ETA: 6:18 - loss: 1.1905 - regression_loss: 1.0046 - classification_loss: 0.1860
1020/1500 [===================>..........] - ETA: 6:17 - loss: 1.1909 - regression_loss: 1.0049 - classification_loss: 0.1860
1021/1500 [===================>..........] - ETA: 6:16 - loss: 1.1908 - regression_loss: 1.0048 - classification_loss: 0.1860
1022/1500 [===================>..........] - ETA: 6:15 - loss: 1.1905 - regression_loss: 1.0046 - classification_loss: 0.1859
1023/1500 [===================>..........] - ETA: 6:15 - loss: 1.1897 - regression_loss: 1.0040 - classification_loss: 0.1857
1024/1500 [===================>..........] - ETA: 6:14 - loss: 1.1915 - regression_loss: 1.0055 - classification_loss: 0.1860
1025/1500 [===================>..........] - ETA: 6:13 - loss: 1.1910 - regression_loss: 1.0051 - classification_loss: 0.1859
1026/1500 [===================>..........] - ETA: 6:12 - loss: 1.1919 - regression_loss: 1.0057 - classification_loss: 0.1862
1027/1500 [===================>..........] - ETA: 6:12 - loss: 1.1927 - regression_loss: 1.0066 - classification_loss: 0.1862
1028/1500 [===================>..........] - ETA: 6:11 - loss: 1.1936 - regression_loss: 1.0074 - classification_loss: 0.1863
1029/1500 [===================>..........] - ETA: 6:10 - loss: 1.1932 - regression_loss: 1.0070 - classification_loss: 0.1862
1030/1500 [===================>..........] - ETA: 6:10 - loss: 1.1933 - regression_loss: 1.0072 - classification_loss: 0.1861
1031/1500 [===================>..........] - ETA: 6:09 - loss: 1.1929 - regression_loss: 1.0068 - classification_loss: 0.1861
1032/1500 [===================>..........] - ETA: 6:09 - loss: 1.1925 - regression_loss: 1.0065 - classification_loss: 0.1861
1033/1500 [===================>..........] - ETA: 6:08 - loss: 1.1923 - regression_loss: 1.0063 - classification_loss: 0.1860
1034/1500 [===================>..........] - ETA: 6:07 - loss: 1.1929 - regression_loss: 1.0067 - classification_loss: 0.1862
1035/1500 [===================>..........] - ETA: 6:06 - loss: 1.1923 - regression_loss: 1.0061 - classification_loss: 0.1862
1036/1500 [===================>..........] - ETA: 6:05 - loss: 1.1921 - regression_loss: 1.0060 - classification_loss: 0.1861
1037/1500 [===================>..........] - ETA: 6:04 - loss: 1.1921 - regression_loss: 1.0058 - classification_loss: 0.1863
1038/1500 [===================>..........] - ETA: 6:04 - loss: 1.1917 - regression_loss: 1.0054 - classification_loss: 0.1862
1039/1500 [===================>..........] - ETA: 6:03 - loss: 1.1913 - regression_loss: 1.0052 - classification_loss: 0.1861
1040/1500 [===================>..........] - ETA: 6:02 - loss: 1.1913 - regression_loss: 1.0053 - classification_loss: 0.1860
1041/1500 [===================>..........] - ETA: 6:01 - loss: 1.1906 - regression_loss: 1.0047 - classification_loss: 0.1859
1042/1500 [===================>..........] - ETA: 6:00 - loss: 1.1904 - regression_loss: 1.0045 - classification_loss: 0.1859
1043/1500 [===================>..........] - ETA: 6:00 - loss: 1.1895 - regression_loss: 1.0038 - classification_loss: 0.1858
1044/1500 [===================>..........] - ETA: 5:59 - loss: 1.1889 - regression_loss: 1.0033 - classification_loss: 0.1856
1045/1500 [===================>..........] - ETA: 5:58 - loss: 1.1883 - regression_loss: 1.0028 - classification_loss: 0.1855
1046/1500 [===================>..........] - ETA: 5:57 - loss: 1.1882 - regression_loss: 1.0027 - classification_loss: 0.1855
1047/1500 [===================>..........] - ETA: 5:57 - loss: 1.1886 - regression_loss: 1.0031 - classification_loss: 0.1856
1048/1500 [===================>..........] - ETA: 5:56 - loss: 1.1896 - regression_loss: 1.0037 - classification_loss: 0.1859
1049/1500 [===================>..........] - ETA: 5:55 - loss: 1.1891 - regression_loss: 1.0033 - classification_loss: 0.1858
1050/1500 [====================>.........] - ETA: 5:54 - loss: 1.1898 - regression_loss: 1.0039 - classification_loss: 0.1859
1051/1500 [====================>.........] - ETA: 5:54 - loss: 1.1896 - regression_loss: 1.0038 - classification_loss: 0.1859
1052/1500 [====================>.........] - ETA: 5:53 - loss: 1.1897 - regression_loss: 1.0039 - classification_loss: 0.1858
1053/1500 [====================>.........] - ETA: 5:52 - loss: 1.1896 - regression_loss: 1.0039 - classification_loss: 0.1857
1054/1500 [====================>.........] - ETA: 5:51 - loss: 1.1898 - regression_loss: 1.0041 - classification_loss: 0.1857
1055/1500 [====================>.........] - ETA: 5:50 - loss: 1.1896 - regression_loss: 1.0040 - classification_loss: 0.1856
1056/1500 [====================>.........] - ETA: 5:49 - loss: 1.1893 - regression_loss: 1.0037 - classification_loss: 0.1855
1057/1500 [====================>.........] - ETA: 5:49 - loss: 1.1904 - regression_loss: 1.0047 - classification_loss: 0.1857
1058/1500 [====================>.........] - ETA: 5:48 - loss: 1.1911 - regression_loss: 1.0053 - classification_loss: 0.1858
1059/1500 [====================>.........] - ETA: 5:47 - loss: 1.1905 - regression_loss: 1.0048 - classification_loss: 0.1857
1060/1500 [====================>.........] - ETA: 5:46 - loss: 1.1899 - regression_loss: 1.0044 - classification_loss: 0.1855
1061/1500 [====================>.........] - ETA: 5:45 - loss: 1.1896 - regression_loss: 1.0042 - classification_loss: 0.1854
1062/1500 [====================>.........] - ETA: 5:45 - loss: 1.1900 - regression_loss: 1.0045 - classification_loss: 0.1856
1063/1500 [====================>.........] - ETA: 5:44 - loss: 1.1902 - regression_loss: 1.0046 - classification_loss: 0.1856
1064/1500 [====================>.........] - ETA: 5:43 - loss: 1.1897 - regression_loss: 1.0043 - classification_loss: 0.1854
1065/1500 [====================>.........] - ETA: 5:42 - loss: 1.1895 - regression_loss: 1.0042 - classification_loss: 0.1853
1066/1500 [====================>.........] - ETA: 5:41 - loss: 1.1893 - regression_loss: 1.0040 - classification_loss: 0.1853
1067/1500 [====================>.........] - ETA: 5:41 - loss: 1.1891 - regression_loss: 1.0039 - classification_loss: 0.1852
1068/1500 [====================>.........] - ETA: 5:40 - loss: 1.1887 - regression_loss: 1.0036 - classification_loss: 0.1852
1069/1500 [====================>.........] - ETA: 5:39 - loss: 1.1881 - regression_loss: 1.0031 - classification_loss: 0.1850
1070/1500 [====================>.........] - ETA: 5:39 - loss: 1.1881 - regression_loss: 1.0031 - classification_loss: 0.1850
1071/1500 [====================>.........] - ETA: 5:38 - loss: 1.1881 - regression_loss: 1.0031 - classification_loss: 0.1850
1072/1500 [====================>.........] - ETA: 5:37 - loss: 1.1874 - regression_loss: 1.0026 - classification_loss: 0.1849
1073/1500 [====================>.........] - ETA: 5:36 - loss: 1.1879 - regression_loss: 1.0030 - classification_loss: 0.1850
1074/1500 [====================>.........] - ETA: 5:35 - loss: 1.1877 - regression_loss: 1.0028 - classification_loss: 0.1849
1075/1500 [====================>.........] - ETA: 5:35 - loss: 1.1873 - regression_loss: 1.0026 - classification_loss: 0.1848
1076/1500 [====================>.........] - ETA: 5:34 - loss: 1.1883 - regression_loss: 1.0034 - classification_loss: 0.1849
1077/1500 [====================>.........] - ETA: 5:33 - loss: 1.1885 - regression_loss: 1.0034 - classification_loss: 0.1851
1078/1500 [====================>.........] - ETA: 5:32 - loss: 1.1880 - regression_loss: 1.0030 - classification_loss: 0.1850
1079/1500 [====================>.........] - ETA: 5:32 - loss: 1.1884 - regression_loss: 1.0033 - classification_loss: 0.1851
1080/1500 [====================>.........] - ETA: 5:31 - loss: 1.1878 - regression_loss: 1.0029 - classification_loss: 0.1850
1081/1500 [====================>.........] - ETA: 5:30 - loss: 1.1890 - regression_loss: 1.0038 - classification_loss: 0.1851
1082/1500 [====================>.........] - ETA: 5:29 - loss: 1.1894 - regression_loss: 1.0043 - classification_loss: 0.1851
1083/1500 [====================>.........] - ETA: 5:28 - loss: 1.1891 - regression_loss: 1.0042 - classification_loss: 0.1850
1084/1500 [====================>.........] - ETA: 5:27 - loss: 1.1890 - regression_loss: 1.0042 - classification_loss: 0.1849
1085/1500 [====================>.........] - ETA: 5:26 - loss: 1.1885 - regression_loss: 1.0037 - classification_loss: 0.1848
1086/1500 [====================>.........] - ETA: 5:26 - loss: 1.1882 - regression_loss: 1.0035 - classification_loss: 0.1847
1087/1500 [====================>.........] - ETA: 5:25 - loss: 1.1880 - regression_loss: 1.0034 - classification_loss: 0.1846
1088/1500 [====================>.........] - ETA: 5:24 - loss: 1.1871 - regression_loss: 1.0027 - classification_loss: 0.1844
1089/1500 [====================>.........] - ETA: 5:23 - loss: 1.1865 - regression_loss: 1.0022 - classification_loss: 0.1843
1090/1500 [====================>.........] - ETA: 5:23 - loss: 1.1856 - regression_loss: 1.0013 - classification_loss: 0.1843
1091/1500 [====================>.........] - ETA: 5:22 - loss: 1.1856 - regression_loss: 1.0014 - classification_loss: 0.1842
1092/1500 [====================>.........] - ETA: 5:21 - loss: 1.1851 - regression_loss: 1.0009 - classification_loss: 0.1841
1093/1500 [====================>.........] - ETA: 5:21 - loss: 1.1847 - regression_loss: 1.0006 - classification_loss: 0.1841
1094/1500 [====================>.........] - ETA: 5:20 - loss: 1.1844 - regression_loss: 1.0005 - classification_loss: 0.1840
1095/1500 [====================>.........] - ETA: 5:19 - loss: 1.1844 - regression_loss: 1.0005 - classification_loss: 0.1839
1096/1500 [====================>.........] - ETA: 5:18 - loss: 1.1841 - regression_loss: 1.0002 - classification_loss: 0.1839
1097/1500 [====================>.........] - ETA: 5:17 - loss: 1.1840 - regression_loss: 1.0002 - classification_loss: 0.1838
1098/1500 [====================>.........] - ETA: 5:16 - loss: 1.1846 - regression_loss: 1.0008 - classification_loss: 0.1838
1099/1500 [====================>.........] - ETA: 5:16 - loss: 1.1840 - regression_loss: 1.0003 - classification_loss: 0.1837
1100/1500 [=====================>........] - ETA: 5:15 - loss: 1.1841 - regression_loss: 1.0004 - classification_loss: 0.1837
1101/1500 [=====================>........] - ETA: 5:14 - loss: 1.1836 - regression_loss: 1.0000 - classification_loss: 0.1835
1102/1500 [=====================>........] - ETA: 5:13 - loss: 1.1847 - regression_loss: 1.0008 - classification_loss: 0.1839
1103/1500 [=====================>........] - ETA: 5:12 - loss: 1.1851 - regression_loss: 1.0011 - classification_loss: 0.1840
1104/1500 [=====================>........] - ETA: 5:11 - loss: 1.1860 - regression_loss: 1.0017 - classification_loss: 0.1843
1105/1500 [=====================>........] - ETA: 5:11 - loss: 1.1856 - regression_loss: 1.0015 - classification_loss: 0.1841
1106/1500 [=====================>........] - ETA: 5:10 - loss: 1.1860 - regression_loss: 1.0018 - classification_loss: 0.1843
1107/1500 [=====================>........] - ETA: 5:09 - loss: 1.1860 - regression_loss: 1.0018 - classification_loss: 0.1842
1108/1500 [=====================>........] - ETA: 5:08 - loss: 1.1858 - regression_loss: 1.0017 - classification_loss: 0.1841
1109/1500 [=====================>........] - ETA: 5:07 - loss: 1.1853 - regression_loss: 1.0013 - classification_loss: 0.1840
1110/1500 [=====================>........] - ETA: 5:06 - loss: 1.1848 - regression_loss: 1.0009 - classification_loss: 0.1840
1111/1500 [=====================>........] - ETA: 5:06 - loss: 1.1857 - regression_loss: 1.0016 - classification_loss: 0.1841
1112/1500 [=====================>........] - ETA: 5:05 - loss: 1.1856 - regression_loss: 1.0014 - classification_loss: 0.1841
1113/1500 [=====================>........] - ETA: 5:04 - loss: 1.1853 - regression_loss: 1.0012 - classification_loss: 0.1841
1114/1500 [=====================>........] - ETA: 5:03 - loss: 1.1857 - regression_loss: 1.0011 - classification_loss: 0.1846
1115/1500 [=====================>........] - ETA: 5:02 - loss: 1.1857 - regression_loss: 1.0012 - classification_loss: 0.1845
1116/1500 [=====================>........] - ETA: 5:02 - loss: 1.1851 - regression_loss: 1.0007 - classification_loss: 0.1844
1117/1500 [=====================>........] - ETA: 5:01 - loss: 1.1850 - regression_loss: 1.0004 - classification_loss: 0.1846
1118/1500 [=====================>........] - ETA: 5:00 - loss: 1.1850 - regression_loss: 1.0005 - classification_loss: 0.1845
1119/1500 [=====================>........] - ETA: 5:00 - loss: 1.1851 - regression_loss: 1.0004 - classification_loss: 0.1847
1120/1500 [=====================>........] - ETA: 4:59 - loss: 1.1849 - regression_loss: 1.0002 - classification_loss: 0.1847
1121/1500 [=====================>........] - ETA: 4:58 - loss: 1.1845 - regression_loss: 0.9998 - classification_loss: 0.1846
1122/1500 [=====================>........] - ETA: 4:57 - loss: 1.1840 - regression_loss: 0.9994 - classification_loss: 0.1845
1123/1500 [=====================>........] - ETA: 4:57 - loss: 1.1835 - regression_loss: 0.9991 - classification_loss: 0.1844
1124/1500 [=====================>........] - ETA: 4:56 - loss: 1.1832 - regression_loss: 0.9988 - classification_loss: 0.1844
1125/1500 [=====================>........] - ETA: 4:55 - loss: 1.1842 - regression_loss: 0.9996 - classification_loss: 0.1846
1126/1500 [=====================>........] - ETA: 4:54 - loss: 1.1836 - regression_loss: 0.9990 - classification_loss: 0.1845
1127/1500 [=====================>........] - ETA: 4:53 - loss: 1.1834 - regression_loss: 0.9989 - classification_loss: 0.1845
1128/1500 [=====================>........] - ETA: 4:52 - loss: 1.1843 - regression_loss: 0.9996 - classification_loss: 0.1847
1129/1500 [=====================>........] - ETA: 4:52 - loss: 1.1841 - regression_loss: 0.9994 - classification_loss: 0.1847
1130/1500 [=====================>........] - ETA: 4:51 - loss: 1.1838 - regression_loss: 0.9992 - classification_loss: 0.1847
1131/1500 [=====================>........] - ETA: 4:50 - loss: 1.1843 - regression_loss: 0.9995 - classification_loss: 0.1847
1132/1500 [=====================>........] - ETA: 4:50 - loss: 1.1837 - regression_loss: 0.9991 - classification_loss: 0.1846
1133/1500 [=====================>........] - ETA: 4:49 - loss: 1.1832 - regression_loss: 0.9987 - classification_loss: 0.1845
1134/1500 [=====================>........] - ETA: 4:48 - loss: 1.1829 - regression_loss: 0.9985 - classification_loss: 0.1844
1135/1500 [=====================>........] - ETA: 4:47 - loss: 1.1830 - regression_loss: 0.9985 - classification_loss: 0.1845
1136/1500 [=====================>........] - ETA: 4:46 - loss: 1.1829 - regression_loss: 0.9984 - classification_loss: 0.1845
1137/1500 [=====================>........] - ETA: 4:45 - loss: 1.1823 - regression_loss: 0.9979 - classification_loss: 0.1844
1138/1500 [=====================>........] - ETA: 4:44 - loss: 1.1820 - regression_loss: 0.9977 - classification_loss: 0.1843
1139/1500 [=====================>........] - ETA: 4:44 - loss: 1.1828 - regression_loss: 0.9983 - classification_loss: 0.1846
1140/1500 [=====================>........] - ETA: 4:43 - loss: 1.1825 - regression_loss: 0.9980 - classification_loss: 0.1845
1141/1500 [=====================>........] - ETA: 4:42 - loss: 1.1831 - regression_loss: 0.9985 - classification_loss: 0.1846
1142/1500 [=====================>........] - ETA: 4:41 - loss: 1.1824 - regression_loss: 0.9979 - classification_loss: 0.1845
1143/1500 [=====================>........] - ETA: 4:40 - loss: 1.1825 - regression_loss: 0.9981 - classification_loss: 0.1844
1144/1500 [=====================>........] - ETA: 4:39 - loss: 1.1823 - regression_loss: 0.9979 - classification_loss: 0.1844
1145/1500 [=====================>........] - ETA: 4:39 - loss: 1.1818 - regression_loss: 0.9974 - classification_loss: 0.1843
1146/1500 [=====================>........] - ETA: 4:38 - loss: 1.1827 - regression_loss: 0.9983 - classification_loss: 0.1845
1147/1500 [=====================>........] - ETA: 4:37 - loss: 1.1827 - regression_loss: 0.9983 - classification_loss: 0.1845
1148/1500 [=====================>........] - ETA: 4:36 - loss: 1.1827 - regression_loss: 0.9983 - classification_loss: 0.1844
1149/1500 [=====================>........] - ETA: 4:35 - loss: 1.1823 - regression_loss: 0.9979 - classification_loss: 0.1843
1150/1500 [======================>.......] - ETA: 4:35 - loss: 1.1831 - regression_loss: 0.9987 - classification_loss: 0.1845
1151/1500 [======================>.......] - ETA: 4:34 - loss: 1.1826 - regression_loss: 0.9982 - classification_loss: 0.1843
1152/1500 [======================>.......] - ETA: 4:33 - loss: 1.1825 - regression_loss: 0.9982 - classification_loss: 0.1843
1153/1500 [======================>.......] - ETA: 4:32 - loss: 1.1822 - regression_loss: 0.9979 - classification_loss: 0.1842
1154/1500 [======================>.......] - ETA: 4:31 - loss: 1.1815 - regression_loss: 0.9974 - classification_loss: 0.1841
1155/1500 [======================>.......] - ETA: 4:30 - loss: 1.1812 - regression_loss: 0.9971 - classification_loss: 0.1840
1156/1500 [======================>.......] - ETA: 4:30 - loss: 1.1809 - regression_loss: 0.9968 - classification_loss: 0.1841
1157/1500 [======================>.......] - ETA: 4:29 - loss: 1.1809 - regression_loss: 0.9968 - classification_loss: 0.1840
1158/1500 [======================>.......] - ETA: 4:28 - loss: 1.1802 - regression_loss: 0.9963 - classification_loss: 0.1839
1159/1500 [======================>.......] - ETA: 4:27 - loss: 1.1798 - regression_loss: 0.9960 - classification_loss: 0.1838
1160/1500 [======================>.......] - ETA: 4:26 - loss: 1.1793 - regression_loss: 0.9956 - classification_loss: 0.1837
1161/1500 [======================>.......] - ETA: 4:25 - loss: 1.1786 - regression_loss: 0.9951 - classification_loss: 0.1835
1162/1500 [======================>.......] - ETA: 4:24 - loss: 1.1786 - regression_loss: 0.9951 - classification_loss: 0.1835
1163/1500 [======================>.......] - ETA: 4:24 - loss: 1.1784 - regression_loss: 0.9950 - classification_loss: 0.1834
1164/1500 [======================>.......] - ETA: 4:23 - loss: 1.1794 - regression_loss: 0.9957 - classification_loss: 0.1837
1165/1500 [======================>.......] - ETA: 4:22 - loss: 1.1799 - regression_loss: 0.9961 - classification_loss: 0.1837
1166/1500 [======================>.......] - ETA: 4:21 - loss: 1.1800 - regression_loss: 0.9963 - classification_loss: 0.1837
1167/1500 [======================>.......] - ETA: 4:20 - loss: 1.1797 - regression_loss: 0.9960 - classification_loss: 0.1837
1168/1500 [======================>.......] - ETA: 4:19 - loss: 1.1792 - regression_loss: 0.9956 - classification_loss: 0.1836
1169/1500 [======================>.......] - ETA: 4:18 - loss: 1.1790 - regression_loss: 0.9954 - classification_loss: 0.1836
1170/1500 [======================>.......] - ETA: 4:17 - loss: 1.1788 - regression_loss: 0.9952 - classification_loss: 0.1836
1171/1500 [======================>.......] - ETA: 4:17 - loss: 1.1785 - regression_loss: 0.9950 - classification_loss: 0.1835
1172/1500 [======================>.......] - ETA: 4:16 - loss: 1.1783 - regression_loss: 0.9949 - classification_loss: 0.1834
1173/1500 [======================>.......] - ETA: 4:15 - loss: 1.1777 - regression_loss: 0.9943 - classification_loss: 0.1834
1174/1500 [======================>.......] - ETA: 4:14 - loss: 1.1778 - regression_loss: 0.9944 - classification_loss: 0.1834
1175/1500 [======================>.......] - ETA: 4:13 - loss: 1.1779 - regression_loss: 0.9944 - classification_loss: 0.1834
1176/1500 [======================>.......] - ETA: 4:13 - loss: 1.1779 - regression_loss: 0.9945 - classification_loss: 0.1834
1177/1500 [======================>.......] - ETA: 4:12 - loss: 1.1791 - regression_loss: 0.9947 - classification_loss: 0.1844
1178/1500 [======================>.......] - ETA: 4:11 - loss: 1.1791 - regression_loss: 0.9948 - classification_loss: 0.1843
1179/1500 [======================>.......] - ETA: 4:10 - loss: 1.1789 - regression_loss: 0.9946 - classification_loss: 0.1842
1180/1500 [======================>.......] - ETA: 4:10 - loss: 1.1787 - regression_loss: 0.9945 - classification_loss: 0.1842
1181/1500 [======================>.......] - ETA: 4:09 - loss: 1.1784 - regression_loss: 0.9943 - classification_loss: 0.1841
1182/1500 [======================>.......] - ETA: 4:08 - loss: 1.1778 - regression_loss: 0.9939 - classification_loss: 0.1840
1183/1500 [======================>.......] - ETA: 4:07 - loss: 1.1776 - regression_loss: 0.9937 - classification_loss: 0.1839
1184/1500 [======================>.......] - ETA: 4:06 - loss: 1.1775 - regression_loss: 0.9936 - classification_loss: 0.1839
1185/1500 [======================>.......] - ETA: 4:06 - loss: 1.1787 - regression_loss: 0.9944 - classification_loss: 0.1842
1186/1500 [======================>.......] - ETA: 4:05 - loss: 1.1786 - regression_loss: 0.9944 - classification_loss: 0.1842
1187/1500 [======================>.......] - ETA: 4:04 - loss: 1.1780 - regression_loss: 0.9939 - classification_loss: 0.1841
1188/1500 [======================>.......] - ETA: 4:03 - loss: 1.1780 - regression_loss: 0.9940 - classification_loss: 0.1841
1189/1500 [======================>.......] - ETA: 4:02 - loss: 1.1775 - regression_loss: 0.9935 - classification_loss: 0.1840
1190/1500 [======================>.......] - ETA: 4:01 - loss: 1.1777 - regression_loss: 0.9938 - classification_loss: 0.1839
1191/1500 [======================>.......] - ETA: 4:01 - loss: 1.1771 - regression_loss: 0.9932 - classification_loss: 0.1839
1192/1500 [======================>.......] - ETA: 4:00 - loss: 1.1765 - regression_loss: 0.9927 - classification_loss: 0.1838
1193/1500 [======================>.......] - ETA: 3:59 - loss: 1.1765 - regression_loss: 0.9927 - classification_loss: 0.1838
1194/1500 [======================>.......] - ETA: 3:59 - loss: 1.1773 - regression_loss: 0.9932 - classification_loss: 0.1841
1195/1500 [======================>.......] - ETA: 3:58 - loss: 1.1772 - regression_loss: 0.9932 - classification_loss: 0.1841
1196/1500 [======================>.......] - ETA: 3:57 - loss: 1.1776 - regression_loss: 0.9935 - classification_loss: 0.1841
1197/1500 [======================>.......] - ETA: 3:56 - loss: 1.1771 - regression_loss: 0.9932 - classification_loss: 0.1840
1198/1500 [======================>.......] - ETA: 3:55 - loss: 1.1768 - regression_loss: 0.9929 - classification_loss: 0.1839
1199/1500 [======================>.......] - ETA: 3:55 - loss: 1.1767 - regression_loss: 0.9928 - classification_loss: 0.1839
1200/1500 [=======================>......] - ETA: 3:54 - loss: 1.1765 - regression_loss: 0.9926 - classification_loss: 0.1838
1201/1500 [=======================>......] - ETA: 3:54 - loss: 1.1761 - regression_loss: 0.9924 - classification_loss: 0.1838
1202/1500 [=======================>......] - ETA: 3:53 - loss: 1.1765 - regression_loss: 0.9927 - classification_loss: 0.1838
1203/1500 [=======================>......] - ETA: 3:52 - loss: 1.1762 - regression_loss: 0.9925 - classification_loss: 0.1838
1204/1500 [=======================>......] - ETA: 3:52 - loss: 1.1765 - regression_loss: 0.9928 - classification_loss: 0.1837
1205/1500 [=======================>......] - ETA: 3:51 - loss: 1.1770 - regression_loss: 0.9932 - classification_loss: 0.1838
1206/1500 [=======================>......] - ETA: 3:50 - loss: 1.1766 - regression_loss: 0.9929 - classification_loss: 0.1837
1207/1500 [=======================>......] - ETA: 3:49 - loss: 1.1765 - regression_loss: 0.9928 - classification_loss: 0.1837
1208/1500 [=======================>......] - ETA: 3:49 - loss: 1.1776 - regression_loss: 0.9936 - classification_loss: 0.1839
1209/1500 [=======================>......] - ETA: 3:48 - loss: 1.1778 - regression_loss: 0.9939 - classification_loss: 0.1839
1210/1500 [=======================>......] - ETA: 3:47 - loss: 1.1775 - regression_loss: 0.9936 - classification_loss: 0.1839
1211/1500 [=======================>......] - ETA: 3:46 - loss: 1.1777 - regression_loss: 0.9938 - classification_loss: 0.1839
1212/1500 [=======================>......] - ETA: 3:46 - loss: 1.1773 - regression_loss: 0.9935 - classification_loss: 0.1838
1213/1500 [=======================>......] - ETA: 3:45 - loss: 1.1775 - regression_loss: 0.9936 - classification_loss: 0.1839
1214/1500 [=======================>......] - ETA: 3:44 - loss: 1.1776 - regression_loss: 0.9935 - classification_loss: 0.1840
1215/1500 [=======================>......] - ETA: 3:44 - loss: 1.1788 - regression_loss: 0.9933 - classification_loss: 0.1855
1216/1500 [=======================>......] - ETA: 3:43 - loss: 1.1798 - regression_loss: 0.9943 - classification_loss: 0.1855
1217/1500 [=======================>......] - ETA: 3:42 - loss: 1.1798 - regression_loss: 0.9943 - classification_loss: 0.1855
1218/1500 [=======================>......] - ETA: 3:41 - loss: 1.1804 - regression_loss: 0.9948 - classification_loss: 0.1855
1219/1500 [=======================>......] - ETA: 3:41 - loss: 1.1803 - regression_loss: 0.9948 - classification_loss: 0.1855
1220/1500 [=======================>......] - ETA: 3:40 - loss: 1.1799 - regression_loss: 0.9945 - classification_loss: 0.1854
1221/1500 [=======================>......] - ETA: 3:39 - loss: 1.1797 - regression_loss: 0.9943 - classification_loss: 0.1853
1222/1500 [=======================>......] - ETA: 3:39 - loss: 1.1806 - regression_loss: 0.9950 - classification_loss: 0.1856
1223/1500 [=======================>......] - ETA: 3:38 - loss: 1.1809 - regression_loss: 0.9953 - classification_loss: 0.1856
1224/1500 [=======================>......] - ETA: 3:37 - loss: 1.1804 - regression_loss: 0.9949 - classification_loss: 0.1855
1225/1500 [=======================>......] - ETA: 3:36 - loss: 1.1806 - regression_loss: 0.9951 - classification_loss: 0.1855
1226/1500 [=======================>......] - ETA: 3:35 - loss: 1.1800 - regression_loss: 0.9946 - classification_loss: 0.1854
1227/1500 [=======================>......] - ETA: 3:35 - loss: 1.1804 - regression_loss: 0.9951 - classification_loss: 0.1853
1228/1500 [=======================>......] - ETA: 3:34 - loss: 1.1800 - regression_loss: 0.9948 - classification_loss: 0.1852
1229/1500 [=======================>......] - ETA: 3:33 - loss: 1.1793 - regression_loss: 0.9942 - classification_loss: 0.1851
1230/1500 [=======================>......] - ETA: 3:32 - loss: 1.1795 - regression_loss: 0.9944 - classification_loss: 0.1851
1231/1500 [=======================>......] - ETA: 3:31 - loss: 1.1804 - regression_loss: 0.9952 - classification_loss: 0.1852
1232/1500 [=======================>......] - ETA: 3:31 - loss: 1.1804 - regression_loss: 0.9952 - classification_loss: 0.1852
1233/1500 [=======================>......] - ETA: 3:30 - loss: 1.1803 - regression_loss: 0.9952 - classification_loss: 0.1851
1234/1500 [=======================>......] - ETA: 3:29 - loss: 1.1802 - regression_loss: 0.9952 - classification_loss: 0.1850
1235/1500 [=======================>......] - ETA: 3:28 - loss: 1.1805 - regression_loss: 0.9956 - classification_loss: 0.1850
1236/1500 [=======================>......] - ETA: 3:28 - loss: 1.1806 - regression_loss: 0.9956 - classification_loss: 0.1850
1237/1500 [=======================>......] - ETA: 3:27 - loss: 1.1811 - regression_loss: 0.9960 - classification_loss: 0.1851
1238/1500 [=======================>......] - ETA: 3:26 - loss: 1.1810 - regression_loss: 0.9959 - classification_loss: 0.1851
1239/1500 [=======================>......] - ETA: 3:25 - loss: 1.1808 - regression_loss: 0.9958 - classification_loss: 0.1851
1240/1500 [=======================>......] - ETA: 3:24 - loss: 1.1804 - regression_loss: 0.9954 - classification_loss: 0.1850
1241/1500 [=======================>......] - ETA: 3:24 - loss: 1.1801 - regression_loss: 0.9951 - classification_loss: 0.1849
1242/1500 [=======================>......] - ETA: 3:23 - loss: 1.1799 - regression_loss: 0.9950 - classification_loss: 0.1849
1243/1500 [=======================>......] - ETA: 3:22 - loss: 1.1797 - regression_loss: 0.9948 - classification_loss: 0.1849
1244/1500 [=======================>......] - ETA: 3:21 - loss: 1.1791 - regression_loss: 0.9943 - classification_loss: 0.1848
1245/1500 [=======================>......] - ETA: 3:20 - loss: 1.1786 - regression_loss: 0.9940 - classification_loss: 0.1847
1246/1500 [=======================>......] - ETA: 3:19 - loss: 1.1780 - regression_loss: 0.9935 - classification_loss: 0.1845
1247/1500 [=======================>......] - ETA: 3:19 - loss: 1.1774 - regression_loss: 0.9930 - classification_loss: 0.1844
1248/1500 [=======================>......] - ETA: 3:18 - loss: 1.1777 - regression_loss: 0.9933 - classification_loss: 0.1844
1249/1500 [=======================>......] - ETA: 3:17 - loss: 1.1774 - regression_loss: 0.9930 - classification_loss: 0.1844
1250/1500 [========================>.....] - ETA: 3:16 - loss: 1.1782 - regression_loss: 0.9935 - classification_loss: 0.1847
1251/1500 [========================>.....] - ETA: 3:15 - loss: 1.1775 - regression_loss: 0.9930 - classification_loss: 0.1846
1252/1500 [========================>.....] - ETA: 3:15 - loss: 1.1770 - regression_loss: 0.9926 - classification_loss: 0.1845
1253/1500 [========================>.....] - ETA: 3:14 - loss: 1.1768 - regression_loss: 0.9924 - classification_loss: 0.1844
1254/1500 [========================>.....] - ETA: 3:13 - loss: 1.1763 - regression_loss: 0.9920 - classification_loss: 0.1843
1255/1500 [========================>.....] - ETA: 3:12 - loss: 1.1760 - regression_loss: 0.9918 - classification_loss: 0.1842
1256/1500 [========================>.....] - ETA: 3:12 - loss: 1.1758 - regression_loss: 0.9915 - classification_loss: 0.1842
1257/1500 [========================>.....] - ETA: 3:11 - loss: 1.1756 - regression_loss: 0.9914 - classification_loss: 0.1842
1258/1500 [========================>.....] - ETA: 3:10 - loss: 1.1752 - regression_loss: 0.9911 - classification_loss: 0.1841
1259/1500 [========================>.....] - ETA: 3:09 - loss: 1.1751 - regression_loss: 0.9911 - classification_loss: 0.1840
1260/1500 [========================>.....] - ETA: 3:08 - loss: 1.1753 - regression_loss: 0.9913 - classification_loss: 0.1840
1261/1500 [========================>.....] - ETA: 3:08 - loss: 1.1753 - regression_loss: 0.9913 - classification_loss: 0.1839
1262/1500 [========================>.....] - ETA: 3:07 - loss: 1.1760 - regression_loss: 0.9918 - classification_loss: 0.1841
1263/1500 [========================>.....] - ETA: 3:06 - loss: 1.1757 - regression_loss: 0.9915 - classification_loss: 0.1841
1264/1500 [========================>.....] - ETA: 3:05 - loss: 1.1754 - regression_loss: 0.9913 - classification_loss: 0.1841
1265/1500 [========================>.....] - ETA: 3:05 - loss: 1.1756 - regression_loss: 0.9914 - classification_loss: 0.1841
1266/1500 [========================>.....] - ETA: 3:04 - loss: 1.1761 - regression_loss: 0.9919 - classification_loss: 0.1842
1267/1500 [========================>.....] - ETA: 3:03 - loss: 1.1758 - regression_loss: 0.9917 - classification_loss: 0.1841
1268/1500 [========================>.....] - ETA: 3:02 - loss: 1.1752 - regression_loss: 0.9912 - classification_loss: 0.1840
1269/1500 [========================>.....] - ETA: 3:01 - loss: 1.1756 - regression_loss: 0.9916 - classification_loss: 0.1840
1270/1500 [========================>.....] - ETA: 3:01 - loss: 1.1758 - regression_loss: 0.9918 - classification_loss: 0.1840
1271/1500 [========================>.....] - ETA: 3:00 - loss: 1.1753 - regression_loss: 0.9914 - classification_loss: 0.1839
1272/1500 [========================>.....] - ETA: 2:59 - loss: 1.1756 - regression_loss: 0.9917 - classification_loss: 0.1840
1273/1500 [========================>.....] - ETA: 2:58 - loss: 1.1755 - regression_loss: 0.9916 - classification_loss: 0.1839
1274/1500 [========================>.....] - ETA: 2:58 - loss: 1.1758 - regression_loss: 0.9919 - classification_loss: 0.1839
1275/1500 [========================>.....] - ETA: 2:57 - loss: 1.1765 - regression_loss: 0.9924 - classification_loss: 0.1840
1276/1500 [========================>.....] - ETA: 2:56 - loss: 1.1762 - regression_loss: 0.9921 - classification_loss: 0.1841
1277/1500 [========================>.....] - ETA: 2:55 - loss: 1.1772 - regression_loss: 0.9931 - classification_loss: 0.1841
1278/1500 [========================>.....] - ETA: 2:55 - loss: 1.1770 - regression_loss: 0.9929 - classification_loss: 0.1841
1279/1500 [========================>.....] - ETA: 2:54 - loss: 1.1765 - regression_loss: 0.9925 - classification_loss: 0.1840
1280/1500 [========================>.....] - ETA: 2:53 - loss: 1.1768 - regression_loss: 0.9928 - classification_loss: 0.1840
1281/1500 [========================>.....] - ETA: 2:52 - loss: 1.1772 - regression_loss: 0.9930 - classification_loss: 0.1842
1282/1500 [========================>.....] - ETA: 2:52 - loss: 1.1773 - regression_loss: 0.9931 - classification_loss: 0.1841
1283/1500 [========================>.....] - ETA: 2:51 - loss: 1.1773 - regression_loss: 0.9932 - classification_loss: 0.1841
1284/1500 [========================>.....] - ETA: 2:50 - loss: 1.1779 - regression_loss: 0.9937 - classification_loss: 0.1842
1285/1500 [========================>.....] - ETA: 2:49 - loss: 1.1775 - regression_loss: 0.9934 - classification_loss: 0.1841
1286/1500 [========================>.....] - ETA: 2:48 - loss: 1.1773 - regression_loss: 0.9933 - classification_loss: 0.1840
1287/1500 [========================>.....] - ETA: 2:48 - loss: 1.1781 - regression_loss: 0.9939 - classification_loss: 0.1842
1288/1500 [========================>.....] - ETA: 2:47 - loss: 1.1779 - regression_loss: 0.9937 - classification_loss: 0.1842
1289/1500 [========================>.....] - ETA: 2:46 - loss: 1.1779 - regression_loss: 0.9937 - classification_loss: 0.1842
1290/1500 [========================>.....] - ETA: 2:45 - loss: 1.1775 - regression_loss: 0.9934 - classification_loss: 0.1841
1291/1500 [========================>.....] - ETA: 2:45 - loss: 1.1775 - regression_loss: 0.9934 - classification_loss: 0.1840
1292/1500 [========================>.....] - ETA: 2:44 - loss: 1.1782 - regression_loss: 0.9941 - classification_loss: 0.1841
1293/1500 [========================>.....] - ETA: 2:43 - loss: 1.1779 - regression_loss: 0.9939 - classification_loss: 0.1841
1294/1500 [========================>.....] - ETA: 2:42 - loss: 1.1775 - regression_loss: 0.9935 - classification_loss: 0.1840
1295/1500 [========================>.....] - ETA: 2:41 - loss: 1.1774 - regression_loss: 0.9935 - classification_loss: 0.1839
1296/1500 [========================>.....] - ETA: 2:41 - loss: 1.1768 - regression_loss: 0.9930 - classification_loss: 0.1838
1297/1500 [========================>.....] - ETA: 2:40 - loss: 1.1766 - regression_loss: 0.9928 - classification_loss: 0.1838
1298/1500 [========================>.....] - ETA: 2:39 - loss: 1.1767 - regression_loss: 0.9929 - classification_loss: 0.1838
1299/1500 [========================>.....] - ETA: 2:38 - loss: 1.1764 - regression_loss: 0.9927 - classification_loss: 0.1837
1300/1500 [=========================>....] - ETA: 2:37 - loss: 1.1772 - regression_loss: 0.9932 - classification_loss: 0.1840
1301/1500 [=========================>....] - ETA: 2:36 - loss: 1.1769 - regression_loss: 0.9931 - classification_loss: 0.1839
1302/1500 [=========================>....] - ETA: 2:36 - loss: 1.1766 - regression_loss: 0.9928 - classification_loss: 0.1838
1303/1500 [=========================>....] - ETA: 2:35 - loss: 1.1761 - regression_loss: 0.9924 - classification_loss: 0.1837
1304/1500 [=========================>....] - ETA: 2:34 - loss: 1.1756 - regression_loss: 0.9920 - classification_loss: 0.1837
1305/1500 [=========================>....] - ETA: 2:33 - loss: 1.1755 - regression_loss: 0.9919 - classification_loss: 0.1836
1306/1500 [=========================>....] - ETA: 2:32 - loss: 1.1755 - regression_loss: 0.9919 - classification_loss: 0.1835
1307/1500 [=========================>....] - ETA: 2:32 - loss: 1.1756 - regression_loss: 0.9921 - classification_loss: 0.1835
1308/1500 [=========================>....] - ETA: 2:31 - loss: 1.1752 - regression_loss: 0.9918 - classification_loss: 0.1834
1309/1500 [=========================>....] - ETA: 2:30 - loss: 1.1750 - regression_loss: 0.9917 - classification_loss: 0.1834
1310/1500 [=========================>....] - ETA: 2:29 - loss: 1.1749 - regression_loss: 0.9916 - classification_loss: 0.1833
1311/1500 [=========================>....] - ETA: 2:28 - loss: 1.1745 - regression_loss: 0.9912 - classification_loss: 0.1833
1312/1500 [=========================>....] - ETA: 2:28 - loss: 1.1746 - regression_loss: 0.9913 - classification_loss: 0.1833
1313/1500 [=========================>....] - ETA: 2:27 - loss: 1.1742 - regression_loss: 0.9911 - classification_loss: 0.1832
1314/1500 [=========================>....] - ETA: 2:26 - loss: 1.1740 - regression_loss: 0.9909 - classification_loss: 0.1831
1315/1500 [=========================>....] - ETA: 2:25 - loss: 1.1744 - regression_loss: 0.9912 - classification_loss: 0.1831
1316/1500 [=========================>....] - ETA: 2:25 - loss: 1.1739 - regression_loss: 0.9909 - classification_loss: 0.1830
1317/1500 [=========================>....] - ETA: 2:24 - loss: 1.1735 - regression_loss: 0.9905 - classification_loss: 0.1829
1318/1500 [=========================>....] - ETA: 2:23 - loss: 1.1738 - regression_loss: 0.9907 - classification_loss: 0.1831
1319/1500 [=========================>....] - ETA: 2:22 - loss: 1.1738 - regression_loss: 0.9907 - classification_loss: 0.1830
1320/1500 [=========================>....] - ETA: 2:21 - loss: 1.1739 - regression_loss: 0.9909 - classification_loss: 0.1830
1321/1500 [=========================>....] - ETA: 2:20 - loss: 1.1745 - regression_loss: 0.9914 - classification_loss: 0.1831
1322/1500 [=========================>....] - ETA: 2:20 - loss: 1.1747 - regression_loss: 0.9915 - classification_loss: 0.1831
1323/1500 [=========================>....] - ETA: 2:19 - loss: 1.1749 - regression_loss: 0.9918 - classification_loss: 0.1831
1324/1500 [=========================>....] - ETA: 2:18 - loss: 1.1750 - regression_loss: 0.9919 - classification_loss: 0.1831
1325/1500 [=========================>....] - ETA: 2:17 - loss: 1.1747 - regression_loss: 0.9917 - classification_loss: 0.1831
1326/1500 [=========================>....] - ETA: 2:17 - loss: 1.1745 - regression_loss: 0.9916 - classification_loss: 0.1829
1327/1500 [=========================>....] - ETA: 2:16 - loss: 1.1753 - regression_loss: 0.9923 - classification_loss: 0.1830
1328/1500 [=========================>....] - ETA: 2:15 - loss: 1.1755 - regression_loss: 0.9924 - classification_loss: 0.1831
1329/1500 [=========================>....] - ETA: 2:14 - loss: 1.1750 - regression_loss: 0.9920 - classification_loss: 0.1830
1330/1500 [=========================>....] - ETA: 2:13 - loss: 1.1760 - regression_loss: 0.9926 - classification_loss: 0.1834
1331/1500 [=========================>....] - ETA: 2:13 - loss: 1.1759 - regression_loss: 0.9925 - classification_loss: 0.1833
1332/1500 [=========================>....] - ETA: 2:12 - loss: 1.1758 - regression_loss: 0.9924 - classification_loss: 0.1834
1333/1500 [=========================>....] - ETA: 2:11 - loss: 1.1753 - regression_loss: 0.9920 - classification_loss: 0.1833
1334/1500 [=========================>....] - ETA: 2:10 - loss: 1.1755 - regression_loss: 0.9923 - classification_loss: 0.1832
1335/1500 [=========================>....] - ETA: 2:09 - loss: 1.1749 - regression_loss: 0.9918 - classification_loss: 0.1831
1336/1500 [=========================>....] - ETA: 2:09 - loss: 1.1745 - regression_loss: 0.9914 - classification_loss: 0.1831
1337/1500 [=========================>....] - ETA: 2:08 - loss: 1.1742 - regression_loss: 0.9911 - classification_loss: 0.1831
1338/1500 [=========================>....] - ETA: 2:07 - loss: 1.1749 - regression_loss: 0.9917 - classification_loss: 0.1832
1339/1500 [=========================>....] - ETA: 2:07 - loss: 1.1745 - regression_loss: 0.9914 - classification_loss: 0.1831
1340/1500 [=========================>....] - ETA: 2:06 - loss: 1.1742 - regression_loss: 0.9912 - classification_loss: 0.1830
1341/1500 [=========================>....] - ETA: 2:05 - loss: 1.1749 - regression_loss: 0.9917 - classification_loss: 0.1832
1342/1500 [=========================>....] - ETA: 2:04 - loss: 1.1745 - regression_loss: 0.9913 - classification_loss: 0.1832
1343/1500 [=========================>....] - ETA: 2:04 - loss: 1.1746 - regression_loss: 0.9913 - classification_loss: 0.1832
1344/1500 [=========================>....] - ETA: 2:03 - loss: 1.1741 - regression_loss: 0.9910 - classification_loss: 0.1831
1345/1500 [=========================>....] - ETA: 2:02 - loss: 1.1743 - regression_loss: 0.9912 - classification_loss: 0.1831
1346/1500 [=========================>....] - ETA: 2:01 - loss: 1.1750 - regression_loss: 0.9917 - classification_loss: 0.1833
1347/1500 [=========================>....] - ETA: 2:00 - loss: 1.1745 - regression_loss: 0.9913 - classification_loss: 0.1832
1348/1500 [=========================>....] - ETA: 2:00 - loss: 1.1743 - regression_loss: 0.9912 - classification_loss: 0.1831
1349/1500 [=========================>....] - ETA: 1:59 - loss: 1.1741 - regression_loss: 0.9910 - classification_loss: 0.1831
1350/1500 [==========================>...] - ETA: 1:58 - loss: 1.1740 - regression_loss: 0.9909 - classification_loss: 0.1831
1351/1500 [==========================>...] - ETA: 1:57 - loss: 1.1740 - regression_loss: 0.9910 - classification_loss: 0.1831
1352/1500 [==========================>...] - ETA: 1:56 - loss: 1.1738 - regression_loss: 0.9908 - classification_loss: 0.1830
1353/1500 [==========================>...] - ETA: 1:56 - loss: 1.1737 - regression_loss: 0.9907 - classification_loss: 0.1830
1354/1500 [==========================>...] - ETA: 1:55 - loss: 1.1731 - regression_loss: 0.9902 - classification_loss: 0.1829
1355/1500 [==========================>...] - ETA: 1:54 - loss: 1.1728 - regression_loss: 0.9899 - classification_loss: 0.1828
1356/1500 [==========================>...] - ETA: 1:53 - loss: 1.1729 - regression_loss: 0.9899 - classification_loss: 0.1830
1357/1500 [==========================>...] - ETA: 1:52 - loss: 1.1725 - regression_loss: 0.9895 - classification_loss: 0.1830
1358/1500 [==========================>...] - ETA: 1:51 - loss: 1.1732 - regression_loss: 0.9900 - classification_loss: 0.1832
1359/1500 [==========================>...] - ETA: 1:51 - loss: 1.1728 - regression_loss: 0.9896 - classification_loss: 0.1832
1360/1500 [==========================>...] - ETA: 1:50 - loss: 1.1729 - regression_loss: 0.9897 - classification_loss: 0.1831
1361/1500 [==========================>...] - ETA: 1:49 - loss: 1.1733 - regression_loss: 0.9902 - classification_loss: 0.1832
1362/1500 [==========================>...] - ETA: 1:48 - loss: 1.1734 - regression_loss: 0.9902 - classification_loss: 0.1832
1363/1500 [==========================>...] - ETA: 1:47 - loss: 1.1731 - regression_loss: 0.9900 - classification_loss: 0.1831
1364/1500 [==========================>...] - ETA: 1:47 - loss: 1.1729 - regression_loss: 0.9898 - classification_loss: 0.1831
1365/1500 [==========================>...] - ETA: 1:46 - loss: 1.1724 - regression_loss: 0.9893 - classification_loss: 0.1831
1366/1500 [==========================>...] - ETA: 1:45 - loss: 1.1729 - regression_loss: 0.9897 - classification_loss: 0.1832
1367/1500 [==========================>...] - ETA: 1:44 - loss: 1.1733 - regression_loss: 0.9901 - classification_loss: 0.1832
1368/1500 [==========================>...] - ETA: 1:43 - loss: 1.1735 - regression_loss: 0.9902 - classification_loss: 0.1833
1369/1500 [==========================>...] - ETA: 1:43 - loss: 1.1732 - regression_loss: 0.9900 - classification_loss: 0.1832
1370/1500 [==========================>...] - ETA: 1:42 - loss: 1.1729 - regression_loss: 0.9897 - classification_loss: 0.1832
1371/1500 [==========================>...] - ETA: 1:41 - loss: 1.1724 - regression_loss: 0.9893 - classification_loss: 0.1831
1372/1500 [==========================>...] - ETA: 1:40 - loss: 1.1726 - regression_loss: 0.9894 - classification_loss: 0.1831
1373/1500 [==========================>...] - ETA: 1:39 - loss: 1.1723 - regression_loss: 0.9893 - classification_loss: 0.1830
1374/1500 [==========================>...] - ETA: 1:39 - loss: 1.1726 - regression_loss: 0.9890 - classification_loss: 0.1836
1375/1500 [==========================>...] - ETA: 1:38 - loss: 1.1724 - regression_loss: 0.9888 - classification_loss: 0.1835
1376/1500 [==========================>...] - ETA: 1:37 - loss: 1.1722 - regression_loss: 0.9888 - classification_loss: 0.1835
1377/1500 [==========================>...] - ETA: 1:36 - loss: 1.1716 - regression_loss: 0.9883 - classification_loss: 0.1834
1378/1500 [==========================>...] - ETA: 1:36 - loss: 1.1713 - regression_loss: 0.9880 - classification_loss: 0.1833
1379/1500 [==========================>...] - ETA: 1:35 - loss: 1.1708 - regression_loss: 0.9876 - classification_loss: 0.1832
1380/1500 [==========================>...] - ETA: 1:34 - loss: 1.1714 - regression_loss: 0.9881 - classification_loss: 0.1833
1381/1500 [==========================>...] - ETA: 1:33 - loss: 1.1709 - regression_loss: 0.9877 - classification_loss: 0.1832
1382/1500 [==========================>...] - ETA: 1:32 - loss: 1.1708 - regression_loss: 0.9875 - classification_loss: 0.1832
1383/1500 [==========================>...] - ETA: 1:32 - loss: 1.1703 - regression_loss: 0.9871 - classification_loss: 0.1832
1384/1500 [==========================>...] - ETA: 1:31 - loss: 1.1701 - regression_loss: 0.9870 - classification_loss: 0.1832
1385/1500 [==========================>...] - ETA: 1:30 - loss: 1.1700 - regression_loss: 0.9869 - classification_loss: 0.1831
1386/1500 [==========================>...] - ETA: 1:29 - loss: 1.1710 - regression_loss: 0.9877 - classification_loss: 0.1833
1387/1500 [==========================>...] - ETA: 1:28 - loss: 1.1722 - regression_loss: 0.9886 - classification_loss: 0.1837
1388/1500 [==========================>...] - ETA: 1:28 - loss: 1.1719 - regression_loss: 0.9883 - classification_loss: 0.1836
1389/1500 [==========================>...] - ETA: 1:27 - loss: 1.1717 - regression_loss: 0.9882 - classification_loss: 0.1835
1390/1500 [==========================>...] - ETA: 1:26 - loss: 1.1713 - regression_loss: 0.9878 - classification_loss: 0.1835
1391/1500 [==========================>...] - ETA: 1:25 - loss: 1.1707 - regression_loss: 0.9873 - classification_loss: 0.1834
1392/1500 [==========================>...] - ETA: 1:25 - loss: 1.1705 - regression_loss: 0.9872 - classification_loss: 0.1833
1393/1500 [==========================>...] - ETA: 1:24 - loss: 1.1702 - regression_loss: 0.9870 - classification_loss: 0.1832
1394/1500 [==========================>...] - ETA: 1:23 - loss: 1.1702 - regression_loss: 0.9870 - classification_loss: 0.1832
1395/1500 [==========================>...] - ETA: 1:22 - loss: 1.1707 - regression_loss: 0.9875 - classification_loss: 0.1832
1396/1500 [==========================>...] - ETA: 1:21 - loss: 1.1707 - regression_loss: 0.9875 - classification_loss: 0.1832
1397/1500 [==========================>...] - ETA: 1:21 - loss: 1.1706 - regression_loss: 0.9874 - classification_loss: 0.1831
1398/1500 [==========================>...] - ETA: 1:20 - loss: 1.1701 - regression_loss: 0.9870 - classification_loss: 0.1830
1399/1500 [==========================>...] - ETA: 1:19 - loss: 1.1703 - regression_loss: 0.9872 - classification_loss: 0.1831
1400/1500 [===========================>..] - ETA: 1:18 - loss: 1.1703 - regression_loss: 0.9872 - classification_loss: 0.1831
1401/1500 [===========================>..] - ETA: 1:17 - loss: 1.1700 - regression_loss: 0.9871 - classification_loss: 0.1830
1402/1500 [===========================>..] - ETA: 1:17 - loss: 1.1697 - regression_loss: 0.9868 - classification_loss: 0.1829
1403/1500 [===========================>..] - ETA: 1:16 - loss: 1.1696 - regression_loss: 0.9867 - classification_loss: 0.1829
1404/1500 [===========================>..] - ETA: 1:15 - loss: 1.1706 - regression_loss: 0.9876 - classification_loss: 0.1830
1405/1500 [===========================>..] - ETA: 1:14 - loss: 1.1710 - regression_loss: 0.9880 - classification_loss: 0.1830
1406/1500 [===========================>..] - ETA: 1:13 - loss: 1.1704 - regression_loss: 0.9875 - classification_loss: 0.1829
1407/1500 [===========================>..] - ETA: 1:13 - loss: 1.1704 - regression_loss: 0.9875 - classification_loss: 0.1829
1408/1500 [===========================>..] - ETA: 1:12 - loss: 1.1708 - regression_loss: 0.9879 - classification_loss: 0.1829
1409/1500 [===========================>..] - ETA: 1:11 - loss: 1.1705 - regression_loss: 0.9876 - classification_loss: 0.1828
1410/1500 [===========================>..] - ETA: 1:10 - loss: 1.1702 - regression_loss: 0.9874 - classification_loss: 0.1828
1411/1500 [===========================>..] - ETA: 1:09 - loss: 1.1698 - regression_loss: 0.9871 - classification_loss: 0.1827
1412/1500 [===========================>..] - ETA: 1:09 - loss: 1.1696 - regression_loss: 0.9870 - classification_loss: 0.1826
1413/1500 [===========================>..] - ETA: 1:08 - loss: 1.1696 - regression_loss: 0.9869 - classification_loss: 0.1826
1414/1500 [===========================>..] - ETA: 1:07 - loss: 1.1690 - regression_loss: 0.9864 - classification_loss: 0.1825
1415/1500 [===========================>..] - ETA: 1:06 - loss: 1.1702 - regression_loss: 0.9873 - classification_loss: 0.1829
1416/1500 [===========================>..] - ETA: 1:05 - loss: 1.1706 - regression_loss: 0.9876 - classification_loss: 0.1830
1417/1500 [===========================>..] - ETA: 1:05 - loss: 1.1704 - regression_loss: 0.9875 - classification_loss: 0.1829
1418/1500 [===========================>..] - ETA: 1:04 - loss: 1.1701 - regression_loss: 0.9872 - classification_loss: 0.1829
1419/1500 [===========================>..] - ETA: 1:03 - loss: 1.1705 - regression_loss: 0.9876 - classification_loss: 0.1829
1420/1500 [===========================>..] - ETA: 1:02 - loss: 1.1703 - regression_loss: 0.9875 - classification_loss: 0.1829
1421/1500 [===========================>..] - ETA: 1:02 - loss: 1.1707 - regression_loss: 0.9877 - classification_loss: 0.1830
1422/1500 [===========================>..] - ETA: 1:01 - loss: 1.1709 - regression_loss: 0.9879 - classification_loss: 0.1830
1423/1500 [===========================>..] - ETA: 1:00 - loss: 1.1710 - regression_loss: 0.9880 - classification_loss: 0.1829
1424/1500 [===========================>..] - ETA: 59s - loss: 1.1709 - regression_loss: 0.9879 - classification_loss: 0.1830 
1425/1500 [===========================>..] - ETA: 58s - loss: 1.1708 - regression_loss: 0.9879 - classification_loss: 0.1829
1426/1500 [===========================>..] - ETA: 58s - loss: 1.1712 - regression_loss: 0.9882 - classification_loss: 0.1830
1427/1500 [===========================>..] - ETA: 57s - loss: 1.1709 - regression_loss: 0.9880 - classification_loss: 0.1829
1428/1500 [===========================>..] - ETA: 56s - loss: 1.1703 - regression_loss: 0.9875 - classification_loss: 0.1829
1429/1500 [===========================>..] - ETA: 55s - loss: 1.1704 - regression_loss: 0.9875 - classification_loss: 0.1829
1430/1500 [===========================>..] - ETA: 55s - loss: 1.1704 - regression_loss: 0.9875 - classification_loss: 0.1828
1431/1500 [===========================>..] - ETA: 54s - loss: 1.1707 - regression_loss: 0.9878 - classification_loss: 0.1829
1432/1500 [===========================>..] - ETA: 53s - loss: 1.1709 - regression_loss: 0.9880 - classification_loss: 0.1829
1433/1500 [===========================>..] - ETA: 52s - loss: 1.1714 - regression_loss: 0.9883 - classification_loss: 0.1831
1434/1500 [===========================>..] - ETA: 51s - loss: 1.1714 - regression_loss: 0.9882 - classification_loss: 0.1832
1435/1500 [===========================>..] - ETA: 51s - loss: 1.1714 - regression_loss: 0.9883 - classification_loss: 0.1831
1436/1500 [===========================>..] - ETA: 50s - loss: 1.1711 - regression_loss: 0.9881 - classification_loss: 0.1831
1437/1500 [===========================>..] - ETA: 49s - loss: 1.1706 - regression_loss: 0.9877 - classification_loss: 0.1830
1438/1500 [===========================>..] - ETA: 48s - loss: 1.1702 - regression_loss: 0.9873 - classification_loss: 0.1829
1439/1500 [===========================>..] - ETA: 47s - loss: 1.1714 - regression_loss: 0.9875 - classification_loss: 0.1839
1440/1500 [===========================>..] - ETA: 47s - loss: 1.1709 - regression_loss: 0.9871 - classification_loss: 0.1838
1441/1500 [===========================>..] - ETA: 46s - loss: 1.1712 - regression_loss: 0.9873 - classification_loss: 0.1839
1442/1500 [===========================>..] - ETA: 45s - loss: 1.1715 - regression_loss: 0.9876 - classification_loss: 0.1839
1443/1500 [===========================>..] - ETA: 44s - loss: 1.1718 - regression_loss: 0.9879 - classification_loss: 0.1840
1444/1500 [===========================>..] - ETA: 44s - loss: 1.1715 - regression_loss: 0.9876 - classification_loss: 0.1839
1445/1500 [===========================>..] - ETA: 43s - loss: 1.1714 - regression_loss: 0.9875 - classification_loss: 0.1839
1446/1500 [===========================>..] - ETA: 42s - loss: 1.1721 - regression_loss: 0.9879 - classification_loss: 0.1841
1447/1500 [===========================>..] - ETA: 41s - loss: 1.1718 - regression_loss: 0.9877 - classification_loss: 0.1841
1448/1500 [===========================>..] - ETA: 40s - loss: 1.1715 - regression_loss: 0.9875 - classification_loss: 0.1840
1449/1500 [===========================>..] - ETA: 40s - loss: 1.1712 - regression_loss: 0.9871 - classification_loss: 0.1840
1450/1500 [============================>.] - ETA: 39s - loss: 1.1711 - regression_loss: 0.9871 - classification_loss: 0.1840
1451/1500 [============================>.] - ETA: 38s - loss: 1.1707 - regression_loss: 0.9868 - classification_loss: 0.1839
1452/1500 [============================>.] - ETA: 37s - loss: 1.1706 - regression_loss: 0.9867 - classification_loss: 0.1839
1453/1500 [============================>.] - ETA: 36s - loss: 1.1703 - regression_loss: 0.9865 - classification_loss: 0.1838
1454/1500 [============================>.] - ETA: 36s - loss: 1.1702 - regression_loss: 0.9864 - classification_loss: 0.1838
1455/1500 [============================>.] - ETA: 35s - loss: 1.1705 - regression_loss: 0.9866 - classification_loss: 0.1839
1456/1500 [============================>.] - ETA: 34s - loss: 1.1710 - regression_loss: 0.9870 - classification_loss: 0.1839
1457/1500 [============================>.] - ETA: 33s - loss: 1.1708 - regression_loss: 0.9870 - classification_loss: 0.1839
1458/1500 [============================>.] - ETA: 33s - loss: 1.1709 - regression_loss: 0.9870 - classification_loss: 0.1839
1459/1500 [============================>.] - ETA: 32s - loss: 1.1707 - regression_loss: 0.9869 - classification_loss: 0.1838
1460/1500 [============================>.] - ETA: 31s - loss: 1.1713 - regression_loss: 0.9873 - classification_loss: 0.1839
1461/1500 [============================>.] - ETA: 30s - loss: 1.1725 - regression_loss: 0.9878 - classification_loss: 0.1847
1462/1500 [============================>.] - ETA: 29s - loss: 1.1726 - regression_loss: 0.9878 - classification_loss: 0.1847
1463/1500 [============================>.] - ETA: 29s - loss: 1.1732 - regression_loss: 0.9883 - classification_loss: 0.1849
1464/1500 [============================>.] - ETA: 28s - loss: 1.1728 - regression_loss: 0.9880 - classification_loss: 0.1848
1465/1500 [============================>.] - ETA: 27s - loss: 1.1727 - regression_loss: 0.9880 - classification_loss: 0.1847
1466/1500 [============================>.] - ETA: 26s - loss: 1.1730 - regression_loss: 0.9883 - classification_loss: 0.1847
1467/1500 [============================>.] - ETA: 25s - loss: 1.1727 - regression_loss: 0.9881 - classification_loss: 0.1846
1468/1500 [============================>.] - ETA: 25s - loss: 1.1726 - regression_loss: 0.9880 - classification_loss: 0.1846
1469/1500 [============================>.] - ETA: 24s - loss: 1.1728 - regression_loss: 0.9881 - classification_loss: 0.1846
1470/1500 [============================>.] - ETA: 23s - loss: 1.1723 - regression_loss: 0.9877 - classification_loss: 0.1846
1471/1500 [============================>.] - ETA: 22s - loss: 1.1719 - regression_loss: 0.9874 - classification_loss: 0.1845
1472/1500 [============================>.] - ETA: 22s - loss: 1.1723 - regression_loss: 0.9877 - classification_loss: 0.1846
1473/1500 [============================>.] - ETA: 21s - loss: 1.1723 - regression_loss: 0.9877 - classification_loss: 0.1846
1474/1500 [============================>.] - ETA: 20s - loss: 1.1729 - regression_loss: 0.9882 - classification_loss: 0.1847
1475/1500 [============================>.] - ETA: 19s - loss: 1.1728 - regression_loss: 0.9881 - classification_loss: 0.1847
1476/1500 [============================>.] - ETA: 18s - loss: 1.1725 - regression_loss: 0.9878 - classification_loss: 0.1846
1477/1500 [============================>.] - ETA: 18s - loss: 1.1722 - regression_loss: 0.9876 - classification_loss: 0.1846
1478/1500 [============================>.] - ETA: 17s - loss: 1.1717 - regression_loss: 0.9873 - classification_loss: 0.1845
1479/1500 [============================>.] - ETA: 16s - loss: 1.1719 - regression_loss: 0.9874 - classification_loss: 0.1845
1480/1500 [============================>.] - ETA: 15s - loss: 1.1720 - regression_loss: 0.9872 - classification_loss: 0.1849
1481/1500 [============================>.] - ETA: 14s - loss: 1.1718 - regression_loss: 0.9870 - classification_loss: 0.1848
1482/1500 [============================>.] - ETA: 14s - loss: 1.1719 - regression_loss: 0.9871 - classification_loss: 0.1848
1483/1500 [============================>.] - ETA: 13s - loss: 1.1717 - regression_loss: 0.9870 - classification_loss: 0.1847
1484/1500 [============================>.] - ETA: 12s - loss: 1.1713 - regression_loss: 0.9867 - classification_loss: 0.1847
1485/1500 [============================>.] - ETA: 11s - loss: 1.1708 - regression_loss: 0.9862 - classification_loss: 0.1846
1486/1500 [============================>.] - ETA: 11s - loss: 1.1707 - regression_loss: 0.9862 - classification_loss: 0.1845
1487/1500 [============================>.] - ETA: 10s - loss: 1.1712 - regression_loss: 0.9865 - classification_loss: 0.1846
1488/1500 [============================>.] - ETA: 9s - loss: 1.1721 - regression_loss: 0.9872 - classification_loss: 0.1849 
1489/1500 [============================>.] - ETA: 8s - loss: 1.1722 - regression_loss: 0.9873 - classification_loss: 0.1849
1490/1500 [============================>.] - ETA: 7s - loss: 1.1730 - regression_loss: 0.9879 - classification_loss: 0.1851
1491/1500 [============================>.] - ETA: 7s - loss: 1.1726 - regression_loss: 0.9876 - classification_loss: 0.1851
1492/1500 [============================>.] - ETA: 6s - loss: 1.1730 - regression_loss: 0.9878 - classification_loss: 0.1852
1493/1500 [============================>.] - ETA: 5s - loss: 1.1730 - regression_loss: 0.9878 - classification_loss: 0.1852
1494/1500 [============================>.] - ETA: 4s - loss: 1.1731 - regression_loss: 0.9879 - classification_loss: 0.1852
1495/1500 [============================>.] - ETA: 3s - loss: 1.1733 - regression_loss: 0.9882 - classification_loss: 0.1851
1496/1500 [============================>.] - ETA: 3s - loss: 1.1733 - regression_loss: 0.9882 - classification_loss: 0.1851
1497/1500 [============================>.] - ETA: 2s - loss: 1.1728 - regression_loss: 0.9877 - classification_loss: 0.1851
1498/1500 [============================>.] - ETA: 1s - loss: 1.1729 - regression_loss: 0.9878 - classification_loss: 0.1851
1499/1500 [============================>.] - ETA: 0s - loss: 1.1736 - regression_loss: 0.9885 - classification_loss: 0.1851
1500/1500 [==============================] - 1182s 788ms/step - loss: 1.1735 - regression_loss: 0.9884 - classification_loss: 0.1851

Epoch 00009: saving model to ./snapshots/resnet50_csv_09.h5
Epoch 10/10

   1/1500 [..............................] - ETA: 11:04 - loss: 0.7256 - regression_loss: 0.5936 - classification_loss: 0.1320
   2/1500 [..............................] - ETA: 10:28 - loss: 0.6306 - regression_loss: 0.5197 - classification_loss: 0.1109
   3/1500 [..............................] - ETA: 10:24 - loss: 0.7046 - regression_loss: 0.6039 - classification_loss: 0.1007
   4/1500 [..............................] - ETA: 9:52 - loss: 0.8228 - regression_loss: 0.7209 - classification_loss: 0.1019 
   5/1500 [..............................] - ETA: 9:41 - loss: 0.9531 - regression_loss: 0.8222 - classification_loss: 0.1309
   6/1500 [..............................] - ETA: 11:53 - loss: 1.0400 - regression_loss: 0.8629 - classification_loss: 0.1771
   7/1500 [..............................] - ETA: 12:05 - loss: 1.2557 - regression_loss: 1.0236 - classification_loss: 0.2322
   8/1500 [..............................] - ETA: 12:58 - loss: 1.2626 - regression_loss: 1.0342 - classification_loss: 0.2285
   9/1500 [..............................] - ETA: 15:01 - loss: 1.3406 - regression_loss: 1.0415 - classification_loss: 0.2991
  10/1500 [..............................] - ETA: 14:59 - loss: 1.3322 - regression_loss: 1.0479 - classification_loss: 0.2844
  11/1500 [..............................] - ETA: 14:38 - loss: 1.4327 - regression_loss: 1.1370 - classification_loss: 0.2957
  12/1500 [..............................] - ETA: 14:15 - loss: 1.4380 - regression_loss: 1.1485 - classification_loss: 0.2896
  13/1500 [..............................] - ETA: 13:55 - loss: 1.4776 - regression_loss: 1.1889 - classification_loss: 0.2887
  14/1500 [..............................] - ETA: 13:36 - loss: 1.3880 - regression_loss: 1.1182 - classification_loss: 0.2698
  15/1500 [..............................] - ETA: 13:17 - loss: 1.3762 - regression_loss: 1.1162 - classification_loss: 0.2600
  16/1500 [..............................] - ETA: 16:32 - loss: 1.3497 - regression_loss: 1.0945 - classification_loss: 0.2551
  17/1500 [..............................] - ETA: 17:22 - loss: 1.2972 - regression_loss: 1.0555 - classification_loss: 0.2416
  18/1500 [..............................] - ETA: 17:06 - loss: 1.2951 - regression_loss: 1.0608 - classification_loss: 0.2343
  19/1500 [..............................] - ETA: 17:10 - loss: 1.2926 - regression_loss: 1.0496 - classification_loss: 0.2430
  20/1500 [..............................] - ETA: 16:50 - loss: 1.2905 - regression_loss: 1.0463 - classification_loss: 0.2442
  21/1500 [..............................] - ETA: 16:29 - loss: 1.2916 - regression_loss: 1.0524 - classification_loss: 0.2392
  22/1500 [..............................] - ETA: 16:14 - loss: 1.2692 - regression_loss: 1.0378 - classification_loss: 0.2314
  23/1500 [..............................] - ETA: 17:44 - loss: 1.3055 - regression_loss: 1.0670 - classification_loss: 0.2385
  24/1500 [..............................] - ETA: 19:58 - loss: 1.3448 - regression_loss: 1.0957 - classification_loss: 0.2491
  25/1500 [..............................] - ETA: 20:43 - loss: 1.3277 - regression_loss: 1.0784 - classification_loss: 0.2493
  26/1500 [..............................] - ETA: 20:38 - loss: 1.3494 - regression_loss: 1.0966 - classification_loss: 0.2529
  27/1500 [..............................] - ETA: 20:28 - loss: 1.3443 - regression_loss: 1.0931 - classification_loss: 0.2512
  28/1500 [..............................] - ETA: 20:39 - loss: 1.3542 - regression_loss: 1.1023 - classification_loss: 0.2519
  29/1500 [..............................] - ETA: 21:06 - loss: 1.3781 - regression_loss: 1.1279 - classification_loss: 0.2502
  30/1500 [..............................] - ETA: 20:44 - loss: 1.4120 - regression_loss: 1.1649 - classification_loss: 0.2471
  31/1500 [..............................] - ETA: 20:43 - loss: 1.4388 - regression_loss: 1.1857 - classification_loss: 0.2531
  32/1500 [..............................] - ETA: 21:01 - loss: 1.4616 - regression_loss: 1.2065 - classification_loss: 0.2551
  33/1500 [..............................] - ETA: 21:17 - loss: 1.4418 - regression_loss: 1.1907 - classification_loss: 0.2511
  34/1500 [..............................] - ETA: 20:56 - loss: 1.4518 - regression_loss: 1.1979 - classification_loss: 0.2539
  35/1500 [..............................] - ETA: 20:42 - loss: 1.4341 - regression_loss: 1.1858 - classification_loss: 0.2483
  36/1500 [..............................] - ETA: 20:54 - loss: 1.4065 - regression_loss: 1.1620 - classification_loss: 0.2445
  37/1500 [..............................] - ETA: 21:35 - loss: 1.3948 - regression_loss: 1.1535 - classification_loss: 0.2413
  38/1500 [..............................] - ETA: 21:34 - loss: 1.3800 - regression_loss: 1.1428 - classification_loss: 0.2371
  39/1500 [..............................] - ETA: 21:15 - loss: 1.4052 - regression_loss: 1.1585 - classification_loss: 0.2467
  40/1500 [..............................] - ETA: 20:56 - loss: 1.3955 - regression_loss: 1.1477 - classification_loss: 0.2478
  41/1500 [..............................] - ETA: 20:54 - loss: 1.4185 - regression_loss: 1.1643 - classification_loss: 0.2543
  42/1500 [..............................] - ETA: 20:35 - loss: 1.3970 - regression_loss: 1.1476 - classification_loss: 0.2494
  43/1500 [..............................] - ETA: 20:24 - loss: 1.4019 - regression_loss: 1.1454 - classification_loss: 0.2565
  44/1500 [..............................] - ETA: 20:28 - loss: 1.3926 - regression_loss: 1.1383 - classification_loss: 0.2543
  45/1500 [..............................] - ETA: 20:24 - loss: 1.3719 - regression_loss: 1.1218 - classification_loss: 0.2501
  46/1500 [..............................] - ETA: 20:10 - loss: 1.3744 - regression_loss: 1.1161 - classification_loss: 0.2583
  47/1500 [..............................] - ETA: 19:53 - loss: 1.3713 - regression_loss: 1.1148 - classification_loss: 0.2564
  48/1500 [..............................] - ETA: 20:06 - loss: 1.3789 - regression_loss: 1.1233 - classification_loss: 0.2556
  49/1500 [..............................] - ETA: 19:55 - loss: 1.3628 - regression_loss: 1.1105 - classification_loss: 0.2524
  50/1500 [>.............................] - ETA: 19:42 - loss: 1.3686 - regression_loss: 1.1165 - classification_loss: 0.2522
  51/1500 [>.............................] - ETA: 19:58 - loss: 1.3979 - regression_loss: 1.1387 - classification_loss: 0.2592
  52/1500 [>.............................] - ETA: 19:43 - loss: 1.3824 - regression_loss: 1.1264 - classification_loss: 0.2561
  53/1500 [>.............................] - ETA: 19:37 - loss: 1.3805 - regression_loss: 1.1241 - classification_loss: 0.2564
  54/1500 [>.............................] - ETA: 19:41 - loss: 1.3672 - regression_loss: 1.1135 - classification_loss: 0.2537
  55/1500 [>.............................] - ETA: 19:51 - loss: 1.3731 - regression_loss: 1.1175 - classification_loss: 0.2556
  56/1500 [>.............................] - ETA: 19:46 - loss: 1.3656 - regression_loss: 1.1128 - classification_loss: 0.2528
  57/1500 [>.............................] - ETA: 19:46 - loss: 1.3581 - regression_loss: 1.1077 - classification_loss: 0.2504
  58/1500 [>.............................] - ETA: 19:53 - loss: 1.3635 - regression_loss: 1.1147 - classification_loss: 0.2488
  59/1500 [>.............................] - ETA: 19:46 - loss: 1.3515 - regression_loss: 1.1049 - classification_loss: 0.2466
  60/1500 [>.............................] - ETA: 19:35 - loss: 1.3437 - regression_loss: 1.0997 - classification_loss: 0.2440
  61/1500 [>.............................] - ETA: 19:25 - loss: 1.3435 - regression_loss: 1.1009 - classification_loss: 0.2426
  62/1500 [>.............................] - ETA: 19:44 - loss: 1.3492 - regression_loss: 1.1083 - classification_loss: 0.2409
  63/1500 [>.............................] - ETA: 19:39 - loss: 1.3426 - regression_loss: 1.1042 - classification_loss: 0.2383
  64/1500 [>.............................] - ETA: 19:47 - loss: 1.3466 - regression_loss: 1.1080 - classification_loss: 0.2386
  65/1500 [>.............................] - ETA: 19:38 - loss: 1.3417 - regression_loss: 1.1053 - classification_loss: 0.2364
  66/1500 [>.............................] - ETA: 19:37 - loss: 1.3484 - regression_loss: 1.1112 - classification_loss: 0.2372
  67/1500 [>.............................] - ETA: 19:28 - loss: 1.3384 - regression_loss: 1.1036 - classification_loss: 0.2349
  68/1500 [>.............................] - ETA: 19:28 - loss: 1.3404 - regression_loss: 1.1046 - classification_loss: 0.2358
  69/1500 [>.............................] - ETA: 19:19 - loss: 1.3324 - regression_loss: 1.0983 - classification_loss: 0.2341
  70/1500 [>.............................] - ETA: 19:11 - loss: 1.3170 - regression_loss: 1.0858 - classification_loss: 0.2311
  71/1500 [>.............................] - ETA: 19:13 - loss: 1.3064 - regression_loss: 1.0777 - classification_loss: 0.2288
  72/1500 [>.............................] - ETA: 19:14 - loss: 1.3032 - regression_loss: 1.0760 - classification_loss: 0.2272
  73/1500 [>.............................] - ETA: 19:23 - loss: 1.3077 - regression_loss: 1.0808 - classification_loss: 0.2269
  74/1500 [>.............................] - ETA: 19:15 - loss: 1.3202 - regression_loss: 1.0923 - classification_loss: 0.2279
  75/1500 [>.............................] - ETA: 19:07 - loss: 1.3207 - regression_loss: 1.0935 - classification_loss: 0.2272
  76/1500 [>.............................] - ETA: 18:58 - loss: 1.3183 - regression_loss: 1.0913 - classification_loss: 0.2270
  77/1500 [>.............................] - ETA: 19:17 - loss: 1.3227 - regression_loss: 1.0941 - classification_loss: 0.2286
  78/1500 [>.............................] - ETA: 19:16 - loss: 1.3266 - regression_loss: 1.0980 - classification_loss: 0.2285
  79/1500 [>.............................] - ETA: 19:14 - loss: 1.3278 - regression_loss: 1.0993 - classification_loss: 0.2285
  80/1500 [>.............................] - ETA: 19:15 - loss: 1.3228 - regression_loss: 1.0961 - classification_loss: 0.2267
  81/1500 [>.............................] - ETA: 19:15 - loss: 1.3341 - regression_loss: 1.1049 - classification_loss: 0.2292
  82/1500 [>.............................] - ETA: 19:18 - loss: 1.3422 - regression_loss: 1.1113 - classification_loss: 0.2310
  83/1500 [>.............................] - ETA: 19:12 - loss: 1.3320 - regression_loss: 1.1032 - classification_loss: 0.2288
  84/1500 [>.............................] - ETA: 19:17 - loss: 1.3238 - regression_loss: 1.0969 - classification_loss: 0.2269
  85/1500 [>.............................] - ETA: 19:14 - loss: 1.3186 - regression_loss: 1.0932 - classification_loss: 0.2254
  86/1500 [>.............................] - ETA: 19:17 - loss: 1.3101 - regression_loss: 1.0862 - classification_loss: 0.2239
  87/1500 [>.............................] - ETA: 19:22 - loss: 1.3025 - regression_loss: 1.0797 - classification_loss: 0.2228
  88/1500 [>.............................] - ETA: 19:14 - loss: 1.2957 - regression_loss: 1.0743 - classification_loss: 0.2214
  89/1500 [>.............................] - ETA: 19:07 - loss: 1.2987 - regression_loss: 1.0776 - classification_loss: 0.2211
  90/1500 [>.............................] - ETA: 19:30 - loss: 1.3095 - regression_loss: 1.0825 - classification_loss: 0.2269
  91/1500 [>.............................] - ETA: 19:24 - loss: 1.3033 - regression_loss: 1.0775 - classification_loss: 0.2258
  92/1500 [>.............................] - ETA: 19:24 - loss: 1.3004 - regression_loss: 1.0752 - classification_loss: 0.2252
  93/1500 [>.............................] - ETA: 19:19 - loss: 1.2958 - regression_loss: 1.0716 - classification_loss: 0.2242
  94/1500 [>.............................] - ETA: 19:21 - loss: 1.2889 - regression_loss: 1.0663 - classification_loss: 0.2226
  95/1500 [>.............................] - ETA: 19:36 - loss: 1.2856 - regression_loss: 1.0640 - classification_loss: 0.2216
  96/1500 [>.............................] - ETA: 19:28 - loss: 1.2803 - regression_loss: 1.0601 - classification_loss: 0.2202
  97/1500 [>.............................] - ETA: 19:31 - loss: 1.2842 - regression_loss: 1.0653 - classification_loss: 0.2189
  98/1500 [>.............................] - ETA: 19:23 - loss: 1.2830 - regression_loss: 1.0655 - classification_loss: 0.2175
  99/1500 [>.............................] - ETA: 19:18 - loss: 1.2855 - regression_loss: 1.0681 - classification_loss: 0.2174
 100/1500 [=>............................] - ETA: 19:17 - loss: 1.2854 - regression_loss: 1.0658 - classification_loss: 0.2196
 101/1500 [=>............................] - ETA: 19:12 - loss: 1.2782 - regression_loss: 1.0598 - classification_loss: 0.2184
 102/1500 [=>............................] - ETA: 19:09 - loss: 1.2734 - regression_loss: 1.0569 - classification_loss: 0.2165
 103/1500 [=>............................] - ETA: 19:04 - loss: 1.2671 - regression_loss: 1.0519 - classification_loss: 0.2152
 104/1500 [=>............................] - ETA: 19:03 - loss: 1.2714 - regression_loss: 1.0563 - classification_loss: 0.2151
 105/1500 [=>............................] - ETA: 19:11 - loss: 1.2821 - regression_loss: 1.0641 - classification_loss: 0.2181
 106/1500 [=>............................] - ETA: 19:10 - loss: 1.2736 - regression_loss: 1.0573 - classification_loss: 0.2163
 107/1500 [=>............................] - ETA: 19:04 - loss: 1.2815 - regression_loss: 1.0631 - classification_loss: 0.2184
 108/1500 [=>............................] - ETA: 19:00 - loss: 1.2768 - regression_loss: 1.0591 - classification_loss: 0.2177
 109/1500 [=>............................] - ETA: 18:57 - loss: 1.2795 - regression_loss: 1.0617 - classification_loss: 0.2178
 110/1500 [=>............................] - ETA: 18:52 - loss: 1.2758 - regression_loss: 1.0583 - classification_loss: 0.2176
 111/1500 [=>............................] - ETA: 18:51 - loss: 1.2732 - regression_loss: 1.0566 - classification_loss: 0.2166
 112/1500 [=>............................] - ETA: 18:47 - loss: 1.2695 - regression_loss: 1.0541 - classification_loss: 0.2154
 113/1500 [=>............................] - ETA: 18:42 - loss: 1.2687 - regression_loss: 1.0537 - classification_loss: 0.2150
 114/1500 [=>............................] - ETA: 19:01 - loss: 1.2739 - regression_loss: 1.0577 - classification_loss: 0.2163
 115/1500 [=>............................] - ETA: 18:57 - loss: 1.2680 - regression_loss: 1.0520 - classification_loss: 0.2160
 116/1500 [=>............................] - ETA: 18:58 - loss: 1.2639 - regression_loss: 1.0492 - classification_loss: 0.2147
 117/1500 [=>............................] - ETA: 18:57 - loss: 1.2680 - regression_loss: 1.0522 - classification_loss: 0.2158
 118/1500 [=>............................] - ETA: 18:54 - loss: 1.2658 - regression_loss: 1.0502 - classification_loss: 0.2156
 119/1500 [=>............................] - ETA: 18:50 - loss: 1.2714 - regression_loss: 1.0552 - classification_loss: 0.2162
 120/1500 [=>............................] - ETA: 18:46 - loss: 1.2764 - regression_loss: 1.0598 - classification_loss: 0.2166
 121/1500 [=>............................] - ETA: 18:50 - loss: 1.2796 - regression_loss: 1.0627 - classification_loss: 0.2169
 122/1500 [=>............................] - ETA: 18:53 - loss: 1.2853 - regression_loss: 1.0673 - classification_loss: 0.2180
 123/1500 [=>............................] - ETA: 18:54 - loss: 1.2797 - regression_loss: 1.0631 - classification_loss: 0.2166
 124/1500 [=>............................] - ETA: 18:48 - loss: 1.2844 - regression_loss: 1.0670 - classification_loss: 0.2174
 125/1500 [=>............................] - ETA: 18:57 - loss: 1.2833 - regression_loss: 1.0647 - classification_loss: 0.2187
 126/1500 [=>............................] - ETA: 18:52 - loss: 1.2876 - regression_loss: 1.0682 - classification_loss: 0.2194
 127/1500 [=>............................] - ETA: 18:48 - loss: 1.2813 - regression_loss: 1.0632 - classification_loss: 0.2181
 128/1500 [=>............................] - ETA: 18:49 - loss: 1.2860 - regression_loss: 1.0670 - classification_loss: 0.2190
 129/1500 [=>............................] - ETA: 18:47 - loss: 1.2850 - regression_loss: 1.0670 - classification_loss: 0.2179
 130/1500 [=>............................] - ETA: 18:42 - loss: 1.2808 - regression_loss: 1.0634 - classification_loss: 0.2174
 131/1500 [=>............................] - ETA: 18:41 - loss: 1.2791 - regression_loss: 1.0619 - classification_loss: 0.2172
 132/1500 [=>............................] - ETA: 18:36 - loss: 1.2822 - regression_loss: 1.0650 - classification_loss: 0.2172
 133/1500 [=>............................] - ETA: 18:35 - loss: 1.2853 - regression_loss: 1.0676 - classification_loss: 0.2177
 134/1500 [=>............................] - ETA: 18:34 - loss: 1.2782 - regression_loss: 1.0617 - classification_loss: 0.2165
 135/1500 [=>............................] - ETA: 18:30 - loss: 1.2783 - regression_loss: 1.0613 - classification_loss: 0.2170
 136/1500 [=>............................] - ETA: 18:25 - loss: 1.2732 - regression_loss: 1.0571 - classification_loss: 0.2161
 137/1500 [=>............................] - ETA: 18:20 - loss: 1.2722 - regression_loss: 1.0567 - classification_loss: 0.2155
 138/1500 [=>............................] - ETA: 18:20 - loss: 1.2814 - regression_loss: 1.0633 - classification_loss: 0.2181
 139/1500 [=>............................] - ETA: 18:25 - loss: 1.2855 - regression_loss: 1.0667 - classification_loss: 0.2188
 140/1500 [=>............................] - ETA: 18:26 - loss: 1.2880 - regression_loss: 1.0693 - classification_loss: 0.2187
 141/1500 [=>............................] - ETA: 18:22 - loss: 1.2849 - regression_loss: 1.0671 - classification_loss: 0.2177
 142/1500 [=>............................] - ETA: 18:23 - loss: 1.2898 - regression_loss: 1.0713 - classification_loss: 0.2185
 143/1500 [=>............................] - ETA: 18:20 - loss: 1.2870 - regression_loss: 1.0692 - classification_loss: 0.2178
 144/1500 [=>............................] - ETA: 18:20 - loss: 1.2920 - regression_loss: 1.0731 - classification_loss: 0.2189
 145/1500 [=>............................] - ETA: 18:16 - loss: 1.2933 - regression_loss: 1.0738 - classification_loss: 0.2195
 146/1500 [=>............................] - ETA: 18:11 - loss: 1.2918 - regression_loss: 1.0733 - classification_loss: 0.2185
 147/1500 [=>............................] - ETA: 18:13 - loss: 1.2873 - regression_loss: 1.0698 - classification_loss: 0.2175
 148/1500 [=>............................] - ETA: 18:11 - loss: 1.2848 - regression_loss: 1.0679 - classification_loss: 0.2168
 149/1500 [=>............................] - ETA: 18:09 - loss: 1.2844 - regression_loss: 1.0681 - classification_loss: 0.2162
 150/1500 [==>...........................] - ETA: 18:12 - loss: 1.2822 - regression_loss: 1.0665 - classification_loss: 0.2157
 151/1500 [==>...........................] - ETA: 18:06 - loss: 1.2790 - regression_loss: 1.0640 - classification_loss: 0.2150
 152/1500 [==>...........................] - ETA: 18:09 - loss: 1.2762 - regression_loss: 1.0615 - classification_loss: 0.2147
 153/1500 [==>...........................] - ETA: 18:26 - loss: 1.2739 - regression_loss: 1.0596 - classification_loss: 0.2143
 154/1500 [==>...........................] - ETA: 18:21 - loss: 1.2709 - regression_loss: 1.0575 - classification_loss: 0.2134
 155/1500 [==>...........................] - ETA: 18:20 - loss: 1.2719 - regression_loss: 1.0586 - classification_loss: 0.2133
 156/1500 [==>...........................] - ETA: 18:21 - loss: 1.2699 - regression_loss: 1.0571 - classification_loss: 0.2128
 157/1500 [==>...........................] - ETA: 18:20 - loss: 1.2672 - regression_loss: 1.0549 - classification_loss: 0.2123
 158/1500 [==>...........................] - ETA: 18:20 - loss: 1.2708 - regression_loss: 1.0582 - classification_loss: 0.2126
 159/1500 [==>...........................] - ETA: 18:17 - loss: 1.2734 - regression_loss: 1.0606 - classification_loss: 0.2128
 160/1500 [==>...........................] - ETA: 18:14 - loss: 1.2694 - regression_loss: 1.0574 - classification_loss: 0.2120
 161/1500 [==>...........................] - ETA: 18:09 - loss: 1.2671 - regression_loss: 1.0560 - classification_loss: 0.2110
 162/1500 [==>...........................] - ETA: 18:05 - loss: 1.2621 - regression_loss: 1.0518 - classification_loss: 0.2103
 163/1500 [==>...........................] - ETA: 18:06 - loss: 1.2621 - regression_loss: 1.0513 - classification_loss: 0.2108
 164/1500 [==>...........................] - ETA: 18:03 - loss: 1.2602 - regression_loss: 1.0498 - classification_loss: 0.2103
 165/1500 [==>...........................] - ETA: 18:00 - loss: 1.2592 - regression_loss: 1.0491 - classification_loss: 0.2101
 166/1500 [==>...........................] - ETA: 17:57 - loss: 1.2572 - regression_loss: 1.0477 - classification_loss: 0.2095
 167/1500 [==>...........................] - ETA: 17:57 - loss: 1.2547 - regression_loss: 1.0460 - classification_loss: 0.2086
 168/1500 [==>...........................] - ETA: 17:53 - loss: 1.2559 - regression_loss: 1.0470 - classification_loss: 0.2089
 169/1500 [==>...........................] - ETA: 17:49 - loss: 1.2558 - regression_loss: 1.0475 - classification_loss: 0.2082
 170/1500 [==>...........................] - ETA: 17:47 - loss: 1.2556 - regression_loss: 1.0476 - classification_loss: 0.2081
 171/1500 [==>...........................] - ETA: 17:45 - loss: 1.2541 - regression_loss: 1.0467 - classification_loss: 0.2074
 172/1500 [==>...........................] - ETA: 17:41 - loss: 1.2515 - regression_loss: 1.0451 - classification_loss: 0.2064
 173/1500 [==>...........................] - ETA: 17:37 - loss: 1.2503 - regression_loss: 1.0446 - classification_loss: 0.2057
 174/1500 [==>...........................] - ETA: 17:36 - loss: 1.2485 - regression_loss: 1.0435 - classification_loss: 0.2050
 175/1500 [==>...........................] - ETA: 17:34 - loss: 1.2462 - regression_loss: 1.0420 - classification_loss: 0.2042
 176/1500 [==>...........................] - ETA: 17:31 - loss: 1.2443 - regression_loss: 1.0407 - classification_loss: 0.2036
 177/1500 [==>...........................] - ETA: 17:29 - loss: 1.2410 - regression_loss: 1.0381 - classification_loss: 0.2029
 178/1500 [==>...........................] - ETA: 17:26 - loss: 1.2382 - regression_loss: 1.0358 - classification_loss: 0.2025
 179/1500 [==>...........................] - ETA: 17:28 - loss: 1.2356 - regression_loss: 1.0339 - classification_loss: 0.2018
 180/1500 [==>...........................] - ETA: 17:31 - loss: 1.2393 - regression_loss: 1.0367 - classification_loss: 0.2026
 181/1500 [==>...........................] - ETA: 17:31 - loss: 1.2414 - regression_loss: 1.0384 - classification_loss: 0.2030
 182/1500 [==>...........................] - ETA: 17:27 - loss: 1.2421 - regression_loss: 1.0392 - classification_loss: 0.2029
 183/1500 [==>...........................] - ETA: 17:31 - loss: 1.2452 - regression_loss: 1.0418 - classification_loss: 0.2034
 184/1500 [==>...........................] - ETA: 17:35 - loss: 1.2523 - regression_loss: 1.0479 - classification_loss: 0.2045
 185/1500 [==>...........................] - ETA: 17:35 - loss: 1.2492 - regression_loss: 1.0454 - classification_loss: 0.2038
 186/1500 [==>...........................] - ETA: 17:32 - loss: 1.2474 - regression_loss: 1.0438 - classification_loss: 0.2036
 187/1500 [==>...........................] - ETA: 17:30 - loss: 1.2458 - regression_loss: 1.0429 - classification_loss: 0.2029
 188/1500 [==>...........................] - ETA: 17:26 - loss: 1.2414 - regression_loss: 1.0394 - classification_loss: 0.2021
 189/1500 [==>...........................] - ETA: 17:23 - loss: 1.2383 - regression_loss: 1.0371 - classification_loss: 0.2012
 190/1500 [==>...........................] - ETA: 17:22 - loss: 1.2334 - regression_loss: 1.0330 - classification_loss: 0.2004
 191/1500 [==>...........................] - ETA: 17:40 - loss: 1.2344 - regression_loss: 1.0343 - classification_loss: 0.2001
 192/1500 [==>...........................] - ETA: 17:36 - loss: 1.2380 - regression_loss: 1.0374 - classification_loss: 0.2007
 193/1500 [==>...........................] - ETA: 17:33 - loss: 1.2392 - regression_loss: 1.0390 - classification_loss: 0.2002
 194/1500 [==>...........................] - ETA: 17:30 - loss: 1.2390 - regression_loss: 1.0388 - classification_loss: 0.2003
 195/1500 [==>...........................] - ETA: 17:27 - loss: 1.2377 - regression_loss: 1.0381 - classification_loss: 0.1996
 196/1500 [==>...........................] - ETA: 17:31 - loss: 1.2389 - regression_loss: 1.0385 - classification_loss: 0.2004
 197/1500 [==>...........................] - ETA: 17:32 - loss: 1.2367 - regression_loss: 1.0369 - classification_loss: 0.1998
 198/1500 [==>...........................] - ETA: 17:28 - loss: 1.2430 - regression_loss: 1.0416 - classification_loss: 0.2014
 199/1500 [==>...........................] - ETA: 17:32 - loss: 1.2470 - regression_loss: 1.0451 - classification_loss: 0.2020
 200/1500 [===>..........................] - ETA: 17:31 - loss: 1.2444 - regression_loss: 1.0431 - classification_loss: 0.2012
 201/1500 [===>..........................] - ETA: 17:30 - loss: 1.2436 - regression_loss: 1.0416 - classification_loss: 0.2019
 202/1500 [===>..........................] - ETA: 17:33 - loss: 1.2428 - regression_loss: 1.0407 - classification_loss: 0.2020
 203/1500 [===>..........................] - ETA: 17:30 - loss: 1.2436 - regression_loss: 1.0413 - classification_loss: 0.2022
 204/1500 [===>..........................] - ETA: 17:27 - loss: 1.2417 - regression_loss: 1.0399 - classification_loss: 0.2018
 205/1500 [===>..........................] - ETA: 17:25 - loss: 1.2375 - regression_loss: 1.0365 - classification_loss: 0.2010
 206/1500 [===>..........................] - ETA: 17:27 - loss: 1.2330 - regression_loss: 1.0328 - classification_loss: 0.2002
 207/1500 [===>..........................] - ETA: 17:25 - loss: 1.2321 - regression_loss: 1.0322 - classification_loss: 0.1999
 208/1500 [===>..........................] - ETA: 17:22 - loss: 1.2296 - regression_loss: 1.0304 - classification_loss: 0.1992
 209/1500 [===>..........................] - ETA: 17:23 - loss: 1.2263 - regression_loss: 1.0277 - classification_loss: 0.1986
 210/1500 [===>..........................] - ETA: 17:20 - loss: 1.2238 - regression_loss: 1.0257 - classification_loss: 0.1981
 211/1500 [===>..........................] - ETA: 17:16 - loss: 1.2273 - regression_loss: 1.0294 - classification_loss: 0.1979
 212/1500 [===>..........................] - ETA: 17:13 - loss: 1.2290 - regression_loss: 1.0303 - classification_loss: 0.1987
 213/1500 [===>..........................] - ETA: 17:10 - loss: 1.2260 - regression_loss: 1.0281 - classification_loss: 0.1980
 214/1500 [===>..........................] - ETA: 17:06 - loss: 1.2230 - regression_loss: 1.0253 - classification_loss: 0.1977
 215/1500 [===>..........................] - ETA: 17:09 - loss: 1.2230 - regression_loss: 1.0257 - classification_loss: 0.1973
 216/1500 [===>..........................] - ETA: 17:06 - loss: 1.2210 - regression_loss: 1.0240 - classification_loss: 0.1970
 217/1500 [===>..........................] - ETA: 17:03 - loss: 1.2193 - regression_loss: 1.0228 - classification_loss: 0.1965
 218/1500 [===>..........................] - ETA: 17:00 - loss: 1.2189 - regression_loss: 1.0226 - classification_loss: 0.1963
 219/1500 [===>..........................] - ETA: 16:57 - loss: 1.2171 - regression_loss: 1.0214 - classification_loss: 0.1957
 220/1500 [===>..........................] - ETA: 16:54 - loss: 1.2137 - regression_loss: 1.0187 - classification_loss: 0.1950
 221/1500 [===>..........................] - ETA: 16:56 - loss: 1.2140 - regression_loss: 1.0190 - classification_loss: 0.1950
 222/1500 [===>..........................] - ETA: 16:53 - loss: 1.2178 - regression_loss: 1.0229 - classification_loss: 0.1949
 223/1500 [===>..........................] - ETA: 16:53 - loss: 1.2180 - regression_loss: 1.0232 - classification_loss: 0.1948
 224/1500 [===>..........................] - ETA: 16:50 - loss: 1.2190 - regression_loss: 1.0242 - classification_loss: 0.1948
 225/1500 [===>..........................] - ETA: 16:54 - loss: 1.2185 - regression_loss: 1.0243 - classification_loss: 0.1942
 226/1500 [===>..........................] - ETA: 16:54 - loss: 1.2197 - regression_loss: 1.0258 - classification_loss: 0.1939
 227/1500 [===>..........................] - ETA: 16:54 - loss: 1.2180 - regression_loss: 1.0244 - classification_loss: 0.1936
 228/1500 [===>..........................] - ETA: 16:57 - loss: 1.2166 - regression_loss: 1.0236 - classification_loss: 0.1930
 229/1500 [===>..........................] - ETA: 16:57 - loss: 1.2135 - regression_loss: 1.0210 - classification_loss: 0.1925
 230/1500 [===>..........................] - ETA: 16:53 - loss: 1.2109 - regression_loss: 1.0191 - classification_loss: 0.1919
 231/1500 [===>..........................] - ETA: 16:52 - loss: 1.2092 - regression_loss: 1.0179 - classification_loss: 0.1913
 232/1500 [===>..........................] - ETA: 16:49 - loss: 1.2066 - regression_loss: 1.0158 - classification_loss: 0.1907
 233/1500 [===>..........................] - ETA: 16:51 - loss: 1.2087 - regression_loss: 1.0179 - classification_loss: 0.1908
 234/1500 [===>..........................] - ETA: 16:56 - loss: 1.2070 - regression_loss: 1.0168 - classification_loss: 0.1902
 235/1500 [===>..........................] - ETA: 17:00 - loss: 1.2074 - regression_loss: 1.0175 - classification_loss: 0.1899
 236/1500 [===>..........................] - ETA: 16:57 - loss: 1.2096 - regression_loss: 1.0190 - classification_loss: 0.1906
 237/1500 [===>..........................] - ETA: 16:58 - loss: 1.2071 - regression_loss: 1.0171 - classification_loss: 0.1900
 238/1500 [===>..........................] - ETA: 16:55 - loss: 1.2071 - regression_loss: 1.0173 - classification_loss: 0.1898
 239/1500 [===>..........................] - ETA: 16:55 - loss: 1.2062 - regression_loss: 1.0165 - classification_loss: 0.1897
 240/1500 [===>..........................] - ETA: 16:54 - loss: 1.2032 - regression_loss: 1.0141 - classification_loss: 0.1891
 241/1500 [===>..........................] - ETA: 16:51 - loss: 1.2007 - regression_loss: 1.0122 - classification_loss: 0.1885
 242/1500 [===>..........................] - ETA: 16:51 - loss: 1.2052 - regression_loss: 1.0113 - classification_loss: 0.1939
 243/1500 [===>..........................] - ETA: 16:54 - loss: 1.2021 - regression_loss: 1.0088 - classification_loss: 0.1933
 244/1500 [===>..........................] - ETA: 16:51 - loss: 1.2047 - regression_loss: 1.0110 - classification_loss: 0.1937
 245/1500 [===>..........................] - ETA: 16:48 - loss: 1.2036 - regression_loss: 1.0102 - classification_loss: 0.1934
 246/1500 [===>..........................] - ETA: 16:46 - loss: 1.2024 - regression_loss: 1.0093 - classification_loss: 0.1931
 247/1500 [===>..........................] - ETA: 16:51 - loss: 1.2025 - regression_loss: 1.0092 - classification_loss: 0.1934
 248/1500 [===>..........................] - ETA: 16:53 - loss: 1.1999 - regression_loss: 1.0071 - classification_loss: 0.1928
 249/1500 [===>..........................] - ETA: 16:51 - loss: 1.1978 - regression_loss: 1.0054 - classification_loss: 0.1924
 250/1500 [====>.........................] - ETA: 16:51 - loss: 1.1999 - regression_loss: 1.0071 - classification_loss: 0.1928
 251/1500 [====>.........................] - ETA: 16:50 - loss: 1.1966 - regression_loss: 1.0045 - classification_loss: 0.1921
 252/1500 [====>.........................] - ETA: 16:47 - loss: 1.1951 - regression_loss: 1.0031 - classification_loss: 0.1920
 253/1500 [====>.........................] - ETA: 16:44 - loss: 1.1939 - regression_loss: 1.0024 - classification_loss: 0.1915
 254/1500 [====>.........................] - ETA: 16:43 - loss: 1.1960 - regression_loss: 1.0038 - classification_loss: 0.1922
 255/1500 [====>.........................] - ETA: 16:40 - loss: 1.1930 - regression_loss: 1.0015 - classification_loss: 0.1916
 256/1500 [====>.........................] - ETA: 16:41 - loss: 1.1935 - regression_loss: 1.0016 - classification_loss: 0.1918
 257/1500 [====>.........................] - ETA: 16:40 - loss: 1.1937 - regression_loss: 1.0014 - classification_loss: 0.1922
 258/1500 [====>.........................] - ETA: 16:41 - loss: 1.1958 - regression_loss: 1.0032 - classification_loss: 0.1926
 259/1500 [====>.........................] - ETA: 16:41 - loss: 1.1938 - regression_loss: 1.0016 - classification_loss: 0.1921
 260/1500 [====>.........................] - ETA: 16:39 - loss: 1.1917 - regression_loss: 1.0002 - classification_loss: 0.1915
 261/1500 [====>.........................] - ETA: 16:36 - loss: 1.1913 - regression_loss: 1.0001 - classification_loss: 0.1912
 262/1500 [====>.........................] - ETA: 16:34 - loss: 1.1903 - regression_loss: 0.9995 - classification_loss: 0.1907
 263/1500 [====>.........................] - ETA: 16:31 - loss: 1.1889 - regression_loss: 0.9986 - classification_loss: 0.1903
 264/1500 [====>.........................] - ETA: 16:29 - loss: 1.1879 - regression_loss: 0.9980 - classification_loss: 0.1899
 265/1500 [====>.........................] - ETA: 16:28 - loss: 1.1891 - regression_loss: 0.9974 - classification_loss: 0.1917
 266/1500 [====>.........................] - ETA: 16:27 - loss: 1.1879 - regression_loss: 0.9968 - classification_loss: 0.1912
 267/1500 [====>.........................] - ETA: 16:29 - loss: 1.1875 - regression_loss: 0.9961 - classification_loss: 0.1914
 268/1500 [====>.........................] - ETA: 16:26 - loss: 1.1861 - regression_loss: 0.9951 - classification_loss: 0.1910
 269/1500 [====>.........................] - ETA: 16:25 - loss: 1.1846 - regression_loss: 0.9940 - classification_loss: 0.1906
 270/1500 [====>.........................] - ETA: 16:23 - loss: 1.1836 - regression_loss: 0.9930 - classification_loss: 0.1906
 271/1500 [====>.........................] - ETA: 16:24 - loss: 1.1850 - regression_loss: 0.9938 - classification_loss: 0.1912
 272/1500 [====>.........................] - ETA: 16:27 - loss: 1.1863 - regression_loss: 0.9952 - classification_loss: 0.1910
 273/1500 [====>.........................] - ETA: 16:27 - loss: 1.1891 - regression_loss: 0.9972 - classification_loss: 0.1920
 274/1500 [====>.........................] - ETA: 16:25 - loss: 1.1907 - regression_loss: 0.9988 - classification_loss: 0.1919
 275/1500 [====>.........................] - ETA: 16:23 - loss: 1.1908 - regression_loss: 0.9992 - classification_loss: 0.1915
 276/1500 [====>.........................] - ETA: 16:20 - loss: 1.1886 - regression_loss: 0.9975 - classification_loss: 0.1910
 277/1500 [====>.........................] - ETA: 16:20 - loss: 1.1913 - regression_loss: 0.9999 - classification_loss: 0.1914
 278/1500 [====>.........................] - ETA: 16:18 - loss: 1.1913 - regression_loss: 1.0000 - classification_loss: 0.1913
 279/1500 [====>.........................] - ETA: 16:21 - loss: 1.1895 - regression_loss: 0.9983 - classification_loss: 0.1912
 280/1500 [====>.........................] - ETA: 16:18 - loss: 1.1885 - regression_loss: 0.9976 - classification_loss: 0.1910
 281/1500 [====>.........................] - ETA: 16:16 - loss: 1.1876 - regression_loss: 0.9968 - classification_loss: 0.1908
 282/1500 [====>.........................] - ETA: 16:20 - loss: 1.1863 - regression_loss: 0.9957 - classification_loss: 0.1906
 283/1500 [====>.........................] - ETA: 16:18 - loss: 1.1842 - regression_loss: 0.9939 - classification_loss: 0.1903
 284/1500 [====>.........................] - ETA: 16:16 - loss: 1.1850 - regression_loss: 0.9945 - classification_loss: 0.1905
 285/1500 [====>.........................] - ETA: 16:14 - loss: 1.1821 - regression_loss: 0.9921 - classification_loss: 0.1900
 286/1500 [====>.........................] - ETA: 16:11 - loss: 1.1840 - regression_loss: 0.9939 - classification_loss: 0.1901
 287/1500 [====>.........................] - ETA: 16:09 - loss: 1.1832 - regression_loss: 0.9933 - classification_loss: 0.1898
 288/1500 [====>.........................] - ETA: 16:06 - loss: 1.1849 - regression_loss: 0.9950 - classification_loss: 0.1900
 289/1500 [====>.........................] - ETA: 16:04 - loss: 1.1821 - regression_loss: 0.9926 - classification_loss: 0.1895
 290/1500 [====>.........................] - ETA: 16:03 - loss: 1.1834 - regression_loss: 0.9939 - classification_loss: 0.1895
 291/1500 [====>.........................] - ETA: 16:05 - loss: 1.1855 - regression_loss: 0.9956 - classification_loss: 0.1899
 292/1500 [====>.........................] - ETA: 16:02 - loss: 1.1843 - regression_loss: 0.9948 - classification_loss: 0.1895
 293/1500 [====>.........................] - ETA: 16:00 - loss: 1.1832 - regression_loss: 0.9939 - classification_loss: 0.1892
 294/1500 [====>.........................] - ETA: 15:58 - loss: 1.1825 - regression_loss: 0.9937 - classification_loss: 0.1888
 295/1500 [====>.........................] - ETA: 15:59 - loss: 1.1821 - regression_loss: 0.9933 - classification_loss: 0.1888
 296/1500 [====>.........................] - ETA: 16:01 - loss: 1.1820 - regression_loss: 0.9932 - classification_loss: 0.1888
 297/1500 [====>.........................] - ETA: 15:58 - loss: 1.1815 - regression_loss: 0.9931 - classification_loss: 0.1884
 298/1500 [====>.........................] - ETA: 15:58 - loss: 1.1797 - regression_loss: 0.9917 - classification_loss: 0.1880
 299/1500 [====>.........................] - ETA: 15:55 - loss: 1.1787 - regression_loss: 0.9908 - classification_loss: 0.1879
 300/1500 [=====>........................] - ETA: 15:53 - loss: 1.1776 - regression_loss: 0.9901 - classification_loss: 0.1875
 301/1500 [=====>........................] - ETA: 15:51 - loss: 1.1745 - regression_loss: 0.9875 - classification_loss: 0.1869
 302/1500 [=====>........................] - ETA: 15:50 - loss: 1.1724 - regression_loss: 0.9860 - classification_loss: 0.1865
 303/1500 [=====>........................] - ETA: 15:48 - loss: 1.1745 - regression_loss: 0.9872 - classification_loss: 0.1873
 304/1500 [=====>........................] - ETA: 15:45 - loss: 1.1725 - regression_loss: 0.9856 - classification_loss: 0.1869
 305/1500 [=====>........................] - ETA: 15:43 - loss: 1.1709 - regression_loss: 0.9842 - classification_loss: 0.1867
 306/1500 [=====>........................] - ETA: 15:44 - loss: 1.1754 - regression_loss: 0.9876 - classification_loss: 0.1878
 307/1500 [=====>........................] - ETA: 15:41 - loss: 1.1758 - regression_loss: 0.9884 - classification_loss: 0.1874
 308/1500 [=====>........................] - ETA: 15:39 - loss: 1.1768 - regression_loss: 0.9891 - classification_loss: 0.1877
 309/1500 [=====>........................] - ETA: 15:40 - loss: 1.1763 - regression_loss: 0.9887 - classification_loss: 0.1876
 310/1500 [=====>........................] - ETA: 15:43 - loss: 1.1792 - regression_loss: 0.9905 - classification_loss: 0.1887
 311/1500 [=====>........................] - ETA: 15:42 - loss: 1.1783 - regression_loss: 0.9899 - classification_loss: 0.1884
 312/1500 [=====>........................] - ETA: 15:41 - loss: 1.1789 - regression_loss: 0.9903 - classification_loss: 0.1886
 313/1500 [=====>........................] - ETA: 15:44 - loss: 1.1786 - regression_loss: 0.9903 - classification_loss: 0.1883
 314/1500 [=====>........................] - ETA: 15:42 - loss: 1.1784 - regression_loss: 0.9901 - classification_loss: 0.1882
 315/1500 [=====>........................] - ETA: 15:44 - loss: 1.1774 - regression_loss: 0.9896 - classification_loss: 0.1878
 316/1500 [=====>........................] - ETA: 15:42 - loss: 1.1775 - regression_loss: 0.9899 - classification_loss: 0.1876
 317/1500 [=====>........................] - ETA: 15:40 - loss: 1.1761 - regression_loss: 0.9889 - classification_loss: 0.1872
 318/1500 [=====>........................] - ETA: 15:37 - loss: 1.1800 - regression_loss: 0.9921 - classification_loss: 0.1879
 319/1500 [=====>........................] - ETA: 15:35 - loss: 1.1827 - regression_loss: 0.9933 - classification_loss: 0.1894
 320/1500 [=====>........................] - ETA: 15:34 - loss: 1.1836 - regression_loss: 0.9944 - classification_loss: 0.1892
 321/1500 [=====>........................] - ETA: 15:32 - loss: 1.1864 - regression_loss: 0.9967 - classification_loss: 0.1896
 322/1500 [=====>........................] - ETA: 15:33 - loss: 1.1872 - regression_loss: 0.9968 - classification_loss: 0.1905
 323/1500 [=====>........................] - ETA: 15:31 - loss: 1.1862 - regression_loss: 0.9960 - classification_loss: 0.1902
 324/1500 [=====>........................] - ETA: 15:29 - loss: 1.1854 - regression_loss: 0.9952 - classification_loss: 0.1901
 325/1500 [=====>........................] - ETA: 15:30 - loss: 1.1846 - regression_loss: 0.9947 - classification_loss: 0.1899
 326/1500 [=====>........................] - ETA: 15:28 - loss: 1.1865 - regression_loss: 0.9970 - classification_loss: 0.1896
 327/1500 [=====>........................] - ETA: 15:26 - loss: 1.1904 - regression_loss: 1.0000 - classification_loss: 0.1905
 328/1500 [=====>........................] - ETA: 15:24 - loss: 1.1886 - regression_loss: 0.9983 - classification_loss: 0.1903
 329/1500 [=====>........................] - ETA: 15:23 - loss: 1.1885 - regression_loss: 0.9985 - classification_loss: 0.1900
 330/1500 [=====>........................] - ETA: 15:21 - loss: 1.1885 - regression_loss: 0.9986 - classification_loss: 0.1899
 331/1500 [=====>........................] - ETA: 15:19 - loss: 1.1899 - regression_loss: 0.9996 - classification_loss: 0.1903
 332/1500 [=====>........................] - ETA: 15:18 - loss: 1.1895 - regression_loss: 0.9995 - classification_loss: 0.1900
 333/1500 [=====>........................] - ETA: 15:17 - loss: 1.1877 - regression_loss: 0.9981 - classification_loss: 0.1895
 334/1500 [=====>........................] - ETA: 15:16 - loss: 1.1909 - regression_loss: 1.0009 - classification_loss: 0.1901
 335/1500 [=====>........................] - ETA: 15:14 - loss: 1.1887 - regression_loss: 0.9992 - classification_loss: 0.1896
 336/1500 [=====>........................] - ETA: 15:14 - loss: 1.1902 - regression_loss: 1.0005 - classification_loss: 0.1897
 337/1500 [=====>........................] - ETA: 15:15 - loss: 1.1901 - regression_loss: 1.0001 - classification_loss: 0.1900
 338/1500 [=====>........................] - ETA: 15:13 - loss: 1.1901 - regression_loss: 1.0002 - classification_loss: 0.1899
 339/1500 [=====>........................] - ETA: 15:12 - loss: 1.1911 - regression_loss: 0.9997 - classification_loss: 0.1914
 340/1500 [=====>........................] - ETA: 15:12 - loss: 1.1938 - regression_loss: 1.0023 - classification_loss: 0.1915
 341/1500 [=====>........................] - ETA: 15:09 - loss: 1.1945 - regression_loss: 1.0028 - classification_loss: 0.1917
 342/1500 [=====>........................] - ETA: 15:08 - loss: 1.1948 - regression_loss: 1.0031 - classification_loss: 0.1917
 343/1500 [=====>........................] - ETA: 15:06 - loss: 1.1931 - regression_loss: 1.0018 - classification_loss: 0.1913
 344/1500 [=====>........................] - ETA: 15:07 - loss: 1.1951 - regression_loss: 1.0033 - classification_loss: 0.1918
 345/1500 [=====>........................] - ETA: 15:07 - loss: 1.1929 - regression_loss: 1.0014 - classification_loss: 0.1915
 346/1500 [=====>........................] - ETA: 15:06 - loss: 1.1934 - regression_loss: 1.0017 - classification_loss: 0.1917
 347/1500 [=====>........................] - ETA: 15:07 - loss: 1.1929 - regression_loss: 1.0013 - classification_loss: 0.1916
 348/1500 [=====>........................] - ETA: 15:06 - loss: 1.1940 - regression_loss: 1.0021 - classification_loss: 0.1919
 349/1500 [=====>........................] - ETA: 15:04 - loss: 1.1924 - regression_loss: 1.0008 - classification_loss: 0.1915
 350/1500 [======>.......................] - ETA: 15:02 - loss: 1.1921 - regression_loss: 1.0005 - classification_loss: 0.1916
 351/1500 [======>.......................] - ETA: 15:01 - loss: 1.1911 - regression_loss: 0.9998 - classification_loss: 0.1913
 352/1500 [======>.......................] - ETA: 14:59 - loss: 1.1905 - regression_loss: 0.9994 - classification_loss: 0.1910
 353/1500 [======>.......................] - ETA: 14:59 - loss: 1.1908 - regression_loss: 0.9992 - classification_loss: 0.1916
 354/1500 [======>.......................] - ETA: 14:57 - loss: 1.1918 - regression_loss: 1.0000 - classification_loss: 0.1918
 355/1500 [======>.......................] - ETA: 14:56 - loss: 1.1909 - regression_loss: 0.9995 - classification_loss: 0.1914
 356/1500 [======>.......................] - ETA: 14:54 - loss: 1.1944 - regression_loss: 1.0024 - classification_loss: 0.1920
 357/1500 [======>.......................] - ETA: 14:52 - loss: 1.1952 - regression_loss: 1.0031 - classification_loss: 0.1921
 358/1500 [======>.......................] - ETA: 14:50 - loss: 1.1937 - regression_loss: 1.0017 - classification_loss: 0.1920
 359/1500 [======>.......................] - ETA: 14:48 - loss: 1.1928 - regression_loss: 1.0011 - classification_loss: 0.1917
 360/1500 [======>.......................] - ETA: 14:47 - loss: 1.1926 - regression_loss: 1.0010 - classification_loss: 0.1915
 361/1500 [======>.......................] - ETA: 14:46 - loss: 1.1931 - regression_loss: 1.0015 - classification_loss: 0.1916
 362/1500 [======>.......................] - ETA: 14:47 - loss: 1.1962 - regression_loss: 1.0040 - classification_loss: 0.1921
 363/1500 [======>.......................] - ETA: 14:47 - loss: 1.1958 - regression_loss: 1.0029 - classification_loss: 0.1930
 364/1500 [======>.......................] - ETA: 14:47 - loss: 1.1969 - regression_loss: 1.0041 - classification_loss: 0.1928
 365/1500 [======>.......................] - ETA: 14:48 - loss: 1.1979 - regression_loss: 1.0049 - classification_loss: 0.1930
 366/1500 [======>.......................] - ETA: 14:46 - loss: 1.1977 - regression_loss: 1.0045 - classification_loss: 0.1932
 367/1500 [======>.......................] - ETA: 14:45 - loss: 1.1952 - regression_loss: 1.0025 - classification_loss: 0.1927
 368/1500 [======>.......................] - ETA: 14:43 - loss: 1.1963 - regression_loss: 1.0032 - classification_loss: 0.1931
 369/1500 [======>.......................] - ETA: 14:41 - loss: 1.1952 - regression_loss: 1.0024 - classification_loss: 0.1928
 370/1500 [======>.......................] - ETA: 14:41 - loss: 1.1936 - regression_loss: 1.0009 - classification_loss: 0.1926
 371/1500 [======>.......................] - ETA: 14:39 - loss: 1.1926 - regression_loss: 1.0002 - classification_loss: 0.1924
 372/1500 [======>.......................] - ETA: 14:39 - loss: 1.1918 - regression_loss: 0.9996 - classification_loss: 0.1922
 373/1500 [======>.......................] - ETA: 14:39 - loss: 1.1912 - regression_loss: 0.9992 - classification_loss: 0.1920
 374/1500 [======>.......................] - ETA: 14:37 - loss: 1.1925 - regression_loss: 1.0004 - classification_loss: 0.1921
 375/1500 [======>.......................] - ETA: 14:37 - loss: 1.1933 - regression_loss: 1.0012 - classification_loss: 0.1921
 376/1500 [======>.......................] - ETA: 14:36 - loss: 1.1940 - regression_loss: 1.0020 - classification_loss: 0.1919
 377/1500 [======>.......................] - ETA: 14:35 - loss: 1.1920 - regression_loss: 1.0004 - classification_loss: 0.1916
 378/1500 [======>.......................] - ETA: 14:33 - loss: 1.1910 - regression_loss: 0.9997 - classification_loss: 0.1913
 379/1500 [======>.......................] - ETA: 14:31 - loss: 1.1922 - regression_loss: 1.0007 - classification_loss: 0.1915
 380/1500 [======>.......................] - ETA: 14:30 - loss: 1.1947 - regression_loss: 1.0031 - classification_loss: 0.1916
 381/1500 [======>.......................] - ETA: 14:28 - loss: 1.1956 - regression_loss: 1.0039 - classification_loss: 0.1917
 382/1500 [======>.......................] - ETA: 14:26 - loss: 1.1971 - regression_loss: 1.0037 - classification_loss: 0.1934
 383/1500 [======>.......................] - ETA: 14:26 - loss: 1.1962 - regression_loss: 1.0032 - classification_loss: 0.1930
 384/1500 [======>.......................] - ETA: 14:24 - loss: 1.1954 - regression_loss: 1.0028 - classification_loss: 0.1926
 385/1500 [======>.......................] - ETA: 14:27 - loss: 1.1950 - regression_loss: 1.0020 - classification_loss: 0.1930
 386/1500 [======>.......................] - ETA: 14:25 - loss: 1.1943 - regression_loss: 1.0017 - classification_loss: 0.1926
 387/1500 [======>.......................] - ETA: 14:23 - loss: 1.1948 - regression_loss: 1.0021 - classification_loss: 0.1927
 388/1500 [======>.......................] - ETA: 14:22 - loss: 1.1933 - regression_loss: 1.0008 - classification_loss: 0.1925
 389/1500 [======>.......................] - ETA: 14:20 - loss: 1.1933 - regression_loss: 1.0008 - classification_loss: 0.1925
 390/1500 [======>.......................] - ETA: 14:20 - loss: 1.1955 - regression_loss: 1.0029 - classification_loss: 0.1926
 391/1500 [======>.......................] - ETA: 14:18 - loss: 1.1953 - regression_loss: 1.0029 - classification_loss: 0.1924
 392/1500 [======>.......................] - ETA: 14:16 - loss: 1.1942 - regression_loss: 1.0019 - classification_loss: 0.1923
 393/1500 [======>.......................] - ETA: 14:16 - loss: 1.1933 - regression_loss: 1.0011 - classification_loss: 0.1922
 394/1500 [======>.......................] - ETA: 14:16 - loss: 1.1948 - regression_loss: 1.0021 - classification_loss: 0.1927
 395/1500 [======>.......................] - ETA: 14:17 - loss: 1.1979 - regression_loss: 1.0043 - classification_loss: 0.1936
 396/1500 [======>.......................] - ETA: 14:15 - loss: 1.1976 - regression_loss: 1.0042 - classification_loss: 0.1935
 397/1500 [======>.......................] - ETA: 14:15 - loss: 1.1970 - regression_loss: 1.0039 - classification_loss: 0.1931
 398/1500 [======>.......................] - ETA: 14:17 - loss: 1.1974 - regression_loss: 1.0042 - classification_loss: 0.1932
 399/1500 [======>.......................] - ETA: 14:15 - loss: 1.1978 - regression_loss: 1.0047 - classification_loss: 0.1931
 400/1500 [=======>......................] - ETA: 14:15 - loss: 1.1996 - regression_loss: 1.0063 - classification_loss: 0.1933
 401/1500 [=======>......................] - ETA: 14:13 - loss: 1.1985 - regression_loss: 1.0055 - classification_loss: 0.1930
 402/1500 [=======>......................] - ETA: 14:12 - loss: 1.1999 - regression_loss: 1.0065 - classification_loss: 0.1934
 403/1500 [=======>......................] - ETA: 14:11 - loss: 1.1989 - regression_loss: 1.0057 - classification_loss: 0.1932
 404/1500 [=======>......................] - ETA: 14:09 - loss: 1.1987 - regression_loss: 1.0056 - classification_loss: 0.1931
 405/1500 [=======>......................] - ETA: 14:08 - loss: 1.1999 - regression_loss: 1.0067 - classification_loss: 0.1932
 406/1500 [=======>......................] - ETA: 14:07 - loss: 1.1988 - regression_loss: 1.0057 - classification_loss: 0.1931
 407/1500 [=======>......................] - ETA: 14:05 - loss: 1.1981 - regression_loss: 1.0049 - classification_loss: 0.1932
 408/1500 [=======>......................] - ETA: 14:04 - loss: 1.2022 - regression_loss: 1.0085 - classification_loss: 0.1937
 409/1500 [=======>......................] - ETA: 14:05 - loss: 1.2016 - regression_loss: 1.0081 - classification_loss: 0.1935
 410/1500 [=======>......................] - ETA: 14:03 - loss: 1.2001 - regression_loss: 1.0069 - classification_loss: 0.1932
 411/1500 [=======>......................] - ETA: 14:01 - loss: 1.1988 - regression_loss: 1.0059 - classification_loss: 0.1929
 412/1500 [=======>......................] - ETA: 14:02 - loss: 1.2003 - regression_loss: 1.0071 - classification_loss: 0.1932
 413/1500 [=======>......................] - ETA: 14:01 - loss: 1.1988 - regression_loss: 1.0058 - classification_loss: 0.1929
 414/1500 [=======>......................] - ETA: 14:00 - loss: 1.2020 - regression_loss: 1.0082 - classification_loss: 0.1938
 415/1500 [=======>......................] - ETA: 14:02 - loss: 1.2000 - regression_loss: 1.0066 - classification_loss: 0.1934
 416/1500 [=======>......................] - ETA: 14:01 - loss: 1.1990 - regression_loss: 1.0059 - classification_loss: 0.1930
 417/1500 [=======>......................] - ETA: 14:02 - loss: 1.1987 - regression_loss: 1.0057 - classification_loss: 0.1930
 418/1500 [=======>......................] - ETA: 14:01 - loss: 1.1986 - regression_loss: 1.0059 - classification_loss: 0.1927
 419/1500 [=======>......................] - ETA: 14:02 - loss: 1.1971 - regression_loss: 1.0047 - classification_loss: 0.1924
 420/1500 [=======>......................] - ETA: 14:00 - loss: 1.1952 - regression_loss: 1.0031 - classification_loss: 0.1921
 421/1500 [=======>......................] - ETA: 14:02 - loss: 1.1977 - regression_loss: 1.0053 - classification_loss: 0.1924
 422/1500 [=======>......................] - ETA: 14:00 - loss: 1.1973 - regression_loss: 1.0049 - classification_loss: 0.1924
 423/1500 [=======>......................] - ETA: 14:01 - loss: 1.1958 - regression_loss: 1.0038 - classification_loss: 0.1920
 424/1500 [=======>......................] - ETA: 14:00 - loss: 1.1952 - regression_loss: 1.0034 - classification_loss: 0.1919
 425/1500 [=======>......................] - ETA: 13:58 - loss: 1.1939 - regression_loss: 1.0023 - classification_loss: 0.1916
 426/1500 [=======>......................] - ETA: 13:56 - loss: 1.1937 - regression_loss: 1.0020 - classification_loss: 0.1917
 427/1500 [=======>......................] - ETA: 13:56 - loss: 1.1941 - regression_loss: 1.0026 - classification_loss: 0.1915
 428/1500 [=======>......................] - ETA: 13:54 - loss: 1.1931 - regression_loss: 1.0019 - classification_loss: 0.1912
 429/1500 [=======>......................] - ETA: 13:53 - loss: 1.1917 - regression_loss: 1.0009 - classification_loss: 0.1908
 430/1500 [=======>......................] - ETA: 13:52 - loss: 1.1911 - regression_loss: 1.0004 - classification_loss: 0.1907
 431/1500 [=======>......................] - ETA: 13:51 - loss: 1.1919 - regression_loss: 1.0009 - classification_loss: 0.1910
 432/1500 [=======>......................] - ETA: 13:50 - loss: 1.1904 - regression_loss: 0.9996 - classification_loss: 0.1907
 433/1500 [=======>......................] - ETA: 13:51 - loss: 1.1897 - regression_loss: 0.9991 - classification_loss: 0.1906
 434/1500 [=======>......................] - ETA: 13:51 - loss: 1.1935 - regression_loss: 1.0008 - classification_loss: 0.1927
 435/1500 [=======>......................] - ETA: 13:51 - loss: 1.1952 - regression_loss: 1.0020 - classification_loss: 0.1932
 436/1500 [=======>......................] - ETA: 13:49 - loss: 1.1936 - regression_loss: 1.0005 - classification_loss: 0.1930
 437/1500 [=======>......................] - ETA: 13:48 - loss: 1.1926 - regression_loss: 0.9998 - classification_loss: 0.1928
 438/1500 [=======>......................] - ETA: 13:48 - loss: 1.1918 - regression_loss: 0.9992 - classification_loss: 0.1926
 439/1500 [=======>......................] - ETA: 13:46 - loss: 1.1900 - regression_loss: 0.9977 - classification_loss: 0.1923
 440/1500 [=======>......................] - ETA: 13:44 - loss: 1.1906 - regression_loss: 0.9983 - classification_loss: 0.1923
 441/1500 [=======>......................] - ETA: 13:43 - loss: 1.1911 - regression_loss: 0.9990 - classification_loss: 0.1921
 442/1500 [=======>......................] - ETA: 13:42 - loss: 1.1895 - regression_loss: 0.9977 - classification_loss: 0.1918
 443/1500 [=======>......................] - ETA: 13:40 - loss: 1.1906 - regression_loss: 0.9985 - classification_loss: 0.1921
 444/1500 [=======>......................] - ETA: 13:39 - loss: 1.1885 - regression_loss: 0.9965 - classification_loss: 0.1920
 445/1500 [=======>......................] - ETA: 13:39 - loss: 1.1894 - regression_loss: 0.9970 - classification_loss: 0.1923
 446/1500 [=======>......................] - ETA: 13:41 - loss: 1.1886 - regression_loss: 0.9965 - classification_loss: 0.1920
 447/1500 [=======>......................] - ETA: 13:39 - loss: 1.1881 - regression_loss: 0.9962 - classification_loss: 0.1919
 448/1500 [=======>......................] - ETA: 13:38 - loss: 1.1871 - regression_loss: 0.9954 - classification_loss: 0.1917
 449/1500 [=======>......................] - ETA: 13:36 - loss: 1.1857 - regression_loss: 0.9943 - classification_loss: 0.1915
 450/1500 [========>.....................] - ETA: 13:34 - loss: 1.1856 - regression_loss: 0.9942 - classification_loss: 0.1914
 451/1500 [========>.....................] - ETA: 13:32 - loss: 1.1878 - regression_loss: 0.9958 - classification_loss: 0.1920
 452/1500 [========>.....................] - ETA: 13:31 - loss: 1.1878 - regression_loss: 0.9960 - classification_loss: 0.1917
 453/1500 [========>.....................] - ETA: 13:30 - loss: 1.1869 - regression_loss: 0.9954 - classification_loss: 0.1915
 454/1500 [========>.....................] - ETA: 13:29 - loss: 1.1853 - regression_loss: 0.9941 - classification_loss: 0.1911
 455/1500 [========>.....................] - ETA: 13:29 - loss: 1.1842 - regression_loss: 0.9932 - classification_loss: 0.1909
 456/1500 [========>.....................] - ETA: 13:34 - loss: 1.1851 - regression_loss: 0.9940 - classification_loss: 0.1911
 457/1500 [========>.....................] - ETA: 13:33 - loss: 1.1852 - regression_loss: 0.9939 - classification_loss: 0.1913
 458/1500 [========>.....................] - ETA: 13:32 - loss: 1.1852 - regression_loss: 0.9941 - classification_loss: 0.1912
 459/1500 [========>.....................] - ETA: 13:31 - loss: 1.1847 - regression_loss: 0.9938 - classification_loss: 0.1909
 460/1500 [========>.....................] - ETA: 13:30 - loss: 1.1829 - regression_loss: 0.9924 - classification_loss: 0.1906
 461/1500 [========>.....................] - ETA: 13:28 - loss: 1.1833 - regression_loss: 0.9926 - classification_loss: 0.1906
 462/1500 [========>.....................] - ETA: 13:27 - loss: 1.1841 - regression_loss: 0.9935 - classification_loss: 0.1906
 463/1500 [========>.....................] - ETA: 13:25 - loss: 1.1830 - regression_loss: 0.9928 - classification_loss: 0.1902
 464/1500 [========>.....................] - ETA: 13:27 - loss: 1.1828 - regression_loss: 0.9928 - classification_loss: 0.1900
 465/1500 [========>.....................] - ETA: 13:26 - loss: 1.1826 - regression_loss: 0.9927 - classification_loss: 0.1899
 466/1500 [========>.....................] - ETA: 13:26 - loss: 1.1847 - regression_loss: 0.9945 - classification_loss: 0.1902
 467/1500 [========>.....................] - ETA: 13:26 - loss: 1.1883 - regression_loss: 0.9972 - classification_loss: 0.1911
 468/1500 [========>.....................] - ETA: 13:24 - loss: 1.1912 - regression_loss: 0.9996 - classification_loss: 0.1916
 469/1500 [========>.....................] - ETA: 13:23 - loss: 1.1916 - regression_loss: 1.0002 - classification_loss: 0.1914
 470/1500 [========>.....................] - ETA: 13:23 - loss: 1.1944 - regression_loss: 1.0023 - classification_loss: 0.1921
 471/1500 [========>.....................] - ETA: 13:24 - loss: 1.1956 - regression_loss: 1.0035 - classification_loss: 0.1921
 472/1500 [========>.....................] - ETA: 13:22 - loss: 1.1952 - regression_loss: 1.0034 - classification_loss: 0.1918
 473/1500 [========>.....................] - ETA: 13:24 - loss: 1.1973 - regression_loss: 1.0053 - classification_loss: 0.1920
 474/1500 [========>.....................] - ETA: 13:23 - loss: 1.1970 - regression_loss: 1.0051 - classification_loss: 0.1919
 475/1500 [========>.....................] - ETA: 13:21 - loss: 1.1968 - regression_loss: 1.0051 - classification_loss: 0.1917
 476/1500 [========>.....................] - ETA: 13:21 - loss: 1.1964 - regression_loss: 1.0049 - classification_loss: 0.1915
 477/1500 [========>.....................] - ETA: 13:19 - loss: 1.1967 - regression_loss: 1.0053 - classification_loss: 0.1914
 478/1500 [========>.....................] - ETA: 13:18 - loss: 1.1962 - regression_loss: 1.0051 - classification_loss: 0.1911
 479/1500 [========>.....................] - ETA: 13:16 - loss: 1.1961 - regression_loss: 1.0051 - classification_loss: 0.1910
 480/1500 [========>.....................] - ETA: 13:15 - loss: 1.1983 - regression_loss: 1.0069 - classification_loss: 0.1914
 481/1500 [========>.....................] - ETA: 13:13 - loss: 1.1992 - regression_loss: 1.0078 - classification_loss: 0.1915
 482/1500 [========>.....................] - ETA: 13:11 - loss: 1.1996 - regression_loss: 1.0082 - classification_loss: 0.1914
 483/1500 [========>.....................] - ETA: 13:10 - loss: 1.2035 - regression_loss: 1.0113 - classification_loss: 0.1922
 484/1500 [========>.....................] - ETA: 13:08 - loss: 1.2065 - regression_loss: 1.0137 - classification_loss: 0.1928
 485/1500 [========>.....................] - ETA: 13:07 - loss: 1.2056 - regression_loss: 1.0128 - classification_loss: 0.1929
 486/1500 [========>.....................] - ETA: 13:06 - loss: 1.2060 - regression_loss: 1.0131 - classification_loss: 0.1930
 487/1500 [========>.....................] - ETA: 13:04 - loss: 1.2045 - regression_loss: 1.0119 - classification_loss: 0.1927
 488/1500 [========>.....................] - ETA: 13:04 - loss: 1.2047 - regression_loss: 1.0118 - classification_loss: 0.1929
 489/1500 [========>.....................] - ETA: 13:04 - loss: 1.2054 - regression_loss: 1.0122 - classification_loss: 0.1932
 490/1500 [========>.....................] - ETA: 13:04 - loss: 1.2043 - regression_loss: 1.0114 - classification_loss: 0.1929
 491/1500 [========>.....................] - ETA: 13:05 - loss: 1.2043 - regression_loss: 1.0115 - classification_loss: 0.1927
 492/1500 [========>.....................] - ETA: 13:04 - loss: 1.2055 - regression_loss: 1.0126 - classification_loss: 0.1929
 493/1500 [========>.....................] - ETA: 13:05 - loss: 1.2045 - regression_loss: 1.0117 - classification_loss: 0.1928
 494/1500 [========>.....................] - ETA: 13:03 - loss: 1.2035 - regression_loss: 1.0109 - classification_loss: 0.1926
 495/1500 [========>.....................] - ETA: 13:04 - loss: 1.2032 - regression_loss: 1.0109 - classification_loss: 0.1924
 496/1500 [========>.....................] - ETA: 13:04 - loss: 1.2016 - regression_loss: 1.0095 - classification_loss: 0.1922
 497/1500 [========>.....................] - ETA: 13:02 - loss: 1.2005 - regression_loss: 1.0084 - classification_loss: 0.1920
 498/1500 [========>.....................] - ETA: 13:01 - loss: 1.2004 - regression_loss: 1.0083 - classification_loss: 0.1921
 499/1500 [========>.....................] - ETA: 13:01 - loss: 1.1993 - regression_loss: 1.0073 - classification_loss: 0.1920
 500/1500 [=========>....................] - ETA: 12:59 - loss: 1.1988 - regression_loss: 1.0070 - classification_loss: 0.1917
 501/1500 [=========>....................] - ETA: 12:58 - loss: 1.1988 - regression_loss: 1.0068 - classification_loss: 0.1919
 502/1500 [=========>....................] - ETA: 12:57 - loss: 1.1999 - regression_loss: 1.0078 - classification_loss: 0.1921
 503/1500 [=========>....................] - ETA: 12:56 - loss: 1.1992 - regression_loss: 1.0074 - classification_loss: 0.1918
 504/1500 [=========>....................] - ETA: 12:55 - loss: 1.1988 - regression_loss: 1.0071 - classification_loss: 0.1917
 505/1500 [=========>....................] - ETA: 12:54 - loss: 1.1983 - regression_loss: 1.0067 - classification_loss: 0.1916
 506/1500 [=========>....................] - ETA: 12:54 - loss: 1.1975 - regression_loss: 1.0061 - classification_loss: 0.1914
 507/1500 [=========>....................] - ETA: 12:54 - loss: 1.1979 - regression_loss: 1.0067 - classification_loss: 0.1912
 508/1500 [=========>....................] - ETA: 12:52 - loss: 1.1973 - regression_loss: 1.0059 - classification_loss: 0.1914
 509/1500 [=========>....................] - ETA: 12:52 - loss: 1.1968 - regression_loss: 1.0056 - classification_loss: 0.1912
 510/1500 [=========>....................] - ETA: 12:50 - loss: 1.1954 - regression_loss: 1.0044 - classification_loss: 0.1910
 511/1500 [=========>....................] - ETA: 12:51 - loss: 1.1980 - regression_loss: 1.0065 - classification_loss: 0.1915
 512/1500 [=========>....................] - ETA: 12:49 - loss: 1.1974 - regression_loss: 1.0060 - classification_loss: 0.1914
 513/1500 [=========>....................] - ETA: 12:49 - loss: 1.1970 - regression_loss: 1.0058 - classification_loss: 0.1912
 514/1500 [=========>....................] - ETA: 12:47 - loss: 1.1965 - regression_loss: 1.0056 - classification_loss: 0.1910
 515/1500 [=========>....................] - ETA: 12:46 - loss: 1.1960 - regression_loss: 1.0053 - classification_loss: 0.1907
 516/1500 [=========>....................] - ETA: 12:44 - loss: 1.1961 - regression_loss: 1.0053 - classification_loss: 0.1908
 517/1500 [=========>....................] - ETA: 12:44 - loss: 1.1977 - regression_loss: 1.0066 - classification_loss: 0.1911
 518/1500 [=========>....................] - ETA: 12:43 - loss: 1.1981 - regression_loss: 1.0070 - classification_loss: 0.1911
 519/1500 [=========>....................] - ETA: 12:41 - loss: 1.2001 - regression_loss: 1.0086 - classification_loss: 0.1915
 520/1500 [=========>....................] - ETA: 12:41 - loss: 1.1994 - regression_loss: 1.0081 - classification_loss: 0.1913
 521/1500 [=========>....................] - ETA: 12:39 - loss: 1.1988 - regression_loss: 1.0074 - classification_loss: 0.1914
 522/1500 [=========>....................] - ETA: 12:38 - loss: 1.1983 - regression_loss: 1.0070 - classification_loss: 0.1913
 523/1500 [=========>....................] - ETA: 12:40 - loss: 1.1988 - regression_loss: 1.0076 - classification_loss: 0.1913
 524/1500 [=========>....................] - ETA: 12:39 - loss: 1.1997 - regression_loss: 1.0069 - classification_loss: 0.1928
 525/1500 [=========>....................] - ETA: 12:39 - loss: 1.2013 - regression_loss: 1.0080 - classification_loss: 0.1933
 526/1500 [=========>....................] - ETA: 12:38 - loss: 1.2012 - regression_loss: 1.0081 - classification_loss: 0.1931
 527/1500 [=========>....................] - ETA: 12:37 - loss: 1.1999 - regression_loss: 1.0070 - classification_loss: 0.1929
 528/1500 [=========>....................] - ETA: 12:36 - loss: 1.1987 - regression_loss: 1.0061 - classification_loss: 0.1926
 529/1500 [=========>....................] - ETA: 12:35 - loss: 1.1991 - regression_loss: 1.0060 - classification_loss: 0.1931
 530/1500 [=========>....................] - ETA: 12:35 - loss: 1.1988 - regression_loss: 1.0058 - classification_loss: 0.1930
 531/1500 [=========>....................] - ETA: 12:34 - loss: 1.1981 - regression_loss: 1.0053 - classification_loss: 0.1929
 532/1500 [=========>....................] - ETA: 12:33 - loss: 1.1980 - regression_loss: 1.0053 - classification_loss: 0.1926
 533/1500 [=========>....................] - ETA: 12:31 - loss: 1.1972 - regression_loss: 1.0048 - classification_loss: 0.1924
 534/1500 [=========>....................] - ETA: 12:35 - loss: 1.1977 - regression_loss: 1.0053 - classification_loss: 0.1924
 535/1500 [=========>....................] - ETA: 12:34 - loss: 1.1987 - regression_loss: 1.0059 - classification_loss: 0.1928
 536/1500 [=========>....................] - ETA: 12:33 - loss: 1.1988 - regression_loss: 1.0061 - classification_loss: 0.1927
 537/1500 [=========>....................] - ETA: 12:32 - loss: 1.1979 - regression_loss: 1.0053 - classification_loss: 0.1926
 538/1500 [=========>....................] - ETA: 12:30 - loss: 1.1966 - regression_loss: 1.0043 - classification_loss: 0.1923
 539/1500 [=========>....................] - ETA: 12:29 - loss: 1.1964 - regression_loss: 1.0042 - classification_loss: 0.1921
 540/1500 [=========>....................] - ETA: 12:29 - loss: 1.1952 - regression_loss: 1.0033 - classification_loss: 0.1918
 541/1500 [=========>....................] - ETA: 12:28 - loss: 1.1955 - regression_loss: 1.0036 - classification_loss: 0.1919
 542/1500 [=========>....................] - ETA: 12:27 - loss: 1.1965 - regression_loss: 1.0045 - classification_loss: 0.1919
 543/1500 [=========>....................] - ETA: 12:26 - loss: 1.1970 - regression_loss: 1.0042 - classification_loss: 0.1928
 544/1500 [=========>....................] - ETA: 12:24 - loss: 1.1969 - regression_loss: 1.0042 - classification_loss: 0.1927
 545/1500 [=========>....................] - ETA: 12:23 - loss: 1.1981 - regression_loss: 1.0051 - classification_loss: 0.1930
 546/1500 [=========>....................] - ETA: 12:22 - loss: 1.1977 - regression_loss: 1.0049 - classification_loss: 0.1928
 547/1500 [=========>....................] - ETA: 12:21 - loss: 1.1966 - regression_loss: 1.0041 - classification_loss: 0.1925
 548/1500 [=========>....................] - ETA: 12:20 - loss: 1.1978 - regression_loss: 1.0047 - classification_loss: 0.1931
 549/1500 [=========>....................] - ETA: 12:19 - loss: 1.1968 - regression_loss: 1.0040 - classification_loss: 0.1928
 550/1500 [==========>...................] - ETA: 12:18 - loss: 1.1958 - regression_loss: 1.0031 - classification_loss: 0.1927
 551/1500 [==========>...................] - ETA: 12:17 - loss: 1.1982 - regression_loss: 1.0048 - classification_loss: 0.1935
 552/1500 [==========>...................] - ETA: 12:15 - loss: 1.1996 - regression_loss: 1.0058 - classification_loss: 0.1938
 553/1500 [==========>...................] - ETA: 12:14 - loss: 1.2012 - regression_loss: 1.0075 - classification_loss: 0.1937
 554/1500 [==========>...................] - ETA: 12:14 - loss: 1.2002 - regression_loss: 1.0067 - classification_loss: 0.1935
 555/1500 [==========>...................] - ETA: 12:14 - loss: 1.1996 - regression_loss: 1.0059 - classification_loss: 0.1937
 556/1500 [==========>...................] - ETA: 12:13 - loss: 1.1984 - regression_loss: 1.0050 - classification_loss: 0.1935
 557/1500 [==========>...................] - ETA: 12:12 - loss: 1.1992 - regression_loss: 1.0056 - classification_loss: 0.1936
 558/1500 [==========>...................] - ETA: 12:11 - loss: 1.2007 - regression_loss: 1.0070 - classification_loss: 0.1937
 559/1500 [==========>...................] - ETA: 12:10 - loss: 1.2000 - regression_loss: 1.0064 - classification_loss: 0.1936
 560/1500 [==========>...................] - ETA: 12:09 - loss: 1.2003 - regression_loss: 1.0067 - classification_loss: 0.1936
 561/1500 [==========>...................] - ETA: 12:09 - loss: 1.2002 - regression_loss: 1.0067 - classification_loss: 0.1935
 562/1500 [==========>...................] - ETA: 12:08 - loss: 1.2003 - regression_loss: 1.0069 - classification_loss: 0.1935
 563/1500 [==========>...................] - ETA: 12:08 - loss: 1.2001 - regression_loss: 1.0068 - classification_loss: 0.1933
 564/1500 [==========>...................] - ETA: 12:07 - loss: 1.1996 - regression_loss: 1.0063 - classification_loss: 0.1932
 565/1500 [==========>...................] - ETA: 12:05 - loss: 1.2002 - regression_loss: 1.0067 - classification_loss: 0.1936
 566/1500 [==========>...................] - ETA: 12:04 - loss: 1.1993 - regression_loss: 1.0059 - classification_loss: 0.1934
 567/1500 [==========>...................] - ETA: 12:04 - loss: 1.1983 - regression_loss: 1.0051 - classification_loss: 0.1932
 568/1500 [==========>...................] - ETA: 12:02 - loss: 1.1976 - regression_loss: 1.0045 - classification_loss: 0.1930
 569/1500 [==========>...................] - ETA: 12:02 - loss: 1.1989 - regression_loss: 1.0056 - classification_loss: 0.1932
 570/1500 [==========>...................] - ETA: 12:01 - loss: 1.2002 - regression_loss: 1.0071 - classification_loss: 0.1932
 571/1500 [==========>...................] - ETA: 12:00 - loss: 1.1997 - regression_loss: 1.0066 - classification_loss: 0.1931
 572/1500 [==========>...................] - ETA: 12:01 - loss: 1.1986 - regression_loss: 1.0058 - classification_loss: 0.1929
 573/1500 [==========>...................] - ETA: 11:59 - loss: 1.2003 - regression_loss: 1.0072 - classification_loss: 0.1931
 574/1500 [==========>...................] - ETA: 12:02 - loss: 1.1990 - regression_loss: 1.0061 - classification_loss: 0.1929
 575/1500 [==========>...................] - ETA: 12:01 - loss: 1.2008 - regression_loss: 1.0075 - classification_loss: 0.1933
 576/1500 [==========>...................] - ETA: 12:01 - loss: 1.1997 - regression_loss: 1.0067 - classification_loss: 0.1930
 577/1500 [==========>...................] - ETA: 11:59 - loss: 1.1988 - regression_loss: 1.0060 - classification_loss: 0.1928
 578/1500 [==========>...................] - ETA: 11:58 - loss: 1.1985 - regression_loss: 1.0057 - classification_loss: 0.1928
 579/1500 [==========>...................] - ETA: 11:57 - loss: 1.1974 - regression_loss: 1.0049 - classification_loss: 0.1925
 580/1500 [==========>...................] - ETA: 11:58 - loss: 1.1965 - regression_loss: 1.0040 - classification_loss: 0.1925
 581/1500 [==========>...................] - ETA: 11:57 - loss: 1.1966 - regression_loss: 1.0042 - classification_loss: 0.1924
 582/1500 [==========>...................] - ETA: 11:56 - loss: 1.1956 - regression_loss: 1.0035 - classification_loss: 0.1921
 583/1500 [==========>...................] - ETA: 11:54 - loss: 1.1952 - regression_loss: 1.0031 - classification_loss: 0.1921
 584/1500 [==========>...................] - ETA: 11:54 - loss: 1.1943 - regression_loss: 1.0023 - classification_loss: 0.1920
 585/1500 [==========>...................] - ETA: 11:53 - loss: 1.1939 - regression_loss: 1.0020 - classification_loss: 0.1919
 586/1500 [==========>...................] - ETA: 11:52 - loss: 1.1935 - regression_loss: 1.0016 - classification_loss: 0.1919
 587/1500 [==========>...................] - ETA: 11:50 - loss: 1.1924 - regression_loss: 1.0006 - classification_loss: 0.1917
 588/1500 [==========>...................] - ETA: 11:49 - loss: 1.1917 - regression_loss: 1.0002 - classification_loss: 0.1915
 589/1500 [==========>...................] - ETA: 11:48 - loss: 1.1916 - regression_loss: 1.0001 - classification_loss: 0.1915
 590/1500 [==========>...................] - ETA: 11:46 - loss: 1.1926 - regression_loss: 1.0008 - classification_loss: 0.1918
 591/1500 [==========>...................] - ETA: 11:45 - loss: 1.1928 - regression_loss: 1.0010 - classification_loss: 0.1918
 592/1500 [==========>...................] - ETA: 11:44 - loss: 1.1923 - regression_loss: 1.0006 - classification_loss: 0.1917
 593/1500 [==========>...................] - ETA: 11:42 - loss: 1.1916 - regression_loss: 1.0000 - classification_loss: 0.1915
 594/1500 [==========>...................] - ETA: 11:41 - loss: 1.1912 - regression_loss: 0.9995 - classification_loss: 0.1917
 595/1500 [==========>...................] - ETA: 11:40 - loss: 1.1916 - regression_loss: 0.9997 - classification_loss: 0.1918
 596/1500 [==========>...................] - ETA: 11:40 - loss: 1.1909 - regression_loss: 0.9992 - classification_loss: 0.1917
 597/1500 [==========>...................] - ETA: 11:39 - loss: 1.1902 - regression_loss: 0.9987 - classification_loss: 0.1915
 598/1500 [==========>...................] - ETA: 11:38 - loss: 1.1903 - regression_loss: 0.9989 - classification_loss: 0.1914
 599/1500 [==========>...................] - ETA: 11:37 - loss: 1.1913 - regression_loss: 0.9997 - classification_loss: 0.1916
 600/1500 [===========>..................] - ETA: 11:38 - loss: 1.1904 - regression_loss: 0.9990 - classification_loss: 0.1914
 601/1500 [===========>..................] - ETA: 11:36 - loss: 1.1898 - regression_loss: 0.9984 - classification_loss: 0.1913
 602/1500 [===========>..................] - ETA: 11:36 - loss: 1.1900 - regression_loss: 0.9987 - classification_loss: 0.1913
 603/1500 [===========>..................] - ETA: 11:35 - loss: 1.1896 - regression_loss: 0.9984 - classification_loss: 0.1911
 604/1500 [===========>..................] - ETA: 11:34 - loss: 1.1892 - regression_loss: 0.9982 - classification_loss: 0.1911
 605/1500 [===========>..................] - ETA: 11:33 - loss: 1.1882 - regression_loss: 0.9974 - classification_loss: 0.1908
 606/1500 [===========>..................] - ETA: 11:32 - loss: 1.1892 - regression_loss: 0.9981 - classification_loss: 0.1911
 607/1500 [===========>..................] - ETA: 11:32 - loss: 1.1881 - regression_loss: 0.9973 - classification_loss: 0.1909
 608/1500 [===========>..................] - ETA: 11:31 - loss: 1.1868 - regression_loss: 0.9962 - classification_loss: 0.1906
 609/1500 [===========>..................] - ETA: 11:29 - loss: 1.1859 - regression_loss: 0.9955 - classification_loss: 0.1904
 610/1500 [===========>..................] - ETA: 11:29 - loss: 1.1846 - regression_loss: 0.9944 - classification_loss: 0.1902
 611/1500 [===========>..................] - ETA: 11:27 - loss: 1.1835 - regression_loss: 0.9936 - classification_loss: 0.1899
 612/1500 [===========>..................] - ETA: 11:26 - loss: 1.1838 - regression_loss: 0.9939 - classification_loss: 0.1899
 613/1500 [===========>..................] - ETA: 11:26 - loss: 1.1830 - regression_loss: 0.9933 - classification_loss: 0.1898
 614/1500 [===========>..................] - ETA: 11:26 - loss: 1.1830 - regression_loss: 0.9931 - classification_loss: 0.1899
 615/1500 [===========>..................] - ETA: 11:25 - loss: 1.1819 - regression_loss: 0.9922 - classification_loss: 0.1897
 616/1500 [===========>..................] - ETA: 11:25 - loss: 1.1822 - regression_loss: 0.9925 - classification_loss: 0.1896
 617/1500 [===========>..................] - ETA: 11:24 - loss: 1.1812 - regression_loss: 0.9918 - classification_loss: 0.1894
 618/1500 [===========>..................] - ETA: 11:24 - loss: 1.1804 - regression_loss: 0.9912 - classification_loss: 0.1892
 619/1500 [===========>..................] - ETA: 11:23 - loss: 1.1798 - regression_loss: 0.9907 - classification_loss: 0.1891
 620/1500 [===========>..................] - ETA: 11:25 - loss: 1.1800 - regression_loss: 0.9909 - classification_loss: 0.1891
 621/1500 [===========>..................] - ETA: 11:23 - loss: 1.1789 - regression_loss: 0.9899 - classification_loss: 0.1890
 622/1500 [===========>..................] - ETA: 11:22 - loss: 1.1789 - regression_loss: 0.9895 - classification_loss: 0.1893
 623/1500 [===========>..................] - ETA: 11:21 - loss: 1.1791 - regression_loss: 0.9899 - classification_loss: 0.1892
 624/1500 [===========>..................] - ETA: 11:20 - loss: 1.1789 - regression_loss: 0.9898 - classification_loss: 0.1891
 625/1500 [===========>..................] - ETA: 11:18 - loss: 1.1779 - regression_loss: 0.9889 - classification_loss: 0.1890
 626/1500 [===========>..................] - ETA: 11:18 - loss: 1.1782 - regression_loss: 0.9893 - classification_loss: 0.1889
 627/1500 [===========>..................] - ETA: 11:17 - loss: 1.1785 - regression_loss: 0.9894 - classification_loss: 0.1891
 628/1500 [===========>..................] - ETA: 11:16 - loss: 1.1786 - regression_loss: 0.9895 - classification_loss: 0.1891
 629/1500 [===========>..................] - ETA: 11:16 - loss: 1.1788 - regression_loss: 0.9897 - classification_loss: 0.1890
 630/1500 [===========>..................] - ETA: 11:15 - loss: 1.1777 - regression_loss: 0.9889 - classification_loss: 0.1888
 631/1500 [===========>..................] - ETA: 11:14 - loss: 1.1790 - regression_loss: 0.9900 - classification_loss: 0.1890
 632/1500 [===========>..................] - ETA: 11:13 - loss: 1.1786 - regression_loss: 0.9897 - classification_loss: 0.1889
 633/1500 [===========>..................] - ETA: 11:11 - loss: 1.1774 - regression_loss: 0.9888 - classification_loss: 0.1886
 634/1500 [===========>..................] - ETA: 11:10 - loss: 1.1776 - regression_loss: 0.9891 - classification_loss: 0.1885
 635/1500 [===========>..................] - ETA: 11:09 - loss: 1.1796 - regression_loss: 0.9909 - classification_loss: 0.1887
 636/1500 [===========>..................] - ETA: 11:09 - loss: 1.1792 - regression_loss: 0.9905 - classification_loss: 0.1887
 637/1500 [===========>..................] - ETA: 11:09 - loss: 1.1800 - regression_loss: 0.9910 - classification_loss: 0.1890
 638/1500 [===========>..................] - ETA: 11:07 - loss: 1.1792 - regression_loss: 0.9904 - classification_loss: 0.1888
 639/1500 [===========>..................] - ETA: 11:07 - loss: 1.1788 - regression_loss: 0.9901 - classification_loss: 0.1887
 640/1500 [===========>..................] - ETA: 11:06 - loss: 1.1796 - regression_loss: 0.9908 - classification_loss: 0.1888
 641/1500 [===========>..................] - ETA: 11:06 - loss: 1.1789 - regression_loss: 0.9903 - classification_loss: 0.1886
 642/1500 [===========>..................] - ETA: 11:05 - loss: 1.1777 - regression_loss: 0.9894 - classification_loss: 0.1884
 643/1500 [===========>..................] - ETA: 11:04 - loss: 1.1771 - regression_loss: 0.9889 - classification_loss: 0.1882
 644/1500 [===========>..................] - ETA: 11:03 - loss: 1.1772 - regression_loss: 0.9890 - classification_loss: 0.1881
 645/1500 [===========>..................] - ETA: 11:02 - loss: 1.1775 - regression_loss: 0.9894 - classification_loss: 0.1881
 646/1500 [===========>..................] - ETA: 11:01 - loss: 1.1787 - regression_loss: 0.9903 - classification_loss: 0.1883
 647/1500 [===========>..................] - ETA: 11:01 - loss: 1.1789 - regression_loss: 0.9906 - classification_loss: 0.1884
 648/1500 [===========>..................] - ETA: 11:00 - loss: 1.1787 - regression_loss: 0.9905 - classification_loss: 0.1882
 649/1500 [===========>..................] - ETA: 11:02 - loss: 1.1785 - regression_loss: 0.9904 - classification_loss: 0.1881
 650/1500 [============>.................] - ETA: 11:01 - loss: 1.1783 - regression_loss: 0.9902 - classification_loss: 0.1882
 651/1500 [============>.................] - ETA: 11:00 - loss: 1.1796 - regression_loss: 0.9911 - classification_loss: 0.1885
 652/1500 [============>.................] - ETA: 10:59 - loss: 1.1799 - regression_loss: 0.9915 - classification_loss: 0.1884
 653/1500 [============>.................] - ETA: 10:59 - loss: 1.1797 - regression_loss: 0.9913 - classification_loss: 0.1883
 654/1500 [============>.................] - ETA: 11:00 - loss: 1.1788 - regression_loss: 0.9908 - classification_loss: 0.1881
 655/1500 [============>.................] - ETA: 10:59 - loss: 1.1796 - regression_loss: 0.9914 - classification_loss: 0.1882
 656/1500 [============>.................] - ETA: 10:59 - loss: 1.1806 - regression_loss: 0.9924 - classification_loss: 0.1882
 657/1500 [============>.................] - ETA: 10:57 - loss: 1.1803 - regression_loss: 0.9922 - classification_loss: 0.1881
 658/1500 [============>.................] - ETA: 10:57 - loss: 1.1793 - regression_loss: 0.9914 - classification_loss: 0.1879
 659/1500 [============>.................] - ETA: 10:56 - loss: 1.1805 - regression_loss: 0.9924 - classification_loss: 0.1881
 660/1500 [============>.................] - ETA: 10:54 - loss: 1.1795 - regression_loss: 0.9915 - classification_loss: 0.1880
 661/1500 [============>.................] - ETA: 10:53 - loss: 1.1793 - regression_loss: 0.9913 - classification_loss: 0.1879
 662/1500 [============>.................] - ETA: 10:53 - loss: 1.1787 - regression_loss: 0.9909 - classification_loss: 0.1878
 663/1500 [============>.................] - ETA: 10:51 - loss: 1.1785 - regression_loss: 0.9906 - classification_loss: 0.1879
 664/1500 [============>.................] - ETA: 10:50 - loss: 1.1783 - regression_loss: 0.9906 - classification_loss: 0.1877
 665/1500 [============>.................] - ETA: 10:50 - loss: 1.1777 - regression_loss: 0.9900 - classification_loss: 0.1877
 666/1500 [============>.................] - ETA: 10:49 - loss: 1.1769 - regression_loss: 0.9894 - classification_loss: 0.1876
 667/1500 [============>.................] - ETA: 10:49 - loss: 1.1766 - regression_loss: 0.9892 - classification_loss: 0.1874
 668/1500 [============>.................] - ETA: 10:48 - loss: 1.1754 - regression_loss: 0.9882 - classification_loss: 0.1872
 669/1500 [============>.................] - ETA: 10:48 - loss: 1.1751 - regression_loss: 0.9880 - classification_loss: 0.1871
 670/1500 [============>.................] - ETA: 10:47 - loss: 1.1740 - regression_loss: 0.9871 - classification_loss: 0.1869
 671/1500 [============>.................] - ETA: 10:46 - loss: 1.1729 - regression_loss: 0.9862 - classification_loss: 0.1866
 672/1500 [============>.................] - ETA: 10:45 - loss: 1.1717 - regression_loss: 0.9853 - classification_loss: 0.1865
 673/1500 [============>.................] - ETA: 10:44 - loss: 1.1711 - regression_loss: 0.9848 - classification_loss: 0.1863
 674/1500 [============>.................] - ETA: 10:43 - loss: 1.1702 - regression_loss: 0.9842 - classification_loss: 0.1861
 675/1500 [============>.................] - ETA: 10:42 - loss: 1.1702 - regression_loss: 0.9843 - classification_loss: 0.1860
 676/1500 [============>.................] - ETA: 10:42 - loss: 1.1720 - regression_loss: 0.9858 - classification_loss: 0.1862
 677/1500 [============>.................] - ETA: 10:41 - loss: 1.1734 - regression_loss: 0.9868 - classification_loss: 0.1866
 678/1500 [============>.................] - ETA: 10:41 - loss: 1.1731 - regression_loss: 0.9865 - classification_loss: 0.1865
 679/1500 [============>.................] - ETA: 10:41 - loss: 1.1724 - regression_loss: 0.9860 - classification_loss: 0.1863
 680/1500 [============>.................] - ETA: 10:40 - loss: 1.1726 - regression_loss: 0.9863 - classification_loss: 0.1863
 681/1500 [============>.................] - ETA: 10:39 - loss: 1.1741 - regression_loss: 0.9875 - classification_loss: 0.1866
 682/1500 [============>.................] - ETA: 10:39 - loss: 1.1733 - regression_loss: 0.9868 - classification_loss: 0.1864
 683/1500 [============>.................] - ETA: 10:39 - loss: 1.1720 - regression_loss: 0.9857 - classification_loss: 0.1863
 684/1500 [============>.................] - ETA: 10:38 - loss: 1.1721 - regression_loss: 0.9859 - classification_loss: 0.1862
 685/1500 [============>.................] - ETA: 10:37 - loss: 1.1716 - regression_loss: 0.9855 - classification_loss: 0.1861
 686/1500 [============>.................] - ETA: 10:37 - loss: 1.1720 - regression_loss: 0.9858 - classification_loss: 0.1862
 687/1500 [============>.................] - ETA: 10:36 - loss: 1.1724 - regression_loss: 0.9862 - classification_loss: 0.1862
 688/1500 [============>.................] - ETA: 10:36 - loss: 1.1722 - regression_loss: 0.9859 - classification_loss: 0.1863
 689/1500 [============>.................] - ETA: 10:35 - loss: 1.1718 - regression_loss: 0.9856 - classification_loss: 0.1862
 690/1500 [============>.................] - ETA: 10:35 - loss: 1.1708 - regression_loss: 0.9847 - classification_loss: 0.1861
 691/1500 [============>.................] - ETA: 10:34 - loss: 1.1702 - regression_loss: 0.9842 - classification_loss: 0.1860
 692/1500 [============>.................] - ETA: 10:33 - loss: 1.1696 - regression_loss: 0.9839 - classification_loss: 0.1858
 693/1500 [============>.................] - ETA: 10:32 - loss: 1.1688 - regression_loss: 0.9833 - classification_loss: 0.1855
 694/1500 [============>.................] - ETA: 10:31 - loss: 1.1686 - regression_loss: 0.9829 - classification_loss: 0.1857
 695/1500 [============>.................] - ETA: 10:30 - loss: 1.1678 - regression_loss: 0.9822 - classification_loss: 0.1855
 696/1500 [============>.................] - ETA: 10:29 - loss: 1.1668 - regression_loss: 0.9814 - classification_loss: 0.1854
 697/1500 [============>.................] - ETA: 10:28 - loss: 1.1662 - regression_loss: 0.9811 - classification_loss: 0.1851
 698/1500 [============>.................] - ETA: 10:28 - loss: 1.1652 - regression_loss: 0.9802 - classification_loss: 0.1850
 699/1500 [============>.................] - ETA: 10:27 - loss: 1.1652 - regression_loss: 0.9801 - classification_loss: 0.1852
 700/1500 [=============>................] - ETA: 10:28 - loss: 1.1658 - regression_loss: 0.9806 - classification_loss: 0.1852
 701/1500 [=============>................] - ETA: 10:27 - loss: 1.1663 - regression_loss: 0.9807 - classification_loss: 0.1855
 702/1500 [=============>................] - ETA: 10:27 - loss: 1.1654 - regression_loss: 0.9801 - classification_loss: 0.1853
 703/1500 [=============>................] - ETA: 10:26 - loss: 1.1660 - regression_loss: 0.9806 - classification_loss: 0.1855
 704/1500 [=============>................] - ETA: 10:27 - loss: 1.1649 - regression_loss: 0.9797 - classification_loss: 0.1853
 705/1500 [=============>................] - ETA: 10:25 - loss: 1.1643 - regression_loss: 0.9792 - classification_loss: 0.1851
 706/1500 [=============>................] - ETA: 10:24 - loss: 1.1644 - regression_loss: 0.9794 - classification_loss: 0.1851
 707/1500 [=============>................] - ETA: 10:24 - loss: 1.1644 - regression_loss: 0.9792 - classification_loss: 0.1852
 708/1500 [=============>................] - ETA: 10:23 - loss: 1.1644 - regression_loss: 0.9792 - classification_loss: 0.1852
 709/1500 [=============>................] - ETA: 10:22 - loss: 1.1634 - regression_loss: 0.9784 - classification_loss: 0.1850
 710/1500 [=============>................] - ETA: 10:22 - loss: 1.1630 - regression_loss: 0.9781 - classification_loss: 0.1848
 711/1500 [=============>................] - ETA: 10:20 - loss: 1.1623 - regression_loss: 0.9776 - classification_loss: 0.1847
 712/1500 [=============>................] - ETA: 10:19 - loss: 1.1626 - regression_loss: 0.9779 - classification_loss: 0.1847
 713/1500 [=============>................] - ETA: 10:18 - loss: 1.1619 - regression_loss: 0.9773 - classification_loss: 0.1846
 714/1500 [=============>................] - ETA: 10:20 - loss: 1.1626 - regression_loss: 0.9777 - classification_loss: 0.1849
 715/1500 [=============>................] - ETA: 10:18 - loss: 1.1618 - regression_loss: 0.9770 - classification_loss: 0.1847
 716/1500 [=============>................] - ETA: 10:17 - loss: 1.1612 - regression_loss: 0.9766 - classification_loss: 0.1846
 717/1500 [=============>................] - ETA: 10:16 - loss: 1.1612 - regression_loss: 0.9766 - classification_loss: 0.1845
 718/1500 [=============>................] - ETA: 10:15 - loss: 1.1614 - regression_loss: 0.9768 - classification_loss: 0.1846
 719/1500 [=============>................] - ETA: 10:14 - loss: 1.1628 - regression_loss: 0.9781 - classification_loss: 0.1847
 720/1500 [=============>................] - ETA: 10:13 - loss: 1.1628 - regression_loss: 0.9782 - classification_loss: 0.1846
 721/1500 [=============>................] - ETA: 10:12 - loss: 1.1633 - regression_loss: 0.9780 - classification_loss: 0.1853
 722/1500 [=============>................] - ETA: 10:11 - loss: 1.1631 - regression_loss: 0.9779 - classification_loss: 0.1852
 723/1500 [=============>................] - ETA: 10:11 - loss: 1.1633 - regression_loss: 0.9781 - classification_loss: 0.1852
 724/1500 [=============>................] - ETA: 10:10 - loss: 1.1623 - regression_loss: 0.9774 - classification_loss: 0.1850
 725/1500 [=============>................] - ETA: 10:09 - loss: 1.1622 - regression_loss: 0.9773 - classification_loss: 0.1849
 726/1500 [=============>................] - ETA: 10:07 - loss: 1.1621 - regression_loss: 0.9773 - classification_loss: 0.1848
 727/1500 [=============>................] - ETA: 10:06 - loss: 1.1629 - regression_loss: 0.9780 - classification_loss: 0.1850
 728/1500 [=============>................] - ETA: 10:05 - loss: 1.1621 - regression_loss: 0.9774 - classification_loss: 0.1848
 729/1500 [=============>................] - ETA: 10:05 - loss: 1.1614 - regression_loss: 0.9768 - classification_loss: 0.1846
 730/1500 [=============>................] - ETA: 10:04 - loss: 1.1608 - regression_loss: 0.9763 - classification_loss: 0.1845
 731/1500 [=============>................] - ETA: 10:03 - loss: 1.1610 - regression_loss: 0.9764 - classification_loss: 0.1846
 732/1500 [=============>................] - ETA: 10:03 - loss: 1.1605 - regression_loss: 0.9760 - classification_loss: 0.1845
 733/1500 [=============>................] - ETA: 10:02 - loss: 1.1605 - regression_loss: 0.9759 - classification_loss: 0.1846
 734/1500 [=============>................] - ETA: 10:00 - loss: 1.1598 - regression_loss: 0.9753 - classification_loss: 0.1845
 735/1500 [=============>................] - ETA: 10:01 - loss: 1.1597 - regression_loss: 0.9753 - classification_loss: 0.1844
 736/1500 [=============>................] - ETA: 10:00 - loss: 1.1588 - regression_loss: 0.9746 - classification_loss: 0.1842
 737/1500 [=============>................] - ETA: 9:59 - loss: 1.1589 - regression_loss: 0.9748 - classification_loss: 0.1842 
 738/1500 [=============>................] - ETA: 9:59 - loss: 1.1582 - regression_loss: 0.9742 - classification_loss: 0.1840
 739/1500 [=============>................] - ETA: 9:58 - loss: 1.1570 - regression_loss: 0.9732 - classification_loss: 0.1838
 740/1500 [=============>................] - ETA: 9:57 - loss: 1.1566 - regression_loss: 0.9729 - classification_loss: 0.1837
 741/1500 [=============>................] - ETA: 9:56 - loss: 1.1569 - regression_loss: 0.9731 - classification_loss: 0.1837
 742/1500 [=============>................] - ETA: 9:56 - loss: 1.1559 - regression_loss: 0.9723 - classification_loss: 0.1836
 743/1500 [=============>................] - ETA: 9:55 - loss: 1.1566 - regression_loss: 0.9728 - classification_loss: 0.1838
 744/1500 [=============>................] - ETA: 9:55 - loss: 1.1561 - regression_loss: 0.9724 - classification_loss: 0.1837
 745/1500 [=============>................] - ETA: 9:54 - loss: 1.1556 - regression_loss: 0.9720 - classification_loss: 0.1836
 746/1500 [=============>................] - ETA: 9:53 - loss: 1.1550 - regression_loss: 0.9716 - classification_loss: 0.1834
 747/1500 [=============>................] - ETA: 9:52 - loss: 1.1557 - regression_loss: 0.9723 - classification_loss: 0.1834
 748/1500 [=============>................] - ETA: 9:52 - loss: 1.1553 - regression_loss: 0.9720 - classification_loss: 0.1833
 749/1500 [=============>................] - ETA: 9:51 - loss: 1.1552 - regression_loss: 0.9719 - classification_loss: 0.1833
 750/1500 [==============>...............] - ETA: 9:50 - loss: 1.1550 - regression_loss: 0.9717 - classification_loss: 0.1833
 751/1500 [==============>...............] - ETA: 9:49 - loss: 1.1541 - regression_loss: 0.9710 - classification_loss: 0.1832
 752/1500 [==============>...............] - ETA: 9:49 - loss: 1.1548 - regression_loss: 0.9716 - classification_loss: 0.1832
 753/1500 [==============>...............] - ETA: 9:48 - loss: 1.1546 - regression_loss: 0.9715 - classification_loss: 0.1832
 754/1500 [==============>...............] - ETA: 9:47 - loss: 1.1542 - regression_loss: 0.9711 - classification_loss: 0.1831
 755/1500 [==============>...............] - ETA: 9:46 - loss: 1.1544 - regression_loss: 0.9714 - classification_loss: 0.1830
 756/1500 [==============>...............] - ETA: 9:46 - loss: 1.1564 - regression_loss: 0.9729 - classification_loss: 0.1835
 757/1500 [==============>...............] - ETA: 9:45 - loss: 1.1558 - regression_loss: 0.9723 - classification_loss: 0.1835
 758/1500 [==============>...............] - ETA: 9:44 - loss: 1.1569 - regression_loss: 0.9732 - classification_loss: 0.1836
 759/1500 [==============>...............] - ETA: 9:44 - loss: 1.1562 - regression_loss: 0.9727 - classification_loss: 0.1835
 760/1500 [==============>...............] - ETA: 9:43 - loss: 1.1567 - regression_loss: 0.9733 - classification_loss: 0.1833
 761/1500 [==============>...............] - ETA: 9:42 - loss: 1.1566 - regression_loss: 0.9732 - classification_loss: 0.1834
 762/1500 [==============>...............] - ETA: 9:41 - loss: 1.1573 - regression_loss: 0.9739 - classification_loss: 0.1834
 763/1500 [==============>...............] - ETA: 9:41 - loss: 1.1570 - regression_loss: 0.9738 - classification_loss: 0.1832
 764/1500 [==============>...............] - ETA: 9:40 - loss: 1.1580 - regression_loss: 0.9746 - classification_loss: 0.1834
 765/1500 [==============>...............] - ETA: 9:39 - loss: 1.1582 - regression_loss: 0.9747 - classification_loss: 0.1836
 766/1500 [==============>...............] - ETA: 9:39 - loss: 1.1586 - regression_loss: 0.9750 - classification_loss: 0.1835
 767/1500 [==============>...............] - ETA: 9:39 - loss: 1.1576 - regression_loss: 0.9742 - classification_loss: 0.1834
 768/1500 [==============>...............] - ETA: 9:38 - loss: 1.1582 - regression_loss: 0.9747 - classification_loss: 0.1834
 769/1500 [==============>...............] - ETA: 9:37 - loss: 1.1575 - regression_loss: 0.9742 - classification_loss: 0.1833
 770/1500 [==============>...............] - ETA: 9:36 - loss: 1.1588 - regression_loss: 0.9750 - classification_loss: 0.1838
 771/1500 [==============>...............] - ETA: 9:35 - loss: 1.1598 - regression_loss: 0.9758 - classification_loss: 0.1840
 772/1500 [==============>...............] - ETA: 9:34 - loss: 1.1592 - regression_loss: 0.9754 - classification_loss: 0.1839
 773/1500 [==============>...............] - ETA: 9:33 - loss: 1.1592 - regression_loss: 0.9754 - classification_loss: 0.1839
 774/1500 [==============>...............] - ETA: 9:32 - loss: 1.1583 - regression_loss: 0.9746 - classification_loss: 0.1837
 775/1500 [==============>...............] - ETA: 9:30 - loss: 1.1575 - regression_loss: 0.9740 - classification_loss: 0.1836
 776/1500 [==============>...............] - ETA: 9:30 - loss: 1.1570 - regression_loss: 0.9735 - classification_loss: 0.1835
 777/1500 [==============>...............] - ETA: 9:29 - loss: 1.1576 - regression_loss: 0.9740 - classification_loss: 0.1836
 778/1500 [==============>...............] - ETA: 9:28 - loss: 1.1591 - regression_loss: 0.9753 - classification_loss: 0.1837
 779/1500 [==============>...............] - ETA: 9:27 - loss: 1.1583 - regression_loss: 0.9746 - classification_loss: 0.1837
 780/1500 [==============>...............] - ETA: 9:26 - loss: 1.1581 - regression_loss: 0.9746 - classification_loss: 0.1835
 781/1500 [==============>...............] - ETA: 9:25 - loss: 1.1574 - regression_loss: 0.9740 - classification_loss: 0.1834
 782/1500 [==============>...............] - ETA: 9:24 - loss: 1.1578 - regression_loss: 0.9745 - classification_loss: 0.1833
 783/1500 [==============>...............] - ETA: 9:23 - loss: 1.1576 - regression_loss: 0.9743 - classification_loss: 0.1833
 784/1500 [==============>...............] - ETA: 9:23 - loss: 1.1572 - regression_loss: 0.9740 - classification_loss: 0.1832
 785/1500 [==============>...............] - ETA: 9:23 - loss: 1.1566 - regression_loss: 0.9735 - classification_loss: 0.1831
 786/1500 [==============>...............] - ETA: 9:21 - loss: 1.1559 - regression_loss: 0.9730 - classification_loss: 0.1830
 787/1500 [==============>...............] - ETA: 9:20 - loss: 1.1552 - regression_loss: 0.9724 - classification_loss: 0.1828
 788/1500 [==============>...............] - ETA: 9:20 - loss: 1.1546 - regression_loss: 0.9719 - classification_loss: 0.1827
 789/1500 [==============>...............] - ETA: 9:19 - loss: 1.1538 - regression_loss: 0.9713 - classification_loss: 0.1825
 790/1500 [==============>...............] - ETA: 9:18 - loss: 1.1541 - regression_loss: 0.9716 - classification_loss: 0.1825
 791/1500 [==============>...............] - ETA: 9:17 - loss: 1.1535 - regression_loss: 0.9711 - classification_loss: 0.1824
 792/1500 [==============>...............] - ETA: 9:16 - loss: 1.1544 - regression_loss: 0.9719 - classification_loss: 0.1825
 793/1500 [==============>...............] - ETA: 9:15 - loss: 1.1545 - regression_loss: 0.9721 - classification_loss: 0.1825
 794/1500 [==============>...............] - ETA: 9:14 - loss: 1.1536 - regression_loss: 0.9713 - classification_loss: 0.1823
 795/1500 [==============>...............] - ETA: 9:14 - loss: 1.1541 - regression_loss: 0.9716 - classification_loss: 0.1825
 796/1500 [==============>...............] - ETA: 9:14 - loss: 1.1533 - regression_loss: 0.9709 - classification_loss: 0.1823
 797/1500 [==============>...............] - ETA: 9:13 - loss: 1.1536 - regression_loss: 0.9714 - classification_loss: 0.1822
 798/1500 [==============>...............] - ETA: 9:12 - loss: 1.1530 - regression_loss: 0.9708 - classification_loss: 0.1821
 799/1500 [==============>...............] - ETA: 9:12 - loss: 1.1534 - regression_loss: 0.9713 - classification_loss: 0.1821
 800/1500 [===============>..............] - ETA: 9:11 - loss: 1.1526 - regression_loss: 0.9707 - classification_loss: 0.1819
 801/1500 [===============>..............] - ETA: 9:11 - loss: 1.1527 - regression_loss: 0.9709 - classification_loss: 0.1818
 802/1500 [===============>..............] - ETA: 9:10 - loss: 1.1520 - regression_loss: 0.9703 - classification_loss: 0.1817
 803/1500 [===============>..............] - ETA: 9:09 - loss: 1.1520 - regression_loss: 0.9704 - classification_loss: 0.1817
 804/1500 [===============>..............] - ETA: 9:08 - loss: 1.1532 - regression_loss: 0.9713 - classification_loss: 0.1818
 805/1500 [===============>..............] - ETA: 9:07 - loss: 1.1526 - regression_loss: 0.9708 - classification_loss: 0.1818
 806/1500 [===============>..............] - ETA: 9:06 - loss: 1.1521 - regression_loss: 0.9704 - classification_loss: 0.1817
 807/1500 [===============>..............] - ETA: 9:05 - loss: 1.1522 - regression_loss: 0.9705 - classification_loss: 0.1817
 808/1500 [===============>..............] - ETA: 9:04 - loss: 1.1521 - regression_loss: 0.9705 - classification_loss: 0.1816
 809/1500 [===============>..............] - ETA: 9:03 - loss: 1.1528 - regression_loss: 0.9712 - classification_loss: 0.1816
 810/1500 [===============>..............] - ETA: 9:04 - loss: 1.1534 - regression_loss: 0.9716 - classification_loss: 0.1817
 811/1500 [===============>..............] - ETA: 9:03 - loss: 1.1538 - regression_loss: 0.9720 - classification_loss: 0.1818
 812/1500 [===============>..............] - ETA: 9:02 - loss: 1.1529 - regression_loss: 0.9714 - classification_loss: 0.1816
 813/1500 [===============>..............] - ETA: 9:02 - loss: 1.1528 - regression_loss: 0.9713 - classification_loss: 0.1815
 814/1500 [===============>..............] - ETA: 9:01 - loss: 1.1539 - regression_loss: 0.9721 - classification_loss: 0.1819
 815/1500 [===============>..............] - ETA: 9:00 - loss: 1.1536 - regression_loss: 0.9720 - classification_loss: 0.1817
 816/1500 [===============>..............] - ETA: 8:59 - loss: 1.1531 - regression_loss: 0.9716 - classification_loss: 0.1815
 817/1500 [===============>..............] - ETA: 8:58 - loss: 1.1532 - regression_loss: 0.9716 - classification_loss: 0.1815
 818/1500 [===============>..............] - ETA: 8:58 - loss: 1.1540 - regression_loss: 0.9722 - classification_loss: 0.1818
 819/1500 [===============>..............] - ETA: 8:57 - loss: 1.1547 - regression_loss: 0.9727 - classification_loss: 0.1819
 820/1500 [===============>..............] - ETA: 8:56 - loss: 1.1542 - regression_loss: 0.9724 - classification_loss: 0.1818
 821/1500 [===============>..............] - ETA: 8:56 - loss: 1.1544 - regression_loss: 0.9726 - classification_loss: 0.1818
 822/1500 [===============>..............] - ETA: 8:55 - loss: 1.1535 - regression_loss: 0.9719 - classification_loss: 0.1816
 823/1500 [===============>..............] - ETA: 8:54 - loss: 1.1530 - regression_loss: 0.9715 - classification_loss: 0.1815
 824/1500 [===============>..............] - ETA: 8:53 - loss: 1.1528 - regression_loss: 0.9714 - classification_loss: 0.1814
 825/1500 [===============>..............] - ETA: 8:53 - loss: 1.1545 - regression_loss: 0.9729 - classification_loss: 0.1816
 826/1500 [===============>..............] - ETA: 8:53 - loss: 1.1541 - regression_loss: 0.9726 - classification_loss: 0.1815
 827/1500 [===============>..............] - ETA: 8:52 - loss: 1.1537 - regression_loss: 0.9723 - classification_loss: 0.1814
 828/1500 [===============>..............] - ETA: 8:51 - loss: 1.1531 - regression_loss: 0.9717 - classification_loss: 0.1814
 829/1500 [===============>..............] - ETA: 8:50 - loss: 1.1528 - regression_loss: 0.9713 - classification_loss: 0.1814
 830/1500 [===============>..............] - ETA: 8:49 - loss: 1.1532 - regression_loss: 0.9718 - classification_loss: 0.1814
 831/1500 [===============>..............] - ETA: 8:48 - loss: 1.1536 - regression_loss: 0.9720 - classification_loss: 0.1816
 832/1500 [===============>..............] - ETA: 8:47 - loss: 1.1534 - regression_loss: 0.9719 - classification_loss: 0.1815
 833/1500 [===============>..............] - ETA: 8:47 - loss: 1.1531 - regression_loss: 0.9716 - classification_loss: 0.1814
 834/1500 [===============>..............] - ETA: 8:46 - loss: 1.1534 - regression_loss: 0.9720 - classification_loss: 0.1814
 835/1500 [===============>..............] - ETA: 8:46 - loss: 1.1550 - regression_loss: 0.9733 - classification_loss: 0.1817
 836/1500 [===============>..............] - ETA: 8:44 - loss: 1.1547 - regression_loss: 0.9731 - classification_loss: 0.1817
 837/1500 [===============>..............] - ETA: 8:44 - loss: 1.1553 - regression_loss: 0.9735 - classification_loss: 0.1818
 838/1500 [===============>..............] - ETA: 8:43 - loss: 1.1567 - regression_loss: 0.9749 - classification_loss: 0.1818
 839/1500 [===============>..............] - ETA: 8:42 - loss: 1.1577 - regression_loss: 0.9757 - classification_loss: 0.1820
 840/1500 [===============>..............] - ETA: 8:40 - loss: 1.1569 - regression_loss: 0.9750 - classification_loss: 0.1819
 841/1500 [===============>..............] - ETA: 8:39 - loss: 1.1589 - regression_loss: 0.9765 - classification_loss: 0.1824
 842/1500 [===============>..............] - ETA: 8:39 - loss: 1.1587 - regression_loss: 0.9764 - classification_loss: 0.1823
 843/1500 [===============>..............] - ETA: 8:38 - loss: 1.1592 - regression_loss: 0.9770 - classification_loss: 0.1822
 844/1500 [===============>..............] - ETA: 8:37 - loss: 1.1595 - regression_loss: 0.9773 - classification_loss: 0.1822
 845/1500 [===============>..............] - ETA: 8:36 - loss: 1.1594 - regression_loss: 0.9772 - classification_loss: 0.1822
 846/1500 [===============>..............] - ETA: 8:35 - loss: 1.1586 - regression_loss: 0.9765 - classification_loss: 0.1821
 847/1500 [===============>..............] - ETA: 8:34 - loss: 1.1579 - regression_loss: 0.9760 - classification_loss: 0.1819
 848/1500 [===============>..............] - ETA: 8:33 - loss: 1.1574 - regression_loss: 0.9755 - classification_loss: 0.1819
 849/1500 [===============>..............] - ETA: 8:33 - loss: 1.1582 - regression_loss: 0.9760 - classification_loss: 0.1822
 850/1500 [================>.............] - ETA: 8:32 - loss: 1.1579 - regression_loss: 0.9758 - classification_loss: 0.1821
 851/1500 [================>.............] - ETA: 8:32 - loss: 1.1573 - regression_loss: 0.9754 - classification_loss: 0.1819
 852/1500 [================>.............] - ETA: 8:31 - loss: 1.1574 - regression_loss: 0.9755 - classification_loss: 0.1819
 853/1500 [================>.............] - ETA: 8:29 - loss: 1.1571 - regression_loss: 0.9753 - classification_loss: 0.1818
 854/1500 [================>.............] - ETA: 8:30 - loss: 1.1581 - regression_loss: 0.9761 - classification_loss: 0.1820
 855/1500 [================>.............] - ETA: 8:29 - loss: 1.1591 - regression_loss: 0.9770 - classification_loss: 0.1821
 856/1500 [================>.............] - ETA: 8:28 - loss: 1.1583 - regression_loss: 0.9764 - classification_loss: 0.1819
 857/1500 [================>.............] - ETA: 8:27 - loss: 1.1583 - regression_loss: 0.9764 - classification_loss: 0.1818
 858/1500 [================>.............] - ETA: 8:25 - loss: 1.1574 - regression_loss: 0.9757 - classification_loss: 0.1817
 859/1500 [================>.............] - ETA: 8:25 - loss: 1.1581 - regression_loss: 0.9764 - classification_loss: 0.1817
 860/1500 [================>.............] - ETA: 8:25 - loss: 1.1577 - regression_loss: 0.9761 - classification_loss: 0.1816
 861/1500 [================>.............] - ETA: 8:25 - loss: 1.1575 - regression_loss: 0.9761 - classification_loss: 0.1815
 862/1500 [================>.............] - ETA: 8:25 - loss: 1.1567 - regression_loss: 0.9754 - classification_loss: 0.1813
 863/1500 [================>.............] - ETA: 8:24 - loss: 1.1558 - regression_loss: 0.9746 - classification_loss: 0.1811
 864/1500 [================>.............] - ETA: 8:23 - loss: 1.1562 - regression_loss: 0.9751 - classification_loss: 0.1811
 865/1500 [================>.............] - ETA: 8:22 - loss: 1.1558 - regression_loss: 0.9748 - classification_loss: 0.1810
 866/1500 [================>.............] - ETA: 8:21 - loss: 1.1560 - regression_loss: 0.9750 - classification_loss: 0.1810
 867/1500 [================>.............] - ETA: 8:21 - loss: 1.1554 - regression_loss: 0.9745 - classification_loss: 0.1809
 868/1500 [================>.............] - ETA: 8:20 - loss: 1.1548 - regression_loss: 0.9740 - classification_loss: 0.1808
 869/1500 [================>.............] - ETA: 8:19 - loss: 1.1550 - regression_loss: 0.9743 - classification_loss: 0.1807
 870/1500 [================>.............] - ETA: 8:18 - loss: 1.1549 - regression_loss: 0.9743 - classification_loss: 0.1807
 871/1500 [================>.............] - ETA: 8:17 - loss: 1.1546 - regression_loss: 0.9740 - classification_loss: 0.1806
 872/1500 [================>.............] - ETA: 8:17 - loss: 1.1538 - regression_loss: 0.9734 - classification_loss: 0.1805
 873/1500 [================>.............] - ETA: 8:16 - loss: 1.1543 - regression_loss: 0.9738 - classification_loss: 0.1805
 874/1500 [================>.............] - ETA: 8:15 - loss: 1.1544 - regression_loss: 0.9741 - classification_loss: 0.1804
 875/1500 [================>.............] - ETA: 8:14 - loss: 1.1541 - regression_loss: 0.9738 - classification_loss: 0.1803
 876/1500 [================>.............] - ETA: 8:13 - loss: 1.1546 - regression_loss: 0.9743 - classification_loss: 0.1803
 877/1500 [================>.............] - ETA: 8:12 - loss: 1.1538 - regression_loss: 0.9737 - classification_loss: 0.1801
 878/1500 [================>.............] - ETA: 8:11 - loss: 1.1557 - regression_loss: 0.9750 - classification_loss: 0.1807
 879/1500 [================>.............] - ETA: 8:10 - loss: 1.1554 - regression_loss: 0.9747 - classification_loss: 0.1807
 880/1500 [================>.............] - ETA: 8:08 - loss: 1.1549 - regression_loss: 0.9743 - classification_loss: 0.1806
 881/1500 [================>.............] - ETA: 8:08 - loss: 1.1548 - regression_loss: 0.9743 - classification_loss: 0.1805
 882/1500 [================>.............] - ETA: 8:07 - loss: 1.1564 - regression_loss: 0.9754 - classification_loss: 0.1810
 883/1500 [================>.............] - ETA: 8:08 - loss: 1.1559 - regression_loss: 0.9750 - classification_loss: 0.1809
 884/1500 [================>.............] - ETA: 8:07 - loss: 1.1573 - regression_loss: 0.9762 - classification_loss: 0.1811
 885/1500 [================>.............] - ETA: 8:06 - loss: 1.1565 - regression_loss: 0.9755 - classification_loss: 0.1809
 886/1500 [================>.............] - ETA: 8:05 - loss: 1.1564 - regression_loss: 0.9755 - classification_loss: 0.1809
 887/1500 [================>.............] - ETA: 8:04 - loss: 1.1560 - regression_loss: 0.9752 - classification_loss: 0.1808
 888/1500 [================>.............] - ETA: 8:03 - loss: 1.1558 - regression_loss: 0.9751 - classification_loss: 0.1807
 889/1500 [================>.............] - ETA: 8:02 - loss: 1.1549 - regression_loss: 0.9744 - classification_loss: 0.1805
 890/1500 [================>.............] - ETA: 8:02 - loss: 1.1548 - regression_loss: 0.9744 - classification_loss: 0.1804
 891/1500 [================>.............] - ETA: 8:01 - loss: 1.1553 - regression_loss: 0.9749 - classification_loss: 0.1804
 892/1500 [================>.............] - ETA: 8:00 - loss: 1.1547 - regression_loss: 0.9743 - classification_loss: 0.1803
 893/1500 [================>.............] - ETA: 7:59 - loss: 1.1542 - regression_loss: 0.9741 - classification_loss: 0.1801
 894/1500 [================>.............] - ETA: 7:58 - loss: 1.1542 - regression_loss: 0.9741 - classification_loss: 0.1801
 895/1500 [================>.............] - ETA: 7:58 - loss: 1.1542 - regression_loss: 0.9741 - classification_loss: 0.1801
 896/1500 [================>.............] - ETA: 7:57 - loss: 1.1534 - regression_loss: 0.9735 - classification_loss: 0.1800
 897/1500 [================>.............] - ETA: 7:56 - loss: 1.1540 - regression_loss: 0.9740 - classification_loss: 0.1799
 898/1500 [================>.............] - ETA: 7:55 - loss: 1.1541 - regression_loss: 0.9741 - classification_loss: 0.1799
 899/1500 [================>.............] - ETA: 7:55 - loss: 1.1538 - regression_loss: 0.9740 - classification_loss: 0.1798
 900/1500 [=================>............] - ETA: 7:54 - loss: 1.1536 - regression_loss: 0.9738 - classification_loss: 0.1798
 901/1500 [=================>............] - ETA: 7:53 - loss: 1.1530 - regression_loss: 0.9734 - classification_loss: 0.1797
 902/1500 [=================>............] - ETA: 7:52 - loss: 1.1526 - regression_loss: 0.9730 - classification_loss: 0.1795
 903/1500 [=================>............] - ETA: 7:51 - loss: 1.1526 - regression_loss: 0.9728 - classification_loss: 0.1798
 904/1500 [=================>............] - ETA: 7:50 - loss: 1.1527 - regression_loss: 0.9730 - classification_loss: 0.1797
 905/1500 [=================>............] - ETA: 7:50 - loss: 1.1522 - regression_loss: 0.9726 - classification_loss: 0.1796
 906/1500 [=================>............] - ETA: 7:49 - loss: 1.1518 - regression_loss: 0.9724 - classification_loss: 0.1794
 907/1500 [=================>............] - ETA: 7:48 - loss: 1.1512 - regression_loss: 0.9718 - classification_loss: 0.1794
 908/1500 [=================>............] - ETA: 7:47 - loss: 1.1514 - regression_loss: 0.9721 - classification_loss: 0.1793
 909/1500 [=================>............] - ETA: 7:47 - loss: 1.1510 - regression_loss: 0.9717 - classification_loss: 0.1793
 910/1500 [=================>............] - ETA: 7:46 - loss: 1.1502 - regression_loss: 0.9711 - classification_loss: 0.1791
 911/1500 [=================>............] - ETA: 7:45 - loss: 1.1496 - regression_loss: 0.9706 - classification_loss: 0.1790
 912/1500 [=================>............] - ETA: 7:45 - loss: 1.1502 - regression_loss: 0.9711 - classification_loss: 0.1791
 913/1500 [=================>............] - ETA: 7:45 - loss: 1.1504 - regression_loss: 0.9712 - classification_loss: 0.1791
 914/1500 [=================>............] - ETA: 7:44 - loss: 1.1497 - regression_loss: 0.9707 - classification_loss: 0.1790
 915/1500 [=================>............] - ETA: 7:43 - loss: 1.1501 - regression_loss: 0.9711 - classification_loss: 0.1790
 916/1500 [=================>............] - ETA: 7:42 - loss: 1.1495 - regression_loss: 0.9707 - classification_loss: 0.1788
 917/1500 [=================>............] - ETA: 7:41 - loss: 1.1492 - regression_loss: 0.9705 - classification_loss: 0.1787
 918/1500 [=================>............] - ETA: 7:41 - loss: 1.1497 - regression_loss: 0.9709 - classification_loss: 0.1788
 919/1500 [=================>............] - ETA: 7:41 - loss: 1.1492 - regression_loss: 0.9705 - classification_loss: 0.1787
 920/1500 [=================>............] - ETA: 7:40 - loss: 1.1486 - regression_loss: 0.9700 - classification_loss: 0.1786
 921/1500 [=================>............] - ETA: 7:40 - loss: 1.1491 - regression_loss: 0.9703 - classification_loss: 0.1788
 922/1500 [=================>............] - ETA: 7:39 - loss: 1.1501 - regression_loss: 0.9712 - classification_loss: 0.1789
 923/1500 [=================>............] - ETA: 7:38 - loss: 1.1499 - regression_loss: 0.9710 - classification_loss: 0.1790
 924/1500 [=================>............] - ETA: 7:38 - loss: 1.1494 - regression_loss: 0.9706 - classification_loss: 0.1788
 925/1500 [=================>............] - ETA: 7:37 - loss: 1.1514 - regression_loss: 0.9720 - classification_loss: 0.1794
 926/1500 [=================>............] - ETA: 7:36 - loss: 1.1522 - regression_loss: 0.9729 - classification_loss: 0.1793
 927/1500 [=================>............] - ETA: 7:35 - loss: 1.1519 - regression_loss: 0.9727 - classification_loss: 0.1792
 928/1500 [=================>............] - ETA: 7:35 - loss: 1.1517 - regression_loss: 0.9726 - classification_loss: 0.1792
 929/1500 [=================>............] - ETA: 7:34 - loss: 1.1514 - regression_loss: 0.9724 - classification_loss: 0.1790
 930/1500 [=================>............] - ETA: 7:33 - loss: 1.1516 - regression_loss: 0.9724 - classification_loss: 0.1791
 931/1500 [=================>............] - ETA: 7:31 - loss: 1.1510 - regression_loss: 0.9720 - classification_loss: 0.1790
 932/1500 [=================>............] - ETA: 7:30 - loss: 1.1503 - regression_loss: 0.9715 - classification_loss: 0.1789
 933/1500 [=================>............] - ETA: 7:29 - loss: 1.1521 - regression_loss: 0.9726 - classification_loss: 0.1795
 934/1500 [=================>............] - ETA: 7:28 - loss: 1.1522 - regression_loss: 0.9728 - classification_loss: 0.1795
 935/1500 [=================>............] - ETA: 7:28 - loss: 1.1527 - regression_loss: 0.9733 - classification_loss: 0.1795
 936/1500 [=================>............] - ETA: 7:27 - loss: 1.1526 - regression_loss: 0.9732 - classification_loss: 0.1794
 937/1500 [=================>............] - ETA: 7:26 - loss: 1.1520 - regression_loss: 0.9727 - classification_loss: 0.1794
 938/1500 [=================>............] - ETA: 7:25 - loss: 1.1513 - regression_loss: 0.9721 - classification_loss: 0.1792
 939/1500 [=================>............] - ETA: 7:25 - loss: 1.1513 - regression_loss: 0.9722 - classification_loss: 0.1792
 940/1500 [=================>............] - ETA: 7:24 - loss: 1.1516 - regression_loss: 0.9724 - classification_loss: 0.1792
 941/1500 [=================>............] - ETA: 7:25 - loss: 1.1527 - regression_loss: 0.9734 - classification_loss: 0.1793
 942/1500 [=================>............] - ETA: 7:24 - loss: 1.1529 - regression_loss: 0.9736 - classification_loss: 0.1793
 943/1500 [=================>............] - ETA: 7:23 - loss: 1.1523 - regression_loss: 0.9732 - classification_loss: 0.1792
 944/1500 [=================>............] - ETA: 7:23 - loss: 1.1521 - regression_loss: 0.9730 - classification_loss: 0.1791
 945/1500 [=================>............] - ETA: 7:22 - loss: 1.1514 - regression_loss: 0.9724 - classification_loss: 0.1790
 946/1500 [=================>............] - ETA: 7:21 - loss: 1.1510 - regression_loss: 0.9721 - classification_loss: 0.1789
 947/1500 [=================>............] - ETA: 7:20 - loss: 1.1505 - regression_loss: 0.9717 - classification_loss: 0.1788
 948/1500 [=================>............] - ETA: 7:19 - loss: 1.1502 - regression_loss: 0.9715 - classification_loss: 0.1787
 949/1500 [=================>............] - ETA: 7:18 - loss: 1.1504 - regression_loss: 0.9718 - classification_loss: 0.1787
 950/1500 [==================>...........] - ETA: 7:17 - loss: 1.1502 - regression_loss: 0.9716 - classification_loss: 0.1786
 951/1500 [==================>...........] - ETA: 7:16 - loss: 1.1494 - regression_loss: 0.9709 - classification_loss: 0.1785
 952/1500 [==================>...........] - ETA: 7:15 - loss: 1.1491 - regression_loss: 0.9705 - classification_loss: 0.1786
 953/1500 [==================>...........] - ETA: 7:14 - loss: 1.1488 - regression_loss: 0.9703 - classification_loss: 0.1786
 954/1500 [==================>...........] - ETA: 7:14 - loss: 1.1489 - regression_loss: 0.9704 - classification_loss: 0.1785
 955/1500 [==================>...........] - ETA: 7:13 - loss: 1.1494 - regression_loss: 0.9707 - classification_loss: 0.1787
 956/1500 [==================>...........] - ETA: 7:12 - loss: 1.1498 - regression_loss: 0.9711 - classification_loss: 0.1787
 957/1500 [==================>...........] - ETA: 7:11 - loss: 1.1504 - regression_loss: 0.9717 - classification_loss: 0.1788
 958/1500 [==================>...........] - ETA: 7:10 - loss: 1.1497 - regression_loss: 0.9711 - classification_loss: 0.1786
 959/1500 [==================>...........] - ETA: 7:09 - loss: 1.1498 - regression_loss: 0.9712 - classification_loss: 0.1785
 960/1500 [==================>...........] - ETA: 7:09 - loss: 1.1490 - regression_loss: 0.9706 - classification_loss: 0.1784
 961/1500 [==================>...........] - ETA: 7:09 - loss: 1.1481 - regression_loss: 0.9699 - classification_loss: 0.1782
 962/1500 [==================>...........] - ETA: 7:08 - loss: 1.1477 - regression_loss: 0.9696 - classification_loss: 0.1781
 963/1500 [==================>...........] - ETA: 7:07 - loss: 1.1486 - regression_loss: 0.9702 - classification_loss: 0.1784
 964/1500 [==================>...........] - ETA: 7:06 - loss: 1.1486 - regression_loss: 0.9702 - classification_loss: 0.1784
 965/1500 [==================>...........] - ETA: 7:05 - loss: 1.1487 - regression_loss: 0.9703 - classification_loss: 0.1784
 966/1500 [==================>...........] - ETA: 7:04 - loss: 1.1486 - regression_loss: 0.9702 - classification_loss: 0.1784
 967/1500 [==================>...........] - ETA: 7:03 - loss: 1.1486 - regression_loss: 0.9700 - classification_loss: 0.1786
 968/1500 [==================>...........] - ETA: 7:02 - loss: 1.1491 - regression_loss: 0.9703 - classification_loss: 0.1788
 969/1500 [==================>...........] - ETA: 7:01 - loss: 1.1488 - regression_loss: 0.9700 - classification_loss: 0.1788
 970/1500 [==================>...........] - ETA: 7:00 - loss: 1.1486 - regression_loss: 0.9699 - classification_loss: 0.1787
 971/1500 [==================>...........] - ETA: 7:00 - loss: 1.1494 - regression_loss: 0.9706 - classification_loss: 0.1787
 972/1500 [==================>...........] - ETA: 6:59 - loss: 1.1486 - regression_loss: 0.9699 - classification_loss: 0.1786
 973/1500 [==================>...........] - ETA: 6:58 - loss: 1.1488 - regression_loss: 0.9702 - classification_loss: 0.1786
 974/1500 [==================>...........] - ETA: 6:57 - loss: 1.1483 - regression_loss: 0.9698 - classification_loss: 0.1785
 975/1500 [==================>...........] - ETA: 6:56 - loss: 1.1478 - regression_loss: 0.9695 - classification_loss: 0.1784
 976/1500 [==================>...........] - ETA: 6:55 - loss: 1.1476 - regression_loss: 0.9692 - classification_loss: 0.1784
 977/1500 [==================>...........] - ETA: 6:55 - loss: 1.1471 - regression_loss: 0.9688 - classification_loss: 0.1783
 978/1500 [==================>...........] - ETA: 6:54 - loss: 1.1470 - regression_loss: 0.9688 - classification_loss: 0.1782
 979/1500 [==================>...........] - ETA: 6:53 - loss: 1.1484 - regression_loss: 0.9699 - classification_loss: 0.1785
 980/1500 [==================>...........] - ETA: 6:53 - loss: 1.1493 - regression_loss: 0.9704 - classification_loss: 0.1789
 981/1500 [==================>...........] - ETA: 6:52 - loss: 1.1485 - regression_loss: 0.9698 - classification_loss: 0.1788
 982/1500 [==================>...........] - ETA: 6:51 - loss: 1.1487 - regression_loss: 0.9699 - classification_loss: 0.1787
 983/1500 [==================>...........] - ETA: 6:50 - loss: 1.1479 - regression_loss: 0.9693 - classification_loss: 0.1786
 984/1500 [==================>...........] - ETA: 6:49 - loss: 1.1483 - regression_loss: 0.9697 - classification_loss: 0.1786
 985/1500 [==================>...........] - ETA: 6:48 - loss: 1.1487 - regression_loss: 0.9701 - classification_loss: 0.1786
 986/1500 [==================>...........] - ETA: 6:47 - loss: 1.1495 - regression_loss: 0.9709 - classification_loss: 0.1786
 987/1500 [==================>...........] - ETA: 6:47 - loss: 1.1492 - regression_loss: 0.9706 - classification_loss: 0.1786
 988/1500 [==================>...........] - ETA: 6:46 - loss: 1.1492 - regression_loss: 0.9707 - classification_loss: 0.1785
 989/1500 [==================>...........] - ETA: 6:45 - loss: 1.1506 - regression_loss: 0.9717 - classification_loss: 0.1789
 990/1500 [==================>...........] - ETA: 6:44 - loss: 1.1503 - regression_loss: 0.9715 - classification_loss: 0.1788
 991/1500 [==================>...........] - ETA: 6:43 - loss: 1.1496 - regression_loss: 0.9709 - classification_loss: 0.1787
 992/1500 [==================>...........] - ETA: 6:42 - loss: 1.1494 - regression_loss: 0.9707 - classification_loss: 0.1786
 993/1500 [==================>...........] - ETA: 6:41 - loss: 1.1493 - regression_loss: 0.9707 - classification_loss: 0.1786
 994/1500 [==================>...........] - ETA: 6:40 - loss: 1.1489 - regression_loss: 0.9704 - classification_loss: 0.1785
 995/1500 [==================>...........] - ETA: 6:40 - loss: 1.1486 - regression_loss: 0.9702 - classification_loss: 0.1784
 996/1500 [==================>...........] - ETA: 6:39 - loss: 1.1488 - regression_loss: 0.9704 - classification_loss: 0.1784
 997/1500 [==================>...........] - ETA: 6:38 - loss: 1.1489 - regression_loss: 0.9706 - classification_loss: 0.1783
 998/1500 [==================>...........] - ETA: 6:38 - loss: 1.1483 - regression_loss: 0.9702 - classification_loss: 0.1782
 999/1500 [==================>...........] - ETA: 6:37 - loss: 1.1495 - regression_loss: 0.9711 - classification_loss: 0.1784
1000/1500 [===================>..........] - ETA: 6:36 - loss: 1.1489 - regression_loss: 0.9706 - classification_loss: 0.1783
1001/1500 [===================>..........] - ETA: 6:36 - loss: 1.1493 - regression_loss: 0.9709 - classification_loss: 0.1785
1002/1500 [===================>..........] - ETA: 6:35 - loss: 1.1489 - regression_loss: 0.9705 - classification_loss: 0.1783
1003/1500 [===================>..........] - ETA: 6:34 - loss: 1.1483 - regression_loss: 0.9700 - classification_loss: 0.1782
1004/1500 [===================>..........] - ETA: 6:33 - loss: 1.1477 - regression_loss: 0.9696 - classification_loss: 0.1781
1005/1500 [===================>..........] - ETA: 6:33 - loss: 1.1482 - regression_loss: 0.9699 - classification_loss: 0.1783
1006/1500 [===================>..........] - ETA: 6:32 - loss: 1.1480 - regression_loss: 0.9697 - classification_loss: 0.1783
1007/1500 [===================>..........] - ETA: 6:31 - loss: 1.1479 - regression_loss: 0.9696 - classification_loss: 0.1783
1008/1500 [===================>..........] - ETA: 6:30 - loss: 1.1471 - regression_loss: 0.9690 - classification_loss: 0.1782
1009/1500 [===================>..........] - ETA: 6:30 - loss: 1.1466 - regression_loss: 0.9686 - classification_loss: 0.1780
1010/1500 [===================>..........] - ETA: 6:29 - loss: 1.1475 - regression_loss: 0.9694 - classification_loss: 0.1781
1011/1500 [===================>..........] - ETA: 6:28 - loss: 1.1468 - regression_loss: 0.9688 - classification_loss: 0.1780
1012/1500 [===================>..........] - ETA: 6:27 - loss: 1.1463 - regression_loss: 0.9683 - classification_loss: 0.1780
1013/1500 [===================>..........] - ETA: 6:26 - loss: 1.1455 - regression_loss: 0.9676 - classification_loss: 0.1779
1014/1500 [===================>..........] - ETA: 6:25 - loss: 1.1461 - regression_loss: 0.9681 - classification_loss: 0.1780
1015/1500 [===================>..........] - ETA: 6:24 - loss: 1.1463 - regression_loss: 0.9682 - classification_loss: 0.1781
1016/1500 [===================>..........] - ETA: 6:24 - loss: 1.1460 - regression_loss: 0.9680 - classification_loss: 0.1780
1017/1500 [===================>..........] - ETA: 6:24 - loss: 1.1465 - regression_loss: 0.9680 - classification_loss: 0.1785
1018/1500 [===================>..........] - ETA: 6:23 - loss: 1.1465 - regression_loss: 0.9680 - classification_loss: 0.1785
1019/1500 [===================>..........] - ETA: 6:22 - loss: 1.1462 - regression_loss: 0.9678 - classification_loss: 0.1784
1020/1500 [===================>..........] - ETA: 6:21 - loss: 1.1454 - regression_loss: 0.9672 - classification_loss: 0.1782
1021/1500 [===================>..........] - ETA: 6:21 - loss: 1.1454 - regression_loss: 0.9672 - classification_loss: 0.1782
1022/1500 [===================>..........] - ETA: 6:20 - loss: 1.1451 - regression_loss: 0.9671 - classification_loss: 0.1781
1023/1500 [===================>..........] - ETA: 6:20 - loss: 1.1445 - regression_loss: 0.9666 - classification_loss: 0.1779
1024/1500 [===================>..........] - ETA: 6:19 - loss: 1.1449 - regression_loss: 0.9669 - classification_loss: 0.1780
1025/1500 [===================>..........] - ETA: 6:18 - loss: 1.1443 - regression_loss: 0.9664 - classification_loss: 0.1779
1026/1500 [===================>..........] - ETA: 6:17 - loss: 1.1448 - regression_loss: 0.9667 - classification_loss: 0.1781
1027/1500 [===================>..........] - ETA: 6:16 - loss: 1.1447 - regression_loss: 0.9667 - classification_loss: 0.1780
1028/1500 [===================>..........] - ETA: 6:15 - loss: 1.1446 - regression_loss: 0.9666 - classification_loss: 0.1780
1029/1500 [===================>..........] - ETA: 6:14 - loss: 1.1441 - regression_loss: 0.9661 - classification_loss: 0.1779
1030/1500 [===================>..........] - ETA: 6:13 - loss: 1.1433 - regression_loss: 0.9655 - classification_loss: 0.1778
1031/1500 [===================>..........] - ETA: 6:12 - loss: 1.1433 - regression_loss: 0.9655 - classification_loss: 0.1778
1032/1500 [===================>..........] - ETA: 6:12 - loss: 1.1431 - regression_loss: 0.9653 - classification_loss: 0.1778
1033/1500 [===================>..........] - ETA: 6:11 - loss: 1.1432 - regression_loss: 0.9654 - classification_loss: 0.1778
1034/1500 [===================>..........] - ETA: 6:10 - loss: 1.1431 - regression_loss: 0.9653 - classification_loss: 0.1778
1035/1500 [===================>..........] - ETA: 6:09 - loss: 1.1426 - regression_loss: 0.9649 - classification_loss: 0.1777
1036/1500 [===================>..........] - ETA: 6:09 - loss: 1.1431 - regression_loss: 0.9653 - classification_loss: 0.1778
1037/1500 [===================>..........] - ETA: 6:08 - loss: 1.1430 - regression_loss: 0.9653 - classification_loss: 0.1777
1038/1500 [===================>..........] - ETA: 6:07 - loss: 1.1431 - regression_loss: 0.9654 - classification_loss: 0.1777
1039/1500 [===================>..........] - ETA: 6:06 - loss: 1.1427 - regression_loss: 0.9651 - classification_loss: 0.1776
1040/1500 [===================>..........] - ETA: 6:05 - loss: 1.1427 - regression_loss: 0.9652 - classification_loss: 0.1775
1041/1500 [===================>..........] - ETA: 6:04 - loss: 1.1428 - regression_loss: 0.9652 - classification_loss: 0.1775
1042/1500 [===================>..........] - ETA: 6:04 - loss: 1.1422 - regression_loss: 0.9647 - classification_loss: 0.1775
1043/1500 [===================>..........] - ETA: 6:03 - loss: 1.1425 - regression_loss: 0.9649 - classification_loss: 0.1775
1044/1500 [===================>..........] - ETA: 6:02 - loss: 1.1423 - regression_loss: 0.9648 - classification_loss: 0.1775
1045/1500 [===================>..........] - ETA: 6:01 - loss: 1.1428 - regression_loss: 0.9653 - classification_loss: 0.1775
1046/1500 [===================>..........] - ETA: 6:00 - loss: 1.1426 - regression_loss: 0.9651 - classification_loss: 0.1775
1047/1500 [===================>..........] - ETA: 6:00 - loss: 1.1418 - regression_loss: 0.9644 - classification_loss: 0.1774
1048/1500 [===================>..........] - ETA: 5:59 - loss: 1.1420 - regression_loss: 0.9645 - classification_loss: 0.1775
1049/1500 [===================>..........] - ETA: 5:58 - loss: 1.1415 - regression_loss: 0.9642 - classification_loss: 0.1773
1050/1500 [====================>.........] - ETA: 5:57 - loss: 1.1420 - regression_loss: 0.9645 - classification_loss: 0.1775
1051/1500 [====================>.........] - ETA: 5:56 - loss: 1.1423 - regression_loss: 0.9649 - classification_loss: 0.1775
1052/1500 [====================>.........] - ETA: 5:56 - loss: 1.1417 - regression_loss: 0.9643 - classification_loss: 0.1773
1053/1500 [====================>.........] - ETA: 5:55 - loss: 1.1413 - regression_loss: 0.9640 - classification_loss: 0.1773
1054/1500 [====================>.........] - ETA: 5:54 - loss: 1.1407 - regression_loss: 0.9635 - classification_loss: 0.1772
1055/1500 [====================>.........] - ETA: 5:53 - loss: 1.1414 - regression_loss: 0.9641 - classification_loss: 0.1773
1056/1500 [====================>.........] - ETA: 5:52 - loss: 1.1411 - regression_loss: 0.9638 - classification_loss: 0.1773
1057/1500 [====================>.........] - ETA: 5:51 - loss: 1.1413 - regression_loss: 0.9640 - classification_loss: 0.1773
1058/1500 [====================>.........] - ETA: 5:50 - loss: 1.1411 - regression_loss: 0.9638 - classification_loss: 0.1773
1059/1500 [====================>.........] - ETA: 5:50 - loss: 1.1413 - regression_loss: 0.9640 - classification_loss: 0.1773
1060/1500 [====================>.........] - ETA: 5:49 - loss: 1.1410 - regression_loss: 0.9638 - classification_loss: 0.1772
1061/1500 [====================>.........] - ETA: 5:48 - loss: 1.1423 - regression_loss: 0.9646 - classification_loss: 0.1776
1062/1500 [====================>.........] - ETA: 5:47 - loss: 1.1416 - regression_loss: 0.9640 - classification_loss: 0.1775
1063/1500 [====================>.........] - ETA: 5:46 - loss: 1.1411 - regression_loss: 0.9637 - classification_loss: 0.1774
1064/1500 [====================>.........] - ETA: 5:45 - loss: 1.1418 - regression_loss: 0.9643 - classification_loss: 0.1774
1065/1500 [====================>.........] - ETA: 5:45 - loss: 1.1418 - regression_loss: 0.9643 - classification_loss: 0.1775
1066/1500 [====================>.........] - ETA: 5:44 - loss: 1.1422 - regression_loss: 0.9646 - classification_loss: 0.1776
1067/1500 [====================>.........] - ETA: 5:43 - loss: 1.1415 - regression_loss: 0.9640 - classification_loss: 0.1775
1068/1500 [====================>.........] - ETA: 5:42 - loss: 1.1413 - regression_loss: 0.9639 - classification_loss: 0.1774
1069/1500 [====================>.........] - ETA: 5:42 - loss: 1.1407 - regression_loss: 0.9634 - classification_loss: 0.1773
1070/1500 [====================>.........] - ETA: 5:42 - loss: 1.1404 - regression_loss: 0.9632 - classification_loss: 0.1772
1071/1500 [====================>.........] - ETA: 5:41 - loss: 1.1397 - regression_loss: 0.9626 - classification_loss: 0.1771
1072/1500 [====================>.........] - ETA: 5:40 - loss: 1.1393 - regression_loss: 0.9622 - classification_loss: 0.1771
1073/1500 [====================>.........] - ETA: 5:39 - loss: 1.1388 - regression_loss: 0.9618 - classification_loss: 0.1770
1074/1500 [====================>.........] - ETA: 5:38 - loss: 1.1387 - regression_loss: 0.9618 - classification_loss: 0.1769
1075/1500 [====================>.........] - ETA: 5:37 - loss: 1.1387 - regression_loss: 0.9618 - classification_loss: 0.1769
1076/1500 [====================>.........] - ETA: 5:36 - loss: 1.1395 - regression_loss: 0.9621 - classification_loss: 0.1774
1077/1500 [====================>.........] - ETA: 5:36 - loss: 1.1401 - regression_loss: 0.9627 - classification_loss: 0.1774
1078/1500 [====================>.........] - ETA: 5:35 - loss: 1.1396 - regression_loss: 0.9623 - classification_loss: 0.1773
1079/1500 [====================>.........] - ETA: 5:34 - loss: 1.1390 - regression_loss: 0.9618 - classification_loss: 0.1772
1080/1500 [====================>.........] - ETA: 5:33 - loss: 1.1392 - regression_loss: 0.9619 - classification_loss: 0.1773
1081/1500 [====================>.........] - ETA: 5:32 - loss: 1.1406 - regression_loss: 0.9631 - classification_loss: 0.1775
1082/1500 [====================>.........] - ETA: 5:31 - loss: 1.1399 - regression_loss: 0.9626 - classification_loss: 0.1773
1083/1500 [====================>.........] - ETA: 5:31 - loss: 1.1401 - regression_loss: 0.9627 - classification_loss: 0.1774
1084/1500 [====================>.........] - ETA: 5:30 - loss: 1.1395 - regression_loss: 0.9623 - classification_loss: 0.1772
1085/1500 [====================>.........] - ETA: 5:29 - loss: 1.1388 - regression_loss: 0.9617 - classification_loss: 0.1771
1086/1500 [====================>.........] - ETA: 5:28 - loss: 1.1391 - regression_loss: 0.9621 - classification_loss: 0.1771
1087/1500 [====================>.........] - ETA: 5:27 - loss: 1.1386 - regression_loss: 0.9616 - classification_loss: 0.1770
1088/1500 [====================>.........] - ETA: 5:26 - loss: 1.1386 - regression_loss: 0.9616 - classification_loss: 0.1769
1089/1500 [====================>.........] - ETA: 5:26 - loss: 1.1394 - regression_loss: 0.9624 - classification_loss: 0.1770
1090/1500 [====================>.........] - ETA: 5:25 - loss: 1.1391 - regression_loss: 0.9622 - classification_loss: 0.1769
1091/1500 [====================>.........] - ETA: 5:24 - loss: 1.1387 - regression_loss: 0.9619 - classification_loss: 0.1768
1092/1500 [====================>.........] - ETA: 5:23 - loss: 1.1382 - regression_loss: 0.9614 - classification_loss: 0.1768
1093/1500 [====================>.........] - ETA: 5:22 - loss: 1.1377 - regression_loss: 0.9610 - classification_loss: 0.1766
1094/1500 [====================>.........] - ETA: 5:22 - loss: 1.1385 - regression_loss: 0.9617 - classification_loss: 0.1768
1095/1500 [====================>.........] - ETA: 5:21 - loss: 1.1388 - regression_loss: 0.9619 - classification_loss: 0.1769
1096/1500 [====================>.........] - ETA: 5:20 - loss: 1.1385 - regression_loss: 0.9617 - classification_loss: 0.1768
1097/1500 [====================>.........] - ETA: 5:19 - loss: 1.1383 - regression_loss: 0.9615 - classification_loss: 0.1767
1098/1500 [====================>.........] - ETA: 5:19 - loss: 1.1383 - regression_loss: 0.9615 - classification_loss: 0.1768
1099/1500 [====================>.........] - ETA: 5:18 - loss: 1.1384 - regression_loss: 0.9616 - classification_loss: 0.1768
1100/1500 [=====================>........] - ETA: 5:17 - loss: 1.1383 - regression_loss: 0.9615 - classification_loss: 0.1767
1101/1500 [=====================>........] - ETA: 5:16 - loss: 1.1380 - regression_loss: 0.9613 - classification_loss: 0.1767
1102/1500 [=====================>........] - ETA: 5:15 - loss: 1.1383 - regression_loss: 0.9616 - classification_loss: 0.1766
1103/1500 [=====================>........] - ETA: 5:14 - loss: 1.1377 - regression_loss: 0.9611 - classification_loss: 0.1765
1104/1500 [=====================>........] - ETA: 5:14 - loss: 1.1386 - regression_loss: 0.9618 - classification_loss: 0.1768
1105/1500 [=====================>........] - ETA: 5:13 - loss: 1.1394 - regression_loss: 0.9625 - classification_loss: 0.1768
1106/1500 [=====================>........] - ETA: 5:12 - loss: 1.1406 - regression_loss: 0.9635 - classification_loss: 0.1771
1107/1500 [=====================>........] - ETA: 5:11 - loss: 1.1400 - regression_loss: 0.9630 - classification_loss: 0.1770
1108/1500 [=====================>........] - ETA: 5:10 - loss: 1.1393 - regression_loss: 0.9625 - classification_loss: 0.1768
1109/1500 [=====================>........] - ETA: 5:09 - loss: 1.1397 - regression_loss: 0.9629 - classification_loss: 0.1768
1110/1500 [=====================>........] - ETA: 5:09 - loss: 1.1393 - regression_loss: 0.9626 - classification_loss: 0.1767
1111/1500 [=====================>........] - ETA: 5:08 - loss: 1.1393 - regression_loss: 0.9625 - classification_loss: 0.1768
1112/1500 [=====================>........] - ETA: 5:07 - loss: 1.1399 - regression_loss: 0.9630 - classification_loss: 0.1769
1113/1500 [=====================>........] - ETA: 5:07 - loss: 1.1402 - regression_loss: 0.9631 - classification_loss: 0.1770
1114/1500 [=====================>........] - ETA: 5:06 - loss: 1.1404 - regression_loss: 0.9634 - classification_loss: 0.1770
1115/1500 [=====================>........] - ETA: 5:05 - loss: 1.1415 - regression_loss: 0.9642 - classification_loss: 0.1773
1116/1500 [=====================>........] - ETA: 5:04 - loss: 1.1421 - regression_loss: 0.9647 - classification_loss: 0.1774
1117/1500 [=====================>........] - ETA: 5:03 - loss: 1.1421 - regression_loss: 0.9647 - classification_loss: 0.1774
1118/1500 [=====================>........] - ETA: 5:02 - loss: 1.1419 - regression_loss: 0.9646 - classification_loss: 0.1774
1119/1500 [=====================>........] - ETA: 5:02 - loss: 1.1417 - regression_loss: 0.9644 - classification_loss: 0.1773
1120/1500 [=====================>........] - ETA: 5:01 - loss: 1.1416 - regression_loss: 0.9643 - classification_loss: 0.1772
1121/1500 [=====================>........] - ETA: 5:00 - loss: 1.1413 - regression_loss: 0.9641 - classification_loss: 0.1772
1122/1500 [=====================>........] - ETA: 4:59 - loss: 1.1411 - regression_loss: 0.9640 - classification_loss: 0.1772
1123/1500 [=====================>........] - ETA: 4:58 - loss: 1.1408 - regression_loss: 0.9637 - classification_loss: 0.1771
1124/1500 [=====================>........] - ETA: 4:57 - loss: 1.1415 - regression_loss: 0.9644 - classification_loss: 0.1772
1125/1500 [=====================>........] - ETA: 4:56 - loss: 1.1414 - regression_loss: 0.9643 - classification_loss: 0.1771
1126/1500 [=====================>........] - ETA: 4:56 - loss: 1.1409 - regression_loss: 0.9640 - classification_loss: 0.1770
1127/1500 [=====================>........] - ETA: 4:55 - loss: 1.1412 - regression_loss: 0.9642 - classification_loss: 0.1770
1128/1500 [=====================>........] - ETA: 4:54 - loss: 1.1411 - regression_loss: 0.9642 - classification_loss: 0.1769
1129/1500 [=====================>........] - ETA: 4:53 - loss: 1.1411 - regression_loss: 0.9642 - classification_loss: 0.1769
1130/1500 [=====================>........] - ETA: 4:52 - loss: 1.1404 - regression_loss: 0.9636 - classification_loss: 0.1768
1131/1500 [=====================>........] - ETA: 4:51 - loss: 1.1412 - regression_loss: 0.9642 - classification_loss: 0.1770
1132/1500 [=====================>........] - ETA: 4:51 - loss: 1.1407 - regression_loss: 0.9638 - classification_loss: 0.1769
1133/1500 [=====================>........] - ETA: 4:50 - loss: 1.1406 - regression_loss: 0.9637 - classification_loss: 0.1768
1134/1500 [=====================>........] - ETA: 4:49 - loss: 1.1403 - regression_loss: 0.9634 - classification_loss: 0.1768
1135/1500 [=====================>........] - ETA: 4:48 - loss: 1.1401 - regression_loss: 0.9633 - classification_loss: 0.1768
1136/1500 [=====================>........] - ETA: 4:47 - loss: 1.1406 - regression_loss: 0.9638 - classification_loss: 0.1768
1137/1500 [=====================>........] - ETA: 4:46 - loss: 1.1403 - regression_loss: 0.9636 - classification_loss: 0.1767
1138/1500 [=====================>........] - ETA: 4:45 - loss: 1.1402 - regression_loss: 0.9635 - classification_loss: 0.1766
1139/1500 [=====================>........] - ETA: 4:45 - loss: 1.1397 - regression_loss: 0.9631 - classification_loss: 0.1766
1140/1500 [=====================>........] - ETA: 4:44 - loss: 1.1401 - regression_loss: 0.9634 - classification_loss: 0.1767
1141/1500 [=====================>........] - ETA: 4:43 - loss: 1.1396 - regression_loss: 0.9630 - classification_loss: 0.1766
1142/1500 [=====================>........] - ETA: 4:42 - loss: 1.1393 - regression_loss: 0.9628 - classification_loss: 0.1765
1143/1500 [=====================>........] - ETA: 4:41 - loss: 1.1395 - regression_loss: 0.9630 - classification_loss: 0.1764
1144/1500 [=====================>........] - ETA: 4:41 - loss: 1.1392 - regression_loss: 0.9629 - classification_loss: 0.1763
1145/1500 [=====================>........] - ETA: 4:40 - loss: 1.1392 - regression_loss: 0.9629 - classification_loss: 0.1763
1146/1500 [=====================>........] - ETA: 4:39 - loss: 1.1389 - regression_loss: 0.9627 - classification_loss: 0.1762
1147/1500 [=====================>........] - ETA: 4:38 - loss: 1.1397 - regression_loss: 0.9633 - classification_loss: 0.1764
1148/1500 [=====================>........] - ETA: 4:37 - loss: 1.1393 - regression_loss: 0.9630 - classification_loss: 0.1763
1149/1500 [=====================>........] - ETA: 4:36 - loss: 1.1388 - regression_loss: 0.9626 - classification_loss: 0.1762
1150/1500 [======================>.......] - ETA: 4:35 - loss: 1.1391 - regression_loss: 0.9629 - classification_loss: 0.1762
1151/1500 [======================>.......] - ETA: 4:35 - loss: 1.1385 - regression_loss: 0.9624 - classification_loss: 0.1762
1152/1500 [======================>.......] - ETA: 4:34 - loss: 1.1380 - regression_loss: 0.9619 - classification_loss: 0.1760
1153/1500 [======================>.......] - ETA: 4:33 - loss: 1.1374 - regression_loss: 0.9615 - classification_loss: 0.1759
1154/1500 [======================>.......] - ETA: 4:32 - loss: 1.1372 - regression_loss: 0.9614 - classification_loss: 0.1758
1155/1500 [======================>.......] - ETA: 4:31 - loss: 1.1372 - regression_loss: 0.9615 - classification_loss: 0.1758
1156/1500 [======================>.......] - ETA: 4:30 - loss: 1.1379 - regression_loss: 0.9621 - classification_loss: 0.1758
1157/1500 [======================>.......] - ETA: 4:29 - loss: 1.1374 - regression_loss: 0.9617 - classification_loss: 0.1757
1158/1500 [======================>.......] - ETA: 4:28 - loss: 1.1375 - regression_loss: 0.9617 - classification_loss: 0.1758
1159/1500 [======================>.......] - ETA: 4:27 - loss: 1.1369 - regression_loss: 0.9613 - classification_loss: 0.1757
1160/1500 [======================>.......] - ETA: 4:27 - loss: 1.1363 - regression_loss: 0.9607 - classification_loss: 0.1756
1161/1500 [======================>.......] - ETA: 4:26 - loss: 1.1361 - regression_loss: 0.9605 - classification_loss: 0.1755
1162/1500 [======================>.......] - ETA: 4:25 - loss: 1.1360 - regression_loss: 0.9605 - classification_loss: 0.1755
1163/1500 [======================>.......] - ETA: 4:24 - loss: 1.1363 - regression_loss: 0.9607 - classification_loss: 0.1756
1164/1500 [======================>.......] - ETA: 4:23 - loss: 1.1360 - regression_loss: 0.9605 - classification_loss: 0.1755
1165/1500 [======================>.......] - ETA: 4:22 - loss: 1.1357 - regression_loss: 0.9603 - classification_loss: 0.1754
1166/1500 [======================>.......] - ETA: 4:21 - loss: 1.1361 - regression_loss: 0.9605 - classification_loss: 0.1755
1167/1500 [======================>.......] - ETA: 4:21 - loss: 1.1356 - regression_loss: 0.9602 - classification_loss: 0.1754
1168/1500 [======================>.......] - ETA: 4:20 - loss: 1.1356 - regression_loss: 0.9602 - classification_loss: 0.1754
1169/1500 [======================>.......] - ETA: 4:19 - loss: 1.1353 - regression_loss: 0.9600 - classification_loss: 0.1753
1170/1500 [======================>.......] - ETA: 4:18 - loss: 1.1351 - regression_loss: 0.9597 - classification_loss: 0.1754
1171/1500 [======================>.......] - ETA: 4:18 - loss: 1.1349 - regression_loss: 0.9595 - classification_loss: 0.1754
1172/1500 [======================>.......] - ETA: 4:17 - loss: 1.1354 - regression_loss: 0.9599 - classification_loss: 0.1756
1173/1500 [======================>.......] - ETA: 4:16 - loss: 1.1349 - regression_loss: 0.9595 - classification_loss: 0.1755
1174/1500 [======================>.......] - ETA: 4:15 - loss: 1.1343 - regression_loss: 0.9589 - classification_loss: 0.1754
1175/1500 [======================>.......] - ETA: 4:14 - loss: 1.1336 - regression_loss: 0.9584 - classification_loss: 0.1752
1176/1500 [======================>.......] - ETA: 4:14 - loss: 1.1340 - regression_loss: 0.9587 - classification_loss: 0.1753
1177/1500 [======================>.......] - ETA: 4:13 - loss: 1.1338 - regression_loss: 0.9586 - classification_loss: 0.1752
1178/1500 [======================>.......] - ETA: 4:12 - loss: 1.1334 - regression_loss: 0.9582 - classification_loss: 0.1752
1179/1500 [======================>.......] - ETA: 4:11 - loss: 1.1329 - regression_loss: 0.9578 - classification_loss: 0.1751
1180/1500 [======================>.......] - ETA: 4:10 - loss: 1.1324 - regression_loss: 0.9573 - classification_loss: 0.1751
1181/1500 [======================>.......] - ETA: 4:09 - loss: 1.1324 - regression_loss: 0.9574 - classification_loss: 0.1750
1182/1500 [======================>.......] - ETA: 4:09 - loss: 1.1320 - regression_loss: 0.9571 - classification_loss: 0.1750
1183/1500 [======================>.......] - ETA: 4:08 - loss: 1.1318 - regression_loss: 0.9569 - classification_loss: 0.1749
1184/1500 [======================>.......] - ETA: 4:07 - loss: 1.1318 - regression_loss: 0.9569 - classification_loss: 0.1749
1185/1500 [======================>.......] - ETA: 4:06 - loss: 1.1314 - regression_loss: 0.9565 - classification_loss: 0.1748
1186/1500 [======================>.......] - ETA: 4:06 - loss: 1.1313 - regression_loss: 0.9565 - classification_loss: 0.1748
1187/1500 [======================>.......] - ETA: 4:05 - loss: 1.1308 - regression_loss: 0.9562 - classification_loss: 0.1747
1188/1500 [======================>.......] - ETA: 4:04 - loss: 1.1303 - regression_loss: 0.9558 - classification_loss: 0.1745
1189/1500 [======================>.......] - ETA: 4:03 - loss: 1.1300 - regression_loss: 0.9555 - classification_loss: 0.1745
1190/1500 [======================>.......] - ETA: 4:03 - loss: 1.1299 - regression_loss: 0.9555 - classification_loss: 0.1744
1191/1500 [======================>.......] - ETA: 4:02 - loss: 1.1295 - regression_loss: 0.9552 - classification_loss: 0.1743
1192/1500 [======================>.......] - ETA: 4:02 - loss: 1.1297 - regression_loss: 0.9554 - classification_loss: 0.1743
1193/1500 [======================>.......] - ETA: 4:01 - loss: 1.1291 - regression_loss: 0.9549 - classification_loss: 0.1742
1194/1500 [======================>.......] - ETA: 4:00 - loss: 1.1298 - regression_loss: 0.9555 - classification_loss: 0.1744
1195/1500 [======================>.......] - ETA: 3:59 - loss: 1.1308 - regression_loss: 0.9558 - classification_loss: 0.1750
1196/1500 [======================>.......] - ETA: 3:58 - loss: 1.1306 - regression_loss: 0.9557 - classification_loss: 0.1749
1197/1500 [======================>.......] - ETA: 3:58 - loss: 1.1306 - regression_loss: 0.9557 - classification_loss: 0.1749
1198/1500 [======================>.......] - ETA: 3:57 - loss: 1.1303 - regression_loss: 0.9555 - classification_loss: 0.1749
1199/1500 [======================>.......] - ETA: 3:56 - loss: 1.1297 - regression_loss: 0.9549 - classification_loss: 0.1748
1200/1500 [=======================>......] - ETA: 3:55 - loss: 1.1293 - regression_loss: 0.9546 - classification_loss: 0.1747
1201/1500 [=======================>......] - ETA: 3:54 - loss: 1.1288 - regression_loss: 0.9542 - classification_loss: 0.1746
1202/1500 [=======================>......] - ETA: 3:54 - loss: 1.1288 - regression_loss: 0.9543 - classification_loss: 0.1745
1203/1500 [=======================>......] - ETA: 3:53 - loss: 1.1283 - regression_loss: 0.9538 - classification_loss: 0.1744
1204/1500 [=======================>......] - ETA: 3:52 - loss: 1.1291 - regression_loss: 0.9545 - classification_loss: 0.1747
1205/1500 [=======================>......] - ETA: 3:51 - loss: 1.1288 - regression_loss: 0.9542 - classification_loss: 0.1746
1206/1500 [=======================>......] - ETA: 3:50 - loss: 1.1282 - regression_loss: 0.9538 - classification_loss: 0.1745
1207/1500 [=======================>......] - ETA: 3:49 - loss: 1.1287 - regression_loss: 0.9542 - classification_loss: 0.1746
1208/1500 [=======================>......] - ETA: 3:49 - loss: 1.1287 - regression_loss: 0.9541 - classification_loss: 0.1746
1209/1500 [=======================>......] - ETA: 3:48 - loss: 1.1286 - regression_loss: 0.9541 - classification_loss: 0.1745
1210/1500 [=======================>......] - ETA: 3:47 - loss: 1.1285 - regression_loss: 0.9541 - classification_loss: 0.1744
1211/1500 [=======================>......] - ETA: 3:46 - loss: 1.1286 - regression_loss: 0.9542 - classification_loss: 0.1744
1212/1500 [=======================>......] - ETA: 3:45 - loss: 1.1286 - regression_loss: 0.9542 - classification_loss: 0.1744
1213/1500 [=======================>......] - ETA: 3:45 - loss: 1.1281 - regression_loss: 0.9538 - classification_loss: 0.1743
1214/1500 [=======================>......] - ETA: 3:44 - loss: 1.1281 - regression_loss: 0.9538 - classification_loss: 0.1742
1215/1500 [=======================>......] - ETA: 3:43 - loss: 1.1280 - regression_loss: 0.9537 - classification_loss: 0.1742
1216/1500 [=======================>......] - ETA: 3:42 - loss: 1.1275 - regression_loss: 0.9534 - classification_loss: 0.1741
1217/1500 [=======================>......] - ETA: 3:41 - loss: 1.1272 - regression_loss: 0.9532 - classification_loss: 0.1741
1218/1500 [=======================>......] - ETA: 3:41 - loss: 1.1267 - regression_loss: 0.9527 - classification_loss: 0.1740
1219/1500 [=======================>......] - ETA: 3:40 - loss: 1.1266 - regression_loss: 0.9525 - classification_loss: 0.1741
1220/1500 [=======================>......] - ETA: 3:39 - loss: 1.1265 - regression_loss: 0.9524 - classification_loss: 0.1741
1221/1500 [=======================>......] - ETA: 3:38 - loss: 1.1273 - regression_loss: 0.9531 - classification_loss: 0.1742
1222/1500 [=======================>......] - ETA: 3:37 - loss: 1.1278 - regression_loss: 0.9528 - classification_loss: 0.1750
1223/1500 [=======================>......] - ETA: 3:36 - loss: 1.1278 - regression_loss: 0.9527 - classification_loss: 0.1750
1224/1500 [=======================>......] - ETA: 3:36 - loss: 1.1283 - regression_loss: 0.9532 - classification_loss: 0.1751
1225/1500 [=======================>......] - ETA: 3:35 - loss: 1.1289 - regression_loss: 0.9536 - classification_loss: 0.1753
1226/1500 [=======================>......] - ETA: 3:35 - loss: 1.1290 - regression_loss: 0.9537 - classification_loss: 0.1753
1227/1500 [=======================>......] - ETA: 3:34 - loss: 1.1287 - regression_loss: 0.9535 - classification_loss: 0.1752
1228/1500 [=======================>......] - ETA: 3:33 - loss: 1.1295 - regression_loss: 0.9540 - classification_loss: 0.1755
1229/1500 [=======================>......] - ETA: 3:32 - loss: 1.1292 - regression_loss: 0.9538 - classification_loss: 0.1754
1230/1500 [=======================>......] - ETA: 3:31 - loss: 1.1304 - regression_loss: 0.9545 - classification_loss: 0.1759
1231/1500 [=======================>......] - ETA: 3:31 - loss: 1.1306 - regression_loss: 0.9548 - classification_loss: 0.1759
1232/1500 [=======================>......] - ETA: 3:30 - loss: 1.1308 - regression_loss: 0.9550 - classification_loss: 0.1759
1233/1500 [=======================>......] - ETA: 3:29 - loss: 1.1304 - regression_loss: 0.9547 - classification_loss: 0.1758
1234/1500 [=======================>......] - ETA: 3:28 - loss: 1.1298 - regression_loss: 0.9542 - classification_loss: 0.1757
1235/1500 [=======================>......] - ETA: 3:28 - loss: 1.1295 - regression_loss: 0.9539 - classification_loss: 0.1757
1236/1500 [=======================>......] - ETA: 3:27 - loss: 1.1290 - regression_loss: 0.9535 - classification_loss: 0.1756
1237/1500 [=======================>......] - ETA: 3:26 - loss: 1.1287 - regression_loss: 0.9532 - classification_loss: 0.1755
1238/1500 [=======================>......] - ETA: 3:26 - loss: 1.1284 - regression_loss: 0.9529 - classification_loss: 0.1755
1239/1500 [=======================>......] - ETA: 3:25 - loss: 1.1294 - regression_loss: 0.9537 - classification_loss: 0.1757
1240/1500 [=======================>......] - ETA: 3:24 - loss: 1.1300 - regression_loss: 0.9541 - classification_loss: 0.1759
1241/1500 [=======================>......] - ETA: 3:23 - loss: 1.1311 - regression_loss: 0.9550 - classification_loss: 0.1761
1242/1500 [=======================>......] - ETA: 3:23 - loss: 1.1310 - regression_loss: 0.9550 - classification_loss: 0.1760
1243/1500 [=======================>......] - ETA: 3:22 - loss: 1.1317 - regression_loss: 0.9556 - classification_loss: 0.1761
1244/1500 [=======================>......] - ETA: 3:21 - loss: 1.1321 - regression_loss: 0.9560 - classification_loss: 0.1761
1245/1500 [=======================>......] - ETA: 3:20 - loss: 1.1321 - regression_loss: 0.9560 - classification_loss: 0.1761
1246/1500 [=======================>......] - ETA: 3:19 - loss: 1.1323 - regression_loss: 0.9562 - classification_loss: 0.1761
1247/1500 [=======================>......] - ETA: 3:19 - loss: 1.1322 - regression_loss: 0.9561 - classification_loss: 0.1760
1248/1500 [=======================>......] - ETA: 3:18 - loss: 1.1319 - regression_loss: 0.9558 - classification_loss: 0.1761
1249/1500 [=======================>......] - ETA: 3:17 - loss: 1.1313 - regression_loss: 0.9554 - classification_loss: 0.1760
1250/1500 [========================>.....] - ETA: 3:16 - loss: 1.1308 - regression_loss: 0.9549 - classification_loss: 0.1759
1251/1500 [========================>.....] - ETA: 3:15 - loss: 1.1309 - regression_loss: 0.9550 - classification_loss: 0.1759
1252/1500 [========================>.....] - ETA: 3:15 - loss: 1.1315 - regression_loss: 0.9557 - classification_loss: 0.1758
1253/1500 [========================>.....] - ETA: 3:14 - loss: 1.1312 - regression_loss: 0.9554 - classification_loss: 0.1758
1254/1500 [========================>.....] - ETA: 3:13 - loss: 1.1307 - regression_loss: 0.9549 - classification_loss: 0.1757
1255/1500 [========================>.....] - ETA: 3:12 - loss: 1.1302 - regression_loss: 0.9546 - classification_loss: 0.1757
1256/1500 [========================>.....] - ETA: 3:11 - loss: 1.1301 - regression_loss: 0.9544 - classification_loss: 0.1757
1257/1500 [========================>.....] - ETA: 3:11 - loss: 1.1301 - regression_loss: 0.9545 - classification_loss: 0.1756
1258/1500 [========================>.....] - ETA: 3:10 - loss: 1.1301 - regression_loss: 0.9545 - classification_loss: 0.1756
1259/1500 [========================>.....] - ETA: 3:09 - loss: 1.1305 - regression_loss: 0.9549 - classification_loss: 0.1756
1260/1500 [========================>.....] - ETA: 3:08 - loss: 1.1302 - regression_loss: 0.9547 - classification_loss: 0.1755
1261/1500 [========================>.....] - ETA: 3:07 - loss: 1.1299 - regression_loss: 0.9545 - classification_loss: 0.1754
1262/1500 [========================>.....] - ETA: 3:06 - loss: 1.1295 - regression_loss: 0.9542 - classification_loss: 0.1753
1263/1500 [========================>.....] - ETA: 3:06 - loss: 1.1292 - regression_loss: 0.9540 - classification_loss: 0.1752
1264/1500 [========================>.....] - ETA: 3:05 - loss: 1.1287 - regression_loss: 0.9535 - classification_loss: 0.1751
1265/1500 [========================>.....] - ETA: 3:04 - loss: 1.1282 - regression_loss: 0.9532 - classification_loss: 0.1750
1266/1500 [========================>.....] - ETA: 3:03 - loss: 1.1275 - regression_loss: 0.9526 - classification_loss: 0.1749
1267/1500 [========================>.....] - ETA: 3:02 - loss: 1.1270 - regression_loss: 0.9522 - classification_loss: 0.1748
1268/1500 [========================>.....] - ETA: 3:02 - loss: 1.1273 - regression_loss: 0.9523 - classification_loss: 0.1750
1269/1500 [========================>.....] - ETA: 3:01 - loss: 1.1271 - regression_loss: 0.9522 - classification_loss: 0.1749
1270/1500 [========================>.....] - ETA: 3:00 - loss: 1.1273 - regression_loss: 0.9524 - classification_loss: 0.1749
1271/1500 [========================>.....] - ETA: 3:00 - loss: 1.1269 - regression_loss: 0.9521 - classification_loss: 0.1748
1272/1500 [========================>.....] - ETA: 2:59 - loss: 1.1264 - regression_loss: 0.9517 - classification_loss: 0.1747
1273/1500 [========================>.....] - ETA: 2:58 - loss: 1.1261 - regression_loss: 0.9515 - classification_loss: 0.1746
1274/1500 [========================>.....] - ETA: 2:57 - loss: 1.1257 - regression_loss: 0.9512 - classification_loss: 0.1745
1275/1500 [========================>.....] - ETA: 2:56 - loss: 1.1254 - regression_loss: 0.9509 - classification_loss: 0.1745
1276/1500 [========================>.....] - ETA: 2:56 - loss: 1.1248 - regression_loss: 0.9504 - classification_loss: 0.1744
1277/1500 [========================>.....] - ETA: 2:55 - loss: 1.1243 - regression_loss: 0.9500 - classification_loss: 0.1743
1278/1500 [========================>.....] - ETA: 2:54 - loss: 1.1241 - regression_loss: 0.9498 - classification_loss: 0.1743
1279/1500 [========================>.....] - ETA: 2:53 - loss: 1.1237 - regression_loss: 0.9495 - classification_loss: 0.1742
1280/1500 [========================>.....] - ETA: 2:52 - loss: 1.1250 - regression_loss: 0.9504 - classification_loss: 0.1746
1281/1500 [========================>.....] - ETA: 2:52 - loss: 1.1250 - regression_loss: 0.9505 - classification_loss: 0.1746
1282/1500 [========================>.....] - ETA: 2:51 - loss: 1.1249 - regression_loss: 0.9505 - classification_loss: 0.1745
1283/1500 [========================>.....] - ETA: 2:50 - loss: 1.1247 - regression_loss: 0.9503 - classification_loss: 0.1744
1284/1500 [========================>.....] - ETA: 2:49 - loss: 1.1241 - regression_loss: 0.9497 - classification_loss: 0.1743
1285/1500 [========================>.....] - ETA: 2:48 - loss: 1.1240 - regression_loss: 0.9497 - classification_loss: 0.1743
1286/1500 [========================>.....] - ETA: 2:48 - loss: 1.1236 - regression_loss: 0.9494 - classification_loss: 0.1742
1287/1500 [========================>.....] - ETA: 2:47 - loss: 1.1234 - regression_loss: 0.9493 - classification_loss: 0.1741
1288/1500 [========================>.....] - ETA: 2:46 - loss: 1.1231 - regression_loss: 0.9491 - classification_loss: 0.1740
1289/1500 [========================>.....] - ETA: 2:46 - loss: 1.1229 - regression_loss: 0.9489 - classification_loss: 0.1740
1290/1500 [========================>.....] - ETA: 2:45 - loss: 1.1229 - regression_loss: 0.9489 - classification_loss: 0.1740
1291/1500 [========================>.....] - ETA: 2:44 - loss: 1.1224 - regression_loss: 0.9485 - classification_loss: 0.1739
1292/1500 [========================>.....] - ETA: 2:43 - loss: 1.1222 - regression_loss: 0.9484 - classification_loss: 0.1738
1293/1500 [========================>.....] - ETA: 2:42 - loss: 1.1226 - regression_loss: 0.9487 - classification_loss: 0.1739
1294/1500 [========================>.....] - ETA: 2:42 - loss: 1.1227 - regression_loss: 0.9487 - classification_loss: 0.1739
1295/1500 [========================>.....] - ETA: 2:41 - loss: 1.1230 - regression_loss: 0.9490 - classification_loss: 0.1740
1296/1500 [========================>.....] - ETA: 2:40 - loss: 1.1236 - regression_loss: 0.9495 - classification_loss: 0.1740
1297/1500 [========================>.....] - ETA: 2:39 - loss: 1.1231 - regression_loss: 0.9492 - classification_loss: 0.1739
1298/1500 [========================>.....] - ETA: 2:38 - loss: 1.1233 - regression_loss: 0.9493 - classification_loss: 0.1740
1299/1500 [========================>.....] - ETA: 2:38 - loss: 1.1232 - regression_loss: 0.9493 - classification_loss: 0.1739
1300/1500 [=========================>....] - ETA: 2:37 - loss: 1.1235 - regression_loss: 0.9496 - classification_loss: 0.1739
1301/1500 [=========================>....] - ETA: 2:36 - loss: 1.1234 - regression_loss: 0.9496 - classification_loss: 0.1739
1302/1500 [=========================>....] - ETA: 2:35 - loss: 1.1245 - regression_loss: 0.9504 - classification_loss: 0.1741
1303/1500 [=========================>....] - ETA: 2:34 - loss: 1.1245 - regression_loss: 0.9502 - classification_loss: 0.1743
1304/1500 [=========================>....] - ETA: 2:33 - loss: 1.1241 - regression_loss: 0.9498 - classification_loss: 0.1743
1305/1500 [=========================>....] - ETA: 2:33 - loss: 1.1237 - regression_loss: 0.9495 - classification_loss: 0.1742
1306/1500 [=========================>....] - ETA: 2:32 - loss: 1.1236 - regression_loss: 0.9495 - classification_loss: 0.1741
1307/1500 [=========================>....] - ETA: 2:31 - loss: 1.1238 - regression_loss: 0.9496 - classification_loss: 0.1742
1308/1500 [=========================>....] - ETA: 2:30 - loss: 1.1251 - regression_loss: 0.9507 - classification_loss: 0.1744
1309/1500 [=========================>....] - ETA: 2:29 - loss: 1.1248 - regression_loss: 0.9504 - classification_loss: 0.1744
1310/1500 [=========================>....] - ETA: 2:28 - loss: 1.1243 - regression_loss: 0.9500 - classification_loss: 0.1743
1311/1500 [=========================>....] - ETA: 2:28 - loss: 1.1238 - regression_loss: 0.9496 - classification_loss: 0.1742
1312/1500 [=========================>....] - ETA: 2:27 - loss: 1.1248 - regression_loss: 0.9503 - classification_loss: 0.1745
1313/1500 [=========================>....] - ETA: 2:26 - loss: 1.1247 - regression_loss: 0.9503 - classification_loss: 0.1744
1314/1500 [=========================>....] - ETA: 2:25 - loss: 1.1242 - regression_loss: 0.9498 - classification_loss: 0.1744
1315/1500 [=========================>....] - ETA: 2:24 - loss: 1.1249 - regression_loss: 0.9506 - classification_loss: 0.1743
1316/1500 [=========================>....] - ETA: 2:23 - loss: 1.1251 - regression_loss: 0.9507 - classification_loss: 0.1744
1317/1500 [=========================>....] - ETA: 2:23 - loss: 1.1251 - regression_loss: 0.9507 - classification_loss: 0.1744
1318/1500 [=========================>....] - ETA: 2:22 - loss: 1.1252 - regression_loss: 0.9507 - classification_loss: 0.1745
1319/1500 [=========================>....] - ETA: 2:21 - loss: 1.1252 - regression_loss: 0.9507 - classification_loss: 0.1745
1320/1500 [=========================>....] - ETA: 2:20 - loss: 1.1253 - regression_loss: 0.9506 - classification_loss: 0.1746
1321/1500 [=========================>....] - ETA: 2:19 - loss: 1.1255 - regression_loss: 0.9508 - classification_loss: 0.1747
1322/1500 [=========================>....] - ETA: 2:19 - loss: 1.1251 - regression_loss: 0.9505 - classification_loss: 0.1746
1323/1500 [=========================>....] - ETA: 2:18 - loss: 1.1247 - regression_loss: 0.9501 - classification_loss: 0.1745
1324/1500 [=========================>....] - ETA: 2:17 - loss: 1.1242 - regression_loss: 0.9497 - classification_loss: 0.1745
1325/1500 [=========================>....] - ETA: 2:16 - loss: 1.1238 - regression_loss: 0.9494 - classification_loss: 0.1744
1326/1500 [=========================>....] - ETA: 2:15 - loss: 1.1242 - regression_loss: 0.9497 - classification_loss: 0.1744
1327/1500 [=========================>....] - ETA: 2:15 - loss: 1.1237 - regression_loss: 0.9493 - classification_loss: 0.1744
1328/1500 [=========================>....] - ETA: 2:14 - loss: 1.1235 - regression_loss: 0.9492 - classification_loss: 0.1743
1329/1500 [=========================>....] - ETA: 2:13 - loss: 1.1233 - regression_loss: 0.9490 - classification_loss: 0.1742
1330/1500 [=========================>....] - ETA: 2:12 - loss: 1.1231 - regression_loss: 0.9490 - classification_loss: 0.1741
1331/1500 [=========================>....] - ETA: 2:11 - loss: 1.1230 - regression_loss: 0.9489 - classification_loss: 0.1741
1332/1500 [=========================>....] - ETA: 2:11 - loss: 1.1232 - regression_loss: 0.9491 - classification_loss: 0.1741
1333/1500 [=========================>....] - ETA: 2:10 - loss: 1.1227 - regression_loss: 0.9487 - classification_loss: 0.1740
1334/1500 [=========================>....] - ETA: 2:09 - loss: 1.1223 - regression_loss: 0.9484 - classification_loss: 0.1739
1335/1500 [=========================>....] - ETA: 2:08 - loss: 1.1223 - regression_loss: 0.9484 - classification_loss: 0.1739
1336/1500 [=========================>....] - ETA: 2:08 - loss: 1.1223 - regression_loss: 0.9485 - classification_loss: 0.1738
1337/1500 [=========================>....] - ETA: 2:07 - loss: 1.1229 - regression_loss: 0.9490 - classification_loss: 0.1740
1338/1500 [=========================>....] - ETA: 2:06 - loss: 1.1226 - regression_loss: 0.9486 - classification_loss: 0.1739
1339/1500 [=========================>....] - ETA: 2:05 - loss: 1.1225 - regression_loss: 0.9485 - classification_loss: 0.1739
1340/1500 [=========================>....] - ETA: 2:04 - loss: 1.1219 - regression_loss: 0.9481 - classification_loss: 0.1738
1341/1500 [=========================>....] - ETA: 2:04 - loss: 1.1224 - regression_loss: 0.9485 - classification_loss: 0.1739
1342/1500 [=========================>....] - ETA: 2:03 - loss: 1.1220 - regression_loss: 0.9482 - classification_loss: 0.1738
1343/1500 [=========================>....] - ETA: 2:02 - loss: 1.1219 - regression_loss: 0.9481 - classification_loss: 0.1737
1344/1500 [=========================>....] - ETA: 2:01 - loss: 1.1215 - regression_loss: 0.9478 - classification_loss: 0.1737
1345/1500 [=========================>....] - ETA: 2:00 - loss: 1.1224 - regression_loss: 0.9486 - classification_loss: 0.1738
1346/1500 [=========================>....] - ETA: 2:00 - loss: 1.1221 - regression_loss: 0.9484 - classification_loss: 0.1737
1347/1500 [=========================>....] - ETA: 1:59 - loss: 1.1213 - regression_loss: 0.9477 - classification_loss: 0.1736
1348/1500 [=========================>....] - ETA: 1:58 - loss: 1.1208 - regression_loss: 0.9472 - classification_loss: 0.1735
1349/1500 [=========================>....] - ETA: 1:57 - loss: 1.1207 - regression_loss: 0.9472 - classification_loss: 0.1735
1350/1500 [==========================>...] - ETA: 1:57 - loss: 1.1209 - regression_loss: 0.9473 - classification_loss: 0.1736
1351/1500 [==========================>...] - ETA: 1:56 - loss: 1.1208 - regression_loss: 0.9472 - classification_loss: 0.1736
1352/1500 [==========================>...] - ETA: 1:55 - loss: 1.1211 - regression_loss: 0.9475 - classification_loss: 0.1737
1353/1500 [==========================>...] - ETA: 1:54 - loss: 1.1206 - regression_loss: 0.9470 - classification_loss: 0.1736
1354/1500 [==========================>...] - ETA: 1:54 - loss: 1.1214 - regression_loss: 0.9476 - classification_loss: 0.1738
1355/1500 [==========================>...] - ETA: 1:53 - loss: 1.1212 - regression_loss: 0.9475 - classification_loss: 0.1737
1356/1500 [==========================>...] - ETA: 1:52 - loss: 1.1211 - regression_loss: 0.9474 - classification_loss: 0.1736
1357/1500 [==========================>...] - ETA: 1:51 - loss: 1.1205 - regression_loss: 0.9470 - classification_loss: 0.1735
1358/1500 [==========================>...] - ETA: 1:50 - loss: 1.1204 - regression_loss: 0.9468 - classification_loss: 0.1736
1359/1500 [==========================>...] - ETA: 1:50 - loss: 1.1207 - regression_loss: 0.9472 - classification_loss: 0.1735
1360/1500 [==========================>...] - ETA: 1:49 - loss: 1.1212 - regression_loss: 0.9476 - classification_loss: 0.1736
1361/1500 [==========================>...] - ETA: 1:48 - loss: 1.1209 - regression_loss: 0.9473 - classification_loss: 0.1735
1362/1500 [==========================>...] - ETA: 1:47 - loss: 1.1205 - regression_loss: 0.9471 - classification_loss: 0.1734
1363/1500 [==========================>...] - ETA: 1:47 - loss: 1.1201 - regression_loss: 0.9467 - classification_loss: 0.1733
1364/1500 [==========================>...] - ETA: 1:46 - loss: 1.1201 - regression_loss: 0.9468 - classification_loss: 0.1733
1365/1500 [==========================>...] - ETA: 1:45 - loss: 1.1206 - regression_loss: 0.9472 - classification_loss: 0.1734
1366/1500 [==========================>...] - ETA: 1:44 - loss: 1.1206 - regression_loss: 0.9473 - classification_loss: 0.1733
1367/1500 [==========================>...] - ETA: 1:43 - loss: 1.1204 - regression_loss: 0.9471 - classification_loss: 0.1732
1368/1500 [==========================>...] - ETA: 1:43 - loss: 1.1199 - regression_loss: 0.9468 - classification_loss: 0.1731
1369/1500 [==========================>...] - ETA: 1:42 - loss: 1.1195 - regression_loss: 0.9465 - classification_loss: 0.1730
1370/1500 [==========================>...] - ETA: 1:41 - loss: 1.1196 - regression_loss: 0.9466 - classification_loss: 0.1730
1371/1500 [==========================>...] - ETA: 1:40 - loss: 1.1193 - regression_loss: 0.9464 - classification_loss: 0.1729
1372/1500 [==========================>...] - ETA: 1:39 - loss: 1.1188 - regression_loss: 0.9460 - classification_loss: 0.1728
1373/1500 [==========================>...] - ETA: 1:39 - loss: 1.1183 - regression_loss: 0.9456 - classification_loss: 0.1727
1374/1500 [==========================>...] - ETA: 1:38 - loss: 1.1184 - regression_loss: 0.9457 - classification_loss: 0.1728
1375/1500 [==========================>...] - ETA: 1:37 - loss: 1.1188 - regression_loss: 0.9461 - classification_loss: 0.1728
1376/1500 [==========================>...] - ETA: 1:36 - loss: 1.1183 - regression_loss: 0.9456 - classification_loss: 0.1727
1377/1500 [==========================>...] - ETA: 1:35 - loss: 1.1185 - regression_loss: 0.9458 - classification_loss: 0.1727
1378/1500 [==========================>...] - ETA: 1:35 - loss: 1.1187 - regression_loss: 0.9459 - classification_loss: 0.1727
1379/1500 [==========================>...] - ETA: 1:34 - loss: 1.1196 - regression_loss: 0.9466 - classification_loss: 0.1730
1380/1500 [==========================>...] - ETA: 1:33 - loss: 1.1194 - regression_loss: 0.9465 - classification_loss: 0.1730
1381/1500 [==========================>...] - ETA: 1:32 - loss: 1.1191 - regression_loss: 0.9462 - classification_loss: 0.1729
1382/1500 [==========================>...] - ETA: 1:32 - loss: 1.1190 - regression_loss: 0.9461 - classification_loss: 0.1729
1383/1500 [==========================>...] - ETA: 1:31 - loss: 1.1188 - regression_loss: 0.9460 - classification_loss: 0.1728
1384/1500 [==========================>...] - ETA: 1:30 - loss: 1.1186 - regression_loss: 0.9459 - classification_loss: 0.1727
1385/1500 [==========================>...] - ETA: 1:29 - loss: 1.1188 - regression_loss: 0.9461 - classification_loss: 0.1727
1386/1500 [==========================>...] - ETA: 1:29 - loss: 1.1188 - regression_loss: 0.9461 - classification_loss: 0.1727
1387/1500 [==========================>...] - ETA: 1:28 - loss: 1.1191 - regression_loss: 0.9462 - classification_loss: 0.1729
1388/1500 [==========================>...] - ETA: 1:27 - loss: 1.1197 - regression_loss: 0.9467 - classification_loss: 0.1730
1389/1500 [==========================>...] - ETA: 1:26 - loss: 1.1204 - regression_loss: 0.9471 - classification_loss: 0.1733
1390/1500 [==========================>...] - ETA: 1:25 - loss: 1.1201 - regression_loss: 0.9469 - classification_loss: 0.1732
1391/1500 [==========================>...] - ETA: 1:25 - loss: 1.1199 - regression_loss: 0.9468 - classification_loss: 0.1731
1392/1500 [==========================>...] - ETA: 1:24 - loss: 1.1197 - regression_loss: 0.9466 - classification_loss: 0.1731
1393/1500 [==========================>...] - ETA: 1:23 - loss: 1.1198 - regression_loss: 0.9467 - classification_loss: 0.1730
1394/1500 [==========================>...] - ETA: 1:22 - loss: 1.1196 - regression_loss: 0.9466 - classification_loss: 0.1730
1395/1500 [==========================>...] - ETA: 1:22 - loss: 1.1202 - regression_loss: 0.9470 - classification_loss: 0.1733
1396/1500 [==========================>...] - ETA: 1:21 - loss: 1.1207 - regression_loss: 0.9474 - classification_loss: 0.1734
1397/1500 [==========================>...] - ETA: 1:20 - loss: 1.1208 - regression_loss: 0.9474 - classification_loss: 0.1733
1398/1500 [==========================>...] - ETA: 1:19 - loss: 1.1205 - regression_loss: 0.9473 - classification_loss: 0.1732
1399/1500 [==========================>...] - ETA: 1:18 - loss: 1.1202 - regression_loss: 0.9470 - classification_loss: 0.1732
1400/1500 [===========================>..] - ETA: 1:18 - loss: 1.1201 - regression_loss: 0.9470 - classification_loss: 0.1732
1401/1500 [===========================>..] - ETA: 1:17 - loss: 1.1197 - regression_loss: 0.9466 - classification_loss: 0.1731
1402/1500 [===========================>..] - ETA: 1:16 - loss: 1.1202 - regression_loss: 0.9471 - classification_loss: 0.1731
1403/1500 [===========================>..] - ETA: 1:15 - loss: 1.1197 - regression_loss: 0.9467 - classification_loss: 0.1730
1404/1500 [===========================>..] - ETA: 1:14 - loss: 1.1198 - regression_loss: 0.9468 - classification_loss: 0.1730
1405/1500 [===========================>..] - ETA: 1:14 - loss: 1.1196 - regression_loss: 0.9466 - classification_loss: 0.1729
1406/1500 [===========================>..] - ETA: 1:13 - loss: 1.1196 - regression_loss: 0.9467 - classification_loss: 0.1730
1407/1500 [===========================>..] - ETA: 1:12 - loss: 1.1193 - regression_loss: 0.9464 - classification_loss: 0.1729
1408/1500 [===========================>..] - ETA: 1:11 - loss: 1.1191 - regression_loss: 0.9463 - classification_loss: 0.1729
1409/1500 [===========================>..] - ETA: 1:11 - loss: 1.1196 - regression_loss: 0.9467 - classification_loss: 0.1729
1410/1500 [===========================>..] - ETA: 1:10 - loss: 1.1196 - regression_loss: 0.9468 - classification_loss: 0.1728
1411/1500 [===========================>..] - ETA: 1:09 - loss: 1.1197 - regression_loss: 0.9470 - classification_loss: 0.1728
1412/1500 [===========================>..] - ETA: 1:08 - loss: 1.1197 - regression_loss: 0.9470 - classification_loss: 0.1728
1413/1500 [===========================>..] - ETA: 1:07 - loss: 1.1195 - regression_loss: 0.9468 - classification_loss: 0.1727
1414/1500 [===========================>..] - ETA: 1:07 - loss: 1.1195 - regression_loss: 0.9468 - classification_loss: 0.1727
1415/1500 [===========================>..] - ETA: 1:06 - loss: 1.1203 - regression_loss: 0.9474 - classification_loss: 0.1729
1416/1500 [===========================>..] - ETA: 1:05 - loss: 1.1205 - regression_loss: 0.9476 - classification_loss: 0.1730
1417/1500 [===========================>..] - ETA: 1:04 - loss: 1.1202 - regression_loss: 0.9473 - classification_loss: 0.1729
1418/1500 [===========================>..] - ETA: 1:04 - loss: 1.1201 - regression_loss: 0.9472 - classification_loss: 0.1728
1419/1500 [===========================>..] - ETA: 1:03 - loss: 1.1201 - regression_loss: 0.9473 - classification_loss: 0.1728
1420/1500 [===========================>..] - ETA: 1:02 - loss: 1.1196 - regression_loss: 0.9469 - classification_loss: 0.1727
1421/1500 [===========================>..] - ETA: 1:01 - loss: 1.1194 - regression_loss: 0.9468 - classification_loss: 0.1726
1422/1500 [===========================>..] - ETA: 1:00 - loss: 1.1197 - regression_loss: 0.9470 - classification_loss: 0.1727
1423/1500 [===========================>..] - ETA: 1:00 - loss: 1.1195 - regression_loss: 0.9468 - classification_loss: 0.1726
1424/1500 [===========================>..] - ETA: 59s - loss: 1.1195 - regression_loss: 0.9469 - classification_loss: 0.1726 
1425/1500 [===========================>..] - ETA: 58s - loss: 1.1193 - regression_loss: 0.9468 - classification_loss: 0.1725
1426/1500 [===========================>..] - ETA: 57s - loss: 1.1193 - regression_loss: 0.9468 - classification_loss: 0.1725
1427/1500 [===========================>..] - ETA: 56s - loss: 1.1191 - regression_loss: 0.9466 - classification_loss: 0.1725
1428/1500 [===========================>..] - ETA: 56s - loss: 1.1190 - regression_loss: 0.9465 - classification_loss: 0.1724
1429/1500 [===========================>..] - ETA: 55s - loss: 1.1196 - regression_loss: 0.9471 - classification_loss: 0.1725
1430/1500 [===========================>..] - ETA: 54s - loss: 1.1201 - regression_loss: 0.9475 - classification_loss: 0.1726
1431/1500 [===========================>..] - ETA: 53s - loss: 1.1200 - regression_loss: 0.9474 - classification_loss: 0.1726
1432/1500 [===========================>..] - ETA: 53s - loss: 1.1196 - regression_loss: 0.9471 - classification_loss: 0.1725
1433/1500 [===========================>..] - ETA: 52s - loss: 1.1197 - regression_loss: 0.9473 - classification_loss: 0.1724
1434/1500 [===========================>..] - ETA: 51s - loss: 1.1198 - regression_loss: 0.9473 - classification_loss: 0.1725
1435/1500 [===========================>..] - ETA: 50s - loss: 1.1197 - regression_loss: 0.9472 - classification_loss: 0.1725
1436/1500 [===========================>..] - ETA: 49s - loss: 1.1193 - regression_loss: 0.9469 - classification_loss: 0.1724
1437/1500 [===========================>..] - ETA: 49s - loss: 1.1199 - regression_loss: 0.9473 - classification_loss: 0.1726
1438/1500 [===========================>..] - ETA: 48s - loss: 1.1197 - regression_loss: 0.9472 - classification_loss: 0.1725
1439/1500 [===========================>..] - ETA: 47s - loss: 1.1193 - regression_loss: 0.9468 - classification_loss: 0.1725
1440/1500 [===========================>..] - ETA: 46s - loss: 1.1191 - regression_loss: 0.9467 - classification_loss: 0.1724
1441/1500 [===========================>..] - ETA: 46s - loss: 1.1201 - regression_loss: 0.9474 - classification_loss: 0.1727
1442/1500 [===========================>..] - ETA: 45s - loss: 1.1197 - regression_loss: 0.9471 - classification_loss: 0.1726
1443/1500 [===========================>..] - ETA: 44s - loss: 1.1196 - regression_loss: 0.9470 - classification_loss: 0.1726
1444/1500 [===========================>..] - ETA: 43s - loss: 1.1191 - regression_loss: 0.9467 - classification_loss: 0.1725
1445/1500 [===========================>..] - ETA: 42s - loss: 1.1189 - regression_loss: 0.9466 - classification_loss: 0.1724
1446/1500 [===========================>..] - ETA: 42s - loss: 1.1188 - regression_loss: 0.9463 - classification_loss: 0.1725
1447/1500 [===========================>..] - ETA: 41s - loss: 1.1189 - regression_loss: 0.9464 - classification_loss: 0.1725
1448/1500 [===========================>..] - ETA: 40s - loss: 1.1185 - regression_loss: 0.9460 - classification_loss: 0.1724
1449/1500 [===========================>..] - ETA: 39s - loss: 1.1185 - regression_loss: 0.9460 - classification_loss: 0.1725
1450/1500 [============================>.] - ETA: 39s - loss: 1.1183 - regression_loss: 0.9458 - classification_loss: 0.1724
1451/1500 [============================>.] - ETA: 38s - loss: 1.1180 - regression_loss: 0.9456 - classification_loss: 0.1724
1452/1500 [============================>.] - ETA: 37s - loss: 1.1178 - regression_loss: 0.9454 - classification_loss: 0.1725
1453/1500 [============================>.] - ETA: 36s - loss: 1.1173 - regression_loss: 0.9450 - classification_loss: 0.1724
1454/1500 [============================>.] - ETA: 35s - loss: 1.1174 - regression_loss: 0.9450 - classification_loss: 0.1723
1455/1500 [============================>.] - ETA: 35s - loss: 1.1170 - regression_loss: 0.9447 - classification_loss: 0.1723
1456/1500 [============================>.] - ETA: 34s - loss: 1.1170 - regression_loss: 0.9447 - classification_loss: 0.1722
1457/1500 [============================>.] - ETA: 33s - loss: 1.1165 - regression_loss: 0.9444 - classification_loss: 0.1722
1458/1500 [============================>.] - ETA: 32s - loss: 1.1168 - regression_loss: 0.9446 - classification_loss: 0.1722
1459/1500 [============================>.] - ETA: 31s - loss: 1.1164 - regression_loss: 0.9443 - classification_loss: 0.1721
1460/1500 [============================>.] - ETA: 31s - loss: 1.1161 - regression_loss: 0.9440 - classification_loss: 0.1721
1461/1500 [============================>.] - ETA: 30s - loss: 1.1160 - regression_loss: 0.9440 - classification_loss: 0.1720
1462/1500 [============================>.] - ETA: 29s - loss: 1.1163 - regression_loss: 0.9442 - classification_loss: 0.1721
1463/1500 [============================>.] - ETA: 28s - loss: 1.1159 - regression_loss: 0.9438 - classification_loss: 0.1720
1464/1500 [============================>.] - ETA: 28s - loss: 1.1158 - regression_loss: 0.9439 - classification_loss: 0.1720
1465/1500 [============================>.] - ETA: 27s - loss: 1.1160 - regression_loss: 0.9439 - classification_loss: 0.1720
1466/1500 [============================>.] - ETA: 26s - loss: 1.1170 - regression_loss: 0.9447 - classification_loss: 0.1723
1467/1500 [============================>.] - ETA: 25s - loss: 1.1173 - regression_loss: 0.9450 - classification_loss: 0.1723
1468/1500 [============================>.] - ETA: 24s - loss: 1.1176 - regression_loss: 0.9452 - classification_loss: 0.1724
1469/1500 [============================>.] - ETA: 24s - loss: 1.1182 - regression_loss: 0.9458 - classification_loss: 0.1725
1470/1500 [============================>.] - ETA: 23s - loss: 1.1185 - regression_loss: 0.9460 - classification_loss: 0.1725
1471/1500 [============================>.] - ETA: 22s - loss: 1.1191 - regression_loss: 0.9463 - classification_loss: 0.1727
1472/1500 [============================>.] - ETA: 21s - loss: 1.1188 - regression_loss: 0.9461 - classification_loss: 0.1727
1473/1500 [============================>.] - ETA: 21s - loss: 1.1186 - regression_loss: 0.9460 - classification_loss: 0.1726
1474/1500 [============================>.] - ETA: 20s - loss: 1.1184 - regression_loss: 0.9458 - classification_loss: 0.1725
1475/1500 [============================>.] - ETA: 19s - loss: 1.1192 - regression_loss: 0.9464 - classification_loss: 0.1727
1476/1500 [============================>.] - ETA: 18s - loss: 1.1199 - regression_loss: 0.9470 - classification_loss: 0.1729
1477/1500 [============================>.] - ETA: 17s - loss: 1.1197 - regression_loss: 0.9468 - classification_loss: 0.1729
1478/1500 [============================>.] - ETA: 17s - loss: 1.1193 - regression_loss: 0.9464 - classification_loss: 0.1729
1479/1500 [============================>.] - ETA: 16s - loss: 1.1188 - regression_loss: 0.9460 - classification_loss: 0.1728
1480/1500 [============================>.] - ETA: 15s - loss: 1.1188 - regression_loss: 0.9460 - classification_loss: 0.1728
1481/1500 [============================>.] - ETA: 14s - loss: 1.1186 - regression_loss: 0.9458 - classification_loss: 0.1728
1482/1500 [============================>.] - ETA: 14s - loss: 1.1184 - regression_loss: 0.9457 - classification_loss: 0.1728
1483/1500 [============================>.] - ETA: 13s - loss: 1.1182 - regression_loss: 0.9455 - classification_loss: 0.1727
1484/1500 [============================>.] - ETA: 12s - loss: 1.1177 - regression_loss: 0.9451 - classification_loss: 0.1726
1485/1500 [============================>.] - ETA: 11s - loss: 1.1174 - regression_loss: 0.9448 - classification_loss: 0.1726
1486/1500 [============================>.] - ETA: 10s - loss: 1.1181 - regression_loss: 0.9453 - classification_loss: 0.1728
1487/1500 [============================>.] - ETA: 10s - loss: 1.1178 - regression_loss: 0.9451 - classification_loss: 0.1727
1488/1500 [============================>.] - ETA: 9s - loss: 1.1179 - regression_loss: 0.9452 - classification_loss: 0.1727 
1489/1500 [============================>.] - ETA: 8s - loss: 1.1185 - regression_loss: 0.9456 - classification_loss: 0.1728
1490/1500 [============================>.] - ETA: 7s - loss: 1.1195 - regression_loss: 0.9465 - classification_loss: 0.1730
1491/1500 [============================>.] - ETA: 7s - loss: 1.1198 - regression_loss: 0.9467 - classification_loss: 0.1731
1492/1500 [============================>.] - ETA: 6s - loss: 1.1195 - regression_loss: 0.9465 - classification_loss: 0.1730
1493/1500 [============================>.] - ETA: 5s - loss: 1.1197 - regression_loss: 0.9467 - classification_loss: 0.1730
1494/1500 [============================>.] - ETA: 4s - loss: 1.1203 - regression_loss: 0.9472 - classification_loss: 0.1731
1495/1500 [============================>.] - ETA: 3s - loss: 1.1212 - regression_loss: 0.9480 - classification_loss: 0.1732
1496/1500 [============================>.] - ETA: 3s - loss: 1.1208 - regression_loss: 0.9477 - classification_loss: 0.1731
1497/1500 [============================>.] - ETA: 2s - loss: 1.1207 - regression_loss: 0.9476 - classification_loss: 0.1731
1498/1500 [============================>.] - ETA: 1s - loss: 1.1217 - regression_loss: 0.9483 - classification_loss: 0.1733
1499/1500 [============================>.] - ETA: 0s - loss: 1.1217 - regression_loss: 0.9483 - classification_loss: 0.1734
1500/1500 [==============================] - 1174s 782ms/step - loss: 1.1216 - regression_loss: 0.9482 - classification_loss: 0.1733
/home/tmandel/.conda/envs/fish_env/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.
  warnings.warn('`epsilon` argument is deprecated and '

Epoch 00010: saving model to ./snapshots/resnet50_csv_10.h5
