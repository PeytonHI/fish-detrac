2019-06-03 12:12:43.910579: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2019-06-03 12:12:44.032336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-06-03 12:12:44.033203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:0c:00.0
totalMemory: 7.92GiB freeMemory: 7.76GiB
2019-06-03 12:12:44.033232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2019-06-03 12:12:44.601821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-03 12:12:44.601880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2019-06-03 12:12:44.601900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2019-06-03 12:12:44.602364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7490 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:0c:00.0, compute capability: 6.1)
Creating model, this may take a second...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0                                            
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9408        input_1[0][0]                    
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             
__________________________________________________________________________________________________
res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              
__________________________________________________________________________________________________
padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             
__________________________________________________________________________________________________
res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              
__________________________________________________________________________________________________
res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              
                                                                 bn2a_branch1[0][0]               
__________________________________________________________________________________________________
res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             
__________________________________________________________________________________________________
res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              
__________________________________________________________________________________________________
padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             
__________________________________________________________________________________________________
res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             
__________________________________________________________________________________________________
res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              
                                                                 res2a_relu[0][0]                 
__________________________________________________________________________________________________
res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             
__________________________________________________________________________________________________
res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              
__________________________________________________________________________________________________
padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             
__________________________________________________________________________________________________
res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             
__________________________________________________________________________________________________
res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              
                                                                 res2b_relu[0][0]                 
__________________________________________________________________________________________________
res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             
__________________________________________________________________________________________________
res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              
__________________________________________________________________________________________________
padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             
__________________________________________________________________________________________________
res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              
__________________________________________________________________________________________________
res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              
                                                                 bn3a_branch1[0][0]               
__________________________________________________________________________________________________
res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             
__________________________________________________________________________________________________
res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              
__________________________________________________________________________________________________
padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             
__________________________________________________________________________________________________
res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             
__________________________________________________________________________________________________
res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              
                                                                 res3a_relu[0][0]                 
__________________________________________________________________________________________________
res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             
__________________________________________________________________________________________________
res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              
__________________________________________________________________________________________________
padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             
__________________________________________________________________________________________________
res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             
__________________________________________________________________________________________________
res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              
                                                                 res3b_relu[0][0]                 
__________________________________________________________________________________________________
res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             
__________________________________________________________________________________________________
res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              
__________________________________________________________________________________________________
padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             
__________________________________________________________________________________________________
res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             
__________________________________________________________________________________________________
res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              
                                                                 res3c_relu[0][0]                 
__________________________________________________________________________________________________
res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             
__________________________________________________________________________________________________
res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              
__________________________________________________________________________________________________
padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             
__________________________________________________________________________________________________
res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              
__________________________________________________________________________________________________
res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              
                                                                 bn4a_branch1[0][0]               
__________________________________________________________________________________________________
res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             
__________________________________________________________________________________________________
res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              
__________________________________________________________________________________________________
padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             
__________________________________________________________________________________________________
res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             
__________________________________________________________________________________________________
res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              
                                                                 res4a_relu[0][0]                 
__________________________________________________________________________________________________
res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             
__________________________________________________________________________________________________
res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              
__________________________________________________________________________________________________
padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             
__________________________________________________________________________________________________
res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             
__________________________________________________________________________________________________
res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              
                                                                 res4b_relu[0][0]                 
__________________________________________________________________________________________________
res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             
__________________________________________________________________________________________________
res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              
__________________________________________________________________________________________________
padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             
__________________________________________________________________________________________________
res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             
__________________________________________________________________________________________________
res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              
                                                                 res4c_relu[0][0]                 
__________________________________________________________________________________________________
res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             
__________________________________________________________________________________________________
res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              
__________________________________________________________________________________________________
padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             
__________________________________________________________________________________________________
res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             
__________________________________________________________________________________________________
res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              
                                                                 res4d_relu[0][0]                 
__________________________________________________________________________________________________
res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             
__________________________________________________________________________________________________
res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              
__________________________________________________________________________________________________
padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             
__________________________________________________________________________________________________
res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             
__________________________________________________________________________________________________
res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              
                                                                 res4e_relu[0][0]                 
__________________________________________________________________________________________________
res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             
__________________________________________________________________________________________________
res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              
__________________________________________________________________________________________________
padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             
__________________________________________________________________________________________________
res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              
__________________________________________________________________________________________________
res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              
                                                                 bn5a_branch1[0][0]               
__________________________________________________________________________________________________
res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             
__________________________________________________________________________________________________
res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              
__________________________________________________________________________________________________
padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             
__________________________________________________________________________________________________
res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             
__________________________________________________________________________________________________
res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              
                                                                 res5a_relu[0][0]                 
__________________________________________________________________________________________________
res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             
__________________________________________________________________________________________________
res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              
__________________________________________________________________________________________________
padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             
__________________________________________________________________________________________________
res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              
                                                                 res5b_relu[0][0]                 
__________________________________________________________________________________________________
res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 524544      res5c_relu[0][0]                 
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 
                                                                 res4f_relu[0][0]                 
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               
                                                                 C4_reduced[0][0]                 
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  
                                                                 res3d_relu[0][0]                 
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               
                                                                 C3_reduced[0][0]                 
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 1)      2381065     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        
                                                                 regression_submodel[2][0]        
                                                                 regression_submodel[3][0]        
                                                                 regression_submodel[4][0]        
                                                                 regression_submodel[5][0]        
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 1)      0           classification_submodel[1][0]    
                                                                 classification_submodel[2][0]    
                                                                 classification_submodel[3][0]    
                                                                 classification_submodel[4][0]    
                                                                 classification_submodel[5][0]    
==================================================================================================
Total params: 36,382,957
Trainable params: 36,276,717
Non-trainable params: 106,240
__________________________________________________________________________________________________
None
Epoch 1/10

   1/1000 [..............................] - ETA: 3:18:42 - loss: 4.0733 - regression_loss: 2.9445 - classification_loss: 1.1288
   2/1000 [..............................] - ETA: 1:42:16 - loss: 4.0819 - regression_loss: 2.9530 - classification_loss: 1.1290
   3/1000 [..............................] - ETA: 1:13:33 - loss: 4.0844 - regression_loss: 2.9553 - classification_loss: 1.1291
   4/1000 [..............................] - ETA: 56:39 - loss: 4.0798 - regression_loss: 2.9509 - classification_loss: 1.1289  
   5/1000 [..............................] - ETA: 46:29 - loss: 4.0870 - regression_loss: 2.9582 - classification_loss: 1.1288
   6/1000 [..............................] - ETA: 39:43 - loss: 4.0695 - regression_loss: 2.9409 - classification_loss: 1.1287
   7/1000 [..............................] - ETA: 38:28 - loss: 4.0662 - regression_loss: 2.9377 - classification_loss: 1.1286
   8/1000 [..............................] - ETA: 38:44 - loss: 4.0634 - regression_loss: 2.9350 - classification_loss: 1.1285
   9/1000 [..............................] - ETA: 38:48 - loss: 4.0536 - regression_loss: 2.9252 - classification_loss: 1.1283
  10/1000 [..............................] - ETA: 38:23 - loss: 4.0554 - regression_loss: 2.9271 - classification_loss: 1.1283
  11/1000 [..............................] - ETA: 37:13 - loss: 4.0565 - regression_loss: 2.9283 - classification_loss: 1.1282
  12/1000 [..............................] - ETA: 37:35 - loss: 4.0555 - regression_loss: 2.9274 - classification_loss: 1.1281
  13/1000 [..............................] - ETA: 36:50 - loss: 4.0592 - regression_loss: 2.9312 - classification_loss: 1.1280
  14/1000 [..............................] - ETA: 37:19 - loss: 4.0574 - regression_loss: 2.9296 - classification_loss: 1.1278
  15/1000 [..............................] - ETA: 37:26 - loss: 4.0505 - regression_loss: 2.9229 - classification_loss: 1.1277
  16/1000 [..............................] - ETA: 37:41 - loss: 4.0491 - regression_loss: 2.9215 - classification_loss: 1.1275
  17/1000 [..............................] - ETA: 38:02 - loss: 4.0473 - regression_loss: 2.9200 - classification_loss: 1.1273
  18/1000 [..............................] - ETA: 37:50 - loss: 4.0468 - regression_loss: 2.9196 - classification_loss: 1.1272
  19/1000 [..............................] - ETA: 37:09 - loss: 4.0453 - regression_loss: 2.9182 - classification_loss: 1.1271
  20/1000 [..............................] - ETA: 37:18 - loss: 4.0425 - regression_loss: 2.9156 - classification_loss: 1.1269
  21/1000 [..............................] - ETA: 36:50 - loss: 4.0444 - regression_loss: 2.9176 - classification_loss: 1.1268
  22/1000 [..............................] - ETA: 36:19 - loss: 4.0418 - regression_loss: 2.9152 - classification_loss: 1.1266
  23/1000 [..............................] - ETA: 36:28 - loss: 4.0386 - regression_loss: 2.9121 - classification_loss: 1.1264
  24/1000 [..............................] - ETA: 36:05 - loss: 4.0400 - regression_loss: 2.9136 - classification_loss: 1.1263
  25/1000 [..............................] - ETA: 35:59 - loss: 4.0378 - regression_loss: 2.9116 - classification_loss: 1.1261
  26/1000 [..............................] - ETA: 36:14 - loss: 4.0344 - regression_loss: 2.9085 - classification_loss: 1.1259
  27/1000 [..............................] - ETA: 36:24 - loss: 4.0309 - regression_loss: 2.9053 - classification_loss: 1.1257
  28/1000 [..............................] - ETA: 36:27 - loss: 4.0251 - regression_loss: 2.8997 - classification_loss: 1.1254
  29/1000 [..............................] - ETA: 36:33 - loss: 4.0209 - regression_loss: 2.8958 - classification_loss: 1.1252
  30/1000 [..............................] - ETA: 36:13 - loss: 4.0216 - regression_loss: 2.8966 - classification_loss: 1.1250
  31/1000 [..............................] - ETA: 35:49 - loss: 4.0157 - regression_loss: 2.8910 - classification_loss: 1.1247
  32/1000 [..............................] - ETA: 35:58 - loss: 4.0112 - regression_loss: 2.8868 - classification_loss: 1.1244
  33/1000 [..............................] - ETA: 36:09 - loss: 4.0063 - regression_loss: 2.8822 - classification_loss: 1.1241
  34/1000 [>.............................] - ETA: 36:04 - loss: 4.0020 - regression_loss: 2.8781 - classification_loss: 1.1238
  35/1000 [>.............................] - ETA: 36:07 - loss: 3.9955 - regression_loss: 2.8720 - classification_loss: 1.1235
  36/1000 [>.............................] - ETA: 36:12 - loss: 3.9896 - regression_loss: 2.8665 - classification_loss: 1.1231
  37/1000 [>.............................] - ETA: 36:07 - loss: 3.9843 - regression_loss: 2.8615 - classification_loss: 1.1228
  38/1000 [>.............................] - ETA: 36:16 - loss: 3.9778 - regression_loss: 2.8554 - classification_loss: 1.1223
  39/1000 [>.............................] - ETA: 36:18 - loss: 3.9706 - regression_loss: 2.8487 - classification_loss: 1.1219
  40/1000 [>.............................] - ETA: 36:02 - loss: 3.9703 - regression_loss: 2.8487 - classification_loss: 1.1216
  41/1000 [>.............................] - ETA: 35:44 - loss: 3.9606 - regression_loss: 2.8396 - classification_loss: 1.1210
  42/1000 [>.............................] - ETA: 35:49 - loss: 3.9532 - regression_loss: 2.8327 - classification_loss: 1.1205
  43/1000 [>.............................] - ETA: 35:45 - loss: 3.9459 - regression_loss: 2.8260 - classification_loss: 1.1199
  44/1000 [>.............................] - ETA: 35:51 - loss: 3.9382 - regression_loss: 2.8190 - classification_loss: 1.1192
  45/1000 [>.............................] - ETA: 35:35 - loss: 3.9270 - regression_loss: 2.8085 - classification_loss: 1.1185
  46/1000 [>.............................] - ETA: 35:42 - loss: 3.9195 - regression_loss: 2.8017 - classification_loss: 1.1177
  47/1000 [>.............................] - ETA: 35:42 - loss: 3.9120 - regression_loss: 2.7950 - classification_loss: 1.1169
  48/1000 [>.............................] - ETA: 35:30 - loss: 3.9119 - regression_loss: 2.7954 - classification_loss: 1.1164
  49/1000 [>.............................] - ETA: 35:33 - loss: 3.9047 - regression_loss: 2.7893 - classification_loss: 1.1154
  50/1000 [>.............................] - ETA: 35:21 - loss: 3.9046 - regression_loss: 2.7897 - classification_loss: 1.1148
  51/1000 [>.............................] - ETA: 35:22 - loss: 3.8978 - regression_loss: 2.7840 - classification_loss: 1.1138
  52/1000 [>.............................] - ETA: 35:18 - loss: 3.8912 - regression_loss: 2.7786 - classification_loss: 1.1127
  53/1000 [>.............................] - ETA: 35:03 - loss: 3.8810 - regression_loss: 2.7697 - classification_loss: 1.1113
  54/1000 [>.............................] - ETA: 35:09 - loss: 3.8749 - regression_loss: 2.7649 - classification_loss: 1.1101
  55/1000 [>.............................] - ETA: 35:13 - loss: 3.8686 - regression_loss: 2.7598 - classification_loss: 1.1088
  56/1000 [>.............................] - ETA: 35:15 - loss: 3.8619 - regression_loss: 2.7544 - classification_loss: 1.1075
  57/1000 [>.............................] - ETA: 35:21 - loss: 3.8553 - regression_loss: 2.7491 - classification_loss: 1.1062
  58/1000 [>.............................] - ETA: 35:24 - loss: 3.8487 - regression_loss: 2.7438 - classification_loss: 1.1049
  59/1000 [>.............................] - ETA: 35:24 - loss: 3.8421 - regression_loss: 2.7384 - classification_loss: 1.1037
  60/1000 [>.............................] - ETA: 35:13 - loss: 3.8412 - regression_loss: 2.7382 - classification_loss: 1.1030
  61/1000 [>.............................] - ETA: 35:15 - loss: 3.8344 - regression_loss: 2.7327 - classification_loss: 1.1016
  62/1000 [>.............................] - ETA: 35:03 - loss: 3.8250 - regression_loss: 2.7250 - classification_loss: 1.1001
  63/1000 [>.............................] - ETA: 34:59 - loss: 3.8186 - regression_loss: 2.7200 - classification_loss: 1.0985
  64/1000 [>.............................] - ETA: 34:59 - loss: 3.8121 - regression_loss: 2.7150 - classification_loss: 1.0971
  65/1000 [>.............................] - ETA: 34:50 - loss: 3.8108 - regression_loss: 2.7147 - classification_loss: 1.0961
  66/1000 [>.............................] - ETA: 34:54 - loss: 3.8040 - regression_loss: 2.7097 - classification_loss: 1.0942
  67/1000 [=>............................] - ETA: 34:42 - loss: 3.7946 - regression_loss: 2.7027 - classification_loss: 1.0919
  68/1000 [=>............................] - ETA: 34:45 - loss: 3.7878 - regression_loss: 2.6982 - classification_loss: 1.0896
  69/1000 [=>............................] - ETA: 34:46 - loss: 3.7805 - regression_loss: 2.6935 - classification_loss: 1.0871
  70/1000 [=>............................] - ETA: 34:43 - loss: 3.7733 - regression_loss: 2.6891 - classification_loss: 1.0842
  71/1000 [=>............................] - ETA: 34:39 - loss: 3.7658 - regression_loss: 2.6848 - classification_loss: 1.0810
  72/1000 [=>............................] - ETA: 34:28 - loss: 3.7560 - regression_loss: 2.6784 - classification_loss: 1.0776
  73/1000 [=>............................] - ETA: 34:30 - loss: 3.7477 - regression_loss: 2.6742 - classification_loss: 1.0735
  74/1000 [=>............................] - ETA: 34:21 - loss: 3.7450 - regression_loss: 2.6737 - classification_loss: 1.0714
  75/1000 [=>............................] - ETA: 34:25 - loss: 3.7361 - regression_loss: 2.6699 - classification_loss: 1.0662
  76/1000 [=>............................] - ETA: 34:27 - loss: 3.7268 - regression_loss: 2.6663 - classification_loss: 1.0604
  77/1000 [=>............................] - ETA: 34:27 - loss: 3.7176 - regression_loss: 2.6629 - classification_loss: 1.0547
  78/1000 [=>............................] - ETA: 34:17 - loss: 3.7444 - regression_loss: 2.6572 - classification_loss: 1.0873
  79/1000 [=>............................] - ETA: 34:18 - loss: 3.7334 - regression_loss: 2.6539 - classification_loss: 1.0796
  80/1000 [=>............................] - ETA: 34:18 - loss: 3.7230 - regression_loss: 2.6506 - classification_loss: 1.0724
  81/1000 [=>............................] - ETA: 34:21 - loss: 3.7121 - regression_loss: 2.6474 - classification_loss: 1.0647
  82/1000 [=>............................] - ETA: 34:13 - loss: 3.7096 - regression_loss: 2.6469 - classification_loss: 1.0627
  83/1000 [=>............................] - ETA: 34:09 - loss: 3.6993 - regression_loss: 2.6435 - classification_loss: 1.0558
  84/1000 [=>............................] - ETA: 34:10 - loss: 3.6886 - regression_loss: 2.6400 - classification_loss: 1.0486
  85/1000 [=>............................] - ETA: 34:11 - loss: 3.6779 - regression_loss: 2.6364 - classification_loss: 1.0414
  86/1000 [=>............................] - ETA: 34:03 - loss: 3.6755 - regression_loss: 2.6358 - classification_loss: 1.0396
  87/1000 [=>............................] - ETA: 34:05 - loss: 3.6645 - regression_loss: 2.6324 - classification_loss: 1.0321
  88/1000 [=>............................] - ETA: 33:56 - loss: 3.7831 - regression_loss: 2.6278 - classification_loss: 1.1553
  89/1000 [=>............................] - ETA: 33:56 - loss: 3.7719 - regression_loss: 2.6248 - classification_loss: 1.1471
  90/1000 [=>............................] - ETA: 33:57 - loss: 3.7606 - regression_loss: 2.6220 - classification_loss: 1.1386
  91/1000 [=>............................] - ETA: 33:54 - loss: 3.7499 - regression_loss: 2.6194 - classification_loss: 1.1306
  92/1000 [=>............................] - ETA: 33:54 - loss: 3.7390 - regression_loss: 2.6165 - classification_loss: 1.1224
  93/1000 [=>............................] - ETA: 33:51 - loss: 3.7284 - regression_loss: 2.6139 - classification_loss: 1.1145
  94/1000 [=>............................] - ETA: 33:53 - loss: 3.7172 - regression_loss: 2.6110 - classification_loss: 1.1063
  95/1000 [=>............................] - ETA: 33:53 - loss: 3.7066 - regression_loss: 2.6082 - classification_loss: 1.0984
  96/1000 [=>............................] - ETA: 33:44 - loss: 3.8887 - regression_loss: 2.6044 - classification_loss: 1.2843
  97/1000 [=>............................] - ETA: 33:37 - loss: 3.8839 - regression_loss: 2.6042 - classification_loss: 1.2797
  98/1000 [=>............................] - ETA: 33:38 - loss: 3.8717 - regression_loss: 2.6016 - classification_loss: 1.2701
  99/1000 [=>............................] - ETA: 33:30 - loss: 4.1078 - regression_loss: 2.5978 - classification_loss: 1.5100
 100/1000 [==>...........................] - ETA: 33:26 - loss: 4.0938 - regression_loss: 2.5953 - classification_loss: 1.4985
 101/1000 [==>...........................] - ETA: 33:27 - loss: 4.0794 - regression_loss: 2.5926 - classification_loss: 1.4868
 102/1000 [==>...........................] - ETA: 33:28 - loss: 4.0650 - regression_loss: 2.5899 - classification_loss: 1.4752
 103/1000 [==>...........................] - ETA: 33:29 - loss: 4.0513 - regression_loss: 2.5874 - classification_loss: 1.4639
 104/1000 [==>...........................] - ETA: 33:28 - loss: 4.0380 - regression_loss: 2.5849 - classification_loss: 1.4531
 105/1000 [==>...........................] - ETA: 33:21 - loss: 4.0313 - regression_loss: 2.5851 - classification_loss: 1.4462
 106/1000 [==>...........................] - ETA: 33:23 - loss: 4.0182 - regression_loss: 2.5828 - classification_loss: 1.4354
 107/1000 [==>...........................] - ETA: 33:23 - loss: 4.0051 - regression_loss: 2.5802 - classification_loss: 1.4249
 108/1000 [==>...........................] - ETA: 33:19 - loss: 3.9927 - regression_loss: 2.5779 - classification_loss: 1.4148
 109/1000 [==>...........................] - ETA: 33:21 - loss: 3.9799 - regression_loss: 2.5755 - classification_loss: 1.4044
 110/1000 [==>...........................] - ETA: 33:13 - loss: 4.1149 - regression_loss: 2.5722 - classification_loss: 1.5427
 111/1000 [==>...........................] - ETA: 33:12 - loss: 4.1016 - regression_loss: 2.5699 - classification_loss: 1.5317
 112/1000 [==>...........................] - ETA: 33:06 - loss: 4.0942 - regression_loss: 2.5702 - classification_loss: 1.5240
 113/1000 [==>...........................] - ETA: 33:07 - loss: 4.0813 - regression_loss: 2.5680 - classification_loss: 1.5132
 114/1000 [==>...........................] - ETA: 32:59 - loss: 4.1380 - regression_loss: 2.5650 - classification_loss: 1.5730
 115/1000 [==>...........................] - ETA: 32:59 - loss: 4.1249 - regression_loss: 2.5628 - classification_loss: 1.5620
 116/1000 [==>...........................] - ETA: 32:56 - loss: 4.1123 - regression_loss: 2.5608 - classification_loss: 1.5515
 117/1000 [==>...........................] - ETA: 32:50 - loss: 4.1046 - regression_loss: 2.5610 - classification_loss: 1.5435
 118/1000 [==>...........................] - ETA: 32:52 - loss: 4.0918 - regression_loss: 2.5588 - classification_loss: 1.5329
 119/1000 [==>...........................] - ETA: 32:51 - loss: 4.0795 - regression_loss: 2.5568 - classification_loss: 1.5227
 120/1000 [==>...........................] - ETA: 32:52 - loss: 4.0670 - regression_loss: 2.5546 - classification_loss: 1.5123
 121/1000 [==>...........................] - ETA: 32:45 - loss: 4.1921 - regression_loss: 2.5519 - classification_loss: 1.6403
 122/1000 [==>...........................] - ETA: 32:45 - loss: 4.1791 - regression_loss: 2.5500 - classification_loss: 1.6291
 123/1000 [==>...........................] - ETA: 32:44 - loss: 4.1666 - regression_loss: 2.5480 - classification_loss: 1.6186
 124/1000 [==>...........................] - ETA: 32:41 - loss: 4.1542 - regression_loss: 2.5461 - classification_loss: 1.6080
 125/1000 [==>...........................] - ETA: 32:35 - loss: 4.1459 - regression_loss: 2.5465 - classification_loss: 1.5994
 126/1000 [==>...........................] - ETA: 32:35 - loss: 4.1335 - regression_loss: 2.5445 - classification_loss: 1.5890
 127/1000 [==>...........................] - ETA: 32:32 - loss: 4.1217 - regression_loss: 2.5427 - classification_loss: 1.5790
 128/1000 [==>...........................] - ETA: 32:26 - loss: 4.1134 - regression_loss: 2.5431 - classification_loss: 1.5704
 129/1000 [==>...........................] - ETA: 32:27 - loss: 4.1015 - regression_loss: 2.5411 - classification_loss: 1.5603
 130/1000 [==>...........................] - ETA: 32:20 - loss: 4.1648 - regression_loss: 2.5386 - classification_loss: 1.6261
 131/1000 [==>...........................] - ETA: 32:20 - loss: 4.1528 - regression_loss: 2.5369 - classification_loss: 1.6158
 132/1000 [==>...........................] - ETA: 32:19 - loss: 4.1412 - regression_loss: 2.5352 - classification_loss: 1.6060
 133/1000 [==>...........................] - ETA: 32:19 - loss: 4.1295 - regression_loss: 2.5334 - classification_loss: 1.5961
 134/1000 [===>..........................] - ETA: 32:17 - loss: 4.1181 - regression_loss: 2.5317 - classification_loss: 1.5864
 135/1000 [===>..........................] - ETA: 32:11 - loss: 4.1100 - regression_loss: 2.5321 - classification_loss: 1.5779
 136/1000 [===>..........................] - ETA: 32:12 - loss: 4.0986 - regression_loss: 2.5303 - classification_loss: 1.5683
 137/1000 [===>..........................] - ETA: 32:11 - loss: 4.0874 - regression_loss: 2.5285 - classification_loss: 1.5589
 138/1000 [===>..........................] - ETA: 32:11 - loss: 4.0765 - regression_loss: 2.5269 - classification_loss: 1.5495
 139/1000 [===>..........................] - ETA: 32:04 - loss: 4.1579 - regression_loss: 2.5246 - classification_loss: 1.6333
 140/1000 [===>..........................] - ETA: 32:03 - loss: 4.1469 - regression_loss: 2.5230 - classification_loss: 1.6239
 141/1000 [===>..........................] - ETA: 32:02 - loss: 4.1360 - regression_loss: 2.5214 - classification_loss: 1.6145
 142/1000 [===>..........................] - ETA: 31:57 - loss: 4.1284 - regression_loss: 2.5220 - classification_loss: 1.6064
 143/1000 [===>..........................] - ETA: 31:51 - loss: 4.1777 - regression_loss: 2.5198 - classification_loss: 1.6579
 144/1000 [===>..........................] - ETA: 31:48 - loss: 4.1668 - regression_loss: 2.5183 - classification_loss: 1.6485
 145/1000 [===>..........................] - ETA: 31:48 - loss: 4.1560 - regression_loss: 2.5169 - classification_loss: 1.6391
 146/1000 [===>..........................] - ETA: 31:48 - loss: 4.1451 - regression_loss: 2.5153 - classification_loss: 1.6299
 147/1000 [===>..........................] - ETA: 31:48 - loss: 4.1344 - regression_loss: 2.5137 - classification_loss: 1.6207
 148/1000 [===>..........................] - ETA: 31:47 - loss: 4.1240 - regression_loss: 2.5122 - classification_loss: 1.6118
 149/1000 [===>..........................] - ETA: 31:46 - loss: 4.1136 - regression_loss: 2.5108 - classification_loss: 1.6028
 150/1000 [===>..........................] - ETA: 31:41 - loss: 4.1070 - regression_loss: 2.5117 - classification_loss: 1.5953
 151/1000 [===>..........................] - ETA: 31:39 - loss: 4.0970 - regression_loss: 2.5103 - classification_loss: 1.5868
 152/1000 [===>..........................] - ETA: 31:38 - loss: 4.0870 - regression_loss: 2.5088 - classification_loss: 1.5783
 153/1000 [===>..........................] - ETA: 31:38 - loss: 4.0770 - regression_loss: 2.5073 - classification_loss: 1.5697
 154/1000 [===>..........................] - ETA: 31:32 - loss: 4.1552 - regression_loss: 2.5053 - classification_loss: 1.6499
 155/1000 [===>..........................] - ETA: 31:26 - loss: 4.2028 - regression_loss: 2.5033 - classification_loss: 1.6995
 156/1000 [===>..........................] - ETA: 31:21 - loss: 4.1954 - regression_loss: 2.5039 - classification_loss: 1.6914
 157/1000 [===>..........................] - ETA: 31:21 - loss: 4.1854 - regression_loss: 2.5027 - classification_loss: 1.6827
 158/1000 [===>..........................] - ETA: 31:21 - loss: 4.1754 - regression_loss: 2.5013 - classification_loss: 1.6741
 159/1000 [===>..........................] - ETA: 31:21 - loss: 4.1657 - regression_loss: 2.5000 - classification_loss: 1.6658
 160/1000 [===>..........................] - ETA: 31:19 - loss: 4.1563 - regression_loss: 2.4987 - classification_loss: 1.6576
 161/1000 [===>..........................] - ETA: 31:17 - loss: 4.1468 - regression_loss: 2.4974 - classification_loss: 1.6494
 162/1000 [===>..........................] - ETA: 31:12 - loss: 4.1397 - regression_loss: 2.4981 - classification_loss: 1.6417
 163/1000 [===>..........................] - ETA: 31:11 - loss: 4.1301 - regression_loss: 2.4968 - classification_loss: 1.6333
 164/1000 [===>..........................] - ETA: 31:10 - loss: 4.1208 - regression_loss: 2.4956 - classification_loss: 1.6252
 165/1000 [===>..........................] - ETA: 31:10 - loss: 4.1113 - regression_loss: 2.4944 - classification_loss: 1.6170
 166/1000 [===>..........................] - ETA: 31:07 - loss: 4.1023 - regression_loss: 2.4931 - classification_loss: 1.6092
 167/1000 [====>.........................] - ETA: 31:06 - loss: 4.0932 - regression_loss: 2.4919 - classification_loss: 1.6013
 168/1000 [====>.........................] - ETA: 31:01 - loss: 4.2056 - regression_loss: 2.4901 - classification_loss: 1.7156
 169/1000 [====>.........................] - ETA: 30:55 - loss: 4.2903 - regression_loss: 2.4883 - classification_loss: 1.8020
 170/1000 [====>.........................] - ETA: 30:52 - loss: 4.2802 - regression_loss: 2.4871 - classification_loss: 1.7931
 171/1000 [====>.........................] - ETA: 30:52 - loss: 4.2702 - regression_loss: 2.4859 - classification_loss: 1.7843
 172/1000 [====>.........................] - ETA: 30:51 - loss: 4.2604 - regression_loss: 2.4848 - classification_loss: 1.7755
 173/1000 [====>.........................] - ETA: 30:50 - loss: 4.2508 - regression_loss: 2.4837 - classification_loss: 1.7670
 174/1000 [====>.........................] - ETA: 30:45 - loss: 4.2436 - regression_loss: 2.4843 - classification_loss: 1.7592
 175/1000 [====>.........................] - ETA: 30:45 - loss: 4.2339 - regression_loss: 2.4832 - classification_loss: 1.7507
 176/1000 [====>.........................] - ETA: 30:45 - loss: 4.2242 - regression_loss: 2.4820 - classification_loss: 1.7422
 177/1000 [====>.........................] - ETA: 30:43 - loss: 4.2150 - regression_loss: 2.4809 - classification_loss: 1.7341
 178/1000 [====>.........................] - ETA: 30:42 - loss: 4.2057 - regression_loss: 2.4799 - classification_loss: 1.7258
 179/1000 [====>.........................] - ETA: 30:38 - loss: 4.1992 - regression_loss: 2.4806 - classification_loss: 1.7186
 180/1000 [====>.........................] - ETA: 30:35 - loss: 4.1902 - regression_loss: 2.4795 - classification_loss: 1.7107
 181/1000 [====>.........................] - ETA: 30:34 - loss: 4.1811 - regression_loss: 2.4784 - classification_loss: 1.7027
 182/1000 [====>.........................] - ETA: 30:29 - loss: 4.2268 - regression_loss: 2.4768 - classification_loss: 1.7500
 183/1000 [====>.........................] - ETA: 30:28 - loss: 4.2176 - regression_loss: 2.4757 - classification_loss: 1.7419
 184/1000 [====>.........................] - ETA: 30:26 - loss: 4.2089 - regression_loss: 2.4747 - classification_loss: 1.7342
 185/1000 [====>.........................] - ETA: 30:25 - loss: 4.2000 - regression_loss: 2.4736 - classification_loss: 1.7264
 186/1000 [====>.........................] - ETA: 30:23 - loss: 4.1914 - regression_loss: 2.4726 - classification_loss: 1.7188
 187/1000 [====>.........................] - ETA: 30:18 - loss: 4.2195 - regression_loss: 2.4710 - classification_loss: 1.7485
 188/1000 [====>.........................] - ETA: 30:13 - loss: 4.2129 - regression_loss: 2.4716 - classification_loss: 1.7413
 189/1000 [====>.........................] - ETA: 30:13 - loss: 4.2042 - regression_loss: 2.4707 - classification_loss: 1.7335
 190/1000 [====>.........................] - ETA: 30:08 - loss: 4.1976 - regression_loss: 2.4712 - classification_loss: 1.7264
 191/1000 [====>.........................] - ETA: 30:07 - loss: 4.1890 - regression_loss: 2.4703 - classification_loss: 1.7187
 192/1000 [====>.........................] - ETA: 30:02 - loss: 4.2551 - regression_loss: 2.4687 - classification_loss: 1.7864
 193/1000 [====>.........................] - ETA: 30:01 - loss: 4.2466 - regression_loss: 2.4679 - classification_loss: 1.7787
 194/1000 [====>.........................] - ETA: 29:58 - loss: 4.2380 - regression_loss: 2.4669 - classification_loss: 1.7711
 195/1000 [====>.........................] - ETA: 29:58 - loss: 4.2293 - regression_loss: 2.4660 - classification_loss: 1.7633
 196/1000 [====>.........................] - ETA: 29:57 - loss: 4.2207 - regression_loss: 2.4650 - classification_loss: 1.7557
 197/1000 [====>.........................] - ETA: 29:56 - loss: 4.2122 - regression_loss: 2.4641 - classification_loss: 1.7481
 198/1000 [====>.........................] - ETA: 29:51 - loss: 4.2057 - regression_loss: 2.4646 - classification_loss: 1.7411
 199/1000 [====>.........................] - ETA: 29:49 - loss: 4.1974 - regression_loss: 2.4637 - classification_loss: 1.7338
 200/1000 [=====>........................] - ETA: 29:47 - loss: 4.1892 - regression_loss: 2.4627 - classification_loss: 1.7265
 201/1000 [=====>........................] - ETA: 29:46 - loss: 4.1812 - regression_loss: 2.4619 - classification_loss: 1.7194
 202/1000 [=====>........................] - ETA: 29:45 - loss: 4.1730 - regression_loss: 2.4609 - classification_loss: 1.7121
 203/1000 [=====>........................] - ETA: 29:41 - loss: 4.2306 - regression_loss: 2.4594 - classification_loss: 1.7712
 204/1000 [=====>........................] - ETA: 29:39 - loss: 4.2225 - regression_loss: 2.4586 - classification_loss: 1.7640
 205/1000 [=====>........................] - ETA: 29:38 - loss: 4.2143 - regression_loss: 2.4576 - classification_loss: 1.7566
 206/1000 [=====>........................] - ETA: 29:37 - loss: 4.2062 - regression_loss: 2.4569 - classification_loss: 1.7494
 207/1000 [=====>........................] - ETA: 29:36 - loss: 4.1982 - regression_loss: 2.4560 - classification_loss: 1.7422
 208/1000 [=====>........................] - ETA: 29:32 - loss: 4.1924 - regression_loss: 2.4566 - classification_loss: 1.7357
 209/1000 [=====>........................] - ETA: 29:27 - loss: 4.2480 - regression_loss: 2.4553 - classification_loss: 1.7927
 210/1000 [=====>........................] - ETA: 29:25 - loss: 4.2399 - regression_loss: 2.4545 - classification_loss: 1.7855
 211/1000 [=====>........................] - ETA: 29:22 - loss: 4.2320 - regression_loss: 2.4537 - classification_loss: 1.7783
 212/1000 [=====>........................] - ETA: 29:22 - loss: 4.2239 - regression_loss: 2.4528 - classification_loss: 1.7711
 213/1000 [=====>........................] - ETA: 29:20 - loss: 4.2161 - regression_loss: 2.4519 - classification_loss: 1.7642
 214/1000 [=====>........................] - ETA: 29:19 - loss: 4.2083 - regression_loss: 2.4512 - classification_loss: 1.7571
 215/1000 [=====>........................] - ETA: 29:15 - loss: 4.2024 - regression_loss: 2.4517 - classification_loss: 1.7507
 216/1000 [=====>........................] - ETA: 29:10 - loss: 4.2562 - regression_loss: 2.4503 - classification_loss: 1.8059
 217/1000 [=====>........................] - ETA: 29:09 - loss: 4.2482 - regression_loss: 2.4494 - classification_loss: 1.7988
 218/1000 [=====>........................] - ETA: 29:09 - loss: 4.2402 - regression_loss: 2.4486 - classification_loss: 1.7917
 219/1000 [=====>........................] - ETA: 29:04 - loss: 4.2789 - regression_loss: 2.4472 - classification_loss: 1.8318
 220/1000 [=====>........................] - ETA: 29:01 - loss: 4.2711 - regression_loss: 2.4464 - classification_loss: 1.8247
 221/1000 [=====>........................] - ETA: 28:57 - loss: 4.2649 - regression_loss: 2.4468 - classification_loss: 1.8181
 222/1000 [=====>........................] - ETA: 28:56 - loss: 4.2573 - regression_loss: 2.4461 - classification_loss: 1.8112
 223/1000 [=====>........................] - ETA: 28:55 - loss: 4.2496 - regression_loss: 2.4453 - classification_loss: 1.8043
 224/1000 [=====>........................] - ETA: 28:54 - loss: 4.2419 - regression_loss: 2.4445 - classification_loss: 1.7974
 225/1000 [=====>........................] - ETA: 28:53 - loss: 4.2342 - regression_loss: 2.4437 - classification_loss: 1.7905
 226/1000 [=====>........................] - ETA: 28:51 - loss: 4.2269 - regression_loss: 2.4430 - classification_loss: 1.7839
 227/1000 [=====>........................] - ETA: 28:47 - loss: 4.2878 - regression_loss: 2.4416 - classification_loss: 1.8462
 228/1000 [=====>........................] - ETA: 28:44 - loss: 4.2802 - regression_loss: 2.4409 - classification_loss: 1.8393
 229/1000 [=====>........................] - ETA: 28:43 - loss: 4.2726 - regression_loss: 2.4402 - classification_loss: 1.8324
 230/1000 [=====>........................] - ETA: 28:39 - loss: 4.2667 - regression_loss: 2.4407 - classification_loss: 1.8260
 231/1000 [=====>........................] - ETA: 28:38 - loss: 4.2591 - regression_loss: 2.4399 - classification_loss: 1.8192
 232/1000 [=====>........................] - ETA: 28:34 - loss: 4.2859 - regression_loss: 2.4385 - classification_loss: 1.8473
 233/1000 [=====>........................] - ETA: 28:33 - loss: 4.2783 - regression_loss: 2.4377 - classification_loss: 1.8405
 234/1000 [======>.......................] - ETA: 28:31 - loss: 4.2710 - regression_loss: 2.4371 - classification_loss: 1.8339
 235/1000 [======>.......................] - ETA: 28:27 - loss: 4.2650 - regression_loss: 2.4374 - classification_loss: 1.8276
 236/1000 [======>.......................] - ETA: 28:26 - loss: 4.2575 - regression_loss: 2.4366 - classification_loss: 1.8209
 237/1000 [======>.......................] - ETA: 28:24 - loss: 4.2503 - regression_loss: 2.4359 - classification_loss: 1.8144
 238/1000 [======>.......................] - ETA: 28:22 - loss: 4.2430 - regression_loss: 2.4352 - classification_loss: 1.8078
 239/1000 [======>.......................] - ETA: 28:21 - loss: 4.2358 - regression_loss: 2.4345 - classification_loss: 1.8014
 240/1000 [======>.......................] - ETA: 28:20 - loss: 4.2287 - regression_loss: 2.4338 - classification_loss: 1.7949
 241/1000 [======>.......................] - ETA: 28:16 - loss: 4.2229 - regression_loss: 2.4341 - classification_loss: 1.7889
 242/1000 [======>.......................] - ETA: 28:12 - loss: 4.2546 - regression_loss: 2.4327 - classification_loss: 1.8219
 243/1000 [======>.......................] - ETA: 28:11 - loss: 4.2475 - regression_loss: 2.4320 - classification_loss: 1.8155
 244/1000 [======>.......................] - ETA: 28:09 - loss: 4.2405 - regression_loss: 2.4313 - classification_loss: 1.8092
 245/1000 [======>.......................] - ETA: 28:07 - loss: 4.2338 - regression_loss: 2.4307 - classification_loss: 1.8030
 246/1000 [======>.......................] - ETA: 28:06 - loss: 4.2268 - regression_loss: 2.4300 - classification_loss: 1.7968
 247/1000 [======>.......................] - ETA: 28:02 - loss: 4.2211 - regression_loss: 2.4302 - classification_loss: 1.7909
 248/1000 [======>.......................] - ETA: 27:59 - loss: 4.2143 - regression_loss: 2.4295 - classification_loss: 1.7848
 249/1000 [======>.......................] - ETA: 27:58 - loss: 4.2075 - regression_loss: 2.4289 - classification_loss: 1.7786
 250/1000 [======>.......................] - ETA: 27:57 - loss: 4.2009 - regression_loss: 2.4283 - classification_loss: 1.7727
 251/1000 [======>.......................] - ETA: 27:56 - loss: 4.1942 - regression_loss: 2.4276 - classification_loss: 1.7666
 252/1000 [======>.......................] - ETA: 27:51 - loss: 4.2419 - regression_loss: 2.4263 - classification_loss: 1.8156
 253/1000 [======>.......................] - ETA: 27:50 - loss: 4.2352 - regression_loss: 2.4256 - classification_loss: 1.8095
 254/1000 [======>.......................] - ETA: 27:46 - loss: 4.2584 - regression_loss: 2.4244 - classification_loss: 1.8340
 255/1000 [======>.......................] - ETA: 27:43 - loss: 4.2517 - regression_loss: 2.4238 - classification_loss: 1.8279
 256/1000 [======>.......................] - ETA: 27:42 - loss: 4.2452 - regression_loss: 2.4233 - classification_loss: 1.8219
 257/1000 [======>.......................] - ETA: 27:38 - loss: 4.2397 - regression_loss: 2.4236 - classification_loss: 1.8162
 258/1000 [======>.......................] - ETA: 27:37 - loss: 4.2332 - regression_loss: 2.4229 - classification_loss: 1.8102
 259/1000 [======>.......................] - ETA: 27:36 - loss: 4.2267 - regression_loss: 2.4223 - classification_loss: 1.8044
 260/1000 [======>.......................] - ETA: 27:35 - loss: 4.2201 - regression_loss: 2.4216 - classification_loss: 1.7984
 261/1000 [======>.......................] - ETA: 27:33 - loss: 4.2136 - regression_loss: 2.4211 - classification_loss: 1.7925
 262/1000 [======>.......................] - ETA: 27:32 - loss: 4.2070 - regression_loss: 2.4204 - classification_loss: 1.7866
 263/1000 [======>.......................] - ETA: 27:29 - loss: 4.2018 - regression_loss: 2.4205 - classification_loss: 1.7814
 264/1000 [======>.......................] - ETA: 27:25 - loss: 4.2966 - regression_loss: 2.4193 - classification_loss: 1.8773
 265/1000 [======>.......................] - ETA: 27:23 - loss: 4.2903 - regression_loss: 2.4188 - classification_loss: 1.8715
 266/1000 [======>.......................] - ETA: 27:21 - loss: 4.2839 - regression_loss: 2.4183 - classification_loss: 1.8656
 267/1000 [=======>......................] - ETA: 27:19 - loss: 4.2773 - regression_loss: 2.4177 - classification_loss: 1.8596
 268/1000 [=======>......................] - ETA: 27:17 - loss: 4.2710 - regression_loss: 2.4172 - classification_loss: 1.8537
 269/1000 [=======>......................] - ETA: 27:15 - loss: 4.2645 - regression_loss: 2.4167 - classification_loss: 1.8479
 270/1000 [=======>......................] - ETA: 27:11 - loss: 4.2794 - regression_loss: 2.4155 - classification_loss: 1.8639
 271/1000 [=======>......................] - ETA: 27:07 - loss: 4.2739 - regression_loss: 2.4156 - classification_loss: 1.8583
 272/1000 [=======>......................] - ETA: 27:06 - loss: 4.2676 - regression_loss: 2.4149 - classification_loss: 1.8526
 273/1000 [=======>......................] - ETA: 27:05 - loss: 4.2615 - regression_loss: 2.4145 - classification_loss: 1.8470
 274/1000 [=======>......................] - ETA: 27:03 - loss: 4.2552 - regression_loss: 2.4139 - classification_loss: 1.8414
 275/1000 [=======>......................] - ETA: 27:00 - loss: 4.2499 - regression_loss: 2.4140 - classification_loss: 1.8360
 276/1000 [=======>......................] - ETA: 26:59 - loss: 4.2436 - regression_loss: 2.4134 - classification_loss: 1.8303
 277/1000 [=======>......................] - ETA: 26:56 - loss: 4.2374 - regression_loss: 2.4128 - classification_loss: 1.8246
 278/1000 [=======>......................] - ETA: 26:52 - loss: 4.2732 - regression_loss: 2.4116 - classification_loss: 1.8616
 279/1000 [=======>......................] - ETA: 26:51 - loss: 4.2671 - regression_loss: 2.4111 - classification_loss: 1.8560
 280/1000 [=======>......................] - ETA: 26:49 - loss: 4.2608 - regression_loss: 2.4105 - classification_loss: 1.8503
 281/1000 [=======>......................] - ETA: 26:46 - loss: 4.2555 - regression_loss: 2.4106 - classification_loss: 1.8449
 282/1000 [=======>......................] - ETA: 26:45 - loss: 4.2492 - regression_loss: 2.4099 - classification_loss: 1.8393
 283/1000 [=======>......................] - ETA: 26:41 - loss: 4.2838 - regression_loss: 2.4088 - classification_loss: 1.8750
 284/1000 [=======>......................] - ETA: 26:40 - loss: 4.2775 - regression_loss: 2.4082 - classification_loss: 1.8693
 285/1000 [=======>......................] - ETA: 26:37 - loss: 4.2713 - regression_loss: 2.4076 - classification_loss: 1.8637
 286/1000 [=======>......................] - ETA: 26:36 - loss: 4.2652 - regression_loss: 2.4071 - classification_loss: 1.8581
 287/1000 [=======>......................] - ETA: 26:34 - loss: 4.2592 - regression_loss: 2.4066 - classification_loss: 1.8526
 288/1000 [=======>......................] - ETA: 26:33 - loss: 4.2531 - regression_loss: 2.4060 - classification_loss: 1.8471
 289/1000 [=======>......................] - ETA: 26:31 - loss: 4.2469 - regression_loss: 2.4054 - classification_loss: 1.8415
 290/1000 [=======>......................] - ETA: 26:30 - loss: 4.2410 - regression_loss: 2.4049 - classification_loss: 1.8362
 291/1000 [=======>......................] - ETA: 26:28 - loss: 4.2351 - regression_loss: 2.4044 - classification_loss: 1.8307
 292/1000 [=======>......................] - ETA: 26:24 - loss: 4.2848 - regression_loss: 2.4032 - classification_loss: 1.8816
 293/1000 [=======>......................] - ETA: 26:22 - loss: 4.2787 - regression_loss: 2.4026 - classification_loss: 1.8761
 294/1000 [=======>......................] - ETA: 26:18 - loss: 4.2736 - regression_loss: 2.4027 - classification_loss: 1.8708
 295/1000 [=======>......................] - ETA: 26:17 - loss: 4.2675 - regression_loss: 2.4021 - classification_loss: 1.8654
 296/1000 [=======>......................] - ETA: 26:15 - loss: 4.2616 - regression_loss: 2.4016 - classification_loss: 1.8600
 297/1000 [=======>......................] - ETA: 26:14 - loss: 4.2557 - regression_loss: 2.4011 - classification_loss: 1.8546
 298/1000 [=======>......................] - ETA: 26:12 - loss: 4.2497 - regression_loss: 2.4005 - classification_loss: 1.8492
 299/1000 [=======>......................] - ETA: 26:10 - loss: 4.2439 - regression_loss: 2.3999 - classification_loss: 1.8439
 300/1000 [========>.....................] - ETA: 26:06 - loss: 4.2389 - regression_loss: 2.4000 - classification_loss: 1.8389
 301/1000 [========>.....................] - ETA: 26:03 - loss: 4.2870 - regression_loss: 2.3988 - classification_loss: 1.8882
 302/1000 [========>.....................] - ETA: 26:01 - loss: 4.2811 - regression_loss: 2.3983 - classification_loss: 1.8828
 303/1000 [========>.....................] - ETA: 25:57 - loss: 4.3248 - regression_loss: 2.3972 - classification_loss: 1.9276
 304/1000 [========>.....................] - ETA: 25:56 - loss: 4.3189 - regression_loss: 2.3967 - classification_loss: 1.9222
 305/1000 [========>.....................] - ETA: 25:54 - loss: 4.3128 - regression_loss: 2.3961 - classification_loss: 1.9167
 306/1000 [========>.....................] - ETA: 25:53 - loss: 4.3068 - regression_loss: 2.3955 - classification_loss: 1.9113
 307/1000 [========>.....................] - ETA: 25:49 - loss: 4.3016 - regression_loss: 2.3955 - classification_loss: 1.9061
 308/1000 [========>.....................] - ETA: 25:47 - loss: 4.2957 - regression_loss: 2.3949 - classification_loss: 1.9007
 309/1000 [========>.....................] - ETA: 25:46 - loss: 4.2899 - regression_loss: 2.3945 - classification_loss: 1.8954
 310/1000 [========>.....................] - ETA: 25:42 - loss: 4.2847 - regression_loss: 2.3944 - classification_loss: 1.8903
 311/1000 [========>.....................] - ETA: 25:38 - loss: 4.3334 - regression_loss: 2.3932 - classification_loss: 1.9402
 312/1000 [========>.....................] - ETA: 25:37 - loss: 4.3275 - regression_loss: 2.3927 - classification_loss: 1.9348
 313/1000 [========>.....................] - ETA: 25:36 - loss: 4.3215 - regression_loss: 2.3921 - classification_loss: 1.9294
 314/1000 [========>.....................] - ETA: 25:33 - loss: 4.3157 - regression_loss: 2.3916 - classification_loss: 1.9241
 315/1000 [========>.....................] - ETA: 25:32 - loss: 4.3100 - regression_loss: 2.3911 - classification_loss: 1.9189
 316/1000 [========>.....................] - ETA: 25:28 - loss: 4.3256 - regression_loss: 2.3900 - classification_loss: 1.9356
 317/1000 [========>.....................] - ETA: 25:26 - loss: 4.3198 - regression_loss: 2.3895 - classification_loss: 1.9303
 318/1000 [========>.....................] - ETA: 25:24 - loss: 4.3141 - regression_loss: 2.3890 - classification_loss: 1.9251
 319/1000 [========>.....................] - ETA: 25:22 - loss: 4.3086 - regression_loss: 2.3885 - classification_loss: 1.9201
 320/1000 [========>.....................] - ETA: 25:20 - loss: 4.3031 - regression_loss: 2.3882 - classification_loss: 1.9150
 321/1000 [========>.....................] - ETA: 25:17 - loss: 4.2980 - regression_loss: 2.3880 - classification_loss: 1.9100
 322/1000 [========>.....................] - ETA: 25:16 - loss: 4.2923 - regression_loss: 2.3875 - classification_loss: 1.9048
 323/1000 [========>.....................] - ETA: 25:14 - loss: 4.2866 - regression_loss: 2.3869 - classification_loss: 1.8997
 324/1000 [========>.....................] - ETA: 25:13 - loss: 4.2809 - regression_loss: 2.3863 - classification_loss: 1.8946
 325/1000 [========>.....................] - ETA: 25:10 - loss: 4.2755 - regression_loss: 2.3859 - classification_loss: 1.8896
 326/1000 [========>.....................] - ETA: 25:07 - loss: 4.3251 - regression_loss: 2.3849 - classification_loss: 1.9402
 327/1000 [========>.....................] - ETA: 25:05 - loss: 4.3195 - regression_loss: 2.3845 - classification_loss: 1.9350
 328/1000 [========>.....................] - ETA: 25:03 - loss: 4.3141 - regression_loss: 2.3841 - classification_loss: 1.9300
 329/1000 [========>.....................] - ETA: 25:00 - loss: 4.3090 - regression_loss: 2.3839 - classification_loss: 1.9251
 330/1000 [========>.....................] - ETA: 24:59 - loss: 4.3033 - regression_loss: 2.3833 - classification_loss: 1.9200
 331/1000 [========>.....................] - ETA: 24:57 - loss: 4.2979 - regression_loss: 2.3828 - classification_loss: 1.9150
 332/1000 [========>.....................] - ETA: 24:55 - loss: 4.2923 - regression_loss: 2.3824 - classification_loss: 1.9100
 333/1000 [========>.....................] - ETA: 24:52 - loss: 4.3145 - regression_loss: 2.3812 - classification_loss: 1.9332
 334/1000 [=========>....................] - ETA: 24:48 - loss: 4.3094 - regression_loss: 2.3810 - classification_loss: 1.9284
 335/1000 [=========>....................] - ETA: 24:46 - loss: 4.3039 - regression_loss: 2.3805 - classification_loss: 1.9234
 336/1000 [=========>....................] - ETA: 24:44 - loss: 4.2986 - regression_loss: 2.3802 - classification_loss: 1.9184
 337/1000 [=========>....................] - ETA: 24:43 - loss: 4.2930 - regression_loss: 2.3796 - classification_loss: 1.9134
 338/1000 [=========>....................] - ETA: 24:41 - loss: 4.2876 - regression_loss: 2.3791 - classification_loss: 1.9085
 339/1000 [=========>....................] - ETA: 24:40 - loss: 4.2822 - regression_loss: 2.3787 - classification_loss: 1.9035
 340/1000 [=========>....................] - ETA: 24:38 - loss: 4.2771 - regression_loss: 2.3783 - classification_loss: 1.8988
 341/1000 [=========>....................] - ETA: 24:35 - loss: 4.2718 - regression_loss: 2.3779 - classification_loss: 1.8939
 342/1000 [=========>....................] - ETA: 24:32 - loss: 4.3056 - regression_loss: 2.3767 - classification_loss: 1.9288
 343/1000 [=========>....................] - ETA: 24:29 - loss: 4.3007 - regression_loss: 2.3766 - classification_loss: 1.9241
 344/1000 [=========>....................] - ETA: 24:26 - loss: 4.2953 - regression_loss: 2.3760 - classification_loss: 1.9193
 345/1000 [=========>....................] - ETA: 24:23 - loss: 4.2904 - regression_loss: 2.3758 - classification_loss: 1.9146
 346/1000 [=========>....................] - ETA: 24:21 - loss: 4.2852 - regression_loss: 2.3755 - classification_loss: 1.9098
 347/1000 [=========>....................] - ETA: 24:20 - loss: 4.2799 - regression_loss: 2.3750 - classification_loss: 1.9050
 348/1000 [=========>....................] - ETA: 24:18 - loss: 4.2746 - regression_loss: 2.3744 - classification_loss: 1.9002
 349/1000 [=========>....................] - ETA: 24:15 - loss: 4.3029 - regression_loss: 2.3734 - classification_loss: 1.9295
 350/1000 [=========>....................] - ETA: 24:13 - loss: 4.2979 - regression_loss: 2.3731 - classification_loss: 1.9248
 351/1000 [=========>....................] - ETA: 24:11 - loss: 4.2929 - regression_loss: 2.3728 - classification_loss: 1.9200
 352/1000 [=========>....................] - ETA: 24:09 - loss: 4.2879 - regression_loss: 2.3726 - classification_loss: 1.9154
 353/1000 [=========>....................] - ETA: 24:07 - loss: 4.2828 - regression_loss: 2.3721 - classification_loss: 1.9106
 354/1000 [=========>....................] - ETA: 24:04 - loss: 4.2779 - regression_loss: 2.3718 - classification_loss: 1.9061
 355/1000 [=========>....................] - ETA: 24:02 - loss: 4.2729 - regression_loss: 2.3714 - classification_loss: 1.9014
 356/1000 [=========>....................] - ETA: 23:59 - loss: 4.2917 - regression_loss: 2.3703 - classification_loss: 1.9215
 357/1000 [=========>....................] - ETA: 23:57 - loss: 4.2866 - regression_loss: 2.3698 - classification_loss: 1.9168
 358/1000 [=========>....................] - ETA: 23:56 - loss: 4.2815 - regression_loss: 2.3693 - classification_loss: 1.9122
 359/1000 [=========>....................] - ETA: 23:54 - loss: 4.2765 - regression_loss: 2.3690 - classification_loss: 1.9075
 360/1000 [=========>....................] - ETA: 23:52 - loss: 4.2714 - regression_loss: 2.3685 - classification_loss: 1.9029
 361/1000 [=========>....................] - ETA: 23:50 - loss: 4.2663 - regression_loss: 2.3680 - classification_loss: 1.8983
 362/1000 [=========>....................] - ETA: 23:47 - loss: 4.2616 - regression_loss: 2.3677 - classification_loss: 1.8939
 363/1000 [=========>....................] - ETA: 23:45 - loss: 4.2566 - regression_loss: 2.3672 - classification_loss: 1.8894
 364/1000 [=========>....................] - ETA: 23:42 - loss: 4.2941 - regression_loss: 2.3662 - classification_loss: 1.9279
 365/1000 [=========>....................] - ETA: 23:39 - loss: 4.2892 - regression_loss: 2.3658 - classification_loss: 1.9234
 366/1000 [=========>....................] - ETA: 23:35 - loss: 4.3139 - regression_loss: 2.3647 - classification_loss: 1.9492
 367/1000 [==========>...................] - ETA: 23:33 - loss: 4.3091 - regression_loss: 2.3645 - classification_loss: 1.9446
 368/1000 [==========>...................] - ETA: 23:32 - loss: 4.3041 - regression_loss: 2.3640 - classification_loss: 1.9401
 369/1000 [==========>...................] - ETA: 23:30 - loss: 4.2992 - regression_loss: 2.3636 - classification_loss: 1.9355
 370/1000 [==========>...................] - ETA: 23:28 - loss: 4.2942 - regression_loss: 2.3632 - classification_loss: 1.9310
 371/1000 [==========>...................] - ETA: 23:26 - loss: 4.2891 - regression_loss: 2.3627 - classification_loss: 1.9265
 372/1000 [==========>...................] - ETA: 23:24 - loss: 4.2841 - regression_loss: 2.3622 - classification_loss: 1.9220
 373/1000 [==========>...................] - ETA: 23:22 - loss: 4.2792 - regression_loss: 2.3617 - classification_loss: 1.9175
 374/1000 [==========>...................] - ETA: 23:19 - loss: 4.3130 - regression_loss: 2.3606 - classification_loss: 1.9524
 375/1000 [==========>...................] - ETA: 23:17 - loss: 4.3079 - regression_loss: 2.3601 - classification_loss: 1.9479
 376/1000 [==========>...................] - ETA: 23:14 - loss: 4.3032 - regression_loss: 2.3597 - classification_loss: 1.9434
 377/1000 [==========>...................] - ETA: 23:12 - loss: 4.2982 - regression_loss: 2.3593 - classification_loss: 1.9389
 378/1000 [==========>...................] - ETA: 23:10 - loss: 4.2934 - regression_loss: 2.3589 - classification_loss: 1.9345
 379/1000 [==========>...................] - ETA: 23:09 - loss: 4.2886 - regression_loss: 2.3585 - classification_loss: 1.9300
 380/1000 [==========>...................] - ETA: 23:07 - loss: 4.2838 - regression_loss: 2.3581 - classification_loss: 1.9256
 381/1000 [==========>...................] - ETA: 23:04 - loss: 4.2789 - regression_loss: 2.3577 - classification_loss: 1.9212
 382/1000 [==========>...................] - ETA: 23:01 - loss: 4.3043 - regression_loss: 2.3565 - classification_loss: 1.9478
 383/1000 [==========>...................] - ETA: 23:00 - loss: 4.2993 - regression_loss: 2.3560 - classification_loss: 1.9433
 384/1000 [==========>...................] - ETA: 22:57 - loss: 4.2946 - regression_loss: 2.3556 - classification_loss: 1.9390
 385/1000 [==========>...................] - ETA: 22:55 - loss: 4.2898 - regression_loss: 2.3552 - classification_loss: 1.9346
 386/1000 [==========>...................] - ETA: 22:52 - loss: 4.2851 - regression_loss: 2.3548 - classification_loss: 1.9303
 387/1000 [==========>...................] - ETA: 22:49 - loss: 4.2802 - regression_loss: 2.3543 - classification_loss: 1.9259
 388/1000 [==========>...................] - ETA: 22:47 - loss: 4.2755 - regression_loss: 2.3538 - classification_loss: 1.9217
 389/1000 [==========>...................] - ETA: 22:44 - loss: 4.2992 - regression_loss: 2.3526 - classification_loss: 1.9465
 390/1000 [==========>...................] - ETA: 22:43 - loss: 4.2943 - regression_loss: 2.3521 - classification_loss: 1.9421
 391/1000 [==========>...................] - ETA: 22:41 - loss: 4.2895 - regression_loss: 2.3518 - classification_loss: 1.9378
 392/1000 [==========>...................] - ETA: 22:39 - loss: 4.2847 - regression_loss: 2.3513 - classification_loss: 1.9334
 393/1000 [==========>...................] - ETA: 22:36 - loss: 4.3089 - regression_loss: 2.3502 - classification_loss: 1.9587
 394/1000 [==========>...................] - ETA: 22:34 - loss: 4.3041 - regression_loss: 2.3497 - classification_loss: 1.9544
 395/1000 [==========>...................] - ETA: 22:31 - loss: 4.2994 - regression_loss: 2.3493 - classification_loss: 1.9501
 396/1000 [==========>...................] - ETA: 22:29 - loss: 4.2948 - regression_loss: 2.3489 - classification_loss: 1.9459
 397/1000 [==========>...................] - ETA: 22:27 - loss: 4.2901 - regression_loss: 2.3484 - classification_loss: 1.9417
 398/1000 [==========>...................] - ETA: 22:25 - loss: 4.2855 - regression_loss: 2.3481 - classification_loss: 1.9374
 399/1000 [==========>...................] - ETA: 22:24 - loss: 4.2808 - regression_loss: 2.3476 - classification_loss: 1.9332
 400/1000 [===========>..................] - ETA: 22:20 - loss: 4.2954 - regression_loss: 2.3463 - classification_loss: 1.9491
 401/1000 [===========>..................] - ETA: 22:19 - loss: 4.2908 - regression_loss: 2.3459 - classification_loss: 1.9448
 402/1000 [===========>..................] - ETA: 22:16 - loss: 4.2862 - regression_loss: 2.3455 - classification_loss: 1.9407
 403/1000 [===========>..................] - ETA: 22:14 - loss: 4.2814 - regression_loss: 2.3450 - classification_loss: 1.9364
 404/1000 [===========>..................] - ETA: 22:12 - loss: 4.2770 - regression_loss: 2.3447 - classification_loss: 1.9323
 405/1000 [===========>..................] - ETA: 22:10 - loss: 4.2724 - regression_loss: 2.3443 - classification_loss: 1.9281
 406/1000 [===========>..................] - ETA: 22:08 - loss: 4.2677 - regression_loss: 2.3438 - classification_loss: 1.9240
 407/1000 [===========>..................] - ETA: 22:05 - loss: 4.2632 - regression_loss: 2.3432 - classification_loss: 1.9199
 408/1000 [===========>..................] - ETA: 22:02 - loss: 4.2816 - regression_loss: 2.3420 - classification_loss: 1.9396
 409/1000 [===========>..................] - ETA: 22:00 - loss: 4.2770 - regression_loss: 2.3416 - classification_loss: 1.9354
 410/1000 [===========>..................] - ETA: 21:58 - loss: 4.2727 - regression_loss: 2.3413 - classification_loss: 1.9313
 411/1000 [===========>..................] - ETA: 21:56 - loss: 4.2682 - regression_loss: 2.3409 - classification_loss: 1.9273
 412/1000 [===========>..................] - ETA: 21:54 - loss: 4.2636 - regression_loss: 2.3405 - classification_loss: 1.9232
 413/1000 [===========>..................] - ETA: 21:52 - loss: 4.2591 - regression_loss: 2.3400 - classification_loss: 1.9191
 414/1000 [===========>..................] - ETA: 21:51 - loss: 4.2546 - regression_loss: 2.3396 - classification_loss: 1.9150
 415/1000 [===========>..................] - ETA: 21:48 - loss: 4.2503 - regression_loss: 2.3392 - classification_loss: 1.9111
 416/1000 [===========>..................] - ETA: 21:46 - loss: 4.2460 - regression_loss: 2.3388 - classification_loss: 1.9071
 417/1000 [===========>..................] - ETA: 21:44 - loss: 4.2416 - regression_loss: 2.3384 - classification_loss: 1.9032
 418/1000 [===========>..................] - ETA: 21:42 - loss: 4.2374 - regression_loss: 2.3381 - classification_loss: 1.8993
 419/1000 [===========>..................] - ETA: 21:39 - loss: 4.2752 - regression_loss: 2.3370 - classification_loss: 1.9383
 420/1000 [===========>..................] - ETA: 21:37 - loss: 4.2707 - regression_loss: 2.3365 - classification_loss: 1.9342
 421/1000 [===========>..................] - ETA: 21:35 - loss: 4.2663 - regression_loss: 2.3360 - classification_loss: 1.9303
 422/1000 [===========>..................] - ETA: 21:32 - loss: 4.2756 - regression_loss: 2.3348 - classification_loss: 1.9408
 423/1000 [===========>..................] - ETA: 21:30 - loss: 4.2712 - regression_loss: 2.3344 - classification_loss: 1.9368
 424/1000 [===========>..................] - ETA: 21:28 - loss: 4.2670 - regression_loss: 2.3341 - classification_loss: 1.9330
 425/1000 [===========>..................] - ETA: 21:25 - loss: 4.2628 - regression_loss: 2.3337 - classification_loss: 1.9291
 426/1000 [===========>..................] - ETA: 21:23 - loss: 4.2588 - regression_loss: 2.3334 - classification_loss: 1.9253
 427/1000 [===========>..................] - ETA: 21:21 - loss: 4.2546 - regression_loss: 2.3331 - classification_loss: 1.9216
 428/1000 [===========>..................] - ETA: 21:19 - loss: 4.2504 - regression_loss: 2.3326 - classification_loss: 1.9178
 429/1000 [===========>..................] - ETA: 21:16 - loss: 4.2530 - regression_loss: 2.3313 - classification_loss: 1.9217
 430/1000 [===========>..................] - ETA: 21:14 - loss: 4.2487 - regression_loss: 2.3309 - classification_loss: 1.9179
 431/1000 [===========>..................] - ETA: 21:12 - loss: 4.2445 - regression_loss: 2.3305 - classification_loss: 1.9140
 432/1000 [===========>..................] - ETA: 21:10 - loss: 4.2404 - regression_loss: 2.3301 - classification_loss: 1.9102
 433/1000 [===========>..................] - ETA: 21:08 - loss: 4.2361 - regression_loss: 2.3297 - classification_loss: 1.9063
 434/1000 [============>.................] - ETA: 21:05 - loss: 4.2318 - regression_loss: 2.3292 - classification_loss: 1.9026
 435/1000 [============>.................] - ETA: 21:04 - loss: 4.2276 - regression_loss: 2.3289 - classification_loss: 1.8987
 436/1000 [============>.................] - ETA: 21:01 - loss: 4.2235 - regression_loss: 2.3284 - classification_loss: 1.8951
 437/1000 [============>.................] - ETA: 20:58 - loss: 4.2661 - regression_loss: 2.3272 - classification_loss: 1.9390
 438/1000 [============>.................] - ETA: 20:56 - loss: 4.2619 - regression_loss: 2.3268 - classification_loss: 1.9351
 439/1000 [============>.................] - ETA: 20:54 - loss: 4.2576 - regression_loss: 2.3263 - classification_loss: 1.9313
 440/1000 [============>.................] - ETA: 20:53 - loss: 4.2533 - regression_loss: 2.3259 - classification_loss: 1.9274
 441/1000 [============>.................] - ETA: 20:50 - loss: 4.2490 - regression_loss: 2.3254 - classification_loss: 1.9237
 442/1000 [============>.................] - ETA: 20:48 - loss: 4.2449 - regression_loss: 2.3250 - classification_loss: 1.9199
 443/1000 [============>.................] - ETA: 20:45 - loss: 4.2407 - regression_loss: 2.3246 - classification_loss: 1.9162
 444/1000 [============>.................] - ETA: 20:44 - loss: 4.2365 - regression_loss: 2.3241 - classification_loss: 1.9124
 445/1000 [============>.................] - ETA: 20:42 - loss: 4.2323 - regression_loss: 2.3237 - classification_loss: 1.9087
 446/1000 [============>.................] - ETA: 20:39 - loss: 4.2487 - regression_loss: 2.3224 - classification_loss: 1.9263
 447/1000 [============>.................] - ETA: 20:37 - loss: 4.2446 - regression_loss: 2.3220 - classification_loss: 1.9225
 448/1000 [============>.................] - ETA: 20:34 - loss: 4.2403 - regression_loss: 2.3214 - classification_loss: 1.9189
 449/1000 [============>.................] - ETA: 20:32 - loss: 4.2362 - regression_loss: 2.3210 - classification_loss: 1.9152
 450/1000 [============>.................] - ETA: 20:30 - loss: 4.2320 - regression_loss: 2.3205 - classification_loss: 1.9115
 451/1000 [============>.................] - ETA: 20:28 - loss: 4.2278 - regression_loss: 2.3201 - classification_loss: 1.9077
 452/1000 [============>.................] - ETA: 20:25 - loss: 4.2410 - regression_loss: 2.3189 - classification_loss: 1.9221
 453/1000 [============>.................] - ETA: 20:23 - loss: 4.2369 - regression_loss: 2.3185 - classification_loss: 1.9184
 454/1000 [============>.................] - ETA: 20:21 - loss: 4.2327 - regression_loss: 2.3180 - classification_loss: 1.9147
 455/1000 [============>.................] - ETA: 20:18 - loss: 4.2287 - regression_loss: 2.3175 - classification_loss: 1.9111
 456/1000 [============>.................] - ETA: 20:16 - loss: 4.2247 - regression_loss: 2.3171 - classification_loss: 1.9075
 457/1000 [============>.................] - ETA: 20:14 - loss: 4.2206 - regression_loss: 2.3167 - classification_loss: 1.9039
 458/1000 [============>.................] - ETA: 20:11 - loss: 4.2324 - regression_loss: 2.3153 - classification_loss: 1.9171
 459/1000 [============>.................] - ETA: 20:09 - loss: 4.2283 - regression_loss: 2.3149 - classification_loss: 1.9134
 460/1000 [============>.................] - ETA: 20:06 - loss: 4.2242 - regression_loss: 2.3144 - classification_loss: 1.9098
 461/1000 [============>.................] - ETA: 20:05 - loss: 4.2202 - regression_loss: 2.3141 - classification_loss: 1.9061
 462/1000 [============>.................] - ETA: 20:03 - loss: 4.2161 - regression_loss: 2.3136 - classification_loss: 1.9025
 463/1000 [============>.................] - ETA: 20:00 - loss: 4.2120 - regression_loss: 2.3130 - classification_loss: 1.8990
 464/1000 [============>.................] - ETA: 19:58 - loss: 4.2081 - regression_loss: 2.3127 - classification_loss: 1.8954
 465/1000 [============>.................] - ETA: 19:55 - loss: 4.2242 - regression_loss: 2.3117 - classification_loss: 1.9125
 466/1000 [============>.................] - ETA: 19:53 - loss: 4.2202 - regression_loss: 2.3113 - classification_loss: 1.9089
 467/1000 [=============>................] - ETA: 19:51 - loss: 4.2163 - regression_loss: 2.3109 - classification_loss: 1.9054
 468/1000 [=============>................] - ETA: 19:49 - loss: 4.2123 - regression_loss: 2.3105 - classification_loss: 1.9018
 469/1000 [=============>................] - ETA: 19:47 - loss: 4.2084 - regression_loss: 2.3101 - classification_loss: 1.8983
 470/1000 [=============>................] - ETA: 19:45 - loss: 4.2044 - regression_loss: 2.3096 - classification_loss: 1.8948
 471/1000 [=============>................] - ETA: 19:43 - loss: 4.2004 - regression_loss: 2.3092 - classification_loss: 1.8912
 472/1000 [=============>................] - ETA: 19:40 - loss: 4.2186 - regression_loss: 2.3080 - classification_loss: 1.9107
 473/1000 [=============>................] - ETA: 19:37 - loss: 4.2146 - regression_loss: 2.3074 - classification_loss: 1.9072
 474/1000 [=============>................] - ETA: 19:35 - loss: 4.2108 - regression_loss: 2.3072 - classification_loss: 1.9036
 475/1000 [=============>................] - ETA: 19:33 - loss: 4.2069 - regression_loss: 2.3067 - classification_loss: 1.9002
 476/1000 [=============>................] - ETA: 19:31 - loss: 4.2029 - regression_loss: 2.3063 - classification_loss: 1.8966
 477/1000 [=============>................] - ETA: 19:29 - loss: 4.1989 - regression_loss: 2.3058 - classification_loss: 1.8932
 478/1000 [=============>................] - ETA: 19:26 - loss: 4.1949 - regression_loss: 2.3052 - classification_loss: 1.8897
 479/1000 [=============>................] - ETA: 19:24 - loss: 4.1911 - regression_loss: 2.3049 - classification_loss: 1.8863
 480/1000 [=============>................] - ETA: 19:21 - loss: 4.2017 - regression_loss: 2.3038 - classification_loss: 1.8980
 481/1000 [=============>................] - ETA: 19:19 - loss: 4.1979 - regression_loss: 2.3034 - classification_loss: 1.8945
 482/1000 [=============>................] - ETA: 19:17 - loss: 4.1943 - regression_loss: 2.3032 - classification_loss: 1.8911
 483/1000 [=============>................] - ETA: 19:15 - loss: 4.1905 - regression_loss: 2.3028 - classification_loss: 1.8877
 484/1000 [=============>................] - ETA: 19:13 - loss: 4.1867 - regression_loss: 2.3023 - classification_loss: 1.8844
 485/1000 [=============>................] - ETA: 19:11 - loss: 4.1828 - regression_loss: 2.3019 - classification_loss: 1.8810
 486/1000 [=============>................] - ETA: 19:09 - loss: 4.1791 - regression_loss: 2.3015 - classification_loss: 1.8776
 487/1000 [=============>................] - ETA: 19:06 - loss: 4.1752 - regression_loss: 2.3010 - classification_loss: 1.8742
 488/1000 [=============>................] - ETA: 19:05 - loss: 4.1714 - regression_loss: 2.3005 - classification_loss: 1.8708
 489/1000 [=============>................] - ETA: 19:03 - loss: 4.1676 - regression_loss: 2.3001 - classification_loss: 1.8675
 490/1000 [=============>................] - ETA: 19:00 - loss: 4.1904 - regression_loss: 2.2989 - classification_loss: 1.8915
 491/1000 [=============>................] - ETA: 18:57 - loss: 4.1866 - regression_loss: 2.2984 - classification_loss: 1.8882
 492/1000 [=============>................] - ETA: 18:55 - loss: 4.1828 - regression_loss: 2.2980 - classification_loss: 1.8848
 493/1000 [=============>................] - ETA: 18:53 - loss: 4.1930 - regression_loss: 2.2967 - classification_loss: 1.8963
 494/1000 [=============>................] - ETA: 18:51 - loss: 4.1892 - regression_loss: 2.2962 - classification_loss: 1.8929
 495/1000 [=============>................] - ETA: 18:48 - loss: 4.1852 - regression_loss: 2.2956 - classification_loss: 1.8896
 496/1000 [=============>................] - ETA: 18:46 - loss: 4.1814 - regression_loss: 2.2951 - classification_loss: 1.8863
 497/1000 [=============>................] - ETA: 18:44 - loss: 4.1778 - regression_loss: 2.2947 - classification_loss: 1.8830
 498/1000 [=============>................] - ETA: 18:41 - loss: 4.1738 - regression_loss: 2.2941 - classification_loss: 1.8797
 499/1000 [=============>................] - ETA: 18:38 - loss: 4.1848 - regression_loss: 2.2928 - classification_loss: 1.8920
 500/1000 [==============>...............] - ETA: 18:36 - loss: 4.1810 - regression_loss: 2.2923 - classification_loss: 1.8887
 501/1000 [==============>...............] - ETA: 18:34 - loss: 4.1773 - regression_loss: 2.2918 - classification_loss: 1.8854
 502/1000 [==============>...............] - ETA: 18:33 - loss: 4.1736 - regression_loss: 2.2915 - classification_loss: 1.8821
 503/1000 [==============>...............] - ETA: 18:30 - loss: 4.1698 - regression_loss: 2.2910 - classification_loss: 1.8788
 504/1000 [==============>...............] - ETA: 18:28 - loss: 4.1660 - regression_loss: 2.2905 - classification_loss: 1.8755
 505/1000 [==============>...............] - ETA: 18:27 - loss: 4.1623 - regression_loss: 2.2901 - classification_loss: 1.8723
 506/1000 [==============>...............] - ETA: 18:25 - loss: 4.1587 - regression_loss: 2.2897 - classification_loss: 1.8690
 507/1000 [==============>...............] - ETA: 18:22 - loss: 4.1735 - regression_loss: 2.2884 - classification_loss: 1.8850
 508/1000 [==============>...............] - ETA: 18:20 - loss: 4.1697 - regression_loss: 2.2879 - classification_loss: 1.8818
 509/1000 [==============>...............] - ETA: 18:17 - loss: 4.1658 - regression_loss: 2.2872 - classification_loss: 1.8786
 510/1000 [==============>...............] - ETA: 18:15 - loss: 4.1621 - regression_loss: 2.2867 - classification_loss: 1.8753
 511/1000 [==============>...............] - ETA: 18:13 - loss: 4.1583 - regression_loss: 2.2862 - classification_loss: 1.8721
 512/1000 [==============>...............] - ETA: 18:11 - loss: 4.1546 - regression_loss: 2.2857 - classification_loss: 1.8689
 513/1000 [==============>...............] - ETA: 18:08 - loss: 4.1508 - regression_loss: 2.2851 - classification_loss: 1.8657
 514/1000 [==============>...............] - ETA: 18:06 - loss: 4.1471 - regression_loss: 2.2846 - classification_loss: 1.8625
 515/1000 [==============>...............] - ETA: 18:04 - loss: 4.1436 - regression_loss: 2.2842 - classification_loss: 1.8594
 516/1000 [==============>...............] - ETA: 18:02 - loss: 4.1399 - regression_loss: 2.2836 - classification_loss: 1.8563
 517/1000 [==============>...............] - ETA: 17:59 - loss: 4.1566 - regression_loss: 2.2824 - classification_loss: 1.8743
 518/1000 [==============>...............] - ETA: 17:57 - loss: 4.1531 - regression_loss: 2.2820 - classification_loss: 1.8711
 519/1000 [==============>...............] - ETA: 17:55 - loss: 4.1495 - regression_loss: 2.2816 - classification_loss: 1.8679
 520/1000 [==============>...............] - ETA: 17:53 - loss: 4.1457 - regression_loss: 2.2809 - classification_loss: 1.8648
 521/1000 [==============>...............] - ETA: 17:51 - loss: 4.1422 - regression_loss: 2.2805 - classification_loss: 1.8617
 522/1000 [==============>...............] - ETA: 17:49 - loss: 4.1386 - regression_loss: 2.2799 - classification_loss: 1.8586
 523/1000 [==============>...............] - ETA: 17:47 - loss: 4.1351 - regression_loss: 2.2795 - classification_loss: 1.8556
 524/1000 [==============>...............] - ETA: 17:44 - loss: 4.1460 - regression_loss: 2.2783 - classification_loss: 1.8676
 525/1000 [==============>...............] - ETA: 17:42 - loss: 4.1423 - regression_loss: 2.2778 - classification_loss: 1.8645
 526/1000 [==============>...............] - ETA: 17:40 - loss: 4.1387 - regression_loss: 2.2773 - classification_loss: 1.8614
 527/1000 [==============>...............] - ETA: 17:38 - loss: 4.1351 - regression_loss: 2.2768 - classification_loss: 1.8583
 528/1000 [==============>...............] - ETA: 17:35 - loss: 4.1538 - regression_loss: 2.2756 - classification_loss: 1.8782
 529/1000 [==============>...............] - ETA: 17:32 - loss: 4.1499 - regression_loss: 2.2748 - classification_loss: 1.8751
 530/1000 [==============>...............] - ETA: 17:30 - loss: 4.1463 - regression_loss: 2.2743 - classification_loss: 1.8720
 531/1000 [==============>...............] - ETA: 17:28 - loss: 4.1427 - regression_loss: 2.2738 - classification_loss: 1.8688
 532/1000 [==============>...............] - ETA: 17:26 - loss: 4.1391 - regression_loss: 2.2733 - classification_loss: 1.8658
 533/1000 [==============>...............] - ETA: 17:24 - loss: 4.1356 - regression_loss: 2.2729 - classification_loss: 1.8628
 534/1000 [===============>..............] - ETA: 17:22 - loss: 4.1320 - regression_loss: 2.2723 - classification_loss: 1.8597
 535/1000 [===============>..............] - ETA: 17:19 - loss: 4.1284 - regression_loss: 2.2718 - classification_loss: 1.8566
 536/1000 [===============>..............] - ETA: 17:17 - loss: 4.1441 - regression_loss: 2.2705 - classification_loss: 1.8736
 537/1000 [===============>..............] - ETA: 17:15 - loss: 4.1406 - regression_loss: 2.2701 - classification_loss: 1.8705
 538/1000 [===============>..............] - ETA: 17:12 - loss: 4.1368 - regression_loss: 2.2693 - classification_loss: 1.8675
 539/1000 [===============>..............] - ETA: 17:10 - loss: 4.1332 - regression_loss: 2.2688 - classification_loss: 1.8644
 540/1000 [===============>..............] - ETA: 17:08 - loss: 4.1297 - regression_loss: 2.2683 - classification_loss: 1.8614
 541/1000 [===============>..............] - ETA: 17:06 - loss: 4.1261 - regression_loss: 2.2678 - classification_loss: 1.8583
 542/1000 [===============>..............] - ETA: 17:04 - loss: 4.1224 - regression_loss: 2.2671 - classification_loss: 1.8553
 543/1000 [===============>..............] - ETA: 17:01 - loss: 4.1304 - regression_loss: 2.2659 - classification_loss: 1.8645
 544/1000 [===============>..............] - ETA: 16:59 - loss: 4.1270 - regression_loss: 2.2654 - classification_loss: 1.8616
 545/1000 [===============>..............] - ETA: 16:57 - loss: 4.1236 - regression_loss: 2.2649 - classification_loss: 1.8586
 546/1000 [===============>..............] - ETA: 16:54 - loss: 4.1201 - regression_loss: 2.2644 - classification_loss: 1.8557
 547/1000 [===============>..............] - ETA: 16:52 - loss: 4.1165 - regression_loss: 2.2638 - classification_loss: 1.8527
 548/1000 [===============>..............] - ETA: 16:50 - loss: 4.1130 - regression_loss: 2.2633 - classification_loss: 1.8497
 549/1000 [===============>..............] - ETA: 16:48 - loss: 4.1096 - regression_loss: 2.2628 - classification_loss: 1.8467
 550/1000 [===============>..............] - ETA: 16:46 - loss: 4.1060 - regression_loss: 2.2621 - classification_loss: 1.8439
 551/1000 [===============>..............] - ETA: 16:44 - loss: 4.1028 - regression_loss: 2.2618 - classification_loss: 1.8410
 552/1000 [===============>..............] - ETA: 16:42 - loss: 4.0995 - regression_loss: 2.2613 - classification_loss: 1.8381
 553/1000 [===============>..............] - ETA: 16:39 - loss: 4.1300 - regression_loss: 2.2603 - classification_loss: 1.8697
 554/1000 [===============>..............] - ETA: 16:37 - loss: 4.1267 - regression_loss: 2.2600 - classification_loss: 1.8667
 555/1000 [===============>..............] - ETA: 16:35 - loss: 4.1233 - regression_loss: 2.2595 - classification_loss: 1.8638
 556/1000 [===============>..............] - ETA: 16:33 - loss: 4.1197 - regression_loss: 2.2589 - classification_loss: 1.8608
 557/1000 [===============>..............] - ETA: 16:30 - loss: 4.1284 - regression_loss: 2.2577 - classification_loss: 1.8706
 558/1000 [===============>..............] - ETA: 16:28 - loss: 4.1249 - regression_loss: 2.2573 - classification_loss: 1.8677
 559/1000 [===============>..............] - ETA: 16:26 - loss: 4.1215 - regression_loss: 2.2567 - classification_loss: 1.8648
 560/1000 [===============>..............] - ETA: 16:23 - loss: 4.1179 - regression_loss: 2.2560 - classification_loss: 1.8619
 561/1000 [===============>..............] - ETA: 16:21 - loss: 4.1206 - regression_loss: 2.2548 - classification_loss: 1.8659
 562/1000 [===============>..............] - ETA: 16:19 - loss: 4.1173 - regression_loss: 2.2542 - classification_loss: 1.8630
 563/1000 [===============>..............] - ETA: 16:16 - loss: 4.1139 - regression_loss: 2.2537 - classification_loss: 1.8602
 564/1000 [===============>..............] - ETA: 16:14 - loss: 4.1105 - regression_loss: 2.2532 - classification_loss: 1.8573
 565/1000 [===============>..............] - ETA: 16:12 - loss: 4.1072 - regression_loss: 2.2527 - classification_loss: 1.8545
 566/1000 [===============>..............] - ETA: 16:10 - loss: 4.1038 - regression_loss: 2.2522 - classification_loss: 1.8516
 567/1000 [================>.............] - ETA: 16:08 - loss: 4.1002 - regression_loss: 2.2514 - classification_loss: 1.8488
 568/1000 [================>.............] - ETA: 16:06 - loss: 4.0968 - regression_loss: 2.2509 - classification_loss: 1.8459
 569/1000 [================>.............] - ETA: 16:03 - loss: 4.0935 - regression_loss: 2.2503 - classification_loss: 1.8432
 570/1000 [================>.............] - ETA: 16:01 - loss: 4.0902 - regression_loss: 2.2498 - classification_loss: 1.8404
 571/1000 [================>.............] - ETA: 15:59 - loss: 4.0868 - regression_loss: 2.2491 - classification_loss: 1.8377
 572/1000 [================>.............] - ETA: 15:57 - loss: 4.0835 - regression_loss: 2.2486 - classification_loss: 1.8349
 573/1000 [================>.............] - ETA: 15:54 - loss: 4.1027 - regression_loss: 2.2474 - classification_loss: 1.8553
 574/1000 [================>.............] - ETA: 15:52 - loss: 4.0993 - regression_loss: 2.2469 - classification_loss: 1.8524
 575/1000 [================>.............] - ETA: 15:50 - loss: 4.0959 - regression_loss: 2.2463 - classification_loss: 1.8496
 576/1000 [================>.............] - ETA: 15:48 - loss: 4.0926 - regression_loss: 2.2458 - classification_loss: 1.8468
 577/1000 [================>.............] - ETA: 15:46 - loss: 4.0893 - regression_loss: 2.2453 - classification_loss: 1.8440
 578/1000 [================>.............] - ETA: 15:43 - loss: 4.0965 - regression_loss: 2.2440 - classification_loss: 1.8525
 579/1000 [================>.............] - ETA: 15:41 - loss: 4.0932 - regression_loss: 2.2435 - classification_loss: 1.8497
 580/1000 [================>.............] - ETA: 15:39 - loss: 4.0896 - regression_loss: 2.2426 - classification_loss: 1.8470
 581/1000 [================>.............] - ETA: 15:36 - loss: 4.0862 - regression_loss: 2.2420 - classification_loss: 1.8442
 582/1000 [================>.............] - ETA: 15:34 - loss: 4.0980 - regression_loss: 2.2408 - classification_loss: 1.8572
 583/1000 [================>.............] - ETA: 15:31 - loss: 4.0946 - regression_loss: 2.2402 - classification_loss: 1.8544
 584/1000 [================>.............] - ETA: 15:29 - loss: 4.0912 - regression_loss: 2.2396 - classification_loss: 1.8516
 585/1000 [================>.............] - ETA: 15:27 - loss: 4.0879 - regression_loss: 2.2391 - classification_loss: 1.8488
 586/1000 [================>.............] - ETA: 15:25 - loss: 4.0846 - regression_loss: 2.2386 - classification_loss: 1.8460
 587/1000 [================>.............] - ETA: 15:23 - loss: 4.0811 - regression_loss: 2.2379 - classification_loss: 1.8432
 588/1000 [================>.............] - ETA: 15:21 - loss: 4.0779 - regression_loss: 2.2374 - classification_loss: 1.8405
 589/1000 [================>.............] - ETA: 15:19 - loss: 4.0747 - regression_loss: 2.2369 - classification_loss: 1.8378
 590/1000 [================>.............] - ETA: 15:16 - loss: 4.0883 - regression_loss: 2.2356 - classification_loss: 1.8527
 591/1000 [================>.............] - ETA: 15:14 - loss: 4.0850 - regression_loss: 2.2351 - classification_loss: 1.8499
 592/1000 [================>.............] - ETA: 15:12 - loss: 4.0819 - regression_loss: 2.2347 - classification_loss: 1.8472
 593/1000 [================>.............] - ETA: 15:10 - loss: 4.0785 - regression_loss: 2.2341 - classification_loss: 1.8444
 594/1000 [================>.............] - ETA: 15:07 - loss: 4.0753 - regression_loss: 2.2335 - classification_loss: 1.8417
 595/1000 [================>.............] - ETA: 15:05 - loss: 4.0720 - regression_loss: 2.2330 - classification_loss: 1.8390
 596/1000 [================>.............] - ETA: 15:03 - loss: 4.0687 - regression_loss: 2.2324 - classification_loss: 1.8363
 597/1000 [================>.............] - ETA: 15:01 - loss: 4.0655 - regression_loss: 2.2319 - classification_loss: 1.8336
 598/1000 [================>.............] - ETA: 14:59 - loss: 4.0623 - regression_loss: 2.2314 - classification_loss: 1.8309
 599/1000 [================>.............] - ETA: 14:57 - loss: 4.0591 - regression_loss: 2.2309 - classification_loss: 1.8282
 600/1000 [=================>............] - ETA: 14:55 - loss: 4.0556 - regression_loss: 2.2300 - classification_loss: 1.8255
 601/1000 [=================>............] - ETA: 14:52 - loss: 4.0688 - regression_loss: 2.2289 - classification_loss: 1.8399
 602/1000 [=================>............] - ETA: 14:50 - loss: 4.0655 - regression_loss: 2.2283 - classification_loss: 1.8372
 603/1000 [=================>............] - ETA: 14:47 - loss: 4.0623 - regression_loss: 2.2277 - classification_loss: 1.8345
 604/1000 [=================>............] - ETA: 14:45 - loss: 4.0589 - regression_loss: 2.2270 - classification_loss: 1.8319
 605/1000 [=================>............] - ETA: 14:42 - loss: 4.0628 - regression_loss: 2.2258 - classification_loss: 1.8370
 606/1000 [=================>............] - ETA: 14:40 - loss: 4.0596 - regression_loss: 2.2252 - classification_loss: 1.8343
 607/1000 [=================>............] - ETA: 14:38 - loss: 4.0565 - regression_loss: 2.2247 - classification_loss: 1.8317
 608/1000 [=================>............] - ETA: 14:36 - loss: 4.0533 - regression_loss: 2.2242 - classification_loss: 1.8291
 609/1000 [=================>............] - ETA: 14:34 - loss: 4.0502 - regression_loss: 2.2236 - classification_loss: 1.8265
 610/1000 [=================>............] - ETA: 14:32 - loss: 4.0469 - regression_loss: 2.2230 - classification_loss: 1.8239
 611/1000 [=================>............] - ETA: 14:29 - loss: 4.0582 - regression_loss: 2.2218 - classification_loss: 1.8364
 612/1000 [=================>............] - ETA: 14:27 - loss: 4.0547 - regression_loss: 2.2209 - classification_loss: 1.8338
 613/1000 [=================>............] - ETA: 14:25 - loss: 4.0515 - regression_loss: 2.2204 - classification_loss: 1.8311
 614/1000 [=================>............] - ETA: 14:22 - loss: 4.0483 - regression_loss: 2.2198 - classification_loss: 1.8285
 615/1000 [=================>............] - ETA: 14:20 - loss: 4.0452 - regression_loss: 2.2193 - classification_loss: 1.8260
 616/1000 [=================>............] - ETA: 14:18 - loss: 4.0421 - regression_loss: 2.2187 - classification_loss: 1.8233
 617/1000 [=================>............] - ETA: 14:16 - loss: 4.0504 - regression_loss: 2.2176 - classification_loss: 1.8329
 618/1000 [=================>............] - ETA: 14:13 - loss: 4.0473 - regression_loss: 2.2170 - classification_loss: 1.8303
 619/1000 [=================>............] - ETA: 14:12 - loss: 4.0442 - regression_loss: 2.2165 - classification_loss: 1.8277
 620/1000 [=================>............] - ETA: 14:09 - loss: 4.0410 - regression_loss: 2.2159 - classification_loss: 1.8251
 621/1000 [=================>............] - ETA: 14:07 - loss: 4.0379 - regression_loss: 2.2154 - classification_loss: 1.8226
 622/1000 [=================>............] - ETA: 14:05 - loss: 4.0347 - regression_loss: 2.2147 - classification_loss: 1.8200
 623/1000 [=================>............] - ETA: 14:03 - loss: 4.0314 - regression_loss: 2.2140 - classification_loss: 1.8175
 624/1000 [=================>............] - ETA: 14:01 - loss: 4.0284 - regression_loss: 2.2135 - classification_loss: 1.8149
 625/1000 [=================>............] - ETA: 13:58 - loss: 4.0252 - regression_loss: 2.2128 - classification_loss: 1.8124
 626/1000 [=================>............] - ETA: 13:56 - loss: 4.0218 - regression_loss: 2.2120 - classification_loss: 1.8099
 627/1000 [=================>............] - ETA: 13:54 - loss: 4.0187 - regression_loss: 2.2114 - classification_loss: 1.8073
 628/1000 [=================>............] - ETA: 13:52 - loss: 4.0156 - regression_loss: 2.2108 - classification_loss: 1.8048
 629/1000 [=================>............] - ETA: 13:49 - loss: 4.0276 - regression_loss: 2.2096 - classification_loss: 1.8181
 630/1000 [=================>............] - ETA: 13:47 - loss: 4.0246 - regression_loss: 2.2090 - classification_loss: 1.8156
 631/1000 [=================>............] - ETA: 13:45 - loss: 4.0215 - regression_loss: 2.2085 - classification_loss: 1.8130
 632/1000 [=================>............] - ETA: 13:42 - loss: 4.0183 - regression_loss: 2.2077 - classification_loss: 1.8106
 633/1000 [=================>............] - ETA: 13:40 - loss: 4.0153 - regression_loss: 2.2072 - classification_loss: 1.8081
 634/1000 [==================>...........] - ETA: 13:38 - loss: 4.0122 - regression_loss: 2.2065 - classification_loss: 1.8057
 635/1000 [==================>...........] - ETA: 13:36 - loss: 4.0091 - regression_loss: 2.2059 - classification_loss: 1.8032
 636/1000 [==================>...........] - ETA: 13:34 - loss: 4.0060 - regression_loss: 2.2053 - classification_loss: 1.8007
 637/1000 [==================>...........] - ETA: 13:31 - loss: 4.0171 - regression_loss: 2.2041 - classification_loss: 1.8130
 638/1000 [==================>...........] - ETA: 13:29 - loss: 4.0140 - regression_loss: 2.2035 - classification_loss: 1.8105
 639/1000 [==================>...........] - ETA: 13:27 - loss: 4.0109 - regression_loss: 2.2029 - classification_loss: 1.8081
 640/1000 [==================>...........] - ETA: 13:24 - loss: 4.0076 - regression_loss: 2.2020 - classification_loss: 1.8056
 641/1000 [==================>...........] - ETA: 13:22 - loss: 4.0046 - regression_loss: 2.2014 - classification_loss: 1.8031
 642/1000 [==================>...........] - ETA: 13:20 - loss: 4.0015 - regression_loss: 2.2009 - classification_loss: 1.8006
 643/1000 [==================>...........] - ETA: 13:18 - loss: 3.9984 - regression_loss: 2.2002 - classification_loss: 1.7982
 644/1000 [==================>...........] - ETA: 13:16 - loss: 4.0058 - regression_loss: 2.1990 - classification_loss: 1.8068
 645/1000 [==================>...........] - ETA: 13:13 - loss: 4.0028 - regression_loss: 2.1984 - classification_loss: 1.8044
 646/1000 [==================>...........] - ETA: 13:11 - loss: 3.9998 - regression_loss: 2.1978 - classification_loss: 1.8019
 647/1000 [==================>...........] - ETA: 13:09 - loss: 3.9968 - regression_loss: 2.1973 - classification_loss: 1.7995
 648/1000 [==================>...........] - ETA: 13:07 - loss: 3.9938 - regression_loss: 2.1967 - classification_loss: 1.7970
 649/1000 [==================>...........] - ETA: 13:05 - loss: 3.9905 - regression_loss: 2.1959 - classification_loss: 1.7946
 650/1000 [==================>...........] - ETA: 13:02 - loss: 3.9874 - regression_loss: 2.1952 - classification_loss: 1.7922
 651/1000 [==================>...........] - ETA: 13:00 - loss: 4.0004 - regression_loss: 2.1940 - classification_loss: 1.8063
 652/1000 [==================>...........] - ETA: 12:58 - loss: 3.9973 - regression_loss: 2.1934 - classification_loss: 1.8039
 653/1000 [==================>...........] - ETA: 12:56 - loss: 3.9942 - regression_loss: 2.1928 - classification_loss: 1.8014
 654/1000 [==================>...........] - ETA: 12:53 - loss: 4.0035 - regression_loss: 2.1916 - classification_loss: 1.8119
 655/1000 [==================>...........] - ETA: 12:51 - loss: 4.0004 - regression_loss: 2.1909 - classification_loss: 1.8094
 656/1000 [==================>...........] - ETA: 12:49 - loss: 3.9973 - regression_loss: 2.1903 - classification_loss: 1.8070
 657/1000 [==================>...........] - ETA: 12:46 - loss: 3.9942 - regression_loss: 2.1895 - classification_loss: 1.8046
 658/1000 [==================>...........] - ETA: 12:44 - loss: 3.9912 - regression_loss: 2.1890 - classification_loss: 1.8022
 659/1000 [==================>...........] - ETA: 12:42 - loss: 3.9882 - regression_loss: 2.1884 - classification_loss: 1.7998
 660/1000 [==================>...........] - ETA: 12:40 - loss: 3.9852 - regression_loss: 2.1878 - classification_loss: 1.7974
 661/1000 [==================>...........] - ETA: 12:37 - loss: 3.9819 - regression_loss: 2.1869 - classification_loss: 1.7951
 662/1000 [==================>...........] - ETA: 12:35 - loss: 3.9986 - regression_loss: 2.1856 - classification_loss: 1.8130
 663/1000 [==================>...........] - ETA: 12:33 - loss: 3.9956 - regression_loss: 2.1849 - classification_loss: 1.8106
 664/1000 [==================>...........] - ETA: 12:31 - loss: 3.9926 - regression_loss: 2.1844 - classification_loss: 1.8082
 665/1000 [==================>...........] - ETA: 12:28 - loss: 3.9895 - regression_loss: 2.1837 - classification_loss: 1.8058
 666/1000 [==================>...........] - ETA: 12:26 - loss: 3.9865 - regression_loss: 2.1831 - classification_loss: 1.8034
 667/1000 [===================>..........] - ETA: 12:24 - loss: 3.9835 - regression_loss: 2.1824 - classification_loss: 1.8011
 668/1000 [===================>..........] - ETA: 12:22 - loss: 3.9804 - regression_loss: 2.1818 - classification_loss: 1.7987
 669/1000 [===================>..........] - ETA: 12:20 - loss: 3.9775 - regression_loss: 2.1812 - classification_loss: 1.7963
 670/1000 [===================>..........] - ETA: 12:17 - loss: 3.9822 - regression_loss: 2.1799 - classification_loss: 1.8023
 671/1000 [===================>..........] - ETA: 12:15 - loss: 3.9790 - regression_loss: 2.1791 - classification_loss: 1.7999
 672/1000 [===================>..........] - ETA: 12:13 - loss: 3.9761 - regression_loss: 2.1785 - classification_loss: 1.7976
 673/1000 [===================>..........] - ETA: 12:11 - loss: 3.9730 - regression_loss: 2.1778 - classification_loss: 1.7952
 674/1000 [===================>..........] - ETA: 12:08 - loss: 3.9780 - regression_loss: 2.1765 - classification_loss: 1.8015
 675/1000 [===================>..........] - ETA: 12:06 - loss: 3.9750 - regression_loss: 2.1759 - classification_loss: 1.7991
 676/1000 [===================>..........] - ETA: 12:04 - loss: 3.9721 - regression_loss: 2.1753 - classification_loss: 1.7968
 677/1000 [===================>..........] - ETA: 12:01 - loss: 3.9689 - regression_loss: 2.1744 - classification_loss: 1.7945
 678/1000 [===================>..........] - ETA: 11:59 - loss: 3.9660 - regression_loss: 2.1739 - classification_loss: 1.7921
 679/1000 [===================>..........] - ETA: 11:57 - loss: 3.9630 - regression_loss: 2.1732 - classification_loss: 1.7898
 680/1000 [===================>..........] - ETA: 11:55 - loss: 3.9600 - regression_loss: 2.1725 - classification_loss: 1.7875
 681/1000 [===================>..........] - ETA: 11:52 - loss: 3.9684 - regression_loss: 2.1712 - classification_loss: 1.7973
 682/1000 [===================>..........] - ETA: 11:50 - loss: 3.9655 - regression_loss: 2.1706 - classification_loss: 1.7949
 683/1000 [===================>..........] - ETA: 11:48 - loss: 3.9623 - regression_loss: 2.1697 - classification_loss: 1.7926
 684/1000 [===================>..........] - ETA: 11:46 - loss: 3.9594 - regression_loss: 2.1690 - classification_loss: 1.7903
 685/1000 [===================>..........] - ETA: 11:44 - loss: 3.9563 - regression_loss: 2.1684 - classification_loss: 1.7880
 686/1000 [===================>..........] - ETA: 11:41 - loss: 3.9534 - regression_loss: 2.1677 - classification_loss: 1.7857
 687/1000 [===================>..........] - ETA: 11:39 - loss: 3.9505 - regression_loss: 2.1670 - classification_loss: 1.7835
 688/1000 [===================>..........] - ETA: 11:37 - loss: 3.9476 - regression_loss: 2.1664 - classification_loss: 1.7812
 689/1000 [===================>..........] - ETA: 11:35 - loss: 3.9446 - regression_loss: 2.1658 - classification_loss: 1.7789
 690/1000 [===================>..........] - ETA: 11:33 - loss: 3.9417 - regression_loss: 2.1651 - classification_loss: 1.7766
 691/1000 [===================>..........] - ETA: 11:31 - loss: 3.9386 - regression_loss: 2.1642 - classification_loss: 1.7743
 692/1000 [===================>..........] - ETA: 11:28 - loss: 3.9357 - regression_loss: 2.1636 - classification_loss: 1.7721
 693/1000 [===================>..........] - ETA: 11:26 - loss: 3.9429 - regression_loss: 2.1623 - classification_loss: 1.7806
 694/1000 [===================>..........] - ETA: 11:23 - loss: 3.9399 - regression_loss: 2.1615 - classification_loss: 1.7784
 695/1000 [===================>..........] - ETA: 11:21 - loss: 3.9369 - regression_loss: 2.1608 - classification_loss: 1.7761
 696/1000 [===================>..........] - ETA: 11:19 - loss: 3.9341 - regression_loss: 2.1602 - classification_loss: 1.7739
 697/1000 [===================>..........] - ETA: 11:16 - loss: 3.9366 - regression_loss: 2.1589 - classification_loss: 1.7777
 698/1000 [===================>..........] - ETA: 11:14 - loss: 3.9337 - regression_loss: 2.1582 - classification_loss: 1.7755
 699/1000 [===================>..........] - ETA: 11:12 - loss: 3.9308 - regression_loss: 2.1575 - classification_loss: 1.7733
 700/1000 [====================>.........] - ETA: 11:10 - loss: 3.9280 - regression_loss: 2.1569 - classification_loss: 1.7711
 701/1000 [====================>.........] - ETA: 11:08 - loss: 3.9251 - regression_loss: 2.1562 - classification_loss: 1.7689
 702/1000 [====================>.........] - ETA: 11:06 - loss: 3.9222 - regression_loss: 2.1556 - classification_loss: 1.7666
 703/1000 [====================>.........] - ETA: 11:04 - loss: 3.9193 - regression_loss: 2.1549 - classification_loss: 1.7644
 704/1000 [====================>.........] - ETA: 11:01 - loss: 3.9325 - regression_loss: 2.1536 - classification_loss: 1.7789
 705/1000 [====================>.........] - ETA: 10:59 - loss: 3.9293 - regression_loss: 2.1526 - classification_loss: 1.7767
 706/1000 [====================>.........] - ETA: 10:57 - loss: 3.9264 - regression_loss: 2.1520 - classification_loss: 1.7744
 707/1000 [====================>.........] - ETA: 10:54 - loss: 3.9235 - regression_loss: 2.1513 - classification_loss: 1.7722
 708/1000 [====================>.........] - ETA: 10:52 - loss: 3.9206 - regression_loss: 2.1506 - classification_loss: 1.7700
 709/1000 [====================>.........] - ETA: 10:50 - loss: 3.9178 - regression_loss: 2.1500 - classification_loss: 1.7678
 710/1000 [====================>.........] - ETA: 10:48 - loss: 3.9149 - regression_loss: 2.1493 - classification_loss: 1.7656
 711/1000 [====================>.........] - ETA: 10:46 - loss: 3.9119 - regression_loss: 2.1486 - classification_loss: 1.7634
 712/1000 [====================>.........] - ETA: 10:43 - loss: 3.9090 - regression_loss: 2.1477 - classification_loss: 1.7612
 713/1000 [====================>.........] - ETA: 10:41 - loss: 3.9164 - regression_loss: 2.1464 - classification_loss: 1.7701
 714/1000 [====================>.........] - ETA: 10:39 - loss: 3.9136 - regression_loss: 2.1457 - classification_loss: 1.7679
 715/1000 [====================>.........] - ETA: 10:37 - loss: 3.9108 - regression_loss: 2.1451 - classification_loss: 1.7657
 716/1000 [====================>.........] - ETA: 10:35 - loss: 3.9080 - regression_loss: 2.1444 - classification_loss: 1.7636
 717/1000 [====================>.........] - ETA: 10:32 - loss: 3.9051 - regression_loss: 2.1437 - classification_loss: 1.7614
 718/1000 [====================>.........] - ETA: 10:30 - loss: 3.9023 - regression_loss: 2.1431 - classification_loss: 1.7592
 719/1000 [====================>.........] - ETA: 10:28 - loss: 3.8994 - regression_loss: 2.1423 - classification_loss: 1.7571
 720/1000 [====================>.........] - ETA: 10:26 - loss: 3.8964 - regression_loss: 2.1414 - classification_loss: 1.7550
 721/1000 [====================>.........] - ETA: 10:23 - loss: 3.9072 - regression_loss: 2.1401 - classification_loss: 1.7670
 722/1000 [====================>.........] - ETA: 10:21 - loss: 3.9173 - regression_loss: 2.1389 - classification_loss: 1.7784
 723/1000 [====================>.........] - ETA: 10:18 - loss: 3.9143 - regression_loss: 2.1380 - classification_loss: 1.7762
 724/1000 [====================>.........] - ETA: 10:16 - loss: 3.9115 - regression_loss: 2.1374 - classification_loss: 1.7740
 725/1000 [====================>.........] - ETA: 10:14 - loss: 3.9087 - regression_loss: 2.1368 - classification_loss: 1.7719
 726/1000 [====================>.........] - ETA: 10:12 - loss: 3.9059 - regression_loss: 2.1362 - classification_loss: 1.7698
 727/1000 [====================>.........] - ETA: 10:10 - loss: 3.9032 - regression_loss: 2.1356 - classification_loss: 1.7676
 728/1000 [====================>.........] - ETA: 10:07 - loss: 3.9004 - regression_loss: 2.1350 - classification_loss: 1.7655
 729/1000 [====================>.........] - ETA: 10:05 - loss: 3.8977 - regression_loss: 2.1343 - classification_loss: 1.7634
 730/1000 [====================>.........] - ETA: 10:03 - loss: 3.8949 - regression_loss: 2.1336 - classification_loss: 1.7613
 731/1000 [====================>.........] - ETA: 10:01 - loss: 3.8920 - regression_loss: 2.1329 - classification_loss: 1.7591
 732/1000 [====================>.........] - ETA: 9:59 - loss: 3.8893 - regression_loss: 2.1323 - classification_loss: 1.7570 
 733/1000 [====================>.........] - ETA: 9:57 - loss: 3.8865 - regression_loss: 2.1317 - classification_loss: 1.7549
 734/1000 [=====================>........] - ETA: 9:54 - loss: 3.8946 - regression_loss: 2.1304 - classification_loss: 1.7643
 735/1000 [=====================>........] - ETA: 9:52 - loss: 3.8917 - regression_loss: 2.1296 - classification_loss: 1.7622
 736/1000 [=====================>........] - ETA: 9:49 - loss: 3.8970 - regression_loss: 2.1283 - classification_loss: 1.7687
 737/1000 [=====================>........] - ETA: 9:47 - loss: 3.8943 - regression_loss: 2.1277 - classification_loss: 1.7665
 738/1000 [=====================>........] - ETA: 9:45 - loss: 3.8916 - regression_loss: 2.1271 - classification_loss: 1.7645
 739/1000 [=====================>........] - ETA: 9:43 - loss: 3.8889 - regression_loss: 2.1265 - classification_loss: 1.7624
 740/1000 [=====================>........] - ETA: 9:41 - loss: 3.8862 - regression_loss: 2.1259 - classification_loss: 1.7603
 741/1000 [=====================>........] - ETA: 9:38 - loss: 3.8833 - regression_loss: 2.1250 - classification_loss: 1.7583
 742/1000 [=====================>........] - ETA: 9:36 - loss: 3.8805 - regression_loss: 2.1243 - classification_loss: 1.7562
 743/1000 [=====================>........] - ETA: 9:34 - loss: 3.8829 - regression_loss: 2.1231 - classification_loss: 1.7599
 744/1000 [=====================>........] - ETA: 9:31 - loss: 3.8802 - regression_loss: 2.1224 - classification_loss: 1.7578
 745/1000 [=====================>........] - ETA: 9:29 - loss: 3.8773 - regression_loss: 2.1216 - classification_loss: 1.7557
 746/1000 [=====================>........] - ETA: 9:27 - loss: 3.8747 - regression_loss: 2.1210 - classification_loss: 1.7536
 747/1000 [=====================>........] - ETA: 9:25 - loss: 3.8719 - regression_loss: 2.1203 - classification_loss: 1.7516
 748/1000 [=====================>........] - ETA: 9:23 - loss: 3.8692 - regression_loss: 2.1197 - classification_loss: 1.7496
 749/1000 [=====================>........] - ETA: 9:20 - loss: 3.8665 - regression_loss: 2.1190 - classification_loss: 1.7475
 750/1000 [=====================>........] - ETA: 9:18 - loss: 3.8636 - regression_loss: 2.1181 - classification_loss: 1.7455
 751/1000 [=====================>........] - ETA: 9:16 - loss: 3.8609 - regression_loss: 2.1175 - classification_loss: 1.7434
 752/1000 [=====================>........] - ETA: 9:14 - loss: 3.8581 - regression_loss: 2.1167 - classification_loss: 1.7414
 753/1000 [=====================>........] - ETA: 9:12 - loss: 3.8554 - regression_loss: 2.1160 - classification_loss: 1.7393
 754/1000 [=====================>........] - ETA: 9:09 - loss: 3.8598 - regression_loss: 2.1147 - classification_loss: 1.7452
 755/1000 [=====================>........] - ETA: 9:07 - loss: 3.8572 - regression_loss: 2.1141 - classification_loss: 1.7431
 756/1000 [=====================>........] - ETA: 9:05 - loss: 3.8546 - regression_loss: 2.1134 - classification_loss: 1.7411
 757/1000 [=====================>........] - ETA: 9:03 - loss: 3.8519 - regression_loss: 2.1127 - classification_loss: 1.7391
 758/1000 [=====================>........] - ETA: 9:01 - loss: 3.8492 - regression_loss: 2.1121 - classification_loss: 1.7371
 759/1000 [=====================>........] - ETA: 8:58 - loss: 3.8465 - regression_loss: 2.1114 - classification_loss: 1.7351
 760/1000 [=====================>........] - ETA: 8:56 - loss: 3.8443 - regression_loss: 2.1111 - classification_loss: 1.7332
 761/1000 [=====================>........] - ETA: 8:54 - loss: 3.8417 - regression_loss: 2.1105 - classification_loss: 1.7312
 762/1000 [=====================>........] - ETA: 8:51 - loss: 3.8516 - regression_loss: 2.1092 - classification_loss: 1.7424
 763/1000 [=====================>........] - ETA: 8:49 - loss: 3.8489 - regression_loss: 2.1085 - classification_loss: 1.7404
 764/1000 [=====================>........] - ETA: 8:47 - loss: 3.8461 - regression_loss: 2.1077 - classification_loss: 1.7384
 765/1000 [=====================>........] - ETA: 8:45 - loss: 3.8435 - regression_loss: 2.1071 - classification_loss: 1.7364
 766/1000 [=====================>........] - ETA: 8:42 - loss: 3.8407 - regression_loss: 2.1063 - classification_loss: 1.7344
 767/1000 [======================>.......] - ETA: 8:40 - loss: 3.8381 - regression_loss: 2.1056 - classification_loss: 1.7325
 768/1000 [======================>.......] - ETA: 8:38 - loss: 3.8355 - regression_loss: 2.1050 - classification_loss: 1.7305
 769/1000 [======================>.......] - ETA: 8:36 - loss: 3.8369 - regression_loss: 2.1037 - classification_loss: 1.7332
 770/1000 [======================>.......] - ETA: 8:33 - loss: 3.8342 - regression_loss: 2.1030 - classification_loss: 1.7312
 771/1000 [======================>.......] - ETA: 8:31 - loss: 3.8349 - regression_loss: 2.1017 - classification_loss: 1.7332
 772/1000 [======================>.......] - ETA: 8:29 - loss: 3.8323 - regression_loss: 2.1009 - classification_loss: 1.7313
 773/1000 [======================>.......] - ETA: 8:27 - loss: 3.8297 - regression_loss: 2.1003 - classification_loss: 1.7294
 774/1000 [======================>.......] - ETA: 8:24 - loss: 3.8272 - regression_loss: 2.0997 - classification_loss: 1.7275
 775/1000 [======================>.......] - ETA: 8:22 - loss: 3.8246 - regression_loss: 2.0990 - classification_loss: 1.7256
 776/1000 [======================>.......] - ETA: 8:20 - loss: 3.8219 - regression_loss: 2.0983 - classification_loss: 1.7236
 777/1000 [======================>.......] - ETA: 8:18 - loss: 3.8193 - regression_loss: 2.0976 - classification_loss: 1.7217
 778/1000 [======================>.......] - ETA: 8:16 - loss: 3.8166 - regression_loss: 2.0969 - classification_loss: 1.7197
 779/1000 [======================>.......] - ETA: 8:13 - loss: 3.8140 - regression_loss: 2.0962 - classification_loss: 1.7178
 780/1000 [======================>.......] - ETA: 8:11 - loss: 3.8115 - regression_loss: 2.0956 - classification_loss: 1.7159
 781/1000 [======================>.......] - ETA: 8:09 - loss: 3.8208 - regression_loss: 2.0942 - classification_loss: 1.7266
 782/1000 [======================>.......] - ETA: 8:07 - loss: 3.8181 - regression_loss: 2.0934 - classification_loss: 1.7247
 783/1000 [======================>.......] - ETA: 8:04 - loss: 3.8154 - regression_loss: 2.0927 - classification_loss: 1.7227
 784/1000 [======================>.......] - ETA: 8:02 - loss: 3.8128 - regression_loss: 2.0920 - classification_loss: 1.7208
 785/1000 [======================>.......] - ETA: 8:00 - loss: 3.8102 - regression_loss: 2.0913 - classification_loss: 1.7189
 786/1000 [======================>.......] - ETA: 7:58 - loss: 3.8110 - regression_loss: 2.0900 - classification_loss: 1.7210
 787/1000 [======================>.......] - ETA: 7:55 - loss: 3.8085 - regression_loss: 2.0894 - classification_loss: 1.7191
 788/1000 [======================>.......] - ETA: 7:53 - loss: 3.8059 - regression_loss: 2.0887 - classification_loss: 1.7172
 789/1000 [======================>.......] - ETA: 7:51 - loss: 3.8033 - regression_loss: 2.0879 - classification_loss: 1.7153
 790/1000 [======================>.......] - ETA: 7:49 - loss: 3.8005 - regression_loss: 2.0871 - classification_loss: 1.7134
 791/1000 [======================>.......] - ETA: 7:47 - loss: 3.7979 - regression_loss: 2.0864 - classification_loss: 1.7115
 792/1000 [======================>.......] - ETA: 7:44 - loss: 3.7952 - regression_loss: 2.0855 - classification_loss: 1.7096
 793/1000 [======================>.......] - ETA: 7:42 - loss: 3.7925 - regression_loss: 2.0848 - classification_loss: 1.7078
 794/1000 [======================>.......] - ETA: 7:40 - loss: 3.7900 - regression_loss: 2.0841 - classification_loss: 1.7059
 795/1000 [======================>.......] - ETA: 7:38 - loss: 3.7874 - regression_loss: 2.0834 - classification_loss: 1.7040
 796/1000 [======================>.......] - ETA: 7:35 - loss: 3.7968 - regression_loss: 2.0820 - classification_loss: 1.7147
 797/1000 [======================>.......] - ETA: 7:33 - loss: 3.7942 - regression_loss: 2.0814 - classification_loss: 1.7128
 798/1000 [======================>.......] - ETA: 7:31 - loss: 3.7916 - regression_loss: 2.0807 - classification_loss: 1.7109
 799/1000 [======================>.......] - ETA: 7:29 - loss: 3.7892 - regression_loss: 2.0801 - classification_loss: 1.7091
 800/1000 [=======================>......] - ETA: 7:27 - loss: 3.7866 - regression_loss: 2.0794 - classification_loss: 1.7072
 801/1000 [=======================>......] - ETA: 7:24 - loss: 3.7867 - regression_loss: 2.0780 - classification_loss: 1.7086
 802/1000 [=======================>......] - ETA: 7:22 - loss: 3.7842 - regression_loss: 2.0774 - classification_loss: 1.7068
 803/1000 [=======================>......] - ETA: 7:20 - loss: 3.7818 - regression_loss: 2.0768 - classification_loss: 1.7050
 804/1000 [=======================>......] - ETA: 7:17 - loss: 3.7793 - regression_loss: 2.0761 - classification_loss: 1.7032
 805/1000 [=======================>......] - ETA: 7:15 - loss: 3.7767 - regression_loss: 2.0754 - classification_loss: 1.7014
 806/1000 [=======================>......] - ETA: 7:13 - loss: 3.7742 - regression_loss: 2.0747 - classification_loss: 1.6995
 807/1000 [=======================>......] - ETA: 7:11 - loss: 3.7717 - regression_loss: 2.0740 - classification_loss: 1.6977
 808/1000 [=======================>......] - ETA: 7:09 - loss: 3.7691 - regression_loss: 2.0733 - classification_loss: 1.6958
 809/1000 [=======================>......] - ETA: 7:06 - loss: 3.7666 - regression_loss: 2.0726 - classification_loss: 1.6940
 810/1000 [=======================>......] - ETA: 7:04 - loss: 3.7640 - regression_loss: 2.0719 - classification_loss: 1.6921
 811/1000 [=======================>......] - ETA: 7:02 - loss: 3.7721 - regression_loss: 2.0706 - classification_loss: 1.7014
 812/1000 [=======================>......] - ETA: 7:00 - loss: 3.7694 - regression_loss: 2.0698 - classification_loss: 1.6996
 813/1000 [=======================>......] - ETA: 6:57 - loss: 3.7669 - regression_loss: 2.0691 - classification_loss: 1.6978
 814/1000 [=======================>......] - ETA: 6:55 - loss: 3.7644 - regression_loss: 2.0685 - classification_loss: 1.6960
 815/1000 [=======================>......] - ETA: 6:53 - loss: 3.7617 - regression_loss: 2.0676 - classification_loss: 1.6941
 816/1000 [=======================>......] - ETA: 6:50 - loss: 3.7638 - regression_loss: 2.0663 - classification_loss: 1.6975
 817/1000 [=======================>......] - ETA: 6:48 - loss: 3.7613 - regression_loss: 2.0656 - classification_loss: 1.6957
 818/1000 [=======================>......] - ETA: 6:46 - loss: 3.7589 - regression_loss: 2.0650 - classification_loss: 1.6939
 819/1000 [=======================>......] - ETA: 6:44 - loss: 3.7564 - regression_loss: 2.0644 - classification_loss: 1.6921
 820/1000 [=======================>......] - ETA: 6:42 - loss: 3.7569 - regression_loss: 2.0631 - classification_loss: 1.6938
 821/1000 [=======================>......] - ETA: 6:39 - loss: 3.7544 - regression_loss: 2.0624 - classification_loss: 1.6920
 822/1000 [=======================>......] - ETA: 6:37 - loss: 3.7520 - regression_loss: 2.0618 - classification_loss: 1.6902
 823/1000 [=======================>......] - ETA: 6:35 - loss: 3.7496 - regression_loss: 2.0612 - classification_loss: 1.6884
 824/1000 [=======================>......] - ETA: 6:33 - loss: 3.7472 - regression_loss: 2.0605 - classification_loss: 1.6867
 825/1000 [=======================>......] - ETA: 6:31 - loss: 3.7447 - regression_loss: 2.0598 - classification_loss: 1.6849
 826/1000 [=======================>......] - ETA: 6:28 - loss: 3.7421 - regression_loss: 2.0590 - classification_loss: 1.6831
 827/1000 [=======================>......] - ETA: 6:26 - loss: 3.7395 - regression_loss: 2.0582 - classification_loss: 1.6814
 828/1000 [=======================>......] - ETA: 6:24 - loss: 3.7370 - regression_loss: 2.0574 - classification_loss: 1.6796
 829/1000 [=======================>......] - ETA: 6:22 - loss: 3.7346 - regression_loss: 2.0568 - classification_loss: 1.6778
 830/1000 [=======================>......] - ETA: 6:19 - loss: 3.7322 - regression_loss: 2.0561 - classification_loss: 1.6761
 831/1000 [=======================>......] - ETA: 6:17 - loss: 3.7297 - regression_loss: 2.0554 - classification_loss: 1.6743
 832/1000 [=======================>......] - ETA: 6:15 - loss: 3.7326 - regression_loss: 2.0541 - classification_loss: 1.6785
 833/1000 [=======================>......] - ETA: 6:13 - loss: 3.7302 - regression_loss: 2.0534 - classification_loss: 1.6767
 834/1000 [========================>.....] - ETA: 6:10 - loss: 3.7303 - regression_loss: 2.0522 - classification_loss: 1.6781
 835/1000 [========================>.....] - ETA: 6:08 - loss: 3.7279 - regression_loss: 2.0515 - classification_loss: 1.6764
 836/1000 [========================>.....] - ETA: 6:06 - loss: 3.7255 - regression_loss: 2.0508 - classification_loss: 1.6748
 837/1000 [========================>.....] - ETA: 6:04 - loss: 3.7231 - regression_loss: 2.0501 - classification_loss: 1.6731
 838/1000 [========================>.....] - ETA: 6:01 - loss: 3.7210 - regression_loss: 2.0496 - classification_loss: 1.6714
 839/1000 [========================>.....] - ETA: 5:59 - loss: 3.7187 - regression_loss: 2.0490 - classification_loss: 1.6697
 840/1000 [========================>.....] - ETA: 5:57 - loss: 3.7163 - regression_loss: 2.0483 - classification_loss: 1.6680
 841/1000 [========================>.....] - ETA: 5:55 - loss: 3.7139 - regression_loss: 2.0476 - classification_loss: 1.6663
 842/1000 [========================>.....] - ETA: 5:53 - loss: 3.7114 - regression_loss: 2.0469 - classification_loss: 1.6645
 843/1000 [========================>.....] - ETA: 5:50 - loss: 3.7090 - regression_loss: 2.0463 - classification_loss: 1.6628
 844/1000 [========================>.....] - ETA: 5:48 - loss: 3.7066 - regression_loss: 2.0455 - classification_loss: 1.6611
 845/1000 [========================>.....] - ETA: 5:46 - loss: 3.7043 - regression_loss: 2.0449 - classification_loss: 1.6594
 846/1000 [========================>.....] - ETA: 5:44 - loss: 3.7019 - regression_loss: 2.0442 - classification_loss: 1.6577
 847/1000 [========================>.....] - ETA: 5:41 - loss: 3.7104 - regression_loss: 2.0429 - classification_loss: 1.6675
 848/1000 [========================>.....] - ETA: 5:39 - loss: 3.7148 - regression_loss: 2.0417 - classification_loss: 1.6731
 849/1000 [========================>.....] - ETA: 5:37 - loss: 3.7125 - regression_loss: 2.0411 - classification_loss: 1.6714
 850/1000 [========================>.....] - ETA: 5:35 - loss: 3.7101 - regression_loss: 2.0404 - classification_loss: 1.6697
 851/1000 [========================>.....] - ETA: 5:32 - loss: 3.7078 - regression_loss: 2.0398 - classification_loss: 1.6680
 852/1000 [========================>.....] - ETA: 5:30 - loss: 3.7056 - regression_loss: 2.0392 - classification_loss: 1.6664
 853/1000 [========================>.....] - ETA: 5:28 - loss: 3.7032 - regression_loss: 2.0385 - classification_loss: 1.6647
 854/1000 [========================>.....] - ETA: 5:26 - loss: 3.7008 - regression_loss: 2.0377 - classification_loss: 1.6630
 855/1000 [========================>.....] - ETA: 5:23 - loss: 3.6984 - regression_loss: 2.0371 - classification_loss: 1.6613
 856/1000 [========================>.....] - ETA: 5:21 - loss: 3.6960 - regression_loss: 2.0364 - classification_loss: 1.6596
 857/1000 [========================>.....] - ETA: 5:19 - loss: 3.7020 - regression_loss: 2.0351 - classification_loss: 1.6670
 858/1000 [========================>.....] - ETA: 5:17 - loss: 3.6997 - regression_loss: 2.0345 - classification_loss: 1.6653
 859/1000 [========================>.....] - ETA: 5:14 - loss: 3.6972 - regression_loss: 2.0336 - classification_loss: 1.6636
 860/1000 [========================>.....] - ETA: 5:12 - loss: 3.6948 - regression_loss: 2.0329 - classification_loss: 1.6619
 861/1000 [========================>.....] - ETA: 5:10 - loss: 3.6924 - regression_loss: 2.0322 - classification_loss: 1.6602
 862/1000 [========================>.....] - ETA: 5:08 - loss: 3.6900 - regression_loss: 2.0315 - classification_loss: 1.6585
 863/1000 [========================>.....] - ETA: 5:06 - loss: 3.6878 - regression_loss: 2.0309 - classification_loss: 1.6568
 864/1000 [========================>.....] - ETA: 5:03 - loss: 3.6855 - regression_loss: 2.0303 - classification_loss: 1.6552
 865/1000 [========================>.....] - ETA: 5:01 - loss: 3.6829 - regression_loss: 2.0295 - classification_loss: 1.6535
 866/1000 [========================>.....] - ETA: 4:59 - loss: 3.6805 - regression_loss: 2.0287 - classification_loss: 1.6518
 867/1000 [=========================>....] - ETA: 4:57 - loss: 3.6832 - regression_loss: 2.0275 - classification_loss: 1.6557
 868/1000 [=========================>....] - ETA: 4:54 - loss: 3.6809 - regression_loss: 2.0268 - classification_loss: 1.6541
 869/1000 [=========================>....] - ETA: 4:52 - loss: 3.6833 - regression_loss: 2.0255 - classification_loss: 1.6578
 870/1000 [=========================>....] - ETA: 4:50 - loss: 3.6810 - regression_loss: 2.0249 - classification_loss: 1.6562
 871/1000 [=========================>....] - ETA: 4:48 - loss: 3.6787 - regression_loss: 2.0242 - classification_loss: 1.6545
 872/1000 [=========================>....] - ETA: 4:45 - loss: 3.6763 - regression_loss: 2.0235 - classification_loss: 1.6528
 873/1000 [=========================>....] - ETA: 4:43 - loss: 3.6738 - regression_loss: 2.0227 - classification_loss: 1.6512
 874/1000 [=========================>....] - ETA: 4:41 - loss: 3.6715 - regression_loss: 2.0219 - classification_loss: 1.6495
 875/1000 [=========================>....] - ETA: 4:39 - loss: 3.6691 - regression_loss: 2.0212 - classification_loss: 1.6479
 876/1000 [=========================>....] - ETA: 4:36 - loss: 3.6727 - regression_loss: 2.0200 - classification_loss: 1.6528
 877/1000 [=========================>....] - ETA: 4:34 - loss: 3.6705 - regression_loss: 2.0194 - classification_loss: 1.6511
 878/1000 [=========================>....] - ETA: 4:32 - loss: 3.6682 - regression_loss: 2.0188 - classification_loss: 1.6494
 879/1000 [=========================>....] - ETA: 4:30 - loss: 3.6657 - regression_loss: 2.0180 - classification_loss: 1.6478
 880/1000 [=========================>....] - ETA: 4:28 - loss: 3.6634 - regression_loss: 2.0172 - classification_loss: 1.6462
 881/1000 [=========================>....] - ETA: 4:25 - loss: 3.6610 - regression_loss: 2.0165 - classification_loss: 1.6445
 882/1000 [=========================>....] - ETA: 4:23 - loss: 3.6587 - regression_loss: 2.0158 - classification_loss: 1.6429
 883/1000 [=========================>....] - ETA: 4:21 - loss: 3.6564 - regression_loss: 2.0152 - classification_loss: 1.6412
 884/1000 [=========================>....] - ETA: 4:19 - loss: 3.6541 - regression_loss: 2.0145 - classification_loss: 1.6396
 885/1000 [=========================>....] - ETA: 4:17 - loss: 3.6517 - regression_loss: 2.0138 - classification_loss: 1.6379
 886/1000 [=========================>....] - ETA: 4:14 - loss: 3.6557 - regression_loss: 2.0125 - classification_loss: 1.6432
 887/1000 [=========================>....] - ETA: 4:12 - loss: 3.6533 - regression_loss: 2.0117 - classification_loss: 1.6416
 888/1000 [=========================>....] - ETA: 4:10 - loss: 3.6510 - regression_loss: 2.0110 - classification_loss: 1.6400
 889/1000 [=========================>....] - ETA: 4:07 - loss: 3.6486 - regression_loss: 2.0102 - classification_loss: 1.6384
 890/1000 [=========================>....] - ETA: 4:05 - loss: 3.6463 - regression_loss: 2.0096 - classification_loss: 1.6367
 891/1000 [=========================>....] - ETA: 4:03 - loss: 3.6469 - regression_loss: 2.0083 - classification_loss: 1.6386
 892/1000 [=========================>....] - ETA: 4:01 - loss: 3.6445 - regression_loss: 2.0075 - classification_loss: 1.6370
 893/1000 [=========================>....] - ETA: 3:58 - loss: 3.6421 - regression_loss: 2.0067 - classification_loss: 1.6354
 894/1000 [=========================>....] - ETA: 3:56 - loss: 3.6399 - regression_loss: 2.0061 - classification_loss: 1.6338
 895/1000 [=========================>....] - ETA: 3:54 - loss: 3.6376 - regression_loss: 2.0054 - classification_loss: 1.6322
 896/1000 [=========================>....] - ETA: 3:52 - loss: 3.6353 - regression_loss: 2.0047 - classification_loss: 1.6306
 897/1000 [=========================>....] - ETA: 3:50 - loss: 3.6330 - regression_loss: 2.0041 - classification_loss: 1.6290
 898/1000 [=========================>....] - ETA: 3:47 - loss: 3.6308 - regression_loss: 2.0034 - classification_loss: 1.6274
 899/1000 [=========================>....] - ETA: 3:45 - loss: 3.6285 - regression_loss: 2.0027 - classification_loss: 1.6258
 900/1000 [==========================>...] - ETA: 3:43 - loss: 3.6267 - regression_loss: 2.0024 - classification_loss: 1.6242
 901/1000 [==========================>...] - ETA: 3:41 - loss: 3.6244 - regression_loss: 2.0017 - classification_loss: 1.6227
 902/1000 [==========================>...] - ETA: 3:38 - loss: 3.6221 - regression_loss: 2.0010 - classification_loss: 1.6211
 903/1000 [==========================>...] - ETA: 3:36 - loss: 3.6261 - regression_loss: 1.9998 - classification_loss: 1.6263
 904/1000 [==========================>...] - ETA: 3:34 - loss: 3.6276 - regression_loss: 1.9987 - classification_loss: 1.6290
 905/1000 [==========================>...] - ETA: 3:32 - loss: 3.6255 - regression_loss: 1.9981 - classification_loss: 1.6274
 906/1000 [==========================>...] - ETA: 3:29 - loss: 3.6234 - regression_loss: 1.9974 - classification_loss: 1.6259
 907/1000 [==========================>...] - ETA: 3:27 - loss: 3.6213 - regression_loss: 1.9968 - classification_loss: 1.6244
 908/1000 [==========================>...] - ETA: 3:25 - loss: 3.6193 - regression_loss: 1.9963 - classification_loss: 1.6230
 909/1000 [==========================>...] - ETA: 3:23 - loss: 3.6171 - regression_loss: 1.9956 - classification_loss: 1.6214
 910/1000 [==========================>...] - ETA: 3:21 - loss: 3.6149 - regression_loss: 1.9950 - classification_loss: 1.6199
 911/1000 [==========================>...] - ETA: 3:18 - loss: 3.6129 - regression_loss: 1.9944 - classification_loss: 1.6184
 912/1000 [==========================>...] - ETA: 3:16 - loss: 3.6107 - regression_loss: 1.9937 - classification_loss: 1.6169
 913/1000 [==========================>...] - ETA: 3:14 - loss: 3.6103 - regression_loss: 1.9924 - classification_loss: 1.6179
 914/1000 [==========================>...] - ETA: 3:12 - loss: 3.6081 - regression_loss: 1.9918 - classification_loss: 1.6164
 915/1000 [==========================>...] - ETA: 3:09 - loss: 3.6060 - regression_loss: 1.9912 - classification_loss: 1.6148
 916/1000 [==========================>...] - ETA: 3:07 - loss: 3.6038 - regression_loss: 1.9906 - classification_loss: 1.6133
 917/1000 [==========================>...] - ETA: 3:05 - loss: 3.6018 - regression_loss: 1.9900 - classification_loss: 1.6117
 918/1000 [==========================>...] - ETA: 3:03 - loss: 3.5996 - regression_loss: 1.9894 - classification_loss: 1.6102
 919/1000 [==========================>...] - ETA: 3:00 - loss: 3.6016 - regression_loss: 1.9881 - classification_loss: 1.6135
 920/1000 [==========================>...] - ETA: 2:58 - loss: 3.5995 - regression_loss: 1.9875 - classification_loss: 1.6119
 921/1000 [==========================>...] - ETA: 2:56 - loss: 3.5974 - regression_loss: 1.9870 - classification_loss: 1.6104
 922/1000 [==========================>...] - ETA: 2:54 - loss: 3.5950 - regression_loss: 1.9862 - classification_loss: 1.6089
 923/1000 [==========================>...] - ETA: 2:51 - loss: 3.5928 - regression_loss: 1.9855 - classification_loss: 1.6073
 924/1000 [==========================>...] - ETA: 2:49 - loss: 3.5907 - regression_loss: 1.9849 - classification_loss: 1.6058
 925/1000 [==========================>...] - ETA: 2:47 - loss: 3.5908 - regression_loss: 1.9836 - classification_loss: 1.6072
 926/1000 [==========================>...] - ETA: 2:45 - loss: 3.5886 - regression_loss: 1.9829 - classification_loss: 1.6056
 927/1000 [==========================>...] - ETA: 2:43 - loss: 3.5864 - regression_loss: 1.9823 - classification_loss: 1.6041
 928/1000 [==========================>...] - ETA: 2:40 - loss: 3.5842 - regression_loss: 1.9816 - classification_loss: 1.6026
 929/1000 [==========================>...] - ETA: 2:38 - loss: 3.5821 - regression_loss: 1.9810 - classification_loss: 1.6011
 930/1000 [==========================>...] - ETA: 2:36 - loss: 3.5800 - regression_loss: 1.9804 - classification_loss: 1.5996
 931/1000 [==========================>...] - ETA: 2:34 - loss: 3.5779 - regression_loss: 1.9798 - classification_loss: 1.5981
 932/1000 [==========================>...] - ETA: 2:31 - loss: 3.5758 - regression_loss: 1.9793 - classification_loss: 1.5966
 933/1000 [==========================>...] - ETA: 2:29 - loss: 3.5736 - regression_loss: 1.9785 - classification_loss: 1.5951
 934/1000 [===========================>..] - ETA: 2:27 - loss: 3.5714 - regression_loss: 1.9778 - classification_loss: 1.5936
 935/1000 [===========================>..] - ETA: 2:25 - loss: 3.5743 - regression_loss: 1.9766 - classification_loss: 1.5977
 936/1000 [===========================>..] - ETA: 2:22 - loss: 3.5721 - regression_loss: 1.9759 - classification_loss: 1.5962
 937/1000 [===========================>..] - ETA: 2:20 - loss: 3.5700 - regression_loss: 1.9753 - classification_loss: 1.5947
 938/1000 [===========================>..] - ETA: 2:18 - loss: 3.5678 - regression_loss: 1.9746 - classification_loss: 1.5932
 939/1000 [===========================>..] - ETA: 2:16 - loss: 3.5679 - regression_loss: 1.9734 - classification_loss: 1.5944
 940/1000 [===========================>..] - ETA: 2:13 - loss: 3.5658 - regression_loss: 1.9728 - classification_loss: 1.5930
 941/1000 [===========================>..] - ETA: 2:11 - loss: 3.5637 - regression_loss: 1.9722 - classification_loss: 1.5915
 942/1000 [===========================>..] - ETA: 2:09 - loss: 3.5617 - regression_loss: 1.9716 - classification_loss: 1.5901
 943/1000 [===========================>..] - ETA: 2:07 - loss: 3.5599 - regression_loss: 1.9713 - classification_loss: 1.5886
 944/1000 [===========================>..] - ETA: 2:05 - loss: 3.5577 - regression_loss: 1.9706 - classification_loss: 1.5871
 945/1000 [===========================>..] - ETA: 2:02 - loss: 3.5556 - regression_loss: 1.9699 - classification_loss: 1.5857
 946/1000 [===========================>..] - ETA: 2:00 - loss: 3.5535 - regression_loss: 1.9693 - classification_loss: 1.5842
 947/1000 [===========================>..] - ETA: 1:58 - loss: 3.5514 - regression_loss: 1.9687 - classification_loss: 1.5827
 948/1000 [===========================>..] - ETA: 1:56 - loss: 3.5492 - regression_loss: 1.9680 - classification_loss: 1.5813
 949/1000 [===========================>..] - ETA: 1:53 - loss: 3.5471 - regression_loss: 1.9673 - classification_loss: 1.5798
 950/1000 [===========================>..] - ETA: 1:51 - loss: 3.5450 - regression_loss: 1.9666 - classification_loss: 1.5783
 951/1000 [===========================>..] - ETA: 1:49 - loss: 3.5476 - regression_loss: 1.9655 - classification_loss: 1.5821
 952/1000 [===========================>..] - ETA: 1:47 - loss: 3.5455 - regression_loss: 1.9649 - classification_loss: 1.5806
 953/1000 [===========================>..] - ETA: 1:44 - loss: 3.5435 - regression_loss: 1.9644 - classification_loss: 1.5791
 954/1000 [===========================>..] - ETA: 1:42 - loss: 3.5415 - regression_loss: 1.9638 - classification_loss: 1.5777
 955/1000 [===========================>..] - ETA: 1:40 - loss: 3.5410 - regression_loss: 1.9626 - classification_loss: 1.5783
 956/1000 [===========================>..] - ETA: 1:38 - loss: 3.5389 - regression_loss: 1.9620 - classification_loss: 1.5769
 957/1000 [===========================>..] - ETA: 1:36 - loss: 3.5368 - regression_loss: 1.9613 - classification_loss: 1.5755
 958/1000 [===========================>..] - ETA: 1:33 - loss: 3.5349 - regression_loss: 1.9608 - classification_loss: 1.5741
 959/1000 [===========================>..] - ETA: 1:31 - loss: 3.5329 - regression_loss: 1.9602 - classification_loss: 1.5727
 960/1000 [===========================>..] - ETA: 1:29 - loss: 3.5319 - regression_loss: 1.9590 - classification_loss: 1.5729
 961/1000 [===========================>..] - ETA: 1:27 - loss: 3.5300 - regression_loss: 1.9585 - classification_loss: 1.5715
 962/1000 [===========================>..] - ETA: 1:24 - loss: 3.5281 - regression_loss: 1.9580 - classification_loss: 1.5701
 963/1000 [===========================>..] - ETA: 1:22 - loss: 3.5259 - regression_loss: 1.9573 - classification_loss: 1.5687
 964/1000 [===========================>..] - ETA: 1:20 - loss: 3.5240 - regression_loss: 1.9567 - classification_loss: 1.5673
 965/1000 [===========================>..] - ETA: 1:18 - loss: 3.5219 - regression_loss: 1.9560 - classification_loss: 1.5658
 966/1000 [===========================>..] - ETA: 1:15 - loss: 3.5198 - regression_loss: 1.9554 - classification_loss: 1.5644
 967/1000 [============================>.] - ETA: 1:13 - loss: 3.5178 - regression_loss: 1.9548 - classification_loss: 1.5630
 968/1000 [============================>.] - ETA: 1:11 - loss: 3.5158 - regression_loss: 1.9542 - classification_loss: 1.5616
 969/1000 [============================>.] - ETA: 1:09 - loss: 3.5137 - regression_loss: 1.9535 - classification_loss: 1.5602
 970/1000 [============================>.] - ETA: 1:07 - loss: 3.5117 - regression_loss: 1.9529 - classification_loss: 1.5588
 971/1000 [============================>.] - ETA: 1:04 - loss: 3.5095 - regression_loss: 1.9521 - classification_loss: 1.5574
 972/1000 [============================>.] - ETA: 1:02 - loss: 3.5075 - regression_loss: 1.9515 - classification_loss: 1.5560
 973/1000 [============================>.] - ETA: 1:00 - loss: 3.5092 - regression_loss: 1.9503 - classification_loss: 1.5589
 974/1000 [============================>.] - ETA: 58s - loss: 3.5101 - regression_loss: 1.9492 - classification_loss: 1.5609 
 975/1000 [============================>.] - ETA: 55s - loss: 3.5081 - regression_loss: 1.9486 - classification_loss: 1.5595
 976/1000 [============================>.] - ETA: 53s - loss: 3.5062 - regression_loss: 1.9481 - classification_loss: 1.5581
 977/1000 [============================>.] - ETA: 51s - loss: 3.5042 - regression_loss: 1.9475 - classification_loss: 1.5568
 978/1000 [============================>.] - ETA: 49s - loss: 3.5022 - regression_loss: 1.9468 - classification_loss: 1.5554
 979/1000 [============================>.] - ETA: 46s - loss: 3.5002 - regression_loss: 1.9461 - classification_loss: 1.5540
 980/1000 [============================>.] - ETA: 44s - loss: 3.4983 - regression_loss: 1.9456 - classification_loss: 1.5527
 981/1000 [============================>.] - ETA: 42s - loss: 3.4963 - regression_loss: 1.9450 - classification_loss: 1.5513
 982/1000 [============================>.] - ETA: 40s - loss: 3.4943 - regression_loss: 1.9444 - classification_loss: 1.5499
 983/1000 [============================>.] - ETA: 37s - loss: 3.4923 - regression_loss: 1.9437 - classification_loss: 1.5485
 984/1000 [============================>.] - ETA: 35s - loss: 3.4901 - regression_loss: 1.9429 - classification_loss: 1.5472
 985/1000 [============================>.] - ETA: 33s - loss: 3.4882 - regression_loss: 1.9424 - classification_loss: 1.5458
 986/1000 [============================>.] - ETA: 31s - loss: 3.4906 - regression_loss: 1.9412 - classification_loss: 1.5494
 987/1000 [============================>.] - ETA: 29s - loss: 3.4888 - regression_loss: 1.9408 - classification_loss: 1.5480
 988/1000 [============================>.] - ETA: 26s - loss: 3.4869 - regression_loss: 1.9402 - classification_loss: 1.5466
 989/1000 [============================>.] - ETA: 24s - loss: 3.4849 - regression_loss: 1.9396 - classification_loss: 1.5453
 990/1000 [============================>.] - ETA: 22s - loss: 3.4844 - regression_loss: 1.9384 - classification_loss: 1.5460
 991/1000 [============================>.] - ETA: 20s - loss: 3.4827 - regression_loss: 1.9381 - classification_loss: 1.5446
 992/1000 [============================>.] - ETA: 17s - loss: 3.4810 - regression_loss: 1.9377 - classification_loss: 1.5433
 993/1000 [============================>.] - ETA: 15s - loss: 3.4793 - regression_loss: 1.9373 - classification_loss: 1.5420
 994/1000 [============================>.] - ETA: 13s - loss: 3.4775 - regression_loss: 1.9368 - classification_loss: 1.5407
 995/1000 [============================>.] - ETA: 11s - loss: 3.4756 - regression_loss: 1.9362 - classification_loss: 1.5394
 996/1000 [============================>.] - ETA: 8s - loss: 3.4740 - regression_loss: 1.9351 - classification_loss: 1.5388 
 997/1000 [============================>.] - ETA: 6s - loss: 3.4722 - regression_loss: 1.9346 - classification_loss: 1.5375
 998/1000 [============================>.] - ETA: 4s - loss: 3.4701 - regression_loss: 1.9339 - classification_loss: 1.5362
 999/1000 [============================>.] - ETA: 2s - loss: 3.4684 - regression_loss: 1.9334 - classification_loss: 1.5349
1000/1000 [==============================] - 2233s 2s/step - loss: 3.4666 - regression_loss: 1.9330 - classification_loss: 1.5337

Epoch 00001: saving model to ./snapshots/resnet50_csv_01.h5
Epoch 2/10

   1/1000 [..............................] - ETA: 6:18 - loss: 1.6234 - regression_loss: 1.4262 - classification_loss: 0.1972
   2/1000 [..............................] - ETA: 6:12 - loss: 1.6063 - regression_loss: 1.4057 - classification_loss: 0.2005
   3/1000 [..............................] - ETA: 6:11 - loss: 1.5650 - regression_loss: 1.3713 - classification_loss: 0.1936
   4/1000 [..............................] - ETA: 6:16 - loss: 1.5505 - regression_loss: 1.3574 - classification_loss: 0.1931
   5/1000 [..............................] - ETA: 8:35 - loss: 1.5334 - regression_loss: 1.3373 - classification_loss: 0.1961
   6/1000 [..............................] - ETA: 8:11 - loss: 1.5354 - regression_loss: 1.3366 - classification_loss: 0.1988
   7/1000 [..............................] - ETA: 7:53 - loss: 2.1234 - regression_loss: 1.2733 - classification_loss: 0.8501
   8/1000 [..............................] - ETA: 7:39 - loss: 2.0635 - regression_loss: 1.2968 - classification_loss: 0.7667
   9/1000 [..............................] - ETA: 10:46 - loss: 2.0117 - regression_loss: 1.3100 - classification_loss: 0.7018
  10/1000 [..............................] - ETA: 13:43 - loss: 1.9613 - regression_loss: 1.3091 - classification_loss: 0.6522
  11/1000 [..............................] - ETA: 16:23 - loss: 1.9253 - regression_loss: 1.3147 - classification_loss: 0.6106
  12/1000 [..............................] - ETA: 17:24 - loss: 1.8942 - regression_loss: 1.3174 - classification_loss: 0.5768
  13/1000 [..............................] - ETA: 18:06 - loss: 1.9950 - regression_loss: 1.2842 - classification_loss: 0.7108
  14/1000 [..............................] - ETA: 19:52 - loss: 1.9584 - regression_loss: 1.2851 - classification_loss: 0.6733
  15/1000 [..............................] - ETA: 20:54 - loss: 1.9272 - regression_loss: 1.2858 - classification_loss: 0.6414
  16/1000 [..............................] - ETA: 22:24 - loss: 1.9055 - regression_loss: 1.2921 - classification_loss: 0.6135
  17/1000 [..............................] - ETA: 23:33 - loss: 1.8885 - regression_loss: 1.2991 - classification_loss: 0.5894
  18/1000 [..............................] - ETA: 23:49 - loss: 1.8603 - regression_loss: 1.2927 - classification_loss: 0.5676
  19/1000 [..............................] - ETA: 24:39 - loss: 1.8401 - regression_loss: 1.2917 - classification_loss: 0.5485
  20/1000 [..............................] - ETA: 24:40 - loss: 1.9155 - regression_loss: 1.2635 - classification_loss: 0.6520
  21/1000 [..............................] - ETA: 25:09 - loss: 1.8941 - regression_loss: 1.2644 - classification_loss: 0.6297
  22/1000 [..............................] - ETA: 25:51 - loss: 1.8743 - regression_loss: 1.2650 - classification_loss: 0.6093
  23/1000 [..............................] - ETA: 26:26 - loss: 1.8568 - regression_loss: 1.2650 - classification_loss: 0.5918
  24/1000 [..............................] - ETA: 26:22 - loss: 1.8859 - regression_loss: 1.2479 - classification_loss: 0.6381
  25/1000 [..............................] - ETA: 26:42 - loss: 1.8717 - regression_loss: 1.2509 - classification_loss: 0.6208
  26/1000 [..............................] - ETA: 27:15 - loss: 1.8627 - regression_loss: 1.2567 - classification_loss: 0.6060
  27/1000 [..............................] - ETA: 27:51 - loss: 1.8529 - regression_loss: 1.2616 - classification_loss: 0.5913
  28/1000 [..............................] - ETA: 27:49 - loss: 1.8354 - regression_loss: 1.2583 - classification_loss: 0.5771
  29/1000 [..............................] - ETA: 28:14 - loss: 1.8225 - regression_loss: 1.2587 - classification_loss: 0.5638
  30/1000 [..............................] - ETA: 28:37 - loss: 1.8100 - regression_loss: 1.2590 - classification_loss: 0.5510
  31/1000 [..............................] - ETA: 28:29 - loss: 1.8620 - regression_loss: 1.2444 - classification_loss: 0.6176
  32/1000 [..............................] - ETA: 28:56 - loss: 1.8533 - regression_loss: 1.2492 - classification_loss: 0.6041
  33/1000 [..............................] - ETA: 29:04 - loss: 1.8411 - regression_loss: 1.2496 - classification_loss: 0.5915
  34/1000 [>.............................] - ETA: 29:25 - loss: 1.8312 - regression_loss: 1.2518 - classification_loss: 0.5794
  35/1000 [>.............................] - ETA: 29:20 - loss: 1.8170 - regression_loss: 1.2485 - classification_loss: 0.5686
  36/1000 [>.............................] - ETA: 29:34 - loss: 1.8078 - regression_loss: 1.2493 - classification_loss: 0.5585
  37/1000 [>.............................] - ETA: 29:55 - loss: 1.8003 - regression_loss: 1.2521 - classification_loss: 0.5482
  38/1000 [>.............................] - ETA: 29:49 - loss: 1.7874 - regression_loss: 1.2485 - classification_loss: 0.5389
  39/1000 [>.............................] - ETA: 30:02 - loss: 1.7789 - regression_loss: 1.2486 - classification_loss: 0.5303
  40/1000 [>.............................] - ETA: 30:17 - loss: 1.7723 - regression_loss: 1.2506 - classification_loss: 0.5216
  41/1000 [>.............................] - ETA: 30:21 - loss: 1.7640 - regression_loss: 1.2506 - classification_loss: 0.5134
  42/1000 [>.............................] - ETA: 30:11 - loss: 1.7989 - regression_loss: 1.2382 - classification_loss: 0.5606
  43/1000 [>.............................] - ETA: 30:23 - loss: 1.7903 - regression_loss: 1.2386 - classification_loss: 0.5517
  44/1000 [>.............................] - ETA: 30:26 - loss: 1.7826 - regression_loss: 1.2393 - classification_loss: 0.5433
  45/1000 [>.............................] - ETA: 30:40 - loss: 1.7777 - regression_loss: 1.2423 - classification_loss: 0.5354
  46/1000 [>.............................] - ETA: 30:50 - loss: 1.7699 - regression_loss: 1.2424 - classification_loss: 0.5276
  47/1000 [>.............................] - ETA: 30:58 - loss: 1.7627 - regression_loss: 1.2420 - classification_loss: 0.5207
  48/1000 [>.............................] - ETA: 30:51 - loss: 1.7558 - regression_loss: 1.2418 - classification_loss: 0.5139
  49/1000 [>.............................] - ETA: 30:42 - loss: 1.7973 - regression_loss: 1.2327 - classification_loss: 0.5647
  50/1000 [>.............................] - ETA: 30:52 - loss: 1.7918 - regression_loss: 1.2348 - classification_loss: 0.5570
  51/1000 [>.............................] - ETA: 31:04 - loss: 1.7873 - regression_loss: 1.2376 - classification_loss: 0.5497
  52/1000 [>.............................] - ETA: 31:05 - loss: 1.7809 - regression_loss: 1.2382 - classification_loss: 0.5427
  53/1000 [>.............................] - ETA: 30:59 - loss: 1.7721 - regression_loss: 1.2361 - classification_loss: 0.5360
  54/1000 [>.............................] - ETA: 31:09 - loss: 1.7674 - regression_loss: 1.2379 - classification_loss: 0.5295
  55/1000 [>.............................] - ETA: 31:14 - loss: 1.7617 - regression_loss: 1.2381 - classification_loss: 0.5236
  56/1000 [>.............................] - ETA: 31:05 - loss: 1.7879 - regression_loss: 1.2296 - classification_loss: 0.5584
  57/1000 [>.............................] - ETA: 31:12 - loss: 1.7815 - regression_loss: 1.2299 - classification_loss: 0.5516
  58/1000 [>.............................] - ETA: 31:20 - loss: 1.7763 - regression_loss: 1.2311 - classification_loss: 0.5453
  59/1000 [>.............................] - ETA: 31:25 - loss: 1.7704 - regression_loss: 1.2310 - classification_loss: 0.5394
  60/1000 [>.............................] - ETA: 31:16 - loss: 1.7914 - regression_loss: 1.2235 - classification_loss: 0.5679
  61/1000 [>.............................] - ETA: 31:16 - loss: 1.7858 - regression_loss: 1.2242 - classification_loss: 0.5616
  62/1000 [>.............................] - ETA: 31:25 - loss: 1.7816 - regression_loss: 1.2261 - classification_loss: 0.5555
  63/1000 [>.............................] - ETA: 31:18 - loss: 1.7739 - regression_loss: 1.2243 - classification_loss: 0.5496
  64/1000 [>.............................] - ETA: 31:24 - loss: 1.7682 - regression_loss: 1.2244 - classification_loss: 0.5439
  65/1000 [>.............................] - ETA: 31:32 - loss: 1.7641 - regression_loss: 1.2258 - classification_loss: 0.5383
  66/1000 [>.............................] - ETA: 31:32 - loss: 1.7586 - regression_loss: 1.2257 - classification_loss: 0.5329
  67/1000 [=>............................] - ETA: 31:38 - loss: 1.7542 - regression_loss: 1.2267 - classification_loss: 0.5275
  68/1000 [=>............................] - ETA: 31:30 - loss: 1.7835 - regression_loss: 1.2191 - classification_loss: 0.5644
  69/1000 [=>............................] - ETA: 31:35 - loss: 1.7782 - regression_loss: 1.2193 - classification_loss: 0.5589
  70/1000 [=>............................] - ETA: 31:28 - loss: 1.7726 - regression_loss: 1.2189 - classification_loss: 0.5537
  71/1000 [=>............................] - ETA: 31:32 - loss: 1.7675 - regression_loss: 1.2188 - classification_loss: 0.5487
  72/1000 [=>............................] - ETA: 31:24 - loss: 1.7788 - regression_loss: 1.2117 - classification_loss: 0.5671
  73/1000 [=>............................] - ETA: 31:29 - loss: 1.7753 - regression_loss: 1.2132 - classification_loss: 0.5621
  74/1000 [=>............................] - ETA: 31:23 - loss: 1.7682 - regression_loss: 1.2112 - classification_loss: 0.5570
  75/1000 [=>............................] - ETA: 31:27 - loss: 1.7635 - regression_loss: 1.2113 - classification_loss: 0.5522
  76/1000 [=>............................] - ETA: 31:29 - loss: 1.7592 - regression_loss: 1.2113 - classification_loss: 0.5479
  77/1000 [=>............................] - ETA: 31:28 - loss: 1.7546 - regression_loss: 1.2114 - classification_loss: 0.5432
  78/1000 [=>............................] - ETA: 31:34 - loss: 1.7516 - regression_loss: 1.2131 - classification_loss: 0.5386
  79/1000 [=>............................] - ETA: 31:26 - loss: 1.7642 - regression_loss: 1.2062 - classification_loss: 0.5580
  80/1000 [=>............................] - ETA: 31:25 - loss: 1.7599 - regression_loss: 1.2066 - classification_loss: 0.5533
  81/1000 [=>............................] - ETA: 31:19 - loss: 1.7539 - regression_loss: 1.2051 - classification_loss: 0.5488
  82/1000 [=>............................] - ETA: 31:22 - loss: 1.7500 - regression_loss: 1.2057 - classification_loss: 0.5443
  83/1000 [=>............................] - ETA: 31:26 - loss: 1.7468 - regression_loss: 1.2069 - classification_loss: 0.5399
  84/1000 [=>............................] - ETA: 31:31 - loss: 1.7436 - regression_loss: 1.2080 - classification_loss: 0.5356
  85/1000 [=>............................] - ETA: 31:33 - loss: 1.7395 - regression_loss: 1.2079 - classification_loss: 0.5316
  86/1000 [=>............................] - ETA: 31:34 - loss: 1.7355 - regression_loss: 1.2077 - classification_loss: 0.5278
  87/1000 [=>............................] - ETA: 31:33 - loss: 1.7318 - regression_loss: 1.2080 - classification_loss: 0.5239
  88/1000 [=>............................] - ETA: 31:35 - loss: 1.7280 - regression_loss: 1.2081 - classification_loss: 0.5199
  89/1000 [=>............................] - ETA: 31:38 - loss: 1.7250 - regression_loss: 1.2089 - classification_loss: 0.5161
  90/1000 [=>............................] - ETA: 31:31 - loss: 1.7394 - regression_loss: 1.2040 - classification_loss: 0.5354
  91/1000 [=>............................] - ETA: 31:25 - loss: 1.7341 - regression_loss: 1.2025 - classification_loss: 0.5316
  92/1000 [=>............................] - ETA: 31:30 - loss: 1.7313 - regression_loss: 1.2035 - classification_loss: 0.5277
  93/1000 [=>............................] - ETA: 31:29 - loss: 1.7277 - regression_loss: 1.2037 - classification_loss: 0.5240
  94/1000 [=>............................] - ETA: 31:30 - loss: 1.7242 - regression_loss: 1.2037 - classification_loss: 0.5205
  95/1000 [=>............................] - ETA: 31:24 - loss: 1.7190 - regression_loss: 1.2020 - classification_loss: 0.5170
  96/1000 [=>............................] - ETA: 31:28 - loss: 1.7170 - regression_loss: 1.2035 - classification_loss: 0.5135
  97/1000 [=>............................] - ETA: 31:21 - loss: 1.7229 - regression_loss: 1.1988 - classification_loss: 0.5240
  98/1000 [=>............................] - ETA: 31:23 - loss: 1.7196 - regression_loss: 1.1991 - classification_loss: 0.5205
  99/1000 [=>............................] - ETA: 31:25 - loss: 1.7176 - regression_loss: 1.2002 - classification_loss: 0.5174
 100/1000 [==>...........................] - ETA: 31:24 - loss: 1.7148 - regression_loss: 1.2005 - classification_loss: 0.5142
 101/1000 [==>...........................] - ETA: 31:27 - loss: 1.7124 - regression_loss: 1.2014 - classification_loss: 0.5109
 102/1000 [==>...........................] - ETA: 31:29 - loss: 1.7099 - regression_loss: 1.2022 - classification_loss: 0.5077
 103/1000 [==>...........................] - ETA: 31:31 - loss: 1.7067 - regression_loss: 1.2022 - classification_loss: 0.5045
 104/1000 [==>...........................] - ETA: 31:25 - loss: 1.7051 - regression_loss: 1.2035 - classification_loss: 0.5015
 105/1000 [==>...........................] - ETA: 31:26 - loss: 1.7025 - regression_loss: 1.2038 - classification_loss: 0.4987
 106/1000 [==>...........................] - ETA: 31:19 - loss: 1.7261 - regression_loss: 1.1987 - classification_loss: 0.5274
 107/1000 [==>...........................] - ETA: 31:13 - loss: 1.7416 - regression_loss: 1.1936 - classification_loss: 0.5480
 108/1000 [==>...........................] - ETA: 31:08 - loss: 1.7373 - regression_loss: 1.1926 - classification_loss: 0.5446
 109/1000 [==>...........................] - ETA: 31:10 - loss: 1.7347 - regression_loss: 1.1933 - classification_loss: 0.5414
 110/1000 [==>...........................] - ETA: 31:08 - loss: 1.7319 - regression_loss: 1.1937 - classification_loss: 0.5383
 111/1000 [==>...........................] - ETA: 31:09 - loss: 1.7288 - regression_loss: 1.1937 - classification_loss: 0.5351
 112/1000 [==>...........................] - ETA: 31:10 - loss: 1.7258 - regression_loss: 1.1936 - classification_loss: 0.5322
 113/1000 [==>...........................] - ETA: 31:13 - loss: 1.7234 - regression_loss: 1.1944 - classification_loss: 0.5290
 114/1000 [==>...........................] - ETA: 31:14 - loss: 1.7209 - regression_loss: 1.1950 - classification_loss: 0.5259
 115/1000 [==>...........................] - ETA: 31:13 - loss: 1.7181 - regression_loss: 1.1952 - classification_loss: 0.5229
 116/1000 [==>...........................] - ETA: 31:08 - loss: 1.7172 - regression_loss: 1.1971 - classification_loss: 0.5201
 117/1000 [==>...........................] - ETA: 31:09 - loss: 1.7144 - regression_loss: 1.1971 - classification_loss: 0.5173
 118/1000 [==>...........................] - ETA: 31:09 - loss: 1.7116 - regression_loss: 1.1969 - classification_loss: 0.5147
 119/1000 [==>...........................] - ETA: 31:03 - loss: 1.7220 - regression_loss: 1.1933 - classification_loss: 0.5286
 120/1000 [==>...........................] - ETA: 31:05 - loss: 1.7201 - regression_loss: 1.1944 - classification_loss: 0.5257
 121/1000 [==>...........................] - ETA: 31:07 - loss: 1.7182 - regression_loss: 1.1951 - classification_loss: 0.5231
 122/1000 [==>...........................] - ETA: 31:07 - loss: 1.7154 - regression_loss: 1.1951 - classification_loss: 0.5203
 123/1000 [==>...........................] - ETA: 31:06 - loss: 1.7126 - regression_loss: 1.1950 - classification_loss: 0.5176
 124/1000 [==>...........................] - ETA: 30:59 - loss: 1.7147 - regression_loss: 1.1909 - classification_loss: 0.5238
 125/1000 [==>...........................] - ETA: 30:55 - loss: 1.7110 - regression_loss: 1.1899 - classification_loss: 0.5211
 126/1000 [==>...........................] - ETA: 30:57 - loss: 1.7090 - regression_loss: 1.1907 - classification_loss: 0.5183
 127/1000 [==>...........................] - ETA: 30:57 - loss: 1.7063 - regression_loss: 1.1905 - classification_loss: 0.5158
 128/1000 [==>...........................] - ETA: 30:51 - loss: 1.7116 - regression_loss: 1.1872 - classification_loss: 0.5244
 129/1000 [==>...........................] - ETA: 30:52 - loss: 1.7098 - regression_loss: 1.1880 - classification_loss: 0.5218
 130/1000 [==>...........................] - ETA: 30:54 - loss: 1.7079 - regression_loss: 1.1887 - classification_loss: 0.5191
 131/1000 [==>...........................] - ETA: 30:52 - loss: 1.7053 - regression_loss: 1.1887 - classification_loss: 0.5166
 132/1000 [==>...........................] - ETA: 30:47 - loss: 1.7022 - regression_loss: 1.1882 - classification_loss: 0.5141
 133/1000 [==>...........................] - ETA: 30:47 - loss: 1.7007 - regression_loss: 1.1891 - classification_loss: 0.5116
 134/1000 [===>..........................] - ETA: 30:47 - loss: 1.6994 - regression_loss: 1.1900 - classification_loss: 0.5094
 135/1000 [===>..........................] - ETA: 30:48 - loss: 1.6988 - regression_loss: 1.1918 - classification_loss: 0.5071
 136/1000 [===>..........................] - ETA: 30:43 - loss: 1.6952 - regression_loss: 1.1906 - classification_loss: 0.5047
 137/1000 [===>..........................] - ETA: 30:43 - loss: 1.6929 - regression_loss: 1.1907 - classification_loss: 0.5022
 138/1000 [===>..........................] - ETA: 30:45 - loss: 1.6911 - regression_loss: 1.1913 - classification_loss: 0.4999
 139/1000 [===>..........................] - ETA: 30:39 - loss: 1.6975 - regression_loss: 1.1881 - classification_loss: 0.5094
 140/1000 [===>..........................] - ETA: 30:37 - loss: 1.6960 - regression_loss: 1.1890 - classification_loss: 0.5070
 141/1000 [===>..........................] - ETA: 30:37 - loss: 1.6945 - regression_loss: 1.1896 - classification_loss: 0.5048
 142/1000 [===>..........................] - ETA: 30:37 - loss: 1.6925 - regression_loss: 1.1900 - classification_loss: 0.5025
 143/1000 [===>..........................] - ETA: 30:38 - loss: 1.6908 - regression_loss: 1.1905 - classification_loss: 0.5003
 144/1000 [===>..........................] - ETA: 30:32 - loss: 1.6885 - regression_loss: 1.1870 - classification_loss: 0.5016
 145/1000 [===>..........................] - ETA: 30:32 - loss: 1.6866 - regression_loss: 1.1870 - classification_loss: 0.4996
 146/1000 [===>..........................] - ETA: 30:27 - loss: 1.6835 - regression_loss: 1.1860 - classification_loss: 0.4975
 147/1000 [===>..........................] - ETA: 30:28 - loss: 1.6829 - regression_loss: 1.1873 - classification_loss: 0.4956
 148/1000 [===>..........................] - ETA: 30:26 - loss: 1.6816 - regression_loss: 1.1880 - classification_loss: 0.4936
 149/1000 [===>..........................] - ETA: 30:25 - loss: 1.6795 - regression_loss: 1.1878 - classification_loss: 0.4917
 150/1000 [===>..........................] - ETA: 30:25 - loss: 1.6778 - regression_loss: 1.1882 - classification_loss: 0.4896
 151/1000 [===>..........................] - ETA: 30:20 - loss: 1.6810 - regression_loss: 1.1847 - classification_loss: 0.4962
 152/1000 [===>..........................] - ETA: 30:20 - loss: 1.6795 - regression_loss: 1.1853 - classification_loss: 0.4942
 153/1000 [===>..........................] - ETA: 30:17 - loss: 1.6779 - regression_loss: 1.1857 - classification_loss: 0.4922
 154/1000 [===>..........................] - ETA: 30:13 - loss: 1.6752 - regression_loss: 1.1849 - classification_loss: 0.4902
 155/1000 [===>..........................] - ETA: 30:14 - loss: 1.6737 - regression_loss: 1.1854 - classification_loss: 0.4883
 156/1000 [===>..........................] - ETA: 30:09 - loss: 1.6707 - regression_loss: 1.1844 - classification_loss: 0.4863
 157/1000 [===>..........................] - ETA: 30:09 - loss: 1.6688 - regression_loss: 1.1845 - classification_loss: 0.4843
 158/1000 [===>..........................] - ETA: 30:09 - loss: 1.6673 - regression_loss: 1.1849 - classification_loss: 0.4824
 159/1000 [===>..........................] - ETA: 30:07 - loss: 1.6655 - regression_loss: 1.1850 - classification_loss: 0.4806
 160/1000 [===>..........................] - ETA: 30:06 - loss: 1.6637 - regression_loss: 1.1849 - classification_loss: 0.4788
 161/1000 [===>..........................] - ETA: 30:07 - loss: 1.6626 - regression_loss: 1.1857 - classification_loss: 0.4769
 162/1000 [===>..........................] - ETA: 30:02 - loss: 1.6656 - regression_loss: 1.1830 - classification_loss: 0.4826
 163/1000 [===>..........................] - ETA: 30:01 - loss: 1.6637 - regression_loss: 1.1830 - classification_loss: 0.4807
 164/1000 [===>..........................] - ETA: 29:59 - loss: 1.6617 - regression_loss: 1.1829 - classification_loss: 0.4788
 165/1000 [===>..........................] - ETA: 29:55 - loss: 1.6609 - regression_loss: 1.1837 - classification_loss: 0.4771
 166/1000 [===>..........................] - ETA: 29:55 - loss: 1.6594 - regression_loss: 1.1841 - classification_loss: 0.4753
 167/1000 [====>.........................] - ETA: 29:55 - loss: 1.6582 - regression_loss: 1.1847 - classification_loss: 0.4735
 168/1000 [====>.........................] - ETA: 29:50 - loss: 1.6584 - regression_loss: 1.1811 - classification_loss: 0.4772
 169/1000 [====>.........................] - ETA: 29:49 - loss: 1.6565 - regression_loss: 1.1809 - classification_loss: 0.4755
 170/1000 [====>.........................] - ETA: 29:48 - loss: 1.6547 - regression_loss: 1.1808 - classification_loss: 0.4739
 171/1000 [====>.........................] - ETA: 29:46 - loss: 1.6530 - regression_loss: 1.1808 - classification_loss: 0.4721
 172/1000 [====>.........................] - ETA: 29:41 - loss: 1.6530 - regression_loss: 1.1782 - classification_loss: 0.4748
 173/1000 [====>.........................] - ETA: 29:37 - loss: 1.6507 - regression_loss: 1.1776 - classification_loss: 0.4731
 174/1000 [====>.........................] - ETA: 29:37 - loss: 1.6497 - regression_loss: 1.1780 - classification_loss: 0.4717
 175/1000 [====>.........................] - ETA: 29:37 - loss: 1.6489 - regression_loss: 1.1788 - classification_loss: 0.4701
 176/1000 [====>.........................] - ETA: 29:36 - loss: 1.6475 - regression_loss: 1.1790 - classification_loss: 0.4685
 177/1000 [====>.........................] - ETA: 29:37 - loss: 1.6462 - regression_loss: 1.1794 - classification_loss: 0.4668
 178/1000 [====>.........................] - ETA: 29:33 - loss: 1.6441 - regression_loss: 1.1789 - classification_loss: 0.4652
 179/1000 [====>.........................] - ETA: 29:30 - loss: 1.6425 - regression_loss: 1.1789 - classification_loss: 0.4636
 180/1000 [====>.........................] - ETA: 29:29 - loss: 1.6410 - regression_loss: 1.1789 - classification_loss: 0.4621
 181/1000 [====>.........................] - ETA: 29:29 - loss: 1.6406 - regression_loss: 1.1801 - classification_loss: 0.4605
 182/1000 [====>.........................] - ETA: 29:24 - loss: 1.6485 - regression_loss: 1.1777 - classification_loss: 0.4707
 183/1000 [====>.........................] - ETA: 29:23 - loss: 1.6472 - regression_loss: 1.1781 - classification_loss: 0.4691
 184/1000 [====>.........................] - ETA: 29:23 - loss: 1.6460 - regression_loss: 1.1785 - classification_loss: 0.4675
 185/1000 [====>.........................] - ETA: 29:21 - loss: 1.6445 - regression_loss: 1.1786 - classification_loss: 0.4659
 186/1000 [====>.........................] - ETA: 29:20 - loss: 1.6431 - regression_loss: 1.1787 - classification_loss: 0.4644
 187/1000 [====>.........................] - ETA: 29:16 - loss: 1.6408 - regression_loss: 1.1778 - classification_loss: 0.4630
 188/1000 [====>.........................] - ETA: 29:16 - loss: 1.6400 - regression_loss: 1.1785 - classification_loss: 0.4614
 189/1000 [====>.........................] - ETA: 29:15 - loss: 1.6385 - regression_loss: 1.1786 - classification_loss: 0.4599
 190/1000 [====>.........................] - ETA: 29:10 - loss: 1.6380 - regression_loss: 1.1760 - classification_loss: 0.4619
 191/1000 [====>.........................] - ETA: 29:09 - loss: 1.6364 - regression_loss: 1.1758 - classification_loss: 0.4606
 192/1000 [====>.........................] - ETA: 29:09 - loss: 1.6357 - regression_loss: 1.1766 - classification_loss: 0.4591
 193/1000 [====>.........................] - ETA: 29:07 - loss: 1.6343 - regression_loss: 1.1766 - classification_loss: 0.4577
 194/1000 [====>.........................] - ETA: 29:06 - loss: 1.6331 - regression_loss: 1.1768 - classification_loss: 0.4562
 195/1000 [====>.........................] - ETA: 29:06 - loss: 1.6316 - regression_loss: 1.1768 - classification_loss: 0.4548
 196/1000 [====>.........................] - ETA: 29:02 - loss: 1.6295 - regression_loss: 1.1762 - classification_loss: 0.4533
 197/1000 [====>.........................] - ETA: 28:57 - loss: 1.6335 - regression_loss: 1.1738 - classification_loss: 0.4597
 198/1000 [====>.........................] - ETA: 28:53 - loss: 1.6310 - regression_loss: 1.1727 - classification_loss: 0.4583
 199/1000 [====>.........................] - ETA: 28:52 - loss: 1.6298 - regression_loss: 1.1729 - classification_loss: 0.4568
 200/1000 [=====>........................] - ETA: 28:50 - loss: 1.6283 - regression_loss: 1.1729 - classification_loss: 0.4554
 201/1000 [=====>........................] - ETA: 28:46 - loss: 1.6263 - regression_loss: 1.1702 - classification_loss: 0.4561
 202/1000 [=====>........................] - ETA: 28:45 - loss: 1.6249 - regression_loss: 1.1702 - classification_loss: 0.4547
 203/1000 [=====>........................] - ETA: 28:45 - loss: 1.6240 - regression_loss: 1.1706 - classification_loss: 0.4534
 204/1000 [=====>........................] - ETA: 28:44 - loss: 1.6229 - regression_loss: 1.1705 - classification_loss: 0.4524
 205/1000 [=====>........................] - ETA: 28:43 - loss: 1.6222 - regression_loss: 1.1711 - classification_loss: 0.4511
 206/1000 [=====>........................] - ETA: 28:40 - loss: 1.6224 - regression_loss: 1.1725 - classification_loss: 0.4499
 207/1000 [=====>........................] - ETA: 28:39 - loss: 1.6211 - regression_loss: 1.1726 - classification_loss: 0.4485
 208/1000 [=====>........................] - ETA: 28:38 - loss: 1.6202 - regression_loss: 1.1730 - classification_loss: 0.4472
 209/1000 [=====>........................] - ETA: 28:37 - loss: 1.6189 - regression_loss: 1.1728 - classification_loss: 0.4461
 210/1000 [=====>........................] - ETA: 28:33 - loss: 1.6199 - regression_loss: 1.1699 - classification_loss: 0.4500
 211/1000 [=====>........................] - ETA: 28:30 - loss: 1.6191 - regression_loss: 1.1704 - classification_loss: 0.4487
 212/1000 [=====>........................] - ETA: 28:29 - loss: 1.6179 - regression_loss: 1.1705 - classification_loss: 0.4475
 213/1000 [=====>........................] - ETA: 28:28 - loss: 1.6170 - regression_loss: 1.1708 - classification_loss: 0.4461
 214/1000 [=====>........................] - ETA: 28:24 - loss: 1.6153 - regression_loss: 1.1686 - classification_loss: 0.4467
 215/1000 [=====>........................] - ETA: 28:24 - loss: 1.6146 - regression_loss: 1.1691 - classification_loss: 0.4455
 216/1000 [=====>........................] - ETA: 28:23 - loss: 1.6139 - regression_loss: 1.1694 - classification_loss: 0.4445
 217/1000 [=====>........................] - ETA: 28:21 - loss: 1.6135 - regression_loss: 1.1700 - classification_loss: 0.4435
 218/1000 [=====>........................] - ETA: 28:17 - loss: 1.6117 - regression_loss: 1.1693 - classification_loss: 0.4424
 219/1000 [=====>........................] - ETA: 28:16 - loss: 1.6108 - regression_loss: 1.1694 - classification_loss: 0.4414
 220/1000 [=====>........................] - ETA: 28:15 - loss: 1.6101 - regression_loss: 1.1698 - classification_loss: 0.4403
 221/1000 [=====>........................] - ETA: 28:14 - loss: 1.6087 - regression_loss: 1.1697 - classification_loss: 0.4390
 222/1000 [=====>........................] - ETA: 28:10 - loss: 1.6082 - regression_loss: 1.1674 - classification_loss: 0.4408
 223/1000 [=====>........................] - ETA: 28:08 - loss: 1.6072 - regression_loss: 1.1676 - classification_loss: 0.4396
 224/1000 [=====>........................] - ETA: 28:07 - loss: 1.6071 - regression_loss: 1.1687 - classification_loss: 0.4384
 225/1000 [=====>........................] - ETA: 28:03 - loss: 1.6052 - regression_loss: 1.1679 - classification_loss: 0.4373
 226/1000 [=====>........................] - ETA: 28:01 - loss: 1.6041 - regression_loss: 1.1680 - classification_loss: 0.4361
 227/1000 [=====>........................] - ETA: 28:00 - loss: 1.6028 - regression_loss: 1.1678 - classification_loss: 0.4350
 228/1000 [=====>........................] - ETA: 27:59 - loss: 1.6019 - regression_loss: 1.1681 - classification_loss: 0.4338
 229/1000 [=====>........................] - ETA: 27:59 - loss: 1.6012 - regression_loss: 1.1685 - classification_loss: 0.4327
 230/1000 [=====>........................] - ETA: 27:54 - loss: 1.6007 - regression_loss: 1.1671 - classification_loss: 0.4337
 231/1000 [=====>........................] - ETA: 27:51 - loss: 1.5997 - regression_loss: 1.1672 - classification_loss: 0.4326
 232/1000 [=====>........................] - ETA: 27:50 - loss: 1.5986 - regression_loss: 1.1672 - classification_loss: 0.4314
 233/1000 [=====>........................] - ETA: 27:46 - loss: 1.5967 - regression_loss: 1.1663 - classification_loss: 0.4303
 234/1000 [======>.......................] - ETA: 27:44 - loss: 1.5957 - regression_loss: 1.1665 - classification_loss: 0.4293
 235/1000 [======>.......................] - ETA: 27:43 - loss: 1.5958 - regression_loss: 1.1675 - classification_loss: 0.4283
 236/1000 [======>.......................] - ETA: 27:42 - loss: 1.5959 - regression_loss: 1.1686 - classification_loss: 0.4273
 237/1000 [======>.......................] - ETA: 27:41 - loss: 1.5952 - regression_loss: 1.1689 - classification_loss: 0.4263
 238/1000 [======>.......................] - ETA: 27:40 - loss: 1.5940 - regression_loss: 1.1687 - classification_loss: 0.4253
 239/1000 [======>.......................] - ETA: 27:36 - loss: 1.5925 - regression_loss: 1.1662 - classification_loss: 0.4263
 240/1000 [======>.......................] - ETA: 27:35 - loss: 1.5920 - regression_loss: 1.1667 - classification_loss: 0.4252
 241/1000 [======>.......................] - ETA: 27:33 - loss: 1.5915 - regression_loss: 1.1674 - classification_loss: 0.4242
 242/1000 [======>.......................] - ETA: 27:31 - loss: 1.5907 - regression_loss: 1.1675 - classification_loss: 0.4232
 243/1000 [======>.......................] - ETA: 27:30 - loss: 1.5900 - regression_loss: 1.1678 - classification_loss: 0.4221
 244/1000 [======>.......................] - ETA: 27:29 - loss: 1.5888 - regression_loss: 1.1678 - classification_loss: 0.4211
 245/1000 [======>.......................] - ETA: 27:25 - loss: 1.5879 - regression_loss: 1.1656 - classification_loss: 0.4223
 246/1000 [======>.......................] - ETA: 27:22 - loss: 1.5883 - regression_loss: 1.1670 - classification_loss: 0.4213
 247/1000 [======>.......................] - ETA: 27:21 - loss: 1.5874 - regression_loss: 1.1671 - classification_loss: 0.4203
 248/1000 [======>.......................] - ETA: 27:20 - loss: 1.5870 - regression_loss: 1.1676 - classification_loss: 0.4194
 249/1000 [======>.......................] - ETA: 27:17 - loss: 1.5861 - regression_loss: 1.1676 - classification_loss: 0.4185
 250/1000 [======>.......................] - ETA: 27:14 - loss: 1.5850 - regression_loss: 1.1674 - classification_loss: 0.4176
 251/1000 [======>.......................] - ETA: 27:13 - loss: 1.5843 - regression_loss: 1.1677 - classification_loss: 0.4166
 252/1000 [======>.......................] - ETA: 27:09 - loss: 1.5823 - regression_loss: 1.1654 - classification_loss: 0.4168
 253/1000 [======>.......................] - ETA: 27:08 - loss: 1.5813 - regression_loss: 1.1654 - classification_loss: 0.4159
 254/1000 [======>.......................] - ETA: 27:07 - loss: 1.5805 - regression_loss: 1.1656 - classification_loss: 0.4150
 255/1000 [======>.......................] - ETA: 27:06 - loss: 1.5794 - regression_loss: 1.1655 - classification_loss: 0.4140
 256/1000 [======>.......................] - ETA: 27:02 - loss: 1.5770 - regression_loss: 1.1632 - classification_loss: 0.4139
 257/1000 [======>.......................] - ETA: 27:00 - loss: 1.5760 - regression_loss: 1.1630 - classification_loss: 0.4130
 258/1000 [======>.......................] - ETA: 26:57 - loss: 1.5746 - regression_loss: 1.1625 - classification_loss: 0.4121
 259/1000 [======>.......................] - ETA: 26:55 - loss: 1.5736 - regression_loss: 1.1624 - classification_loss: 0.4111
 260/1000 [======>.......................] - ETA: 26:54 - loss: 1.5730 - regression_loss: 1.1628 - classification_loss: 0.4102
 261/1000 [======>.......................] - ETA: 26:53 - loss: 1.5723 - regression_loss: 1.1631 - classification_loss: 0.4093
 262/1000 [======>.......................] - ETA: 26:52 - loss: 1.5717 - regression_loss: 1.1633 - classification_loss: 0.4084
 263/1000 [======>.......................] - ETA: 26:48 - loss: 1.5705 - regression_loss: 1.1610 - classification_loss: 0.4095
 264/1000 [======>.......................] - ETA: 26:44 - loss: 1.5689 - regression_loss: 1.1603 - classification_loss: 0.4086
 265/1000 [======>.......................] - ETA: 26:43 - loss: 1.5685 - regression_loss: 1.1607 - classification_loss: 0.4078
 266/1000 [======>.......................] - ETA: 26:41 - loss: 1.5680 - regression_loss: 1.1612 - classification_loss: 0.4069
 267/1000 [=======>......................] - ETA: 26:39 - loss: 1.5673 - regression_loss: 1.1613 - classification_loss: 0.4060
 268/1000 [=======>......................] - ETA: 26:36 - loss: 1.5657 - regression_loss: 1.1606 - classification_loss: 0.4051
 269/1000 [=======>......................] - ETA: 26:35 - loss: 1.5651 - regression_loss: 1.1608 - classification_loss: 0.4043
 270/1000 [=======>......................] - ETA: 26:34 - loss: 1.5642 - regression_loss: 1.1606 - classification_loss: 0.4035
 271/1000 [=======>......................] - ETA: 26:32 - loss: 1.5636 - regression_loss: 1.1609 - classification_loss: 0.4027
 272/1000 [=======>......................] - ETA: 26:31 - loss: 1.5626 - regression_loss: 1.1608 - classification_loss: 0.4018
 273/1000 [=======>......................] - ETA: 26:27 - loss: 1.5619 - regression_loss: 1.1587 - classification_loss: 0.4032
 274/1000 [=======>......................] - ETA: 26:25 - loss: 1.5610 - regression_loss: 1.1587 - classification_loss: 0.4023
 275/1000 [=======>......................] - ETA: 26:23 - loss: 1.5601 - regression_loss: 1.1587 - classification_loss: 0.4014
 276/1000 [=======>......................] - ETA: 26:21 - loss: 1.5591 - regression_loss: 1.1586 - classification_loss: 0.4006
 277/1000 [=======>......................] - ETA: 26:17 - loss: 1.5574 - regression_loss: 1.1567 - classification_loss: 0.4007
 278/1000 [=======>......................] - ETA: 26:16 - loss: 1.5571 - regression_loss: 1.1572 - classification_loss: 0.3999
 279/1000 [=======>......................] - ETA: 26:13 - loss: 1.5561 - regression_loss: 1.1568 - classification_loss: 0.3992
 280/1000 [=======>......................] - ETA: 26:11 - loss: 1.5554 - regression_loss: 1.1568 - classification_loss: 0.3986
 281/1000 [=======>......................] - ETA: 26:10 - loss: 1.5550 - regression_loss: 1.1572 - classification_loss: 0.3978
 282/1000 [=======>......................] - ETA: 26:07 - loss: 1.5523 - regression_loss: 1.1550 - classification_loss: 0.3973
 283/1000 [=======>......................] - ETA: 26:03 - loss: 1.5507 - regression_loss: 1.1542 - classification_loss: 0.3965
 284/1000 [=======>......................] - ETA: 26:01 - loss: 1.5498 - regression_loss: 1.1541 - classification_loss: 0.3957
 285/1000 [=======>......................] - ETA: 26:00 - loss: 1.5493 - regression_loss: 1.1544 - classification_loss: 0.3949
 286/1000 [=======>......................] - ETA: 25:58 - loss: 1.5485 - regression_loss: 1.1544 - classification_loss: 0.3941
 287/1000 [=======>......................] - ETA: 25:57 - loss: 1.5481 - regression_loss: 1.1547 - classification_loss: 0.3934
 288/1000 [=======>......................] - ETA: 25:56 - loss: 1.5472 - regression_loss: 1.1545 - classification_loss: 0.3927
 289/1000 [=======>......................] - ETA: 25:52 - loss: 1.5476 - regression_loss: 1.1557 - classification_loss: 0.3920
 290/1000 [=======>......................] - ETA: 25:49 - loss: 1.5452 - regression_loss: 1.1535 - classification_loss: 0.3917
 291/1000 [=======>......................] - ETA: 25:47 - loss: 1.5447 - regression_loss: 1.1537 - classification_loss: 0.3910
 292/1000 [=======>......................] - ETA: 25:45 - loss: 1.5446 - regression_loss: 1.1542 - classification_loss: 0.3904
 293/1000 [=======>......................] - ETA: 25:44 - loss: 1.5446 - regression_loss: 1.1545 - classification_loss: 0.3900
 294/1000 [=======>......................] - ETA: 25:43 - loss: 1.5445 - regression_loss: 1.1550 - classification_loss: 0.3896
 295/1000 [=======>......................] - ETA: 25:41 - loss: 1.5441 - regression_loss: 1.1550 - classification_loss: 0.3890
 296/1000 [=======>......................] - ETA: 25:40 - loss: 1.5438 - regression_loss: 1.1554 - classification_loss: 0.3884
 297/1000 [=======>......................] - ETA: 25:39 - loss: 1.5435 - regression_loss: 1.1558 - classification_loss: 0.3877
 298/1000 [=======>......................] - ETA: 25:37 - loss: 1.5428 - regression_loss: 1.1558 - classification_loss: 0.3870
 299/1000 [=======>......................] - ETA: 25:34 - loss: 1.5415 - regression_loss: 1.1552 - classification_loss: 0.3863
 300/1000 [========>.....................] - ETA: 25:33 - loss: 1.5408 - regression_loss: 1.1552 - classification_loss: 0.3856
 301/1000 [========>.....................] - ETA: 25:29 - loss: 1.5454 - regression_loss: 1.1533 - classification_loss: 0.3921
 302/1000 [========>.....................] - ETA: 25:27 - loss: 1.5452 - regression_loss: 1.1538 - classification_loss: 0.3915
 303/1000 [========>.....................] - ETA: 25:26 - loss: 1.5449 - regression_loss: 1.1542 - classification_loss: 0.3907
 304/1000 [========>.....................] - ETA: 25:24 - loss: 1.5441 - regression_loss: 1.1541 - classification_loss: 0.3900
 305/1000 [========>.....................] - ETA: 25:22 - loss: 1.5433 - regression_loss: 1.1541 - classification_loss: 0.3893
 306/1000 [========>.....................] - ETA: 25:20 - loss: 1.5426 - regression_loss: 1.1541 - classification_loss: 0.3885
 307/1000 [========>.....................] - ETA: 25:19 - loss: 1.5422 - regression_loss: 1.1543 - classification_loss: 0.3879
 308/1000 [========>.....................] - ETA: 25:16 - loss: 1.5411 - regression_loss: 1.1534 - classification_loss: 0.3877
 309/1000 [========>.....................] - ETA: 25:12 - loss: 1.5410 - regression_loss: 1.1539 - classification_loss: 0.3871
 310/1000 [========>.....................] - ETA: 25:09 - loss: 1.5403 - regression_loss: 1.1539 - classification_loss: 0.3864
 311/1000 [========>.....................] - ETA: 25:08 - loss: 1.5400 - regression_loss: 1.1542 - classification_loss: 0.3858
 312/1000 [========>.....................] - ETA: 25:05 - loss: 1.5401 - regression_loss: 1.1547 - classification_loss: 0.3854
 313/1000 [========>.....................] - ETA: 25:04 - loss: 1.5396 - regression_loss: 1.1548 - classification_loss: 0.3849
 314/1000 [========>.....................] - ETA: 25:00 - loss: 1.5377 - regression_loss: 1.1531 - classification_loss: 0.3845
 315/1000 [========>.....................] - ETA: 24:59 - loss: 1.5375 - regression_loss: 1.1536 - classification_loss: 0.3839
 316/1000 [========>.....................] - ETA: 24:58 - loss: 1.5371 - regression_loss: 1.1538 - classification_loss: 0.3832
 317/1000 [========>.....................] - ETA: 24:56 - loss: 1.5364 - regression_loss: 1.1538 - classification_loss: 0.3825
 318/1000 [========>.....................] - ETA: 24:55 - loss: 1.5360 - regression_loss: 1.1542 - classification_loss: 0.3818
 319/1000 [========>.....................] - ETA: 24:52 - loss: 1.5354 - regression_loss: 1.1541 - classification_loss: 0.3813
 320/1000 [========>.....................] - ETA: 24:49 - loss: 1.5353 - regression_loss: 1.1547 - classification_loss: 0.3806
 321/1000 [========>.....................] - ETA: 24:46 - loss: 1.5357 - regression_loss: 1.1539 - classification_loss: 0.3818
 322/1000 [========>.....................] - ETA: 24:44 - loss: 1.5352 - regression_loss: 1.1540 - classification_loss: 0.3812
 323/1000 [========>.....................] - ETA: 24:43 - loss: 1.5347 - regression_loss: 1.1542 - classification_loss: 0.3805
 324/1000 [========>.....................] - ETA: 24:41 - loss: 1.5342 - regression_loss: 1.1544 - classification_loss: 0.3798
 325/1000 [========>.....................] - ETA: 24:38 - loss: 1.5334 - regression_loss: 1.1542 - classification_loss: 0.3792
 326/1000 [========>.....................] - ETA: 24:36 - loss: 1.5326 - regression_loss: 1.1540 - classification_loss: 0.3786
 327/1000 [========>.....................] - ETA: 24:33 - loss: 1.5303 - regression_loss: 1.1520 - classification_loss: 0.3783
 328/1000 [========>.....................] - ETA: 24:31 - loss: 1.5297 - regression_loss: 1.1519 - classification_loss: 0.3778
 329/1000 [========>.....................] - ETA: 24:29 - loss: 1.5290 - regression_loss: 1.1519 - classification_loss: 0.3771
 330/1000 [========>.....................] - ETA: 24:28 - loss: 1.5285 - regression_loss: 1.1520 - classification_loss: 0.3765
 331/1000 [========>.....................] - ETA: 24:26 - loss: 1.5277 - regression_loss: 1.1518 - classification_loss: 0.3759
 332/1000 [========>.....................] - ETA: 24:25 - loss: 1.5270 - regression_loss: 1.1517 - classification_loss: 0.3752
 333/1000 [========>.....................] - ETA: 24:23 - loss: 1.5265 - regression_loss: 1.1519 - classification_loss: 0.3746
 334/1000 [=========>....................] - ETA: 24:20 - loss: 1.5243 - regression_loss: 1.1498 - classification_loss: 0.3745
 335/1000 [=========>....................] - ETA: 24:17 - loss: 1.5236 - regression_loss: 1.1498 - classification_loss: 0.3739
 336/1000 [=========>....................] - ETA: 24:14 - loss: 1.5224 - regression_loss: 1.1492 - classification_loss: 0.3733
 337/1000 [=========>....................] - ETA: 24:13 - loss: 1.5221 - regression_loss: 1.1495 - classification_loss: 0.3726
 338/1000 [=========>....................] - ETA: 24:10 - loss: 1.5210 - regression_loss: 1.1490 - classification_loss: 0.3720
 339/1000 [=========>....................] - ETA: 24:08 - loss: 1.5203 - regression_loss: 1.1489 - classification_loss: 0.3714
 340/1000 [=========>....................] - ETA: 24:07 - loss: 1.5197 - regression_loss: 1.1490 - classification_loss: 0.3708
 341/1000 [=========>....................] - ETA: 24:06 - loss: 1.5194 - regression_loss: 1.1492 - classification_loss: 0.3702
 342/1000 [=========>....................] - ETA: 24:04 - loss: 1.5187 - regression_loss: 1.1491 - classification_loss: 0.3696
 343/1000 [=========>....................] - ETA: 24:02 - loss: 1.5180 - regression_loss: 1.1489 - classification_loss: 0.3690
 344/1000 [=========>....................] - ETA: 23:58 - loss: 1.5158 - regression_loss: 1.1470 - classification_loss: 0.3689
 345/1000 [=========>....................] - ETA: 23:57 - loss: 1.5156 - regression_loss: 1.1473 - classification_loss: 0.3683
 346/1000 [=========>....................] - ETA: 23:53 - loss: 1.5135 - regression_loss: 1.1456 - classification_loss: 0.3679
 347/1000 [=========>....................] - ETA: 23:52 - loss: 1.5130 - regression_loss: 1.1456 - classification_loss: 0.3674
 348/1000 [=========>....................] - ETA: 23:50 - loss: 1.5127 - regression_loss: 1.1456 - classification_loss: 0.3671
 349/1000 [=========>....................] - ETA: 23:48 - loss: 1.5126 - regression_loss: 1.1457 - classification_loss: 0.3669
 350/1000 [=========>....................] - ETA: 23:45 - loss: 1.5116 - regression_loss: 1.1453 - classification_loss: 0.3663
 351/1000 [=========>....................] - ETA: 23:43 - loss: 1.5111 - regression_loss: 1.1452 - classification_loss: 0.3659
 352/1000 [=========>....................] - ETA: 23:40 - loss: 1.5091 - regression_loss: 1.1437 - classification_loss: 0.3654
 353/1000 [=========>....................] - ETA: 23:38 - loss: 1.5087 - regression_loss: 1.1438 - classification_loss: 0.3649
 354/1000 [=========>....................] - ETA: 23:37 - loss: 1.5083 - regression_loss: 1.1440 - classification_loss: 0.3643
 355/1000 [=========>....................] - ETA: 23:34 - loss: 1.5072 - regression_loss: 1.1434 - classification_loss: 0.3637
 356/1000 [=========>....................] - ETA: 23:32 - loss: 1.5066 - regression_loss: 1.1434 - classification_loss: 0.3632
 357/1000 [=========>....................] - ETA: 23:30 - loss: 1.5060 - regression_loss: 1.1433 - classification_loss: 0.3627
 358/1000 [=========>....................] - ETA: 23:28 - loss: 1.5054 - regression_loss: 1.1431 - classification_loss: 0.3622
 359/1000 [=========>....................] - ETA: 23:26 - loss: 1.5047 - regression_loss: 1.1430 - classification_loss: 0.3617
 360/1000 [=========>....................] - ETA: 23:25 - loss: 1.5040 - regression_loss: 1.1429 - classification_loss: 0.3611
 361/1000 [=========>....................] - ETA: 23:23 - loss: 1.5036 - regression_loss: 1.1430 - classification_loss: 0.3606
 362/1000 [=========>....................] - ETA: 23:21 - loss: 1.5030 - regression_loss: 1.1429 - classification_loss: 0.3601
 363/1000 [=========>....................] - ETA: 23:19 - loss: 1.5026 - regression_loss: 1.1431 - classification_loss: 0.3595
 364/1000 [=========>....................] - ETA: 23:16 - loss: 1.5008 - regression_loss: 1.1415 - classification_loss: 0.3593
 365/1000 [=========>....................] - ETA: 23:13 - loss: 1.5001 - regression_loss: 1.1413 - classification_loss: 0.3588
 366/1000 [=========>....................] - ETA: 23:11 - loss: 1.4997 - regression_loss: 1.1414 - classification_loss: 0.3583
 367/1000 [==========>...................] - ETA: 23:09 - loss: 1.4995 - regression_loss: 1.1417 - classification_loss: 0.3578
 368/1000 [==========>...................] - ETA: 23:08 - loss: 1.4993 - regression_loss: 1.1420 - classification_loss: 0.3573
 369/1000 [==========>...................] - ETA: 23:06 - loss: 1.4987 - regression_loss: 1.1418 - classification_loss: 0.3568
 370/1000 [==========>...................] - ETA: 23:04 - loss: 1.4984 - regression_loss: 1.1420 - classification_loss: 0.3563
 371/1000 [==========>...................] - ETA: 23:01 - loss: 1.4966 - regression_loss: 1.1405 - classification_loss: 0.3561
 372/1000 [==========>...................] - ETA: 22:58 - loss: 1.4962 - regression_loss: 1.1405 - classification_loss: 0.3557
 373/1000 [==========>...................] - ETA: 22:57 - loss: 1.4959 - regression_loss: 1.1407 - classification_loss: 0.3552
 374/1000 [==========>...................] - ETA: 22:55 - loss: 1.4952 - regression_loss: 1.1405 - classification_loss: 0.3547
 375/1000 [==========>...................] - ETA: 22:53 - loss: 1.4948 - regression_loss: 1.1406 - classification_loss: 0.3542
 376/1000 [==========>...................] - ETA: 22:50 - loss: 1.4937 - regression_loss: 1.1395 - classification_loss: 0.3542
 377/1000 [==========>...................] - ETA: 22:49 - loss: 1.4931 - regression_loss: 1.1394 - classification_loss: 0.3537
 378/1000 [==========>...................] - ETA: 22:46 - loss: 1.4919 - regression_loss: 1.1387 - classification_loss: 0.3532
 379/1000 [==========>...................] - ETA: 22:43 - loss: 1.4914 - regression_loss: 1.1386 - classification_loss: 0.3528
 380/1000 [==========>...................] - ETA: 22:40 - loss: 1.4894 - regression_loss: 1.1370 - classification_loss: 0.3524
 381/1000 [==========>...................] - ETA: 22:37 - loss: 1.4882 - regression_loss: 1.1363 - classification_loss: 0.3519
 382/1000 [==========>...................] - ETA: 22:35 - loss: 1.4877 - regression_loss: 1.1363 - classification_loss: 0.3514
 383/1000 [==========>...................] - ETA: 22:34 - loss: 1.4872 - regression_loss: 1.1361 - classification_loss: 0.3510
 384/1000 [==========>...................] - ETA: 22:32 - loss: 1.4869 - regression_loss: 1.1363 - classification_loss: 0.3506
 385/1000 [==========>...................] - ETA: 22:31 - loss: 1.4865 - regression_loss: 1.1365 - classification_loss: 0.3501
 386/1000 [==========>...................] - ETA: 22:28 - loss: 1.4863 - regression_loss: 1.1367 - classification_loss: 0.3496
 387/1000 [==========>...................] - ETA: 22:26 - loss: 1.4859 - regression_loss: 1.1367 - classification_loss: 0.3492
 388/1000 [==========>...................] - ETA: 22:23 - loss: 1.4850 - regression_loss: 1.1352 - classification_loss: 0.3498
 389/1000 [==========>...................] - ETA: 22:22 - loss: 1.4847 - regression_loss: 1.1353 - classification_loss: 0.3493
 390/1000 [==========>...................] - ETA: 22:19 - loss: 1.4837 - regression_loss: 1.1348 - classification_loss: 0.3489
 391/1000 [==========>...................] - ETA: 22:17 - loss: 1.4836 - regression_loss: 1.1351 - classification_loss: 0.3484
 392/1000 [==========>...................] - ETA: 22:16 - loss: 1.4832 - regression_loss: 1.1352 - classification_loss: 0.3479
 393/1000 [==========>...................] - ETA: 22:13 - loss: 1.4828 - regression_loss: 1.1353 - classification_loss: 0.3475
 394/1000 [==========>...................] - ETA: 22:11 - loss: 1.4823 - regression_loss: 1.1353 - classification_loss: 0.3471
 395/1000 [==========>...................] - ETA: 22:09 - loss: 1.4820 - regression_loss: 1.1353 - classification_loss: 0.3466
 396/1000 [==========>...................] - ETA: 22:07 - loss: 1.4809 - regression_loss: 1.1348 - classification_loss: 0.3462
 397/1000 [==========>...................] - ETA: 22:05 - loss: 1.4807 - regression_loss: 1.1350 - classification_loss: 0.3457
 398/1000 [==========>...................] - ETA: 22:03 - loss: 1.4803 - regression_loss: 1.1351 - classification_loss: 0.3452
 399/1000 [==========>...................] - ETA: 22:00 - loss: 1.4794 - regression_loss: 1.1335 - classification_loss: 0.3458
 400/1000 [===========>..................] - ETA: 21:58 - loss: 1.4788 - regression_loss: 1.1334 - classification_loss: 0.3454
 401/1000 [===========>..................] - ETA: 21:55 - loss: 1.4780 - regression_loss: 1.1330 - classification_loss: 0.3450
 402/1000 [===========>..................] - ETA: 21:53 - loss: 1.4775 - regression_loss: 1.1330 - classification_loss: 0.3445
 403/1000 [===========>..................] - ETA: 21:51 - loss: 1.4769 - regression_loss: 1.1329 - classification_loss: 0.3440
 404/1000 [===========>..................] - ETA: 21:50 - loss: 1.4763 - regression_loss: 1.1326 - classification_loss: 0.3436
 405/1000 [===========>..................] - ETA: 21:46 - loss: 1.4741 - regression_loss: 1.1309 - classification_loss: 0.3432
 406/1000 [===========>..................] - ETA: 21:45 - loss: 1.4738 - regression_loss: 1.1310 - classification_loss: 0.3428
 407/1000 [===========>..................] - ETA: 21:43 - loss: 1.4734 - regression_loss: 1.1311 - classification_loss: 0.3423
 408/1000 [===========>..................] - ETA: 21:42 - loss: 1.4730 - regression_loss: 1.1312 - classification_loss: 0.3419
 409/1000 [===========>..................] - ETA: 21:40 - loss: 1.4724 - regression_loss: 1.1310 - classification_loss: 0.3414
 410/1000 [===========>..................] - ETA: 21:37 - loss: 1.4714 - regression_loss: 1.1305 - classification_loss: 0.3410
 411/1000 [===========>..................] - ETA: 21:36 - loss: 1.4713 - regression_loss: 1.1308 - classification_loss: 0.3405
 412/1000 [===========>..................] - ETA: 21:33 - loss: 1.4708 - regression_loss: 1.1307 - classification_loss: 0.3401
 413/1000 [===========>..................] - ETA: 21:30 - loss: 1.4691 - regression_loss: 1.1290 - classification_loss: 0.3401
 414/1000 [===========>..................] - ETA: 21:28 - loss: 1.4686 - regression_loss: 1.1288 - classification_loss: 0.3397
 415/1000 [===========>..................] - ETA: 21:27 - loss: 1.4683 - regression_loss: 1.1290 - classification_loss: 0.3393
 416/1000 [===========>..................] - ETA: 21:25 - loss: 1.4678 - regression_loss: 1.1289 - classification_loss: 0.3389
 417/1000 [===========>..................] - ETA: 21:22 - loss: 1.4667 - regression_loss: 1.1283 - classification_loss: 0.3384
 418/1000 [===========>..................] - ETA: 21:20 - loss: 1.4663 - regression_loss: 1.1283 - classification_loss: 0.3380
 419/1000 [===========>..................] - ETA: 21:18 - loss: 1.4658 - regression_loss: 1.1282 - classification_loss: 0.3376
 420/1000 [===========>..................] - ETA: 21:15 - loss: 1.4643 - regression_loss: 1.1269 - classification_loss: 0.3374
 421/1000 [===========>..................] - ETA: 21:13 - loss: 1.4638 - regression_loss: 1.1268 - classification_loss: 0.3370
 422/1000 [===========>..................] - ETA: 21:11 - loss: 1.4635 - regression_loss: 1.1269 - classification_loss: 0.3366
 423/1000 [===========>..................] - ETA: 21:08 - loss: 1.4626 - regression_loss: 1.1264 - classification_loss: 0.3362
 424/1000 [===========>..................] - ETA: 21:05 - loss: 1.4606 - regression_loss: 1.1249 - classification_loss: 0.3358
 425/1000 [===========>..................] - ETA: 21:04 - loss: 1.4604 - regression_loss: 1.1250 - classification_loss: 0.3354
 426/1000 [===========>..................] - ETA: 21:01 - loss: 1.4600 - regression_loss: 1.1250 - classification_loss: 0.3350
 427/1000 [===========>..................] - ETA: 21:00 - loss: 1.4598 - regression_loss: 1.1252 - classification_loss: 0.3346
 428/1000 [===========>..................] - ETA: 20:58 - loss: 1.4592 - regression_loss: 1.1250 - classification_loss: 0.3342
 429/1000 [===========>..................] - ETA: 20:56 - loss: 1.4587 - regression_loss: 1.1249 - classification_loss: 0.3338
 430/1000 [===========>..................] - ETA: 20:54 - loss: 1.4581 - regression_loss: 1.1247 - classification_loss: 0.3334
 431/1000 [===========>..................] - ETA: 20:52 - loss: 1.4576 - regression_loss: 1.1247 - classification_loss: 0.3330
 432/1000 [===========>..................] - ETA: 20:50 - loss: 1.4573 - regression_loss: 1.1248 - classification_loss: 0.3326
 433/1000 [===========>..................] - ETA: 20:48 - loss: 1.4563 - regression_loss: 1.1241 - classification_loss: 0.3322
 434/1000 [============>.................] - ETA: 20:46 - loss: 1.4560 - regression_loss: 1.1242 - classification_loss: 0.3317
 435/1000 [============>.................] - ETA: 20:43 - loss: 1.4544 - regression_loss: 1.1226 - classification_loss: 0.3317
 436/1000 [============>.................] - ETA: 20:41 - loss: 1.4540 - regression_loss: 1.1227 - classification_loss: 0.3313
 437/1000 [============>.................] - ETA: 20:39 - loss: 1.4535 - regression_loss: 1.1226 - classification_loss: 0.3309
 438/1000 [============>.................] - ETA: 20:36 - loss: 1.4529 - regression_loss: 1.1224 - classification_loss: 0.3305
 439/1000 [============>.................] - ETA: 20:33 - loss: 1.4509 - regression_loss: 1.1208 - classification_loss: 0.3301
 440/1000 [============>.................] - ETA: 20:32 - loss: 1.4507 - regression_loss: 1.1210 - classification_loss: 0.3298
 441/1000 [============>.................] - ETA: 20:30 - loss: 1.4502 - regression_loss: 1.1209 - classification_loss: 0.3294
 442/1000 [============>.................] - ETA: 20:28 - loss: 1.4497 - regression_loss: 1.1207 - classification_loss: 0.3290
 443/1000 [============>.................] - ETA: 20:27 - loss: 1.4494 - regression_loss: 1.1207 - classification_loss: 0.3287
 444/1000 [============>.................] - ETA: 20:24 - loss: 1.4484 - regression_loss: 1.1202 - classification_loss: 0.3283
 445/1000 [============>.................] - ETA: 20:22 - loss: 1.4480 - regression_loss: 1.1202 - classification_loss: 0.3279
 446/1000 [============>.................] - ETA: 20:19 - loss: 1.4463 - regression_loss: 1.1187 - classification_loss: 0.3276
 447/1000 [============>.................] - ETA: 20:17 - loss: 1.4457 - regression_loss: 1.1186 - classification_loss: 0.3272
 448/1000 [============>.................] - ETA: 20:15 - loss: 1.4452 - regression_loss: 1.1184 - classification_loss: 0.3268
 449/1000 [============>.................] - ETA: 20:13 - loss: 1.4446 - regression_loss: 1.1182 - classification_loss: 0.3264
 450/1000 [============>.................] - ETA: 20:10 - loss: 1.4427 - regression_loss: 1.1166 - classification_loss: 0.3261
 451/1000 [============>.................] - ETA: 20:08 - loss: 1.4422 - regression_loss: 1.1165 - classification_loss: 0.3257
 452/1000 [============>.................] - ETA: 20:06 - loss: 1.4417 - regression_loss: 1.1164 - classification_loss: 0.3253
 453/1000 [============>.................] - ETA: 20:03 - loss: 1.4408 - regression_loss: 1.1159 - classification_loss: 0.3249
 454/1000 [============>.................] - ETA: 20:02 - loss: 1.4405 - regression_loss: 1.1160 - classification_loss: 0.3245
 455/1000 [============>.................] - ETA: 20:00 - loss: 1.4402 - regression_loss: 1.1161 - classification_loss: 0.3242
 456/1000 [============>.................] - ETA: 19:58 - loss: 1.4397 - regression_loss: 1.1159 - classification_loss: 0.3238
 457/1000 [============>.................] - ETA: 19:56 - loss: 1.4393 - regression_loss: 1.1159 - classification_loss: 0.3235
 458/1000 [============>.................] - ETA: 19:53 - loss: 1.4384 - regression_loss: 1.1153 - classification_loss: 0.3231
 459/1000 [============>.................] - ETA: 19:51 - loss: 1.4381 - regression_loss: 1.1154 - classification_loss: 0.3227
 460/1000 [============>.................] - ETA: 19:49 - loss: 1.4375 - regression_loss: 1.1151 - classification_loss: 0.3224
 461/1000 [============>.................] - ETA: 19:48 - loss: 1.4373 - regression_loss: 1.1153 - classification_loss: 0.3220
 462/1000 [============>.................] - ETA: 19:45 - loss: 1.4354 - regression_loss: 1.1137 - classification_loss: 0.3217
 463/1000 [============>.................] - ETA: 19:43 - loss: 1.4349 - regression_loss: 1.1136 - classification_loss: 0.3213
 464/1000 [============>.................] - ETA: 19:41 - loss: 1.4344 - regression_loss: 1.1135 - classification_loss: 0.3209
 465/1000 [============>.................] - ETA: 19:38 - loss: 1.4335 - regression_loss: 1.1130 - classification_loss: 0.3205
 466/1000 [============>.................] - ETA: 19:36 - loss: 1.4316 - regression_loss: 1.1114 - classification_loss: 0.3202
 467/1000 [=============>................] - ETA: 19:33 - loss: 1.4310 - regression_loss: 1.1112 - classification_loss: 0.3198
 468/1000 [=============>................] - ETA: 19:32 - loss: 1.4308 - regression_loss: 1.1113 - classification_loss: 0.3195
 469/1000 [=============>................] - ETA: 19:30 - loss: 1.4305 - regression_loss: 1.1114 - classification_loss: 0.3191
 470/1000 [=============>................] - ETA: 19:28 - loss: 1.4300 - regression_loss: 1.1112 - classification_loss: 0.3188
 471/1000 [=============>................] - ETA: 19:26 - loss: 1.4297 - regression_loss: 1.1113 - classification_loss: 0.3184
 472/1000 [=============>................] - ETA: 19:24 - loss: 1.4292 - regression_loss: 1.1112 - classification_loss: 0.3180
 473/1000 [=============>................] - ETA: 19:22 - loss: 1.4284 - regression_loss: 1.1107 - classification_loss: 0.3177
 474/1000 [=============>................] - ETA: 19:20 - loss: 1.4281 - regression_loss: 1.1108 - classification_loss: 0.3173
 475/1000 [=============>................] - ETA: 19:18 - loss: 1.4277 - regression_loss: 1.1107 - classification_loss: 0.3170
 476/1000 [=============>................] - ETA: 19:16 - loss: 1.4272 - regression_loss: 1.1105 - classification_loss: 0.3166
 477/1000 [=============>................] - ETA: 19:13 - loss: 1.4253 - regression_loss: 1.1090 - classification_loss: 0.3163
 478/1000 [=============>................] - ETA: 19:11 - loss: 1.4250 - regression_loss: 1.1090 - classification_loss: 0.3160
 479/1000 [=============>................] - ETA: 19:09 - loss: 1.4245 - regression_loss: 1.1089 - classification_loss: 0.3156
 480/1000 [=============>................] - ETA: 19:07 - loss: 1.4240 - regression_loss: 1.1087 - classification_loss: 0.3153
 481/1000 [=============>................] - ETA: 19:04 - loss: 1.4222 - regression_loss: 1.1073 - classification_loss: 0.3149
 482/1000 [=============>................] - ETA: 19:01 - loss: 1.4215 - regression_loss: 1.1069 - classification_loss: 0.3146
 483/1000 [=============>................] - ETA: 19:00 - loss: 1.4210 - regression_loss: 1.1067 - classification_loss: 0.3143
 484/1000 [=============>................] - ETA: 18:58 - loss: 1.4207 - regression_loss: 1.1068 - classification_loss: 0.3139
 485/1000 [=============>................] - ETA: 18:56 - loss: 1.4202 - regression_loss: 1.1066 - classification_loss: 0.3136
 486/1000 [=============>................] - ETA: 18:54 - loss: 1.4199 - regression_loss: 1.1067 - classification_loss: 0.3132
 487/1000 [=============>................] - ETA: 18:53 - loss: 1.4196 - regression_loss: 1.1068 - classification_loss: 0.3129
 488/1000 [=============>................] - ETA: 18:50 - loss: 1.4179 - regression_loss: 1.1053 - classification_loss: 0.3126
 489/1000 [=============>................] - ETA: 18:48 - loss: 1.4173 - regression_loss: 1.1051 - classification_loss: 0.3123
 490/1000 [=============>................] - ETA: 18:45 - loss: 1.4165 - regression_loss: 1.1046 - classification_loss: 0.3119
 491/1000 [=============>................] - ETA: 18:43 - loss: 1.4161 - regression_loss: 1.1045 - classification_loss: 0.3116
 492/1000 [=============>................] - ETA: 18:41 - loss: 1.4155 - regression_loss: 1.1042 - classification_loss: 0.3113
 493/1000 [=============>................] - ETA: 18:38 - loss: 1.4151 - regression_loss: 1.1042 - classification_loss: 0.3110
 494/1000 [=============>................] - ETA: 18:36 - loss: 1.4136 - regression_loss: 1.1029 - classification_loss: 0.3107
 495/1000 [=============>................] - ETA: 18:34 - loss: 1.4134 - regression_loss: 1.1030 - classification_loss: 0.3104
 496/1000 [=============>................] - ETA: 18:32 - loss: 1.4132 - regression_loss: 1.1031 - classification_loss: 0.3101
 497/1000 [=============>................] - ETA: 18:29 - loss: 1.4123 - regression_loss: 1.1025 - classification_loss: 0.3098
 498/1000 [=============>................] - ETA: 18:28 - loss: 1.4119 - regression_loss: 1.1025 - classification_loss: 0.3095
 499/1000 [=============>................] - ETA: 18:26 - loss: 1.4117 - regression_loss: 1.1026 - classification_loss: 0.3091
 500/1000 [==============>...............] - ETA: 18:23 - loss: 1.4101 - regression_loss: 1.1013 - classification_loss: 0.3088
 501/1000 [==============>...............] - ETA: 18:21 - loss: 1.4096 - regression_loss: 1.1012 - classification_loss: 0.3085
 502/1000 [==============>...............] - ETA: 18:19 - loss: 1.4091 - regression_loss: 1.1009 - classification_loss: 0.3082
 503/1000 [==============>...............] - ETA: 18:17 - loss: 1.4086 - regression_loss: 1.1008 - classification_loss: 0.3078
 504/1000 [==============>...............] - ETA: 18:15 - loss: 1.4083 - regression_loss: 1.1008 - classification_loss: 0.3075
 505/1000 [==============>...............] - ETA: 18:12 - loss: 1.4076 - regression_loss: 1.1004 - classification_loss: 0.3072
 506/1000 [==============>...............] - ETA: 18:10 - loss: 1.4071 - regression_loss: 1.1003 - classification_loss: 0.3069
 507/1000 [==============>...............] - ETA: 18:07 - loss: 1.4055 - regression_loss: 1.0989 - classification_loss: 0.3066
 508/1000 [==============>...............] - ETA: 18:06 - loss: 1.4053 - regression_loss: 1.0990 - classification_loss: 0.3063
 509/1000 [==============>...............] - ETA: 18:04 - loss: 1.4048 - regression_loss: 1.0989 - classification_loss: 0.3059
 510/1000 [==============>...............] - ETA: 18:02 - loss: 1.4045 - regression_loss: 1.0989 - classification_loss: 0.3056
 511/1000 [==============>...............] - ETA: 18:00 - loss: 1.4041 - regression_loss: 1.0987 - classification_loss: 0.3053
 512/1000 [==============>...............] - ETA: 17:57 - loss: 1.4034 - regression_loss: 1.0984 - classification_loss: 0.3050
 513/1000 [==============>...............] - ETA: 17:55 - loss: 1.4030 - regression_loss: 1.0982 - classification_loss: 0.3047
 514/1000 [==============>...............] - ETA: 17:53 - loss: 1.4025 - regression_loss: 1.0981 - classification_loss: 0.3044
 515/1000 [==============>...............] - ETA: 17:51 - loss: 1.4017 - regression_loss: 1.0976 - classification_loss: 0.3041
 516/1000 [==============>...............] - ETA: 17:49 - loss: 1.4015 - regression_loss: 1.0977 - classification_loss: 0.3038
 517/1000 [==============>...............] - ETA: 17:46 - loss: 1.3997 - regression_loss: 1.0962 - classification_loss: 0.3035
 518/1000 [==============>...............] - ETA: 17:44 - loss: 1.3992 - regression_loss: 1.0961 - classification_loss: 0.3032
 519/1000 [==============>...............] - ETA: 17:42 - loss: 1.3990 - regression_loss: 1.0961 - classification_loss: 0.3029
 520/1000 [==============>...............] - ETA: 17:40 - loss: 1.3985 - regression_loss: 1.0960 - classification_loss: 0.3025
 521/1000 [==============>...............] - ETA: 17:38 - loss: 1.3983 - regression_loss: 1.0960 - classification_loss: 0.3022
 522/1000 [==============>...............] - ETA: 17:35 - loss: 1.3966 - regression_loss: 1.0947 - classification_loss: 0.3019
 523/1000 [==============>...............] - ETA: 17:33 - loss: 1.3962 - regression_loss: 1.0945 - classification_loss: 0.3016
 524/1000 [==============>...............] - ETA: 17:31 - loss: 1.3959 - regression_loss: 1.0946 - classification_loss: 0.3013
 525/1000 [==============>...............] - ETA: 17:29 - loss: 1.3957 - regression_loss: 1.0947 - classification_loss: 0.3010
 526/1000 [==============>...............] - ETA: 17:27 - loss: 1.3956 - regression_loss: 1.0949 - classification_loss: 0.3007
 527/1000 [==============>...............] - ETA: 17:24 - loss: 1.3943 - regression_loss: 1.0938 - classification_loss: 0.3005
 528/1000 [==============>...............] - ETA: 17:22 - loss: 1.3941 - regression_loss: 1.0939 - classification_loss: 0.3002
 529/1000 [==============>...............] - ETA: 17:20 - loss: 1.3937 - regression_loss: 1.0938 - classification_loss: 0.3000
 530/1000 [==============>...............] - ETA: 17:18 - loss: 1.3933 - regression_loss: 1.0937 - classification_loss: 0.2997
 531/1000 [==============>...............] - ETA: 17:16 - loss: 1.3931 - regression_loss: 1.0938 - classification_loss: 0.2994
 532/1000 [==============>...............] - ETA: 17:14 - loss: 1.3927 - regression_loss: 1.0937 - classification_loss: 0.2991
 533/1000 [==============>...............] - ETA: 17:12 - loss: 1.3920 - regression_loss: 1.0932 - classification_loss: 0.2988
 534/1000 [===============>..............] - ETA: 17:09 - loss: 1.3912 - regression_loss: 1.0927 - classification_loss: 0.2985
 535/1000 [===============>..............] - ETA: 17:06 - loss: 1.3894 - regression_loss: 1.0913 - classification_loss: 0.2982
 536/1000 [===============>..............] - ETA: 17:04 - loss: 1.3893 - regression_loss: 1.0914 - classification_loss: 0.2979
 537/1000 [===============>..............] - ETA: 17:02 - loss: 1.3888 - regression_loss: 1.0913 - classification_loss: 0.2976
 538/1000 [===============>..............] - ETA: 17:00 - loss: 1.3884 - regression_loss: 1.0911 - classification_loss: 0.2973
 539/1000 [===============>..............] - ETA: 16:59 - loss: 1.3882 - regression_loss: 1.0911 - classification_loss: 0.2970
 540/1000 [===============>..............] - ETA: 16:56 - loss: 1.3878 - regression_loss: 1.0910 - classification_loss: 0.2967
 541/1000 [===============>..............] - ETA: 16:54 - loss: 1.3864 - regression_loss: 1.0899 - classification_loss: 0.2965
 542/1000 [===============>..............] - ETA: 16:51 - loss: 1.3860 - regression_loss: 1.0898 - classification_loss: 0.2962
 543/1000 [===============>..............] - ETA: 16:49 - loss: 1.3857 - regression_loss: 1.0898 - classification_loss: 0.2959
 544/1000 [===============>..............] - ETA: 16:48 - loss: 1.3855 - regression_loss: 1.0899 - classification_loss: 0.2956
 545/1000 [===============>..............] - ETA: 16:46 - loss: 1.3852 - regression_loss: 1.0898 - classification_loss: 0.2954
 546/1000 [===============>..............] - ETA: 16:43 - loss: 1.3849 - regression_loss: 1.0898 - classification_loss: 0.2951
 547/1000 [===============>..............] - ETA: 16:41 - loss: 1.3847 - regression_loss: 1.0899 - classification_loss: 0.2948
 548/1000 [===============>..............] - ETA: 16:39 - loss: 1.3845 - regression_loss: 1.0899 - classification_loss: 0.2946
 549/1000 [===============>..............] - ETA: 16:37 - loss: 1.3839 - regression_loss: 1.0896 - classification_loss: 0.2943
 550/1000 [===============>..............] - ETA: 16:34 - loss: 1.3835 - regression_loss: 1.0894 - classification_loss: 0.2940
 551/1000 [===============>..............] - ETA: 16:32 - loss: 1.3820 - regression_loss: 1.0882 - classification_loss: 0.2938
 552/1000 [===============>..............] - ETA: 16:29 - loss: 1.3817 - regression_loss: 1.0882 - classification_loss: 0.2935
 553/1000 [===============>..............] - ETA: 16:28 - loss: 1.3815 - regression_loss: 1.0883 - classification_loss: 0.2932
 554/1000 [===============>..............] - ETA: 16:26 - loss: 1.3811 - regression_loss: 1.0882 - classification_loss: 0.2929
 555/1000 [===============>..............] - ETA: 16:24 - loss: 1.3807 - regression_loss: 1.0880 - classification_loss: 0.2927
 556/1000 [===============>..............] - ETA: 16:21 - loss: 1.3794 - regression_loss: 1.0870 - classification_loss: 0.2924
 557/1000 [===============>..............] - ETA: 16:19 - loss: 1.3790 - regression_loss: 1.0869 - classification_loss: 0.2922
 558/1000 [===============>..............] - ETA: 16:16 - loss: 1.3786 - regression_loss: 1.0866 - classification_loss: 0.2919
 559/1000 [===============>..............] - ETA: 16:14 - loss: 1.3784 - regression_loss: 1.0867 - classification_loss: 0.2917
 560/1000 [===============>..............] - ETA: 16:13 - loss: 1.3783 - regression_loss: 1.0868 - classification_loss: 0.2915
 561/1000 [===============>..............] - ETA: 16:10 - loss: 1.3779 - regression_loss: 1.0867 - classification_loss: 0.2912
 562/1000 [===============>..............] - ETA: 16:08 - loss: 1.3764 - regression_loss: 1.0855 - classification_loss: 0.2909
 563/1000 [===============>..............] - ETA: 16:05 - loss: 1.3757 - regression_loss: 1.0850 - classification_loss: 0.2906
 564/1000 [===============>..............] - ETA: 16:03 - loss: 1.3755 - regression_loss: 1.0851 - classification_loss: 0.2904
 565/1000 [===============>..............] - ETA: 16:01 - loss: 1.3752 - regression_loss: 1.0850 - classification_loss: 0.2902
 566/1000 [===============>..............] - ETA: 15:59 - loss: 1.3748 - regression_loss: 1.0849 - classification_loss: 0.2900
 567/1000 [================>.............] - ETA: 15:57 - loss: 1.3746 - regression_loss: 1.0849 - classification_loss: 0.2897
 568/1000 [================>.............] - ETA: 15:55 - loss: 1.3742 - regression_loss: 1.0848 - classification_loss: 0.2894
 569/1000 [================>.............] - ETA: 15:53 - loss: 1.3739 - regression_loss: 1.0847 - classification_loss: 0.2892
 570/1000 [================>.............] - ETA: 15:51 - loss: 1.3735 - regression_loss: 1.0846 - classification_loss: 0.2889
 571/1000 [================>.............] - ETA: 15:48 - loss: 1.3727 - regression_loss: 1.0841 - classification_loss: 0.2887
 572/1000 [================>.............] - ETA: 15:46 - loss: 1.3725 - regression_loss: 1.0841 - classification_loss: 0.2884
 573/1000 [================>.............] - ETA: 15:44 - loss: 1.3720 - regression_loss: 1.0839 - classification_loss: 0.2882
 574/1000 [================>.............] - ETA: 15:42 - loss: 1.3706 - regression_loss: 1.0827 - classification_loss: 0.2879
 575/1000 [================>.............] - ETA: 15:40 - loss: 1.3703 - regression_loss: 1.0827 - classification_loss: 0.2876
 576/1000 [================>.............] - ETA: 15:38 - loss: 1.3701 - regression_loss: 1.0827 - classification_loss: 0.2874
 577/1000 [================>.............] - ETA: 15:35 - loss: 1.3694 - regression_loss: 1.0822 - classification_loss: 0.2871
 578/1000 [================>.............] - ETA: 15:33 - loss: 1.3690 - regression_loss: 1.0821 - classification_loss: 0.2869
 579/1000 [================>.............] - ETA: 15:31 - loss: 1.3688 - regression_loss: 1.0822 - classification_loss: 0.2867
 580/1000 [================>.............] - ETA: 15:28 - loss: 1.3675 - regression_loss: 1.0811 - classification_loss: 0.2864
 581/1000 [================>.............] - ETA: 15:26 - loss: 1.3671 - regression_loss: 1.0810 - classification_loss: 0.2862
 582/1000 [================>.............] - ETA: 15:24 - loss: 1.3667 - regression_loss: 1.0808 - classification_loss: 0.2859
 583/1000 [================>.............] - ETA: 15:22 - loss: 1.3655 - regression_loss: 1.0799 - classification_loss: 0.2856
 584/1000 [================>.............] - ETA: 15:19 - loss: 1.3649 - regression_loss: 1.0795 - classification_loss: 0.2854
 585/1000 [================>.............] - ETA: 15:17 - loss: 1.3647 - regression_loss: 1.0796 - classification_loss: 0.2851
 586/1000 [================>.............] - ETA: 15:15 - loss: 1.3643 - regression_loss: 1.0794 - classification_loss: 0.2849
 587/1000 [================>.............] - ETA: 15:13 - loss: 1.3639 - regression_loss: 1.0793 - classification_loss: 0.2846
 588/1000 [================>.............] - ETA: 15:11 - loss: 1.3637 - regression_loss: 1.0793 - classification_loss: 0.2844
 589/1000 [================>.............] - ETA: 15:09 - loss: 1.3635 - regression_loss: 1.0793 - classification_loss: 0.2842
 590/1000 [================>.............] - ETA: 15:07 - loss: 1.3631 - regression_loss: 1.0793 - classification_loss: 0.2839
 591/1000 [================>.............] - ETA: 15:04 - loss: 1.3626 - regression_loss: 1.0790 - classification_loss: 0.2837
 592/1000 [================>.............] - ETA: 15:02 - loss: 1.3624 - regression_loss: 1.0790 - classification_loss: 0.2834
 593/1000 [================>.............] - ETA: 15:00 - loss: 1.3621 - regression_loss: 1.0789 - classification_loss: 0.2832
 594/1000 [================>.............] - ETA: 14:58 - loss: 1.3619 - regression_loss: 1.0790 - classification_loss: 0.2829
 595/1000 [================>.............] - ETA: 14:56 - loss: 1.3615 - regression_loss: 1.0788 - classification_loss: 0.2827
 596/1000 [================>.............] - ETA: 14:54 - loss: 1.3602 - regression_loss: 1.0777 - classification_loss: 0.2826
 597/1000 [================>.............] - ETA: 14:51 - loss: 1.3595 - regression_loss: 1.0772 - classification_loss: 0.2823
 598/1000 [================>.............] - ETA: 14:48 - loss: 1.3581 - regression_loss: 1.0761 - classification_loss: 0.2821
 599/1000 [================>.............] - ETA: 14:46 - loss: 1.3580 - regression_loss: 1.0762 - classification_loss: 0.2819
 600/1000 [=================>............] - ETA: 14:44 - loss: 1.3577 - regression_loss: 1.0760 - classification_loss: 0.2817
 601/1000 [=================>............] - ETA: 14:42 - loss: 1.3574 - regression_loss: 1.0759 - classification_loss: 0.2815
 602/1000 [=================>............] - ETA: 14:41 - loss: 1.3573 - regression_loss: 1.0760 - classification_loss: 0.2813
 603/1000 [=================>............] - ETA: 14:38 - loss: 1.3570 - regression_loss: 1.0759 - classification_loss: 0.2810
 604/1000 [=================>............] - ETA: 14:36 - loss: 1.3563 - regression_loss: 1.0755 - classification_loss: 0.2808
 605/1000 [=================>............] - ETA: 14:34 - loss: 1.3561 - regression_loss: 1.0755 - classification_loss: 0.2806
 606/1000 [=================>............] - ETA: 14:32 - loss: 1.3557 - regression_loss: 1.0753 - classification_loss: 0.2804
 607/1000 [=================>............] - ETA: 14:29 - loss: 1.3544 - regression_loss: 1.0742 - classification_loss: 0.2802
 608/1000 [=================>............] - ETA: 14:27 - loss: 1.3541 - regression_loss: 1.0741 - classification_loss: 0.2800
 609/1000 [=================>............] - ETA: 14:25 - loss: 1.3537 - regression_loss: 1.0739 - classification_loss: 0.2798
 610/1000 [=================>............] - ETA: 14:23 - loss: 1.3536 - regression_loss: 1.0740 - classification_loss: 0.2796
 611/1000 [=================>............] - ETA: 14:20 - loss: 1.3521 - regression_loss: 1.0728 - classification_loss: 0.2793
 612/1000 [=================>............] - ETA: 14:18 - loss: 1.3515 - regression_loss: 1.0724 - classification_loss: 0.2790
 613/1000 [=================>............] - ETA: 14:16 - loss: 1.3513 - regression_loss: 1.0725 - classification_loss: 0.2788
 614/1000 [=================>............] - ETA: 14:13 - loss: 1.3509 - regression_loss: 1.0724 - classification_loss: 0.2786
 615/1000 [=================>............] - ETA: 14:11 - loss: 1.3505 - regression_loss: 1.0722 - classification_loss: 0.2784
 616/1000 [=================>............] - ETA: 14:09 - loss: 1.3504 - regression_loss: 1.0722 - classification_loss: 0.2781
 617/1000 [=================>............] - ETA: 14:07 - loss: 1.3500 - regression_loss: 1.0721 - classification_loss: 0.2779
 618/1000 [=================>............] - ETA: 14:05 - loss: 1.3498 - regression_loss: 1.0722 - classification_loss: 0.2777
 619/1000 [=================>............] - ETA: 14:03 - loss: 1.3495 - regression_loss: 1.0721 - classification_loss: 0.2774
 620/1000 [=================>............] - ETA: 14:01 - loss: 1.3490 - regression_loss: 1.0717 - classification_loss: 0.2772
 621/1000 [=================>............] - ETA: 13:58 - loss: 1.3477 - regression_loss: 1.0707 - classification_loss: 0.2770
 622/1000 [=================>............] - ETA: 13:56 - loss: 1.3473 - regression_loss: 1.0705 - classification_loss: 0.2768
 623/1000 [=================>............] - ETA: 13:54 - loss: 1.3471 - regression_loss: 1.0706 - classification_loss: 0.2766
 624/1000 [=================>............] - ETA: 13:52 - loss: 1.3468 - regression_loss: 1.0704 - classification_loss: 0.2763
 625/1000 [=================>............] - ETA: 13:49 - loss: 1.3456 - regression_loss: 1.0694 - classification_loss: 0.2762
 626/1000 [=================>............] - ETA: 13:47 - loss: 1.3450 - regression_loss: 1.0689 - classification_loss: 0.2760
 627/1000 [=================>............] - ETA: 13:45 - loss: 1.3446 - regression_loss: 1.0688 - classification_loss: 0.2758
 628/1000 [=================>............] - ETA: 13:43 - loss: 1.3443 - regression_loss: 1.0687 - classification_loss: 0.2756
 629/1000 [=================>............] - ETA: 13:41 - loss: 1.3441 - regression_loss: 1.0688 - classification_loss: 0.2753
 630/1000 [=================>............] - ETA: 13:39 - loss: 1.3437 - regression_loss: 1.0685 - classification_loss: 0.2751
 631/1000 [=================>............] - ETA: 13:37 - loss: 1.3435 - regression_loss: 1.0686 - classification_loss: 0.2749
 632/1000 [=================>............] - ETA: 13:35 - loss: 1.3433 - regression_loss: 1.0686 - classification_loss: 0.2747
 633/1000 [=================>............] - ETA: 13:33 - loss: 1.3429 - regression_loss: 1.0684 - classification_loss: 0.2745
 634/1000 [==================>...........] - ETA: 13:30 - loss: 1.3424 - regression_loss: 1.0680 - classification_loss: 0.2743
 635/1000 [==================>...........] - ETA: 13:28 - loss: 1.3420 - regression_loss: 1.0679 - classification_loss: 0.2741
 636/1000 [==================>...........] - ETA: 13:26 - loss: 1.3418 - regression_loss: 1.0680 - classification_loss: 0.2739
 637/1000 [==================>...........] - ETA: 13:24 - loss: 1.3408 - regression_loss: 1.0669 - classification_loss: 0.2739
 638/1000 [==================>...........] - ETA: 13:21 - loss: 1.3405 - regression_loss: 1.0668 - classification_loss: 0.2737
 639/1000 [==================>...........] - ETA: 13:19 - loss: 1.3402 - regression_loss: 1.0667 - classification_loss: 0.2735
 640/1000 [==================>...........] - ETA: 13:17 - loss: 1.3388 - regression_loss: 1.0656 - classification_loss: 0.2732
 641/1000 [==================>...........] - ETA: 13:15 - loss: 1.3385 - regression_loss: 1.0654 - classification_loss: 0.2731
 642/1000 [==================>...........] - ETA: 13:12 - loss: 1.3382 - regression_loss: 1.0653 - classification_loss: 0.2729
 643/1000 [==================>...........] - ETA: 13:10 - loss: 1.3381 - regression_loss: 1.0654 - classification_loss: 0.2727
 644/1000 [==================>...........] - ETA: 13:08 - loss: 1.3376 - regression_loss: 1.0651 - classification_loss: 0.2725
 645/1000 [==================>...........] - ETA: 13:06 - loss: 1.3374 - regression_loss: 1.0651 - classification_loss: 0.2723
 646/1000 [==================>...........] - ETA: 13:04 - loss: 1.3373 - regression_loss: 1.0652 - classification_loss: 0.2721
 647/1000 [==================>...........] - ETA: 13:02 - loss: 1.3370 - regression_loss: 1.0650 - classification_loss: 0.2720
 648/1000 [==================>...........] - ETA: 13:00 - loss: 1.3368 - regression_loss: 1.0651 - classification_loss: 0.2718
 649/1000 [==================>...........] - ETA: 12:58 - loss: 1.3365 - regression_loss: 1.0650 - classification_loss: 0.2716
 650/1000 [==================>...........] - ETA: 12:55 - loss: 1.3354 - regression_loss: 1.0641 - classification_loss: 0.2713
 651/1000 [==================>...........] - ETA: 12:53 - loss: 1.3347 - regression_loss: 1.0636 - classification_loss: 0.2711
 652/1000 [==================>...........] - ETA: 12:51 - loss: 1.3344 - regression_loss: 1.0635 - classification_loss: 0.2709
 653/1000 [==================>...........] - ETA: 12:48 - loss: 1.3341 - regression_loss: 1.0634 - classification_loss: 0.2707
 654/1000 [==================>...........] - ETA: 12:46 - loss: 1.3339 - regression_loss: 1.0634 - classification_loss: 0.2705
 655/1000 [==================>...........] - ETA: 12:44 - loss: 1.3333 - regression_loss: 1.0629 - classification_loss: 0.2704
 656/1000 [==================>...........] - ETA: 12:42 - loss: 1.3330 - regression_loss: 1.0628 - classification_loss: 0.2701
 657/1000 [==================>...........] - ETA: 12:40 - loss: 1.3327 - regression_loss: 1.0627 - classification_loss: 0.2700
 658/1000 [==================>...........] - ETA: 12:38 - loss: 1.3325 - regression_loss: 1.0627 - classification_loss: 0.2698
 659/1000 [==================>...........] - ETA: 12:35 - loss: 1.3314 - regression_loss: 1.0617 - classification_loss: 0.2697
 660/1000 [==================>...........] - ETA: 12:32 - loss: 1.3301 - regression_loss: 1.0606 - classification_loss: 0.2695
 661/1000 [==================>...........] - ETA: 12:30 - loss: 1.3298 - regression_loss: 1.0605 - classification_loss: 0.2693
 662/1000 [==================>...........] - ETA: 12:28 - loss: 1.3297 - regression_loss: 1.0605 - classification_loss: 0.2691
 663/1000 [==================>...........] - ETA: 12:26 - loss: 1.3291 - regression_loss: 1.0602 - classification_loss: 0.2689
 664/1000 [==================>...........] - ETA: 12:24 - loss: 1.3288 - regression_loss: 1.0600 - classification_loss: 0.2688
 665/1000 [==================>...........] - ETA: 12:22 - loss: 1.3284 - regression_loss: 1.0599 - classification_loss: 0.2686
 666/1000 [==================>...........] - ETA: 12:20 - loss: 1.3282 - regression_loss: 1.0599 - classification_loss: 0.2684
 667/1000 [===================>..........] - ETA: 12:17 - loss: 1.3271 - regression_loss: 1.0589 - classification_loss: 0.2682
 668/1000 [===================>..........] - ETA: 12:15 - loss: 1.3270 - regression_loss: 1.0589 - classification_loss: 0.2680
 669/1000 [===================>..........] - ETA: 12:13 - loss: 1.3268 - regression_loss: 1.0589 - classification_loss: 0.2678
 670/1000 [===================>..........] - ETA: 12:11 - loss: 1.3264 - regression_loss: 1.0588 - classification_loss: 0.2676
 671/1000 [===================>..........] - ETA: 12:09 - loss: 1.3261 - regression_loss: 1.0586 - classification_loss: 0.2674
 672/1000 [===================>..........] - ETA: 12:07 - loss: 1.3257 - regression_loss: 1.0585 - classification_loss: 0.2672
 673/1000 [===================>..........] - ETA: 12:04 - loss: 1.3252 - regression_loss: 1.0581 - classification_loss: 0.2670
 674/1000 [===================>..........] - ETA: 12:02 - loss: 1.3249 - regression_loss: 1.0580 - classification_loss: 0.2669
 675/1000 [===================>..........] - ETA: 12:00 - loss: 1.3242 - regression_loss: 1.0576 - classification_loss: 0.2667
 676/1000 [===================>..........] - ETA: 11:57 - loss: 1.3231 - regression_loss: 1.0567 - classification_loss: 0.2664
 677/1000 [===================>..........] - ETA: 11:55 - loss: 1.3230 - regression_loss: 1.0567 - classification_loss: 0.2662
 678/1000 [===================>..........] - ETA: 11:53 - loss: 1.3228 - regression_loss: 1.0568 - classification_loss: 0.2660
 679/1000 [===================>..........] - ETA: 11:51 - loss: 1.3226 - regression_loss: 1.0567 - classification_loss: 0.2658
 680/1000 [===================>..........] - ETA: 11:49 - loss: 1.3222 - regression_loss: 1.0565 - classification_loss: 0.2657
 681/1000 [===================>..........] - ETA: 11:47 - loss: 1.3221 - regression_loss: 1.0566 - classification_loss: 0.2655
 682/1000 [===================>..........] - ETA: 11:45 - loss: 1.3219 - regression_loss: 1.0566 - classification_loss: 0.2653
 683/1000 [===================>..........] - ETA: 11:42 - loss: 1.3208 - regression_loss: 1.0558 - classification_loss: 0.2651
 684/1000 [===================>..........] - ETA: 11:40 - loss: 1.3205 - regression_loss: 1.0557 - classification_loss: 0.2649
 685/1000 [===================>..........] - ETA: 11:38 - loss: 1.3202 - regression_loss: 1.0555 - classification_loss: 0.2647
 686/1000 [===================>..........] - ETA: 11:36 - loss: 1.3199 - regression_loss: 1.0554 - classification_loss: 0.2645
 687/1000 [===================>..........] - ETA: 11:33 - loss: 1.3200 - regression_loss: 1.0557 - classification_loss: 0.2643
 688/1000 [===================>..........] - ETA: 11:31 - loss: 1.3197 - regression_loss: 1.0556 - classification_loss: 0.2642
 689/1000 [===================>..........] - ETA: 11:29 - loss: 1.3195 - regression_loss: 1.0555 - classification_loss: 0.2640
 690/1000 [===================>..........] - ETA: 11:26 - loss: 1.3192 - regression_loss: 1.0554 - classification_loss: 0.2638
 691/1000 [===================>..........] - ETA: 11:24 - loss: 1.3189 - regression_loss: 1.0553 - classification_loss: 0.2636
 692/1000 [===================>..........] - ETA: 11:22 - loss: 1.3176 - regression_loss: 1.0542 - classification_loss: 0.2634
 693/1000 [===================>..........] - ETA: 11:20 - loss: 1.3175 - regression_loss: 1.0543 - classification_loss: 0.2632
 694/1000 [===================>..........] - ETA: 11:18 - loss: 1.3173 - regression_loss: 1.0543 - classification_loss: 0.2630
 695/1000 [===================>..........] - ETA: 11:15 - loss: 1.3169 - regression_loss: 1.0540 - classification_loss: 0.2629
 696/1000 [===================>..........] - ETA: 11:13 - loss: 1.3167 - regression_loss: 1.0540 - classification_loss: 0.2627
 697/1000 [===================>..........] - ETA: 11:11 - loss: 1.3156 - regression_loss: 1.0532 - classification_loss: 0.2625
 698/1000 [===================>..........] - ETA: 11:08 - loss: 1.3153 - regression_loss: 1.0530 - classification_loss: 0.2623
 699/1000 [===================>..........] - ETA: 11:06 - loss: 1.3152 - regression_loss: 1.0530 - classification_loss: 0.2621
 700/1000 [====================>.........] - ETA: 11:04 - loss: 1.3149 - regression_loss: 1.0529 - classification_loss: 0.2619
 701/1000 [====================>.........] - ETA: 11:02 - loss: 1.3147 - regression_loss: 1.0530 - classification_loss: 0.2617
 702/1000 [====================>.........] - ETA: 11:00 - loss: 1.3145 - regression_loss: 1.0530 - classification_loss: 0.2616
 703/1000 [====================>.........] - ETA: 10:58 - loss: 1.3143 - regression_loss: 1.0529 - classification_loss: 0.2614
 704/1000 [====================>.........] - ETA: 10:56 - loss: 1.3139 - regression_loss: 1.0528 - classification_loss: 0.2612
 705/1000 [====================>.........] - ETA: 10:54 - loss: 1.3133 - regression_loss: 1.0523 - classification_loss: 0.2610
 706/1000 [====================>.........] - ETA: 10:51 - loss: 1.3124 - regression_loss: 1.0516 - classification_loss: 0.2608
 707/1000 [====================>.........] - ETA: 10:49 - loss: 1.3121 - regression_loss: 1.0515 - classification_loss: 0.2606
 708/1000 [====================>.........] - ETA: 10:47 - loss: 1.3119 - regression_loss: 1.0515 - classification_loss: 0.2604
 709/1000 [====================>.........] - ETA: 10:44 - loss: 1.3108 - regression_loss: 1.0505 - classification_loss: 0.2602
 710/1000 [====================>.........] - ETA: 10:42 - loss: 1.3106 - regression_loss: 1.0505 - classification_loss: 0.2600
 711/1000 [====================>.........] - ETA: 10:40 - loss: 1.3103 - regression_loss: 1.0504 - classification_loss: 0.2599
 712/1000 [====================>.........] - ETA: 10:38 - loss: 1.3101 - regression_loss: 1.0504 - classification_loss: 0.2597
 713/1000 [====================>.........] - ETA: 10:36 - loss: 1.3098 - regression_loss: 1.0503 - classification_loss: 0.2595
 714/1000 [====================>.........] - ETA: 10:34 - loss: 1.3095 - regression_loss: 1.0502 - classification_loss: 0.2593
 715/1000 [====================>.........] - ETA: 10:31 - loss: 1.3089 - regression_loss: 1.0498 - classification_loss: 0.2591
 716/1000 [====================>.........] - ETA: 10:29 - loss: 1.3077 - regression_loss: 1.0488 - classification_loss: 0.2589
 717/1000 [====================>.........] - ETA: 10:27 - loss: 1.3079 - regression_loss: 1.0491 - classification_loss: 0.2588
 718/1000 [====================>.........] - ETA: 10:25 - loss: 1.3079 - regression_loss: 1.0493 - classification_loss: 0.2586
 719/1000 [====================>.........] - ETA: 10:23 - loss: 1.3080 - regression_loss: 1.0496 - classification_loss: 0.2584
 720/1000 [====================>.........] - ETA: 10:20 - loss: 1.3076 - regression_loss: 1.0494 - classification_loss: 0.2583
 721/1000 [====================>.........] - ETA: 10:18 - loss: 1.3074 - regression_loss: 1.0493 - classification_loss: 0.2581
 722/1000 [====================>.........] - ETA: 10:16 - loss: 1.3073 - regression_loss: 1.0493 - classification_loss: 0.2580
 723/1000 [====================>.........] - ETA: 10:14 - loss: 1.3072 - regression_loss: 1.0494 - classification_loss: 0.2578
 724/1000 [====================>.........] - ETA: 10:12 - loss: 1.3070 - regression_loss: 1.0494 - classification_loss: 0.2576
 725/1000 [====================>.........] - ETA: 10:10 - loss: 1.3069 - regression_loss: 1.0494 - classification_loss: 0.2575
 726/1000 [====================>.........] - ETA: 10:07 - loss: 1.3063 - regression_loss: 1.0490 - classification_loss: 0.2573
 727/1000 [====================>.........] - ETA: 10:05 - loss: 1.3062 - regression_loss: 1.0490 - classification_loss: 0.2572
 728/1000 [====================>.........] - ETA: 10:03 - loss: 1.3052 - regression_loss: 1.0480 - classification_loss: 0.2571
 729/1000 [====================>.........] - ETA: 10:00 - loss: 1.3050 - regression_loss: 1.0480 - classification_loss: 0.2570
 730/1000 [====================>.........] - ETA: 9:58 - loss: 1.3047 - regression_loss: 1.0479 - classification_loss: 0.2568 
 731/1000 [====================>.........] - ETA: 9:56 - loss: 1.3041 - regression_loss: 1.0475 - classification_loss: 0.2566
 732/1000 [====================>.........] - ETA: 9:54 - loss: 1.3030 - regression_loss: 1.0465 - classification_loss: 0.2564
 733/1000 [====================>.........] - ETA: 9:51 - loss: 1.3028 - regression_loss: 1.0466 - classification_loss: 0.2563
 734/1000 [=====================>........] - ETA: 9:49 - loss: 1.3026 - regression_loss: 1.0465 - classification_loss: 0.2562
 735/1000 [=====================>........] - ETA: 9:47 - loss: 1.3024 - regression_loss: 1.0464 - classification_loss: 0.2560
 736/1000 [=====================>........] - ETA: 9:45 - loss: 1.3022 - regression_loss: 1.0464 - classification_loss: 0.2558
 737/1000 [=====================>........] - ETA: 9:43 - loss: 1.3019 - regression_loss: 1.0463 - classification_loss: 0.2556
 738/1000 [=====================>........] - ETA: 9:41 - loss: 1.3018 - regression_loss: 1.0463 - classification_loss: 0.2555
 739/1000 [=====================>........] - ETA: 9:39 - loss: 1.3015 - regression_loss: 1.0462 - classification_loss: 0.2553
 740/1000 [=====================>........] - ETA: 9:36 - loss: 1.3010 - regression_loss: 1.0458 - classification_loss: 0.2552
 741/1000 [=====================>........] - ETA: 9:34 - loss: 1.3002 - regression_loss: 1.0449 - classification_loss: 0.2553
 742/1000 [=====================>........] - ETA: 9:32 - loss: 1.3001 - regression_loss: 1.0450 - classification_loss: 0.2551
 743/1000 [=====================>........] - ETA: 9:29 - loss: 1.2997 - regression_loss: 1.0448 - classification_loss: 0.2550
 744/1000 [=====================>........] - ETA: 9:27 - loss: 1.2996 - regression_loss: 1.0448 - classification_loss: 0.2548
 745/1000 [=====================>........] - ETA: 9:25 - loss: 1.2991 - regression_loss: 1.0444 - classification_loss: 0.2547
 746/1000 [=====================>........] - ETA: 9:23 - loss: 1.2990 - regression_loss: 1.0445 - classification_loss: 0.2545
 747/1000 [=====================>........] - ETA: 9:21 - loss: 1.2986 - regression_loss: 1.0443 - classification_loss: 0.2544
 748/1000 [=====================>........] - ETA: 9:18 - loss: 1.2976 - regression_loss: 1.0434 - classification_loss: 0.2542
 749/1000 [=====================>........] - ETA: 9:16 - loss: 1.2974 - regression_loss: 1.0434 - classification_loss: 0.2540
 750/1000 [=====================>........] - ETA: 9:14 - loss: 1.2974 - regression_loss: 1.0435 - classification_loss: 0.2539
 751/1000 [=====================>........] - ETA: 9:12 - loss: 1.2974 - regression_loss: 1.0437 - classification_loss: 0.2537
 752/1000 [=====================>........] - ETA: 9:09 - loss: 1.2964 - regression_loss: 1.0428 - classification_loss: 0.2536
 753/1000 [=====================>........] - ETA: 9:07 - loss: 1.2958 - regression_loss: 1.0424 - classification_loss: 0.2534
 754/1000 [=====================>........] - ETA: 9:05 - loss: 1.2955 - regression_loss: 1.0422 - classification_loss: 0.2533
 755/1000 [=====================>........] - ETA: 9:03 - loss: 1.2954 - regression_loss: 1.0423 - classification_loss: 0.2531
 756/1000 [=====================>........] - ETA: 9:01 - loss: 1.2952 - regression_loss: 1.0423 - classification_loss: 0.2530
 757/1000 [=====================>........] - ETA: 8:58 - loss: 1.2950 - regression_loss: 1.0422 - classification_loss: 0.2528
 758/1000 [=====================>........] - ETA: 8:56 - loss: 1.2940 - regression_loss: 1.0413 - classification_loss: 0.2526
 759/1000 [=====================>........] - ETA: 8:54 - loss: 1.2939 - regression_loss: 1.0414 - classification_loss: 0.2525
 760/1000 [=====================>........] - ETA: 8:52 - loss: 1.2938 - regression_loss: 1.0414 - classification_loss: 0.2523
 761/1000 [=====================>........] - ETA: 8:50 - loss: 1.2935 - regression_loss: 1.0414 - classification_loss: 0.2522
 762/1000 [=====================>........] - ETA: 8:47 - loss: 1.2931 - regression_loss: 1.0411 - classification_loss: 0.2520
 763/1000 [=====================>........] - ETA: 8:45 - loss: 1.2928 - regression_loss: 1.0410 - classification_loss: 0.2518
 764/1000 [=====================>........] - ETA: 8:43 - loss: 1.2926 - regression_loss: 1.0409 - classification_loss: 0.2517
 765/1000 [=====================>........] - ETA: 8:41 - loss: 1.2924 - regression_loss: 1.0409 - classification_loss: 0.2515
 766/1000 [=====================>........] - ETA: 8:38 - loss: 1.2918 - regression_loss: 1.0404 - classification_loss: 0.2513
 767/1000 [======================>.......] - ETA: 8:36 - loss: 1.2907 - regression_loss: 1.0395 - classification_loss: 0.2512
 768/1000 [======================>.......] - ETA: 8:34 - loss: 1.2904 - regression_loss: 1.0395 - classification_loss: 0.2510
 769/1000 [======================>.......] - ETA: 8:32 - loss: 1.2902 - regression_loss: 1.0394 - classification_loss: 0.2508
 770/1000 [======================>.......] - ETA: 8:30 - loss: 1.2901 - regression_loss: 1.0394 - classification_loss: 0.2507
 771/1000 [======================>.......] - ETA: 8:27 - loss: 1.2897 - regression_loss: 1.0392 - classification_loss: 0.2505
 772/1000 [======================>.......] - ETA: 8:25 - loss: 1.2887 - regression_loss: 1.0383 - classification_loss: 0.2503
 773/1000 [======================>.......] - ETA: 8:23 - loss: 1.2884 - regression_loss: 1.0382 - classification_loss: 0.2502
 774/1000 [======================>.......] - ETA: 8:21 - loss: 1.2881 - regression_loss: 1.0381 - classification_loss: 0.2500
 775/1000 [======================>.......] - ETA: 8:18 - loss: 1.2877 - regression_loss: 1.0379 - classification_loss: 0.2499
 776/1000 [======================>.......] - ETA: 8:16 - loss: 1.2876 - regression_loss: 1.0379 - classification_loss: 0.2497
 777/1000 [======================>.......] - ETA: 8:14 - loss: 1.2874 - regression_loss: 1.0379 - classification_loss: 0.2496
 778/1000 [======================>.......] - ETA: 8:12 - loss: 1.2872 - regression_loss: 1.0378 - classification_loss: 0.2494
 779/1000 [======================>.......] - ETA: 8:10 - loss: 1.2870 - regression_loss: 1.0378 - classification_loss: 0.2492
 780/1000 [======================>.......] - ETA: 8:08 - loss: 1.2867 - regression_loss: 1.0376 - classification_loss: 0.2491
 781/1000 [======================>.......] - ETA: 8:06 - loss: 1.2865 - regression_loss: 1.0376 - classification_loss: 0.2489
 782/1000 [======================>.......] - ETA: 8:03 - loss: 1.2863 - regression_loss: 1.0375 - classification_loss: 0.2488
 783/1000 [======================>.......] - ETA: 8:01 - loss: 1.2852 - regression_loss: 1.0366 - classification_loss: 0.2486
 784/1000 [======================>.......] - ETA: 7:59 - loss: 1.2848 - regression_loss: 1.0363 - classification_loss: 0.2484
 785/1000 [======================>.......] - ETA: 7:56 - loss: 1.2845 - regression_loss: 1.0362 - classification_loss: 0.2483
 786/1000 [======================>.......] - ETA: 7:54 - loss: 1.2842 - regression_loss: 1.0361 - classification_loss: 0.2481
 787/1000 [======================>.......] - ETA: 7:52 - loss: 1.2841 - regression_loss: 1.0361 - classification_loss: 0.2480
 788/1000 [======================>.......] - ETA: 7:50 - loss: 1.2839 - regression_loss: 1.0361 - classification_loss: 0.2478
 789/1000 [======================>.......] - ETA: 7:48 - loss: 1.2836 - regression_loss: 1.0360 - classification_loss: 0.2477
 790/1000 [======================>.......] - ETA: 7:45 - loss: 1.2826 - regression_loss: 1.0352 - classification_loss: 0.2475
 791/1000 [======================>.......] - ETA: 7:43 - loss: 1.2821 - regression_loss: 1.0348 - classification_loss: 0.2473
 792/1000 [======================>.......] - ETA: 7:41 - loss: 1.2819 - regression_loss: 1.0347 - classification_loss: 0.2472
 793/1000 [======================>.......] - ETA: 7:39 - loss: 1.2818 - regression_loss: 1.0348 - classification_loss: 0.2471
 794/1000 [======================>.......] - ETA: 7:36 - loss: 1.2808 - regression_loss: 1.0339 - classification_loss: 0.2469
 795/1000 [======================>.......] - ETA: 7:34 - loss: 1.2805 - regression_loss: 1.0338 - classification_loss: 0.2468
 796/1000 [======================>.......] - ETA: 7:32 - loss: 1.2803 - regression_loss: 1.0336 - classification_loss: 0.2466
 797/1000 [======================>.......] - ETA: 7:30 - loss: 1.2801 - regression_loss: 1.0337 - classification_loss: 0.2465
 798/1000 [======================>.......] - ETA: 7:28 - loss: 1.2796 - regression_loss: 1.0333 - classification_loss: 0.2463
 799/1000 [======================>.......] - ETA: 7:25 - loss: 1.2794 - regression_loss: 1.0332 - classification_loss: 0.2462
 800/1000 [=======================>......] - ETA: 7:23 - loss: 1.2788 - regression_loss: 1.0328 - classification_loss: 0.2460
 801/1000 [=======================>......] - ETA: 7:21 - loss: 1.2778 - regression_loss: 1.0319 - classification_loss: 0.2458
 802/1000 [=======================>......] - ETA: 7:18 - loss: 1.2775 - regression_loss: 1.0318 - classification_loss: 0.2457
 803/1000 [=======================>......] - ETA: 7:16 - loss: 1.2773 - regression_loss: 1.0318 - classification_loss: 0.2455
 804/1000 [=======================>......] - ETA: 7:14 - loss: 1.2771 - regression_loss: 1.0317 - classification_loss: 0.2454
 805/1000 [=======================>......] - ETA: 7:12 - loss: 1.2767 - regression_loss: 1.0315 - classification_loss: 0.2452
 806/1000 [=======================>......] - ETA: 7:10 - loss: 1.2766 - regression_loss: 1.0315 - classification_loss: 0.2451
 807/1000 [=======================>......] - ETA: 7:08 - loss: 1.2765 - regression_loss: 1.0315 - classification_loss: 0.2449
 808/1000 [=======================>......] - ETA: 7:06 - loss: 1.2763 - regression_loss: 1.0315 - classification_loss: 0.2448
 809/1000 [=======================>......] - ETA: 7:03 - loss: 1.2761 - regression_loss: 1.0314 - classification_loss: 0.2446
 810/1000 [=======================>......] - ETA: 7:01 - loss: 1.2758 - regression_loss: 1.0313 - classification_loss: 0.2445
 811/1000 [=======================>......] - ETA: 6:59 - loss: 1.2755 - regression_loss: 1.0312 - classification_loss: 0.2443
 812/1000 [=======================>......] - ETA: 6:57 - loss: 1.2753 - regression_loss: 1.0311 - classification_loss: 0.2442
 813/1000 [=======================>......] - ETA: 6:54 - loss: 1.2742 - regression_loss: 1.0302 - classification_loss: 0.2440
 814/1000 [=======================>......] - ETA: 6:52 - loss: 1.2731 - regression_loss: 1.0293 - classification_loss: 0.2438
 815/1000 [=======================>......] - ETA: 6:50 - loss: 1.2731 - regression_loss: 1.0294 - classification_loss: 0.2437
 816/1000 [=======================>......] - ETA: 6:48 - loss: 1.2730 - regression_loss: 1.0295 - classification_loss: 0.2435
 817/1000 [=======================>......] - ETA: 6:46 - loss: 1.2728 - regression_loss: 1.0294 - classification_loss: 0.2434
 818/1000 [=======================>......] - ETA: 6:43 - loss: 1.2725 - regression_loss: 1.0293 - classification_loss: 0.2432
 819/1000 [=======================>......] - ETA: 6:41 - loss: 1.2723 - regression_loss: 1.0292 - classification_loss: 0.2431
 820/1000 [=======================>......] - ETA: 6:39 - loss: 1.2718 - regression_loss: 1.0289 - classification_loss: 0.2429
 821/1000 [=======================>......] - ETA: 6:37 - loss: 1.2716 - regression_loss: 1.0288 - classification_loss: 0.2428
 822/1000 [=======================>......] - ETA: 6:34 - loss: 1.2706 - regression_loss: 1.0280 - classification_loss: 0.2426
 823/1000 [=======================>......] - ETA: 6:32 - loss: 1.2706 - regression_loss: 1.0281 - classification_loss: 0.2425
 824/1000 [=======================>......] - ETA: 6:30 - loss: 1.2703 - regression_loss: 1.0280 - classification_loss: 0.2423
 825/1000 [=======================>......] - ETA: 6:28 - loss: 1.2698 - regression_loss: 1.0277 - classification_loss: 0.2422
 826/1000 [=======================>......] - ETA: 6:25 - loss: 1.2697 - regression_loss: 1.0276 - classification_loss: 0.2420
 827/1000 [=======================>......] - ETA: 6:23 - loss: 1.2696 - regression_loss: 1.0277 - classification_loss: 0.2419
 828/1000 [=======================>......] - ETA: 6:21 - loss: 1.2693 - regression_loss: 1.0275 - classification_loss: 0.2418
 829/1000 [=======================>......] - ETA: 6:19 - loss: 1.2691 - regression_loss: 1.0275 - classification_loss: 0.2416
 830/1000 [=======================>......] - ETA: 6:17 - loss: 1.2683 - regression_loss: 1.0269 - classification_loss: 0.2414
 831/1000 [=======================>......] - ETA: 6:15 - loss: 1.2682 - regression_loss: 1.0269 - classification_loss: 0.2413
 832/1000 [=======================>......] - ETA: 6:12 - loss: 1.2680 - regression_loss: 1.0268 - classification_loss: 0.2411
 833/1000 [=======================>......] - ETA: 6:10 - loss: 1.2676 - regression_loss: 1.0266 - classification_loss: 0.2410
 834/1000 [========================>.....] - ETA: 6:08 - loss: 1.2674 - regression_loss: 1.0265 - classification_loss: 0.2409
 835/1000 [========================>.....] - ETA: 6:06 - loss: 1.2673 - regression_loss: 1.0266 - classification_loss: 0.2407
 836/1000 [========================>.....] - ETA: 6:04 - loss: 1.2671 - regression_loss: 1.0266 - classification_loss: 0.2406
 837/1000 [========================>.....] - ETA: 6:01 - loss: 1.2669 - regression_loss: 1.0265 - classification_loss: 0.2404
 838/1000 [========================>.....] - ETA: 5:59 - loss: 1.2666 - regression_loss: 1.0263 - classification_loss: 0.2403
 839/1000 [========================>.....] - ETA: 5:57 - loss: 1.2661 - regression_loss: 1.0259 - classification_loss: 0.2402
 840/1000 [========================>.....] - ETA: 5:55 - loss: 1.2651 - regression_loss: 1.0251 - classification_loss: 0.2400
 841/1000 [========================>.....] - ETA: 5:52 - loss: 1.2648 - regression_loss: 1.0250 - classification_loss: 0.2399
 842/1000 [========================>.....] - ETA: 5:50 - loss: 1.2646 - regression_loss: 1.0248 - classification_loss: 0.2397
 843/1000 [========================>.....] - ETA: 5:48 - loss: 1.2643 - regression_loss: 1.0248 - classification_loss: 0.2396
 844/1000 [========================>.....] - ETA: 5:46 - loss: 1.2642 - regression_loss: 1.0248 - classification_loss: 0.2394
 845/1000 [========================>.....] - ETA: 5:44 - loss: 1.2640 - regression_loss: 1.0247 - classification_loss: 0.2393
 846/1000 [========================>.....] - ETA: 5:41 - loss: 1.2639 - regression_loss: 1.0248 - classification_loss: 0.2392
 847/1000 [========================>.....] - ETA: 5:39 - loss: 1.2630 - regression_loss: 1.0240 - classification_loss: 0.2390
 848/1000 [========================>.....] - ETA: 5:37 - loss: 1.2626 - regression_loss: 1.0237 - classification_loss: 0.2389
 849/1000 [========================>.....] - ETA: 5:35 - loss: 1.2616 - regression_loss: 1.0229 - classification_loss: 0.2387
 850/1000 [========================>.....] - ETA: 5:32 - loss: 1.2614 - regression_loss: 1.0229 - classification_loss: 0.2386
 851/1000 [========================>.....] - ETA: 5:30 - loss: 1.2612 - regression_loss: 1.0228 - classification_loss: 0.2385
 852/1000 [========================>.....] - ETA: 5:28 - loss: 1.2609 - regression_loss: 1.0226 - classification_loss: 0.2383
 853/1000 [========================>.....] - ETA: 5:26 - loss: 1.2608 - regression_loss: 1.0227 - classification_loss: 0.2382
 854/1000 [========================>.....] - ETA: 5:24 - loss: 1.2607 - regression_loss: 1.0227 - classification_loss: 0.2381
 855/1000 [========================>.....] - ETA: 5:21 - loss: 1.2604 - regression_loss: 1.0225 - classification_loss: 0.2379
 856/1000 [========================>.....] - ETA: 5:19 - loss: 1.2599 - regression_loss: 1.0222 - classification_loss: 0.2378
 857/1000 [========================>.....] - ETA: 5:17 - loss: 1.2599 - regression_loss: 1.0223 - classification_loss: 0.2376
 858/1000 [========================>.....] - ETA: 5:15 - loss: 1.2598 - regression_loss: 1.0223 - classification_loss: 0.2375
 859/1000 [========================>.....] - ETA: 5:12 - loss: 1.2597 - regression_loss: 1.0223 - classification_loss: 0.2374
 860/1000 [========================>.....] - ETA: 5:10 - loss: 1.2597 - regression_loss: 1.0224 - classification_loss: 0.2373
 861/1000 [========================>.....] - ETA: 5:08 - loss: 1.2594 - regression_loss: 1.0223 - classification_loss: 0.2371
 862/1000 [========================>.....] - ETA: 5:06 - loss: 1.2585 - regression_loss: 1.0215 - classification_loss: 0.2369
 863/1000 [========================>.....] - ETA: 5:04 - loss: 1.2584 - regression_loss: 1.0216 - classification_loss: 0.2368
 864/1000 [========================>.....] - ETA: 5:01 - loss: 1.2582 - regression_loss: 1.0216 - classification_loss: 0.2367
 865/1000 [========================>.....] - ETA: 4:59 - loss: 1.2581 - regression_loss: 1.0215 - classification_loss: 0.2365
 866/1000 [========================>.....] - ETA: 4:57 - loss: 1.2578 - regression_loss: 1.0214 - classification_loss: 0.2364
 867/1000 [=========================>....] - ETA: 4:55 - loss: 1.2577 - regression_loss: 1.0214 - classification_loss: 0.2363
 868/1000 [=========================>....] - ETA: 4:53 - loss: 1.2574 - regression_loss: 1.0213 - classification_loss: 0.2362
 869/1000 [=========================>....] - ETA: 4:50 - loss: 1.2566 - regression_loss: 1.0206 - classification_loss: 0.2360
 870/1000 [=========================>....] - ETA: 4:48 - loss: 1.2564 - regression_loss: 1.0205 - classification_loss: 0.2358
 871/1000 [=========================>....] - ETA: 4:46 - loss: 1.2561 - regression_loss: 1.0204 - classification_loss: 0.2357
 872/1000 [=========================>....] - ETA: 4:44 - loss: 1.2551 - regression_loss: 1.0196 - classification_loss: 0.2355
 873/1000 [=========================>....] - ETA: 4:41 - loss: 1.2549 - regression_loss: 1.0195 - classification_loss: 0.2354
 874/1000 [=========================>....] - ETA: 4:39 - loss: 1.2547 - regression_loss: 1.0195 - classification_loss: 0.2353
 875/1000 [=========================>....] - ETA: 4:37 - loss: 1.2546 - regression_loss: 1.0195 - classification_loss: 0.2351
 876/1000 [=========================>....] - ETA: 4:35 - loss: 1.2541 - regression_loss: 1.0190 - classification_loss: 0.2350
 877/1000 [=========================>....] - ETA: 4:33 - loss: 1.2538 - regression_loss: 1.0189 - classification_loss: 0.2349
 878/1000 [=========================>....] - ETA: 4:30 - loss: 1.2537 - regression_loss: 1.0189 - classification_loss: 0.2348
 879/1000 [=========================>....] - ETA: 4:28 - loss: 1.2530 - regression_loss: 1.0183 - classification_loss: 0.2347
 880/1000 [=========================>....] - ETA: 4:26 - loss: 1.2526 - regression_loss: 1.0181 - classification_loss: 0.2345
 881/1000 [=========================>....] - ETA: 4:24 - loss: 1.2525 - regression_loss: 1.0181 - classification_loss: 0.2344
 882/1000 [=========================>....] - ETA: 4:21 - loss: 1.2523 - regression_loss: 1.0180 - classification_loss: 0.2343
 883/1000 [=========================>....] - ETA: 4:19 - loss: 1.2520 - regression_loss: 1.0179 - classification_loss: 0.2341
 884/1000 [=========================>....] - ETA: 4:17 - loss: 1.2510 - regression_loss: 1.0171 - classification_loss: 0.2340
 885/1000 [=========================>....] - ETA: 4:15 - loss: 1.2508 - regression_loss: 1.0170 - classification_loss: 0.2338
 886/1000 [=========================>....] - ETA: 4:13 - loss: 1.2506 - regression_loss: 1.0169 - classification_loss: 0.2337
 887/1000 [=========================>....] - ETA: 4:10 - loss: 1.2505 - regression_loss: 1.0169 - classification_loss: 0.2336
 888/1000 [=========================>....] - ETA: 4:08 - loss: 1.2500 - regression_loss: 1.0165 - classification_loss: 0.2335
 889/1000 [=========================>....] - ETA: 4:06 - loss: 1.2498 - regression_loss: 1.0164 - classification_loss: 0.2333
 890/1000 [=========================>....] - ETA: 4:04 - loss: 1.2497 - regression_loss: 1.0165 - classification_loss: 0.2332
 891/1000 [=========================>....] - ETA: 4:02 - loss: 1.2496 - regression_loss: 1.0165 - classification_loss: 0.2331
 892/1000 [=========================>....] - ETA: 3:59 - loss: 1.2493 - regression_loss: 1.0163 - classification_loss: 0.2329
 893/1000 [=========================>....] - ETA: 3:57 - loss: 1.2491 - regression_loss: 1.0163 - classification_loss: 0.2328
 894/1000 [=========================>....] - ETA: 3:55 - loss: 1.2490 - regression_loss: 1.0163 - classification_loss: 0.2327
 895/1000 [=========================>....] - ETA: 3:53 - loss: 1.2487 - regression_loss: 1.0162 - classification_loss: 0.2325
 896/1000 [=========================>....] - ETA: 3:50 - loss: 1.2478 - regression_loss: 1.0155 - classification_loss: 0.2324
 897/1000 [=========================>....] - ETA: 3:48 - loss: 1.2476 - regression_loss: 1.0153 - classification_loss: 0.2323
 898/1000 [=========================>....] - ETA: 3:46 - loss: 1.2473 - regression_loss: 1.0151 - classification_loss: 0.2322
 899/1000 [=========================>....] - ETA: 3:44 - loss: 1.2471 - regression_loss: 1.0150 - classification_loss: 0.2320
 900/1000 [==========================>...] - ETA: 3:42 - loss: 1.2469 - regression_loss: 1.0150 - classification_loss: 0.2319
 901/1000 [==========================>...] - ETA: 3:39 - loss: 1.2468 - regression_loss: 1.0150 - classification_loss: 0.2318
 902/1000 [==========================>...] - ETA: 3:37 - loss: 1.2462 - regression_loss: 1.0146 - classification_loss: 0.2316
 903/1000 [==========================>...] - ETA: 3:35 - loss: 1.2454 - regression_loss: 1.0139 - classification_loss: 0.2315
 904/1000 [==========================>...] - ETA: 3:33 - loss: 1.2452 - regression_loss: 1.0138 - classification_loss: 0.2314
 905/1000 [==========================>...] - ETA: 3:30 - loss: 1.2442 - regression_loss: 1.0130 - classification_loss: 0.2312
 906/1000 [==========================>...] - ETA: 3:28 - loss: 1.2440 - regression_loss: 1.0129 - classification_loss: 0.2311
 907/1000 [==========================>...] - ETA: 3:26 - loss: 1.2439 - regression_loss: 1.0129 - classification_loss: 0.2309
 908/1000 [==========================>...] - ETA: 3:24 - loss: 1.2436 - regression_loss: 1.0128 - classification_loss: 0.2308
 909/1000 [==========================>...] - ETA: 3:22 - loss: 1.2434 - regression_loss: 1.0127 - classification_loss: 0.2307
 910/1000 [==========================>...] - ETA: 3:19 - loss: 1.2432 - regression_loss: 1.0127 - classification_loss: 0.2306
 911/1000 [==========================>...] - ETA: 3:17 - loss: 1.2428 - regression_loss: 1.0124 - classification_loss: 0.2304
 912/1000 [==========================>...] - ETA: 3:15 - loss: 1.2421 - regression_loss: 1.0118 - classification_loss: 0.2303
 913/1000 [==========================>...] - ETA: 3:13 - loss: 1.2420 - regression_loss: 1.0118 - classification_loss: 0.2302
 914/1000 [==========================>...] - ETA: 3:10 - loss: 1.2414 - regression_loss: 1.0114 - classification_loss: 0.2300
 915/1000 [==========================>...] - ETA: 3:08 - loss: 1.2412 - regression_loss: 1.0113 - classification_loss: 0.2299
 916/1000 [==========================>...] - ETA: 3:06 - loss: 1.2409 - regression_loss: 1.0111 - classification_loss: 0.2298
 917/1000 [==========================>...] - ETA: 3:04 - loss: 1.2408 - regression_loss: 1.0111 - classification_loss: 0.2297
 918/1000 [==========================>...] - ETA: 3:02 - loss: 1.2405 - regression_loss: 1.0110 - classification_loss: 0.2295
 919/1000 [==========================>...] - ETA: 2:59 - loss: 1.2404 - regression_loss: 1.0110 - classification_loss: 0.2294
 920/1000 [==========================>...] - ETA: 2:57 - loss: 1.2394 - regression_loss: 1.0102 - classification_loss: 0.2293
 921/1000 [==========================>...] - ETA: 2:55 - loss: 1.2392 - regression_loss: 1.0101 - classification_loss: 0.2291
 922/1000 [==========================>...] - ETA: 2:53 - loss: 1.2390 - regression_loss: 1.0100 - classification_loss: 0.2290
 923/1000 [==========================>...] - ETA: 2:50 - loss: 1.2389 - regression_loss: 1.0100 - classification_loss: 0.2289
 924/1000 [==========================>...] - ETA: 2:48 - loss: 1.2385 - regression_loss: 1.0097 - classification_loss: 0.2288
 925/1000 [==========================>...] - ETA: 2:46 - loss: 1.2383 - regression_loss: 1.0096 - classification_loss: 0.2287
 926/1000 [==========================>...] - ETA: 2:44 - loss: 1.2381 - regression_loss: 1.0096 - classification_loss: 0.2285
 927/1000 [==========================>...] - ETA: 2:42 - loss: 1.2380 - regression_loss: 1.0096 - classification_loss: 0.2284
 928/1000 [==========================>...] - ETA: 2:39 - loss: 1.2370 - regression_loss: 1.0088 - classification_loss: 0.2282
 929/1000 [==========================>...] - ETA: 2:37 - loss: 1.2369 - regression_loss: 1.0088 - classification_loss: 0.2281
 930/1000 [==========================>...] - ETA: 2:35 - loss: 1.2367 - regression_loss: 1.0087 - classification_loss: 0.2280
 931/1000 [==========================>...] - ETA: 2:33 - loss: 1.2364 - regression_loss: 1.0085 - classification_loss: 0.2279
 932/1000 [==========================>...] - ETA: 2:30 - loss: 1.2360 - regression_loss: 1.0082 - classification_loss: 0.2278
 933/1000 [==========================>...] - ETA: 2:28 - loss: 1.2358 - regression_loss: 1.0082 - classification_loss: 0.2277
 934/1000 [===========================>..] - ETA: 2:26 - loss: 1.2358 - regression_loss: 1.0082 - classification_loss: 0.2276
 935/1000 [===========================>..] - ETA: 2:24 - loss: 1.2357 - regression_loss: 1.0082 - classification_loss: 0.2274
 936/1000 [===========================>..] - ETA: 2:22 - loss: 1.2356 - regression_loss: 1.0083 - classification_loss: 0.2273
 937/1000 [===========================>..] - ETA: 2:19 - loss: 1.2347 - regression_loss: 1.0075 - classification_loss: 0.2272
 938/1000 [===========================>..] - ETA: 2:17 - loss: 1.2343 - regression_loss: 1.0072 - classification_loss: 0.2271
 939/1000 [===========================>..] - ETA: 2:15 - loss: 1.2343 - regression_loss: 1.0074 - classification_loss: 0.2270
 940/1000 [===========================>..] - ETA: 2:13 - loss: 1.2338 - regression_loss: 1.0070 - classification_loss: 0.2268
 941/1000 [===========================>..] - ETA: 2:10 - loss: 1.2339 - regression_loss: 1.0071 - classification_loss: 0.2267
 942/1000 [===========================>..] - ETA: 2:08 - loss: 1.2337 - regression_loss: 1.0070 - classification_loss: 0.2266
 943/1000 [===========================>..] - ETA: 2:06 - loss: 1.2336 - regression_loss: 1.0071 - classification_loss: 0.2265
 944/1000 [===========================>..] - ETA: 2:04 - loss: 1.2328 - regression_loss: 1.0065 - classification_loss: 0.2264
 945/1000 [===========================>..] - ETA: 2:02 - loss: 1.2326 - regression_loss: 1.0064 - classification_loss: 0.2262
 946/1000 [===========================>..] - ETA: 1:59 - loss: 1.2326 - regression_loss: 1.0065 - classification_loss: 0.2261
 947/1000 [===========================>..] - ETA: 1:57 - loss: 1.2325 - regression_loss: 1.0065 - classification_loss: 0.2260
 948/1000 [===========================>..] - ETA: 1:55 - loss: 1.2316 - regression_loss: 1.0058 - classification_loss: 0.2258
 949/1000 [===========================>..] - ETA: 1:53 - loss: 1.2315 - regression_loss: 1.0057 - classification_loss: 0.2258
 950/1000 [===========================>..] - ETA: 1:51 - loss: 1.2314 - regression_loss: 1.0057 - classification_loss: 0.2257
 951/1000 [===========================>..] - ETA: 1:48 - loss: 1.2312 - regression_loss: 1.0056 - classification_loss: 0.2256
 952/1000 [===========================>..] - ETA: 1:46 - loss: 1.2310 - regression_loss: 1.0055 - classification_loss: 0.2255
 953/1000 [===========================>..] - ETA: 1:44 - loss: 1.2307 - regression_loss: 1.0054 - classification_loss: 0.2254
 954/1000 [===========================>..] - ETA: 1:42 - loss: 1.2304 - regression_loss: 1.0051 - classification_loss: 0.2253
 955/1000 [===========================>..] - ETA: 1:39 - loss: 1.2295 - regression_loss: 1.0044 - classification_loss: 0.2251
 956/1000 [===========================>..] - ETA: 1:37 - loss: 1.2293 - regression_loss: 1.0043 - classification_loss: 0.2250
 957/1000 [===========================>..] - ETA: 1:35 - loss: 1.2291 - regression_loss: 1.0043 - classification_loss: 0.2249
 958/1000 [===========================>..] - ETA: 1:33 - loss: 1.2290 - regression_loss: 1.0043 - classification_loss: 0.2247
 959/1000 [===========================>..] - ETA: 1:31 - loss: 1.2288 - regression_loss: 1.0042 - classification_loss: 0.2246
 960/1000 [===========================>..] - ETA: 1:28 - loss: 1.2286 - regression_loss: 1.0040 - classification_loss: 0.2245
 961/1000 [===========================>..] - ETA: 1:26 - loss: 1.2284 - regression_loss: 1.0040 - classification_loss: 0.2244
 962/1000 [===========================>..] - ETA: 1:24 - loss: 1.2282 - regression_loss: 1.0040 - classification_loss: 0.2243
 963/1000 [===========================>..] - ETA: 1:22 - loss: 1.2274 - regression_loss: 1.0032 - classification_loss: 0.2241
 964/1000 [===========================>..] - ETA: 1:19 - loss: 1.2269 - regression_loss: 1.0029 - classification_loss: 0.2240
 965/1000 [===========================>..] - ETA: 1:17 - loss: 1.2267 - regression_loss: 1.0028 - classification_loss: 0.2239
 966/1000 [===========================>..] - ETA: 1:15 - loss: 1.2267 - regression_loss: 1.0029 - classification_loss: 0.2238
 967/1000 [============================>.] - ETA: 1:13 - loss: 1.2265 - regression_loss: 1.0028 - classification_loss: 0.2237
 968/1000 [============================>.] - ETA: 1:11 - loss: 1.2263 - regression_loss: 1.0027 - classification_loss: 0.2236
 969/1000 [============================>.] - ETA: 1:08 - loss: 1.2262 - regression_loss: 1.0027 - classification_loss: 0.2235
 970/1000 [============================>.] - ETA: 1:06 - loss: 1.2257 - regression_loss: 1.0023 - classification_loss: 0.2234
 971/1000 [============================>.] - ETA: 1:04 - loss: 1.2255 - regression_loss: 1.0022 - classification_loss: 0.2233
 972/1000 [============================>.] - ETA: 1:02 - loss: 1.2246 - regression_loss: 1.0015 - classification_loss: 0.2231
 973/1000 [============================>.] - ETA: 59s - loss: 1.2245 - regression_loss: 1.0015 - classification_loss: 0.2230 
 974/1000 [============================>.] - ETA: 57s - loss: 1.2243 - regression_loss: 1.0014 - classification_loss: 0.2229
 975/1000 [============================>.] - ETA: 55s - loss: 1.2241 - regression_loss: 1.0013 - classification_loss: 0.2228
 976/1000 [============================>.] - ETA: 53s - loss: 1.2238 - regression_loss: 1.0012 - classification_loss: 0.2227
 977/1000 [============================>.] - ETA: 51s - loss: 1.2236 - regression_loss: 1.0011 - classification_loss: 0.2225
 978/1000 [============================>.] - ETA: 48s - loss: 1.2233 - regression_loss: 1.0009 - classification_loss: 0.2224
 979/1000 [============================>.] - ETA: 46s - loss: 1.2233 - regression_loss: 1.0009 - classification_loss: 0.2223
 980/1000 [============================>.] - ETA: 44s - loss: 1.2225 - regression_loss: 1.0004 - classification_loss: 0.2222
 981/1000 [============================>.] - ETA: 42s - loss: 1.2225 - regression_loss: 1.0005 - classification_loss: 0.2221
 982/1000 [============================>.] - ETA: 39s - loss: 1.2225 - regression_loss: 1.0005 - classification_loss: 0.2220
 983/1000 [============================>.] - ETA: 37s - loss: 1.2216 - regression_loss: 0.9998 - classification_loss: 0.2218
 984/1000 [============================>.] - ETA: 35s - loss: 1.2214 - regression_loss: 0.9997 - classification_loss: 0.2217
 985/1000 [============================>.] - ETA: 33s - loss: 1.2209 - regression_loss: 0.9994 - classification_loss: 0.2216
 986/1000 [============================>.] - ETA: 31s - loss: 1.2207 - regression_loss: 0.9993 - classification_loss: 0.2215
 987/1000 [============================>.] - ETA: 28s - loss: 1.2207 - regression_loss: 0.9993 - classification_loss: 0.2214
 988/1000 [============================>.] - ETA: 26s - loss: 1.2204 - regression_loss: 0.9992 - classification_loss: 0.2213
 989/1000 [============================>.] - ETA: 24s - loss: 1.2203 - regression_loss: 0.9992 - classification_loss: 0.2212
 990/1000 [============================>.] - ETA: 22s - loss: 1.2202 - regression_loss: 0.9991 - classification_loss: 0.2210
 991/1000 [============================>.] - ETA: 19s - loss: 1.2199 - regression_loss: 0.9990 - classification_loss: 0.2209
 992/1000 [============================>.] - ETA: 17s - loss: 1.2197 - regression_loss: 0.9988 - classification_loss: 0.2208
 993/1000 [============================>.] - ETA: 15s - loss: 1.2190 - regression_loss: 0.9983 - classification_loss: 0.2207
 994/1000 [============================>.] - ETA: 13s - loss: 1.2186 - regression_loss: 0.9980 - classification_loss: 0.2206
 995/1000 [============================>.] - ETA: 11s - loss: 1.2185 - regression_loss: 0.9981 - classification_loss: 0.2205
 996/1000 [============================>.] - ETA: 8s - loss: 1.2185 - regression_loss: 0.9981 - classification_loss: 0.2204 
 997/1000 [============================>.] - ETA: 6s - loss: 1.2183 - regression_loss: 0.9980 - classification_loss: 0.2203
 998/1000 [============================>.] - ETA: 4s - loss: 1.2174 - regression_loss: 0.9972 - classification_loss: 0.2202
 999/1000 [============================>.] - ETA: 2s - loss: 1.2173 - regression_loss: 0.9972 - classification_loss: 0.2201
1000/1000 [==============================] - 2221s 2s/step - loss: 1.2173 - regression_loss: 0.9973 - classification_loss: 0.2200

Epoch 00002: saving model to ./snapshots/resnet50_csv_02.h5
Epoch 3/10

   1/1000 [..............................] - ETA: 22:19 - loss: 1.1191 - regression_loss: 0.9887 - classification_loss: 0.1304
   2/1000 [..............................] - ETA: 31:51 - loss: 1.1256 - regression_loss: 0.9913 - classification_loss: 0.1342
   3/1000 [..............................] - ETA: 30:42 - loss: 1.0029 - regression_loss: 0.8729 - classification_loss: 0.1300
   4/1000 [..............................] - ETA: 32:59 - loss: 1.0129 - regression_loss: 0.8728 - classification_loss: 0.1400
   5/1000 [..............................] - ETA: 34:54 - loss: 1.0593 - regression_loss: 0.9190 - classification_loss: 0.1403
   6/1000 [..............................] - ETA: 36:28 - loss: 1.0788 - regression_loss: 0.9415 - classification_loss: 0.1373
   7/1000 [..............................] - ETA: 34:56 - loss: 1.0081 - regression_loss: 0.8813 - classification_loss: 0.1268
   8/1000 [..............................] - ETA: 35:39 - loss: 1.0110 - regression_loss: 0.8866 - classification_loss: 0.1243
   9/1000 [..............................] - ETA: 35:34 - loss: 1.0148 - regression_loss: 0.8902 - classification_loss: 0.1246
  10/1000 [..............................] - ETA: 36:05 - loss: 1.0222 - regression_loss: 0.8994 - classification_loss: 0.1228
  11/1000 [..............................] - ETA: 36:48 - loss: 1.0422 - regression_loss: 0.9199 - classification_loss: 0.1223
  12/1000 [..............................] - ETA: 36:02 - loss: 1.0246 - regression_loss: 0.9032 - classification_loss: 0.1214
  13/1000 [..............................] - ETA: 35:12 - loss: 0.9833 - regression_loss: 0.8659 - classification_loss: 0.1174
  14/1000 [..............................] - ETA: 35:41 - loss: 0.9993 - regression_loss: 0.8823 - classification_loss: 0.1170
  15/1000 [..............................] - ETA: 35:55 - loss: 0.9977 - regression_loss: 0.8805 - classification_loss: 0.1172
  16/1000 [..............................] - ETA: 35:50 - loss: 0.9980 - regression_loss: 0.8810 - classification_loss: 0.1170
  17/1000 [..............................] - ETA: 35:45 - loss: 0.9997 - regression_loss: 0.8832 - classification_loss: 0.1165
  18/1000 [..............................] - ETA: 35:55 - loss: 0.9983 - regression_loss: 0.8816 - classification_loss: 0.1166
  19/1000 [..............................] - ETA: 36:09 - loss: 0.9989 - regression_loss: 0.8828 - classification_loss: 0.1161
  20/1000 [..............................] - ETA: 35:35 - loss: 0.9680 - regression_loss: 0.8539 - classification_loss: 0.1140
  21/1000 [..............................] - ETA: 35:51 - loss: 0.9754 - regression_loss: 0.8614 - classification_loss: 0.1140
  22/1000 [..............................] - ETA: 35:28 - loss: 0.9648 - regression_loss: 0.8510 - classification_loss: 0.1138
  23/1000 [..............................] - ETA: 35:48 - loss: 0.9710 - regression_loss: 0.8571 - classification_loss: 0.1139
  24/1000 [..............................] - ETA: 35:43 - loss: 0.9715 - regression_loss: 0.8578 - classification_loss: 0.1136
  25/1000 [..............................] - ETA: 35:23 - loss: 0.9635 - regression_loss: 0.8502 - classification_loss: 0.1133
  26/1000 [..............................] - ETA: 35:40 - loss: 0.9705 - regression_loss: 0.8570 - classification_loss: 0.1135
  27/1000 [..............................] - ETA: 35:15 - loss: 0.9502 - regression_loss: 0.8386 - classification_loss: 0.1116
  28/1000 [..............................] - ETA: 35:27 - loss: 0.9559 - regression_loss: 0.8440 - classification_loss: 0.1118
  29/1000 [..............................] - ETA: 35:33 - loss: 0.9562 - regression_loss: 0.8441 - classification_loss: 0.1121
  30/1000 [..............................] - ETA: 35:40 - loss: 0.9590 - regression_loss: 0.8470 - classification_loss: 0.1119
  31/1000 [..............................] - ETA: 35:47 - loss: 0.9607 - regression_loss: 0.8490 - classification_loss: 0.1117
  32/1000 [..............................] - ETA: 36:00 - loss: 0.9650 - regression_loss: 0.8532 - classification_loss: 0.1118
  33/1000 [..............................] - ETA: 36:07 - loss: 0.9700 - regression_loss: 0.8581 - classification_loss: 0.1119
  34/1000 [>.............................] - ETA: 35:50 - loss: 0.9634 - regression_loss: 0.8517 - classification_loss: 0.1117
  35/1000 [>.............................] - ETA: 35:30 - loss: 0.9462 - regression_loss: 0.8355 - classification_loss: 0.1107
  36/1000 [>.............................] - ETA: 35:33 - loss: 0.9473 - regression_loss: 0.8361 - classification_loss: 0.1112
  37/1000 [>.............................] - ETA: 35:29 - loss: 0.9484 - regression_loss: 0.8372 - classification_loss: 0.1112
  38/1000 [>.............................] - ETA: 35:14 - loss: 0.9430 - regression_loss: 0.8317 - classification_loss: 0.1112
  39/1000 [>.............................] - ETA: 35:20 - loss: 0.9450 - regression_loss: 0.8338 - classification_loss: 0.1111
  40/1000 [>.............................] - ETA: 35:29 - loss: 0.9497 - regression_loss: 0.8384 - classification_loss: 0.1112
  41/1000 [>.............................] - ETA: 35:36 - loss: 0.9529 - regression_loss: 0.8417 - classification_loss: 0.1112
  42/1000 [>.............................] - ETA: 35:31 - loss: 0.9534 - regression_loss: 0.8423 - classification_loss: 0.1111
  43/1000 [>.............................] - ETA: 35:35 - loss: 0.9548 - regression_loss: 0.8433 - classification_loss: 0.1115
  44/1000 [>.............................] - ETA: 35:18 - loss: 0.9407 - regression_loss: 0.8301 - classification_loss: 0.1106
  45/1000 [>.............................] - ETA: 35:23 - loss: 0.9451 - regression_loss: 0.8345 - classification_loss: 0.1106
  46/1000 [>.............................] - ETA: 35:07 - loss: 0.9320 - regression_loss: 0.8226 - classification_loss: 0.1094
  47/1000 [>.............................] - ETA: 35:15 - loss: 0.9361 - regression_loss: 0.8264 - classification_loss: 0.1097
  48/1000 [>.............................] - ETA: 35:18 - loss: 0.9376 - regression_loss: 0.8279 - classification_loss: 0.1097
  49/1000 [>.............................] - ETA: 35:20 - loss: 0.9378 - regression_loss: 0.8279 - classification_loss: 0.1099
  50/1000 [>.............................] - ETA: 35:15 - loss: 0.9382 - regression_loss: 0.8285 - classification_loss: 0.1098
  51/1000 [>.............................] - ETA: 35:04 - loss: 0.9364 - regression_loss: 0.8266 - classification_loss: 0.1098
  52/1000 [>.............................] - ETA: 35:07 - loss: 0.9395 - regression_loss: 0.8297 - classification_loss: 0.1098
  53/1000 [>.............................] - ETA: 35:11 - loss: 0.9453 - regression_loss: 0.8356 - classification_loss: 0.1098
  54/1000 [>.............................] - ETA: 35:12 - loss: 0.9471 - regression_loss: 0.8372 - classification_loss: 0.1100
  55/1000 [>.............................] - ETA: 35:01 - loss: 0.9438 - regression_loss: 0.8339 - classification_loss: 0.1099
  56/1000 [>.............................] - ETA: 34:57 - loss: 0.9449 - regression_loss: 0.8350 - classification_loss: 0.1099
  57/1000 [>.............................] - ETA: 34:45 - loss: 0.9370 - regression_loss: 0.8279 - classification_loss: 0.1091
  58/1000 [>.............................] - ETA: 34:51 - loss: 0.9403 - regression_loss: 0.8311 - classification_loss: 0.1093
  59/1000 [>.............................] - ETA: 34:38 - loss: 0.9323 - regression_loss: 0.8236 - classification_loss: 0.1086
  60/1000 [>.............................] - ETA: 34:35 - loss: 0.9330 - regression_loss: 0.8243 - classification_loss: 0.1087
  61/1000 [>.............................] - ETA: 34:37 - loss: 0.9342 - regression_loss: 0.8255 - classification_loss: 0.1087
  62/1000 [>.............................] - ETA: 34:38 - loss: 0.9350 - regression_loss: 0.8262 - classification_loss: 0.1088
  63/1000 [>.............................] - ETA: 34:41 - loss: 0.9378 - regression_loss: 0.8290 - classification_loss: 0.1088
  64/1000 [>.............................] - ETA: 34:46 - loss: 0.9404 - regression_loss: 0.8315 - classification_loss: 0.1089
  65/1000 [>.............................] - ETA: 34:36 - loss: 0.9377 - regression_loss: 0.8287 - classification_loss: 0.1090
  66/1000 [>.............................] - ETA: 34:38 - loss: 0.9389 - regression_loss: 0.8299 - classification_loss: 0.1089
  67/1000 [=>............................] - ETA: 34:26 - loss: 0.9310 - regression_loss: 0.8229 - classification_loss: 0.1082
  68/1000 [=>............................] - ETA: 34:31 - loss: 0.9336 - regression_loss: 0.8254 - classification_loss: 0.1082
  69/1000 [=>............................] - ETA: 34:31 - loss: 0.9336 - regression_loss: 0.8252 - classification_loss: 0.1084
  70/1000 [=>............................] - ETA: 34:22 - loss: 0.9308 - regression_loss: 0.8225 - classification_loss: 0.1083
  71/1000 [=>............................] - ETA: 34:19 - loss: 0.9315 - regression_loss: 0.8232 - classification_loss: 0.1082
  72/1000 [=>............................] - ETA: 34:21 - loss: 0.9338 - regression_loss: 0.8255 - classification_loss: 0.1082
  73/1000 [=>............................] - ETA: 34:25 - loss: 0.9360 - regression_loss: 0.8277 - classification_loss: 0.1083
  74/1000 [=>............................] - ETA: 34:27 - loss: 0.9380 - regression_loss: 0.8297 - classification_loss: 0.1083
  75/1000 [=>............................] - ETA: 34:29 - loss: 0.9387 - regression_loss: 0.8305 - classification_loss: 0.1082
  76/1000 [=>............................] - ETA: 34:25 - loss: 0.9389 - regression_loss: 0.8307 - classification_loss: 0.1082
  77/1000 [=>............................] - ETA: 34:15 - loss: 0.9311 - regression_loss: 0.8234 - classification_loss: 0.1076
  78/1000 [=>............................] - ETA: 34:15 - loss: 0.9314 - regression_loss: 0.8236 - classification_loss: 0.1078
  79/1000 [=>............................] - ETA: 34:06 - loss: 0.9303 - regression_loss: 0.8226 - classification_loss: 0.1077
  80/1000 [=>............................] - ETA: 34:07 - loss: 0.9303 - regression_loss: 0.8225 - classification_loss: 0.1078
  81/1000 [=>............................] - ETA: 33:59 - loss: 0.9284 - regression_loss: 0.8207 - classification_loss: 0.1078
  82/1000 [=>............................] - ETA: 33:49 - loss: 0.9217 - regression_loss: 0.8145 - classification_loss: 0.1072
  83/1000 [=>............................] - ETA: 33:51 - loss: 0.9239 - regression_loss: 0.8166 - classification_loss: 0.1073
  84/1000 [=>............................] - ETA: 33:48 - loss: 0.9246 - regression_loss: 0.8173 - classification_loss: 0.1073
  85/1000 [=>............................] - ETA: 33:51 - loss: 0.9265 - regression_loss: 0.8191 - classification_loss: 0.1074
  86/1000 [=>............................] - ETA: 33:51 - loss: 0.9271 - regression_loss: 0.8198 - classification_loss: 0.1073
  87/1000 [=>............................] - ETA: 33:53 - loss: 0.9289 - regression_loss: 0.8215 - classification_loss: 0.1074
  88/1000 [=>............................] - ETA: 33:44 - loss: 0.9222 - regression_loss: 0.8149 - classification_loss: 0.1073
  89/1000 [=>............................] - ETA: 33:45 - loss: 0.9229 - regression_loss: 0.8156 - classification_loss: 0.1073
  90/1000 [=>............................] - ETA: 33:41 - loss: 0.9232 - regression_loss: 0.8160 - classification_loss: 0.1072
  91/1000 [=>............................] - ETA: 33:44 - loss: 0.9250 - regression_loss: 0.8177 - classification_loss: 0.1073
  92/1000 [=>............................] - ETA: 33:37 - loss: 0.9278 - regression_loss: 0.8204 - classification_loss: 0.1074
  93/1000 [=>............................] - ETA: 33:37 - loss: 0.9279 - regression_loss: 0.8204 - classification_loss: 0.1075
  94/1000 [=>............................] - ETA: 33:36 - loss: 0.9280 - regression_loss: 0.8203 - classification_loss: 0.1077
  95/1000 [=>............................] - ETA: 33:29 - loss: 0.9274 - regression_loss: 0.8198 - classification_loss: 0.1077
  96/1000 [=>............................] - ETA: 33:26 - loss: 0.9279 - regression_loss: 0.8202 - classification_loss: 0.1077
  97/1000 [=>............................] - ETA: 33:27 - loss: 0.9294 - regression_loss: 0.8217 - classification_loss: 0.1077
  98/1000 [=>............................] - ETA: 33:30 - loss: 0.9310 - regression_loss: 0.8232 - classification_loss: 0.1077
  99/1000 [=>............................] - ETA: 33:30 - loss: 0.9314 - regression_loss: 0.8237 - classification_loss: 0.1077
 100/1000 [==>...........................] - ETA: 33:22 - loss: 0.9252 - regression_loss: 0.8180 - classification_loss: 0.1072
 101/1000 [==>...........................] - ETA: 33:22 - loss: 0.9258 - regression_loss: 0.8187 - classification_loss: 0.1072
 102/1000 [==>...........................] - ETA: 33:14 - loss: 0.9197 - regression_loss: 0.8130 - classification_loss: 0.1067
 103/1000 [==>...........................] - ETA: 33:15 - loss: 0.9211 - regression_loss: 0.8144 - classification_loss: 0.1067
 104/1000 [==>...........................] - ETA: 33:12 - loss: 0.9215 - regression_loss: 0.8148 - classification_loss: 0.1067
 105/1000 [==>...........................] - ETA: 33:05 - loss: 0.9205 - regression_loss: 0.8138 - classification_loss: 0.1067
 106/1000 [==>...........................] - ETA: 33:07 - loss: 0.9221 - regression_loss: 0.8153 - classification_loss: 0.1068
 107/1000 [==>...........................] - ETA: 33:06 - loss: 0.9220 - regression_loss: 0.8152 - classification_loss: 0.1068
 108/1000 [==>...........................] - ETA: 33:06 - loss: 0.9225 - regression_loss: 0.8157 - classification_loss: 0.1068
 109/1000 [==>...........................] - ETA: 33:06 - loss: 0.9224 - regression_loss: 0.8156 - classification_loss: 0.1069
 110/1000 [==>...........................] - ETA: 33:03 - loss: 0.9227 - regression_loss: 0.8158 - classification_loss: 0.1068
 111/1000 [==>...........................] - ETA: 32:57 - loss: 0.9208 - regression_loss: 0.8140 - classification_loss: 0.1068
 112/1000 [==>...........................] - ETA: 32:58 - loss: 0.9221 - regression_loss: 0.8152 - classification_loss: 0.1068
 113/1000 [==>...........................] - ETA: 32:58 - loss: 0.9235 - regression_loss: 0.8166 - classification_loss: 0.1069
 114/1000 [==>...........................] - ETA: 32:51 - loss: 0.9192 - regression_loss: 0.8126 - classification_loss: 0.1066
 115/1000 [==>...........................] - ETA: 32:51 - loss: 0.9204 - regression_loss: 0.8138 - classification_loss: 0.1066
 116/1000 [==>...........................] - ETA: 32:44 - loss: 0.9156 - regression_loss: 0.8094 - classification_loss: 0.1062
 117/1000 [==>...........................] - ETA: 32:45 - loss: 0.9170 - regression_loss: 0.8107 - classification_loss: 0.1063
 118/1000 [==>...........................] - ETA: 32:44 - loss: 0.9173 - regression_loss: 0.8109 - classification_loss: 0.1064
 119/1000 [==>...........................] - ETA: 32:41 - loss: 0.9177 - regression_loss: 0.8112 - classification_loss: 0.1064
 120/1000 [==>...........................] - ETA: 32:35 - loss: 0.9168 - regression_loss: 0.8104 - classification_loss: 0.1064
 121/1000 [==>...........................] - ETA: 32:35 - loss: 0.9175 - regression_loss: 0.8111 - classification_loss: 0.1064
 122/1000 [==>...........................] - ETA: 32:32 - loss: 0.9180 - regression_loss: 0.8117 - classification_loss: 0.1064
 123/1000 [==>...........................] - ETA: 32:34 - loss: 0.9194 - regression_loss: 0.8129 - classification_loss: 0.1064
 124/1000 [==>...........................] - ETA: 32:33 - loss: 0.9194 - regression_loss: 0.8128 - classification_loss: 0.1066
 125/1000 [==>...........................] - ETA: 32:33 - loss: 0.9211 - regression_loss: 0.8145 - classification_loss: 0.1066
 126/1000 [==>...........................] - ETA: 32:27 - loss: 0.9196 - regression_loss: 0.8131 - classification_loss: 0.1065
 127/1000 [==>...........................] - ETA: 32:27 - loss: 0.9210 - regression_loss: 0.8144 - classification_loss: 0.1066
 128/1000 [==>...........................] - ETA: 32:20 - loss: 0.9177 - regression_loss: 0.8114 - classification_loss: 0.1063
 129/1000 [==>...........................] - ETA: 32:17 - loss: 0.9181 - regression_loss: 0.8117 - classification_loss: 0.1064
 130/1000 [==>...........................] - ETA: 32:12 - loss: 0.9166 - regression_loss: 0.8102 - classification_loss: 0.1064
 131/1000 [==>...........................] - ETA: 32:05 - loss: 0.9121 - regression_loss: 0.8061 - classification_loss: 0.1060
 132/1000 [==>...........................] - ETA: 32:06 - loss: 0.9137 - regression_loss: 0.8076 - classification_loss: 0.1061
 133/1000 [==>...........................] - ETA: 32:05 - loss: 0.9138 - regression_loss: 0.8077 - classification_loss: 0.1061
 134/1000 [===>..........................] - ETA: 32:05 - loss: 0.9150 - regression_loss: 0.8089 - classification_loss: 0.1061
 135/1000 [===>..........................] - ETA: 32:05 - loss: 0.9156 - regression_loss: 0.8096 - classification_loss: 0.1061
 136/1000 [===>..........................] - ETA: 31:59 - loss: 0.9156 - regression_loss: 0.8094 - classification_loss: 0.1061
 137/1000 [===>..........................] - ETA: 32:00 - loss: 0.9166 - regression_loss: 0.8104 - classification_loss: 0.1062
 138/1000 [===>..........................] - ETA: 32:00 - loss: 0.9177 - regression_loss: 0.8116 - classification_loss: 0.1061
 139/1000 [===>..........................] - ETA: 31:57 - loss: 0.9181 - regression_loss: 0.8120 - classification_loss: 0.1061
 140/1000 [===>..........................] - ETA: 31:56 - loss: 0.9184 - regression_loss: 0.8123 - classification_loss: 0.1061
 141/1000 [===>..........................] - ETA: 31:50 - loss: 0.9146 - regression_loss: 0.8089 - classification_loss: 0.1057
 142/1000 [===>..........................] - ETA: 31:49 - loss: 0.9147 - regression_loss: 0.8090 - classification_loss: 0.1058
 143/1000 [===>..........................] - ETA: 31:48 - loss: 0.9160 - regression_loss: 0.8102 - classification_loss: 0.1057
 144/1000 [===>..........................] - ETA: 31:43 - loss: 0.9146 - regression_loss: 0.8089 - classification_loss: 0.1057
 145/1000 [===>..........................] - ETA: 31:37 - loss: 0.9111 - regression_loss: 0.8056 - classification_loss: 0.1055
 146/1000 [===>..........................] - ETA: 31:37 - loss: 0.9116 - regression_loss: 0.8061 - classification_loss: 0.1055
 147/1000 [===>..........................] - ETA: 31:35 - loss: 0.9117 - regression_loss: 0.8062 - classification_loss: 0.1056
 148/1000 [===>..........................] - ETA: 31:36 - loss: 0.9128 - regression_loss: 0.8072 - classification_loss: 0.1056
 149/1000 [===>..........................] - ETA: 31:33 - loss: 0.9131 - regression_loss: 0.8075 - classification_loss: 0.1056
 150/1000 [===>..........................] - ETA: 31:28 - loss: 0.9117 - regression_loss: 0.8062 - classification_loss: 0.1055
 151/1000 [===>..........................] - ETA: 31:25 - loss: 0.9118 - regression_loss: 0.8063 - classification_loss: 0.1055
 152/1000 [===>..........................] - ETA: 31:25 - loss: 0.9121 - regression_loss: 0.8066 - classification_loss: 0.1055
 153/1000 [===>..........................] - ETA: 31:25 - loss: 0.9131 - regression_loss: 0.8076 - classification_loss: 0.1055
 154/1000 [===>..........................] - ETA: 31:19 - loss: 0.9091 - regression_loss: 0.8039 - classification_loss: 0.1052
 155/1000 [===>..........................] - ETA: 31:18 - loss: 0.9091 - regression_loss: 0.8038 - classification_loss: 0.1052
 156/1000 [===>..........................] - ETA: 31:18 - loss: 0.9101 - regression_loss: 0.8049 - classification_loss: 0.1052
 157/1000 [===>..........................] - ETA: 31:16 - loss: 0.9101 - regression_loss: 0.8048 - classification_loss: 0.1053
 158/1000 [===>..........................] - ETA: 31:16 - loss: 0.9104 - regression_loss: 0.8052 - classification_loss: 0.1052
 159/1000 [===>..........................] - ETA: 31:10 - loss: 0.9065 - regression_loss: 0.8016 - classification_loss: 0.1049
 160/1000 [===>..........................] - ETA: 31:05 - loss: 0.9060 - regression_loss: 0.8011 - classification_loss: 0.1049
 161/1000 [===>..........................] - ETA: 31:02 - loss: 0.9071 - regression_loss: 0.8022 - classification_loss: 0.1049
 162/1000 [===>..........................] - ETA: 31:02 - loss: 0.9086 - regression_loss: 0.8036 - classification_loss: 0.1049
 163/1000 [===>..........................] - ETA: 31:02 - loss: 0.9099 - regression_loss: 0.8049 - classification_loss: 0.1050
 164/1000 [===>..........................] - ETA: 31:02 - loss: 0.9111 - regression_loss: 0.8060 - classification_loss: 0.1050
 165/1000 [===>..........................] - ETA: 31:02 - loss: 0.9121 - regression_loss: 0.8071 - classification_loss: 0.1050
 166/1000 [===>..........................] - ETA: 30:57 - loss: 0.9111 - regression_loss: 0.8060 - classification_loss: 0.1050
 167/1000 [====>.........................] - ETA: 30:54 - loss: 0.9115 - regression_loss: 0.8063 - classification_loss: 0.1051
 168/1000 [====>.........................] - ETA: 30:53 - loss: 0.9116 - regression_loss: 0.8063 - classification_loss: 0.1052
 169/1000 [====>.........................] - ETA: 30:47 - loss: 0.9078 - regression_loss: 0.8028 - classification_loss: 0.1050
 170/1000 [====>.........................] - ETA: 30:46 - loss: 0.9083 - regression_loss: 0.8033 - classification_loss: 0.1050
 171/1000 [====>.........................] - ETA: 30:44 - loss: 0.9085 - regression_loss: 0.8036 - classification_loss: 0.1050
 172/1000 [====>.........................] - ETA: 30:38 - loss: 0.9064 - regression_loss: 0.8016 - classification_loss: 0.1048
 173/1000 [====>.........................] - ETA: 30:37 - loss: 0.9068 - regression_loss: 0.8020 - classification_loss: 0.1048
 174/1000 [====>.........................] - ETA: 30:37 - loss: 0.9077 - regression_loss: 0.8029 - classification_loss: 0.1048
 175/1000 [====>.........................] - ETA: 30:36 - loss: 0.9076 - regression_loss: 0.8028 - classification_loss: 0.1048
 176/1000 [====>.........................] - ETA: 30:35 - loss: 0.9084 - regression_loss: 0.8036 - classification_loss: 0.1048
 177/1000 [====>.........................] - ETA: 30:30 - loss: 0.9072 - regression_loss: 0.8025 - classification_loss: 0.1048
 178/1000 [====>.........................] - ETA: 30:30 - loss: 0.9081 - regression_loss: 0.8034 - classification_loss: 0.1048
 179/1000 [====>.........................] - ETA: 30:25 - loss: 0.9068 - regression_loss: 0.8021 - classification_loss: 0.1047
 180/1000 [====>.........................] - ETA: 30:20 - loss: 0.9033 - regression_loss: 0.7988 - classification_loss: 0.1044
 181/1000 [====>.........................] - ETA: 30:20 - loss: 0.9042 - regression_loss: 0.7998 - classification_loss: 0.1045
 182/1000 [====>.........................] - ETA: 30:19 - loss: 0.9042 - regression_loss: 0.7997 - classification_loss: 0.1045
 183/1000 [====>.........................] - ETA: 30:18 - loss: 0.9044 - regression_loss: 0.8000 - classification_loss: 0.1045
 184/1000 [====>.........................] - ETA: 30:15 - loss: 0.9048 - regression_loss: 0.8003 - classification_loss: 0.1044
 185/1000 [====>.........................] - ETA: 30:10 - loss: 0.9020 - regression_loss: 0.7979 - classification_loss: 0.1041
 186/1000 [====>.........................] - ETA: 30:09 - loss: 0.9028 - regression_loss: 0.7986 - classification_loss: 0.1042
 187/1000 [====>.........................] - ETA: 30:09 - loss: 0.9036 - regression_loss: 0.7994 - classification_loss: 0.1042
 188/1000 [====>.........................] - ETA: 30:08 - loss: 0.9038 - regression_loss: 0.7997 - classification_loss: 0.1042
 189/1000 [====>.........................] - ETA: 30:07 - loss: 0.9038 - regression_loss: 0.7996 - classification_loss: 0.1042
 190/1000 [====>.........................] - ETA: 30:04 - loss: 0.9039 - regression_loss: 0.7997 - classification_loss: 0.1042
 191/1000 [====>.........................] - ETA: 30:00 - loss: 0.9036 - regression_loss: 0.7994 - classification_loss: 0.1042
 192/1000 [====>.........................] - ETA: 29:59 - loss: 0.9039 - regression_loss: 0.7998 - classification_loss: 0.1041
 193/1000 [====>.........................] - ETA: 29:57 - loss: 0.9038 - regression_loss: 0.7996 - classification_loss: 0.1042
 194/1000 [====>.........................] - ETA: 29:53 - loss: 0.9033 - regression_loss: 0.7991 - classification_loss: 0.1042
 195/1000 [====>.........................] - ETA: 29:48 - loss: 0.9002 - regression_loss: 0.7963 - classification_loss: 0.1039
 196/1000 [====>.........................] - ETA: 29:45 - loss: 0.9004 - regression_loss: 0.7965 - classification_loss: 0.1039
 197/1000 [====>.........................] - ETA: 29:45 - loss: 0.9012 - regression_loss: 0.7973 - classification_loss: 0.1039
 198/1000 [====>.........................] - ETA: 29:44 - loss: 0.9020 - regression_loss: 0.7981 - classification_loss: 0.1039
 199/1000 [====>.........................] - ETA: 29:39 - loss: 0.8989 - regression_loss: 0.7952 - classification_loss: 0.1037
 200/1000 [=====>........................] - ETA: 29:38 - loss: 0.8994 - regression_loss: 0.7957 - classification_loss: 0.1037
 201/1000 [=====>........................] - ETA: 29:36 - loss: 0.8996 - regression_loss: 0.7960 - classification_loss: 0.1036
 202/1000 [=====>........................] - ETA: 29:34 - loss: 0.8996 - regression_loss: 0.7959 - classification_loss: 0.1037
 203/1000 [=====>........................] - ETA: 29:30 - loss: 0.9002 - regression_loss: 0.7966 - classification_loss: 0.1037
 204/1000 [=====>........................] - ETA: 29:29 - loss: 0.9009 - regression_loss: 0.7973 - classification_loss: 0.1037
 205/1000 [=====>........................] - ETA: 29:29 - loss: 0.9016 - regression_loss: 0.7980 - classification_loss: 0.1037
 206/1000 [=====>........................] - ETA: 29:25 - loss: 0.9010 - regression_loss: 0.7974 - classification_loss: 0.1037
 207/1000 [=====>........................] - ETA: 29:22 - loss: 0.9012 - regression_loss: 0.7976 - classification_loss: 0.1036
 208/1000 [=====>........................] - ETA: 29:21 - loss: 0.9022 - regression_loss: 0.7986 - classification_loss: 0.1036
 209/1000 [=====>........................] - ETA: 29:20 - loss: 0.9022 - regression_loss: 0.7986 - classification_loss: 0.1037
 210/1000 [=====>........................] - ETA: 29:19 - loss: 0.9030 - regression_loss: 0.7993 - classification_loss: 0.1037
 211/1000 [=====>........................] - ETA: 29:18 - loss: 0.9033 - regression_loss: 0.7997 - classification_loss: 0.1037
 212/1000 [=====>........................] - ETA: 29:14 - loss: 0.9006 - regression_loss: 0.7972 - classification_loss: 0.1034
 213/1000 [=====>........................] - ETA: 29:10 - loss: 0.8999 - regression_loss: 0.7965 - classification_loss: 0.1034
 214/1000 [=====>........................] - ETA: 29:09 - loss: 0.9004 - regression_loss: 0.7971 - classification_loss: 0.1034
 215/1000 [=====>........................] - ETA: 29:05 - loss: 0.8977 - regression_loss: 0.7945 - classification_loss: 0.1032
 216/1000 [=====>........................] - ETA: 29:04 - loss: 0.8978 - regression_loss: 0.7946 - classification_loss: 0.1032
 217/1000 [=====>........................] - ETA: 29:02 - loss: 0.8980 - regression_loss: 0.7947 - classification_loss: 0.1033
 218/1000 [=====>........................] - ETA: 29:01 - loss: 0.8988 - regression_loss: 0.7955 - classification_loss: 0.1033
 219/1000 [=====>........................] - ETA: 29:00 - loss: 0.8997 - regression_loss: 0.7964 - classification_loss: 0.1033
 220/1000 [=====>........................] - ETA: 28:56 - loss: 0.8992 - regression_loss: 0.7960 - classification_loss: 0.1033
 221/1000 [=====>........................] - ETA: 28:54 - loss: 0.8994 - regression_loss: 0.7961 - classification_loss: 0.1033
 222/1000 [=====>........................] - ETA: 28:53 - loss: 0.9000 - regression_loss: 0.7968 - classification_loss: 0.1033
 223/1000 [=====>........................] - ETA: 28:48 - loss: 0.8976 - regression_loss: 0.7945 - classification_loss: 0.1031
 224/1000 [=====>........................] - ETA: 28:47 - loss: 0.8979 - regression_loss: 0.7948 - classification_loss: 0.1031
 225/1000 [=====>........................] - ETA: 28:46 - loss: 0.8987 - regression_loss: 0.7956 - classification_loss: 0.1031
 226/1000 [=====>........................] - ETA: 28:45 - loss: 0.8987 - regression_loss: 0.7955 - classification_loss: 0.1031
 227/1000 [=====>........................] - ETA: 28:43 - loss: 0.8990 - regression_loss: 0.7958 - classification_loss: 0.1031
 228/1000 [=====>........................] - ETA: 28:42 - loss: 0.8996 - regression_loss: 0.7965 - classification_loss: 0.1031
 229/1000 [=====>........................] - ETA: 28:41 - loss: 0.8995 - regression_loss: 0.7964 - classification_loss: 0.1032
 230/1000 [=====>........................] - ETA: 28:38 - loss: 0.8996 - regression_loss: 0.7965 - classification_loss: 0.1031
 231/1000 [=====>........................] - ETA: 28:35 - loss: 0.8993 - regression_loss: 0.7961 - classification_loss: 0.1031
 232/1000 [=====>........................] - ETA: 28:34 - loss: 0.8998 - regression_loss: 0.7967 - classification_loss: 0.1031
 233/1000 [=====>........................] - ETA: 28:29 - loss: 0.8971 - regression_loss: 0.7942 - classification_loss: 0.1029
 234/1000 [======>.......................] - ETA: 28:25 - loss: 0.8944 - regression_loss: 0.7917 - classification_loss: 0.1027
 235/1000 [======>.......................] - ETA: 28:24 - loss: 0.8951 - regression_loss: 0.7924 - classification_loss: 0.1027
 236/1000 [======>.......................] - ETA: 28:23 - loss: 0.8954 - regression_loss: 0.7927 - classification_loss: 0.1027
 237/1000 [======>.......................] - ETA: 28:19 - loss: 0.8945 - regression_loss: 0.7918 - classification_loss: 0.1028
 238/1000 [======>.......................] - ETA: 28:18 - loss: 0.8952 - regression_loss: 0.7924 - classification_loss: 0.1028
 239/1000 [======>.......................] - ETA: 28:16 - loss: 0.8952 - regression_loss: 0.7924 - classification_loss: 0.1028
 240/1000 [======>.......................] - ETA: 28:14 - loss: 0.8953 - regression_loss: 0.7925 - classification_loss: 0.1028
 241/1000 [======>.......................] - ETA: 28:13 - loss: 0.8959 - regression_loss: 0.7930 - classification_loss: 0.1028
 242/1000 [======>.......................] - ETA: 28:12 - loss: 0.8965 - regression_loss: 0.7936 - classification_loss: 0.1028
 243/1000 [======>.......................] - ETA: 28:10 - loss: 0.8966 - regression_loss: 0.7937 - classification_loss: 0.1028
 244/1000 [======>.......................] - ETA: 28:06 - loss: 0.8964 - regression_loss: 0.7934 - classification_loss: 0.1030
 245/1000 [======>.......................] - ETA: 28:04 - loss: 0.8963 - regression_loss: 0.7933 - classification_loss: 0.1030
 246/1000 [======>.......................] - ETA: 28:00 - loss: 0.8941 - regression_loss: 0.7912 - classification_loss: 0.1028
 247/1000 [======>.......................] - ETA: 27:59 - loss: 0.8944 - regression_loss: 0.7916 - classification_loss: 0.1028
 248/1000 [======>.......................] - ETA: 27:57 - loss: 0.8947 - regression_loss: 0.7918 - classification_loss: 0.1028
 249/1000 [======>.......................] - ETA: 27:53 - loss: 0.8938 - regression_loss: 0.7910 - classification_loss: 0.1028
 250/1000 [======>.......................] - ETA: 27:52 - loss: 0.8945 - regression_loss: 0.7917 - classification_loss: 0.1028
 251/1000 [======>.......................] - ETA: 27:51 - loss: 0.8949 - regression_loss: 0.7920 - classification_loss: 0.1029
 252/1000 [======>.......................] - ETA: 27:46 - loss: 0.8931 - regression_loss: 0.7904 - classification_loss: 0.1027
 253/1000 [======>.......................] - ETA: 27:44 - loss: 0.8934 - regression_loss: 0.7907 - classification_loss: 0.1027
 254/1000 [======>.......................] - ETA: 27:43 - loss: 0.8939 - regression_loss: 0.7912 - classification_loss: 0.1027
 255/1000 [======>.......................] - ETA: 27:42 - loss: 0.8946 - regression_loss: 0.7919 - classification_loss: 0.1027
 256/1000 [======>.......................] - ETA: 27:40 - loss: 0.8948 - regression_loss: 0.7922 - classification_loss: 0.1027
 257/1000 [======>.......................] - ETA: 27:37 - loss: 0.8940 - regression_loss: 0.7914 - classification_loss: 0.1027
 258/1000 [======>.......................] - ETA: 27:34 - loss: 0.8941 - regression_loss: 0.7915 - classification_loss: 0.1026
 259/1000 [======>.......................] - ETA: 27:30 - loss: 0.8920 - regression_loss: 0.7896 - classification_loss: 0.1024
 260/1000 [======>.......................] - ETA: 27:29 - loss: 0.8928 - regression_loss: 0.7904 - classification_loss: 0.1025
 261/1000 [======>.......................] - ETA: 27:27 - loss: 0.8929 - regression_loss: 0.7904 - classification_loss: 0.1025
 262/1000 [======>.......................] - ETA: 27:26 - loss: 0.8936 - regression_loss: 0.7911 - classification_loss: 0.1025
 263/1000 [======>.......................] - ETA: 27:23 - loss: 0.8927 - regression_loss: 0.7903 - classification_loss: 0.1024
 264/1000 [======>.......................] - ETA: 27:21 - loss: 0.8929 - regression_loss: 0.7905 - classification_loss: 0.1024
 265/1000 [======>.......................] - ETA: 27:17 - loss: 0.8906 - regression_loss: 0.7884 - classification_loss: 0.1022
 266/1000 [======>.......................] - ETA: 27:15 - loss: 0.8906 - regression_loss: 0.7884 - classification_loss: 0.1022
 267/1000 [=======>......................] - ETA: 27:13 - loss: 0.8906 - regression_loss: 0.7884 - classification_loss: 0.1022
 268/1000 [=======>......................] - ETA: 27:12 - loss: 0.8912 - regression_loss: 0.7890 - classification_loss: 0.1022
 269/1000 [=======>......................] - ETA: 27:08 - loss: 0.8907 - regression_loss: 0.7885 - classification_loss: 0.1022
 270/1000 [=======>......................] - ETA: 27:07 - loss: 0.8912 - regression_loss: 0.7891 - classification_loss: 0.1022
 271/1000 [=======>......................] - ETA: 27:05 - loss: 0.8911 - regression_loss: 0.7890 - classification_loss: 0.1022
 272/1000 [=======>......................] - ETA: 27:03 - loss: 0.8913 - regression_loss: 0.7891 - classification_loss: 0.1021
 273/1000 [=======>......................] - ETA: 26:59 - loss: 0.8891 - regression_loss: 0.7871 - classification_loss: 0.1020
 274/1000 [=======>......................] - ETA: 26:57 - loss: 0.8895 - regression_loss: 0.7876 - classification_loss: 0.1019
 275/1000 [=======>......................] - ETA: 26:56 - loss: 0.8907 - regression_loss: 0.7887 - classification_loss: 0.1020
 276/1000 [=======>......................] - ETA: 26:52 - loss: 0.8905 - regression_loss: 0.7885 - classification_loss: 0.1019
 277/1000 [=======>......................] - ETA: 26:51 - loss: 0.8912 - regression_loss: 0.7893 - classification_loss: 0.1019
 278/1000 [=======>......................] - ETA: 26:47 - loss: 0.8891 - regression_loss: 0.7873 - classification_loss: 0.1018
 279/1000 [=======>......................] - ETA: 26:45 - loss: 0.8894 - regression_loss: 0.7876 - classification_loss: 0.1018
 280/1000 [=======>......................] - ETA: 26:43 - loss: 0.8894 - regression_loss: 0.7876 - classification_loss: 0.1018
 281/1000 [=======>......................] - ETA: 26:42 - loss: 0.8896 - regression_loss: 0.7878 - classification_loss: 0.1018
 282/1000 [=======>......................] - ETA: 26:41 - loss: 0.8901 - regression_loss: 0.7884 - classification_loss: 0.1018
 283/1000 [=======>......................] - ETA: 26:40 - loss: 0.8907 - regression_loss: 0.7889 - classification_loss: 0.1018
 284/1000 [=======>......................] - ETA: 26:36 - loss: 0.8899 - regression_loss: 0.7882 - classification_loss: 0.1017
 285/1000 [=======>......................] - ETA: 26:35 - loss: 0.8903 - regression_loss: 0.7886 - classification_loss: 0.1017
 286/1000 [=======>......................] - ETA: 26:32 - loss: 0.8904 - regression_loss: 0.7887 - classification_loss: 0.1017
 287/1000 [=======>......................] - ETA: 26:31 - loss: 0.8909 - regression_loss: 0.7892 - classification_loss: 0.1017
 288/1000 [=======>......................] - ETA: 26:29 - loss: 0.8908 - regression_loss: 0.7892 - classification_loss: 0.1017
 289/1000 [=======>......................] - ETA: 26:26 - loss: 0.8888 - regression_loss: 0.7873 - classification_loss: 0.1015
 290/1000 [=======>......................] - ETA: 26:25 - loss: 0.8896 - regression_loss: 0.7880 - classification_loss: 0.1015
 291/1000 [=======>......................] - ETA: 26:23 - loss: 0.8896 - regression_loss: 0.7881 - classification_loss: 0.1015
 292/1000 [=======>......................] - ETA: 26:21 - loss: 0.8901 - regression_loss: 0.7885 - classification_loss: 0.1015
 293/1000 [=======>......................] - ETA: 26:18 - loss: 0.8902 - regression_loss: 0.7886 - classification_loss: 0.1016
 294/1000 [=======>......................] - ETA: 26:14 - loss: 0.8886 - regression_loss: 0.7872 - classification_loss: 0.1014
 295/1000 [=======>......................] - ETA: 26:13 - loss: 0.8894 - regression_loss: 0.7880 - classification_loss: 0.1014
 296/1000 [=======>......................] - ETA: 26:10 - loss: 0.8895 - regression_loss: 0.7881 - classification_loss: 0.1014
 297/1000 [=======>......................] - ETA: 26:08 - loss: 0.8896 - regression_loss: 0.7882 - classification_loss: 0.1014
 298/1000 [=======>......................] - ETA: 26:06 - loss: 0.8896 - regression_loss: 0.7882 - classification_loss: 0.1014
 299/1000 [=======>......................] - ETA: 26:05 - loss: 0.8899 - regression_loss: 0.7885 - classification_loss: 0.1014
 300/1000 [========>.....................] - ETA: 26:01 - loss: 0.8894 - regression_loss: 0.7880 - classification_loss: 0.1014
 301/1000 [========>.....................] - ETA: 26:00 - loss: 0.8901 - regression_loss: 0.7888 - classification_loss: 0.1014
 302/1000 [========>.....................] - ETA: 25:56 - loss: 0.8885 - regression_loss: 0.7873 - classification_loss: 0.1012
 303/1000 [========>.....................] - ETA: 25:55 - loss: 0.8890 - regression_loss: 0.7878 - classification_loss: 0.1012
 304/1000 [========>.....................] - ETA: 25:51 - loss: 0.8884 - regression_loss: 0.7871 - classification_loss: 0.1012
 305/1000 [========>.....................] - ETA: 25:50 - loss: 0.8883 - regression_loss: 0.7871 - classification_loss: 0.1012
 306/1000 [========>.....................] - ETA: 25:48 - loss: 0.8888 - regression_loss: 0.7876 - classification_loss: 0.1012
 307/1000 [========>.....................] - ETA: 25:45 - loss: 0.8871 - regression_loss: 0.7860 - classification_loss: 0.1011
 308/1000 [========>.....................] - ETA: 25:42 - loss: 0.8874 - regression_loss: 0.7863 - classification_loss: 0.1011
 309/1000 [========>.....................] - ETA: 25:41 - loss: 0.8876 - regression_loss: 0.7866 - classification_loss: 0.1010
 310/1000 [========>.....................] - ETA: 25:39 - loss: 0.8881 - regression_loss: 0.7870 - classification_loss: 0.1010
 311/1000 [========>.....................] - ETA: 25:38 - loss: 0.8885 - regression_loss: 0.7875 - classification_loss: 0.1010
 312/1000 [========>.....................] - ETA: 25:36 - loss: 0.8885 - regression_loss: 0.7874 - classification_loss: 0.1010
 313/1000 [========>.....................] - ETA: 25:34 - loss: 0.8885 - regression_loss: 0.7875 - classification_loss: 0.1010
 314/1000 [========>.....................] - ETA: 25:30 - loss: 0.8866 - regression_loss: 0.7858 - classification_loss: 0.1008
 315/1000 [========>.....................] - ETA: 25:27 - loss: 0.8860 - regression_loss: 0.7852 - classification_loss: 0.1009
 316/1000 [========>.....................] - ETA: 25:25 - loss: 0.8866 - regression_loss: 0.7857 - classification_loss: 0.1009
 317/1000 [========>.....................] - ETA: 25:24 - loss: 0.8868 - regression_loss: 0.7859 - classification_loss: 0.1008
 318/1000 [========>.....................] - ETA: 25:22 - loss: 0.8867 - regression_loss: 0.7858 - classification_loss: 0.1009
 319/1000 [========>.....................] - ETA: 25:19 - loss: 0.8862 - regression_loss: 0.7854 - classification_loss: 0.1008
 320/1000 [========>.....................] - ETA: 25:17 - loss: 0.8866 - regression_loss: 0.7858 - classification_loss: 0.1008
 321/1000 [========>.....................] - ETA: 25:14 - loss: 0.8848 - regression_loss: 0.7841 - classification_loss: 0.1007
 322/1000 [========>.....................] - ETA: 25:11 - loss: 0.8849 - regression_loss: 0.7842 - classification_loss: 0.1007
 323/1000 [========>.....................] - ETA: 25:09 - loss: 0.8850 - regression_loss: 0.7844 - classification_loss: 0.1006
 324/1000 [========>.....................] - ETA: 25:08 - loss: 0.8855 - regression_loss: 0.7848 - classification_loss: 0.1006
 325/1000 [========>.....................] - ETA: 25:05 - loss: 0.8835 - regression_loss: 0.7830 - classification_loss: 0.1005
 326/1000 [========>.....................] - ETA: 25:03 - loss: 0.8835 - regression_loss: 0.7830 - classification_loss: 0.1005
 327/1000 [========>.....................] - ETA: 25:00 - loss: 0.8835 - regression_loss: 0.7830 - classification_loss: 0.1005
 328/1000 [========>.....................] - ETA: 24:57 - loss: 0.8828 - regression_loss: 0.7824 - classification_loss: 0.1005
 329/1000 [========>.....................] - ETA: 24:56 - loss: 0.8833 - regression_loss: 0.7828 - classification_loss: 0.1005
 330/1000 [========>.....................] - ETA: 24:54 - loss: 0.8837 - regression_loss: 0.7832 - classification_loss: 0.1005
 331/1000 [========>.....................] - ETA: 24:53 - loss: 0.8838 - regression_loss: 0.7833 - classification_loss: 0.1005
 332/1000 [========>.....................] - ETA: 24:50 - loss: 0.8838 - regression_loss: 0.7834 - classification_loss: 0.1004
 333/1000 [========>.....................] - ETA: 24:47 - loss: 0.8819 - regression_loss: 0.7816 - classification_loss: 0.1003
 334/1000 [=========>....................] - ETA: 24:44 - loss: 0.8814 - regression_loss: 0.7811 - classification_loss: 0.1003
 335/1000 [=========>....................] - ETA: 24:42 - loss: 0.8821 - regression_loss: 0.7817 - classification_loss: 0.1004
 336/1000 [=========>....................] - ETA: 24:41 - loss: 0.8827 - regression_loss: 0.7823 - classification_loss: 0.1004
 337/1000 [=========>....................] - ETA: 24:39 - loss: 0.8829 - regression_loss: 0.7825 - classification_loss: 0.1004
 338/1000 [=========>....................] - ETA: 24:37 - loss: 0.8828 - regression_loss: 0.7824 - classification_loss: 0.1004
 339/1000 [=========>....................] - ETA: 24:35 - loss: 0.8827 - regression_loss: 0.7823 - classification_loss: 0.1004
 340/1000 [=========>....................] - ETA: 24:34 - loss: 0.8832 - regression_loss: 0.7828 - classification_loss: 0.1004
 341/1000 [=========>....................] - ETA: 24:32 - loss: 0.8835 - regression_loss: 0.7831 - classification_loss: 0.1004
 342/1000 [=========>....................] - ETA: 24:30 - loss: 0.8836 - regression_loss: 0.7832 - classification_loss: 0.1004
 343/1000 [=========>....................] - ETA: 24:26 - loss: 0.8820 - regression_loss: 0.7818 - classification_loss: 0.1003
 344/1000 [=========>....................] - ETA: 24:25 - loss: 0.8822 - regression_loss: 0.7820 - classification_loss: 0.1002
 345/1000 [=========>....................] - ETA: 24:22 - loss: 0.8816 - regression_loss: 0.7813 - classification_loss: 0.1002
 346/1000 [=========>....................] - ETA: 24:20 - loss: 0.8815 - regression_loss: 0.7813 - classification_loss: 0.1002
 347/1000 [=========>....................] - ETA: 24:18 - loss: 0.8817 - regression_loss: 0.7815 - classification_loss: 0.1002
 348/1000 [=========>....................] - ETA: 24:15 - loss: 0.8810 - regression_loss: 0.7809 - classification_loss: 0.1002
 349/1000 [=========>....................] - ETA: 24:13 - loss: 0.8815 - regression_loss: 0.7813 - classification_loss: 0.1002
 350/1000 [=========>....................] - ETA: 24:11 - loss: 0.8814 - regression_loss: 0.7813 - classification_loss: 0.1002
 351/1000 [=========>....................] - ETA: 24:09 - loss: 0.8818 - regression_loss: 0.7817 - classification_loss: 0.1001
 352/1000 [=========>....................] - ETA: 24:06 - loss: 0.8805 - regression_loss: 0.7804 - classification_loss: 0.1000
 353/1000 [=========>....................] - ETA: 24:02 - loss: 0.8790 - regression_loss: 0.7790 - classification_loss: 0.0999
 354/1000 [=========>....................] - ETA: 24:01 - loss: 0.8792 - regression_loss: 0.7793 - classification_loss: 0.0999
 355/1000 [=========>....................] - ETA: 23:58 - loss: 0.8789 - regression_loss: 0.7790 - classification_loss: 0.0999
 356/1000 [=========>....................] - ETA: 23:56 - loss: 0.8794 - regression_loss: 0.7794 - classification_loss: 0.0999
 357/1000 [=========>....................] - ETA: 23:55 - loss: 0.8798 - regression_loss: 0.7798 - classification_loss: 0.0999
 358/1000 [=========>....................] - ETA: 23:52 - loss: 0.8798 - regression_loss: 0.7799 - classification_loss: 0.0999
 359/1000 [=========>....................] - ETA: 23:50 - loss: 0.8798 - regression_loss: 0.7798 - classification_loss: 0.0999
 360/1000 [=========>....................] - ETA: 23:49 - loss: 0.8802 - regression_loss: 0.7802 - classification_loss: 0.1000
 361/1000 [=========>....................] - ETA: 23:46 - loss: 0.8796 - regression_loss: 0.7796 - classification_loss: 0.1000
 362/1000 [=========>....................] - ETA: 23:44 - loss: 0.8797 - regression_loss: 0.7797 - classification_loss: 0.0999
 363/1000 [=========>....................] - ETA: 23:41 - loss: 0.8782 - regression_loss: 0.7784 - classification_loss: 0.0998
 364/1000 [=========>....................] - ETA: 23:38 - loss: 0.8783 - regression_loss: 0.7785 - classification_loss: 0.0998
 365/1000 [=========>....................] - ETA: 23:37 - loss: 0.8787 - regression_loss: 0.7789 - classification_loss: 0.0998
 366/1000 [=========>....................] - ETA: 23:35 - loss: 0.8786 - regression_loss: 0.7788 - classification_loss: 0.0998
 367/1000 [==========>...................] - ETA: 23:33 - loss: 0.8786 - regression_loss: 0.7787 - classification_loss: 0.0999
 368/1000 [==========>...................] - ETA: 23:30 - loss: 0.8781 - regression_loss: 0.7783 - classification_loss: 0.0999
 369/1000 [==========>...................] - ETA: 23:28 - loss: 0.8782 - regression_loss: 0.7784 - classification_loss: 0.0999
 370/1000 [==========>...................] - ETA: 23:26 - loss: 0.8784 - regression_loss: 0.7784 - classification_loss: 0.0999
 371/1000 [==========>...................] - ETA: 23:22 - loss: 0.8769 - regression_loss: 0.7771 - classification_loss: 0.0998
 372/1000 [==========>...................] - ETA: 23:21 - loss: 0.8772 - regression_loss: 0.7774 - classification_loss: 0.0998
 373/1000 [==========>...................] - ETA: 23:19 - loss: 0.8776 - regression_loss: 0.7778 - classification_loss: 0.0998
 374/1000 [==========>...................] - ETA: 23:16 - loss: 0.8770 - regression_loss: 0.7772 - classification_loss: 0.0998
 375/1000 [==========>...................] - ETA: 23:14 - loss: 0.8772 - regression_loss: 0.7774 - classification_loss: 0.0998
 376/1000 [==========>...................] - ETA: 23:12 - loss: 0.8774 - regression_loss: 0.7776 - classification_loss: 0.0998
 377/1000 [==========>...................] - ETA: 23:10 - loss: 0.8773 - regression_loss: 0.7774 - classification_loss: 0.0999
 378/1000 [==========>...................] - ETA: 23:08 - loss: 0.8777 - regression_loss: 0.7778 - classification_loss: 0.0999
 379/1000 [==========>...................] - ETA: 23:07 - loss: 0.8780 - regression_loss: 0.7781 - classification_loss: 0.0999
 380/1000 [==========>...................] - ETA: 23:04 - loss: 0.8765 - regression_loss: 0.7767 - classification_loss: 0.0998
 381/1000 [==========>...................] - ETA: 23:02 - loss: 0.8768 - regression_loss: 0.7770 - classification_loss: 0.0998
 382/1000 [==========>...................] - ETA: 23:00 - loss: 0.8772 - regression_loss: 0.7774 - classification_loss: 0.0998
 383/1000 [==========>...................] - ETA: 22:57 - loss: 0.8767 - regression_loss: 0.7768 - classification_loss: 0.0998
 384/1000 [==========>...................] - ETA: 22:54 - loss: 0.8750 - regression_loss: 0.7753 - classification_loss: 0.0997
 385/1000 [==========>...................] - ETA: 22:52 - loss: 0.8749 - regression_loss: 0.7752 - classification_loss: 0.0997
 386/1000 [==========>...................] - ETA: 22:50 - loss: 0.8750 - regression_loss: 0.7753 - classification_loss: 0.0997
 387/1000 [==========>...................] - ETA: 22:48 - loss: 0.8750 - regression_loss: 0.7753 - classification_loss: 0.0996
 388/1000 [==========>...................] - ETA: 22:46 - loss: 0.8749 - regression_loss: 0.7753 - classification_loss: 0.0996
 389/1000 [==========>...................] - ETA: 22:43 - loss: 0.8732 - regression_loss: 0.7737 - classification_loss: 0.0995
 390/1000 [==========>...................] - ETA: 22:41 - loss: 0.8736 - regression_loss: 0.7741 - classification_loss: 0.0995
 391/1000 [==========>...................] - ETA: 22:39 - loss: 0.8736 - regression_loss: 0.7741 - classification_loss: 0.0995
 392/1000 [==========>...................] - ETA: 22:37 - loss: 0.8737 - regression_loss: 0.7742 - classification_loss: 0.0994
 393/1000 [==========>...................] - ETA: 22:36 - loss: 0.8740 - regression_loss: 0.7746 - classification_loss: 0.0994
 394/1000 [==========>...................] - ETA: 22:33 - loss: 0.8735 - regression_loss: 0.7740 - classification_loss: 0.0994
 395/1000 [==========>...................] - ETA: 22:29 - loss: 0.8719 - regression_loss: 0.7727 - classification_loss: 0.0993
 396/1000 [==========>...................] - ETA: 22:28 - loss: 0.8721 - regression_loss: 0.7728 - classification_loss: 0.0993
 397/1000 [==========>...................] - ETA: 22:26 - loss: 0.8724 - regression_loss: 0.7731 - classification_loss: 0.0993
 398/1000 [==========>...................] - ETA: 22:24 - loss: 0.8727 - regression_loss: 0.7735 - classification_loss: 0.0993
 399/1000 [==========>...................] - ETA: 22:23 - loss: 0.8726 - regression_loss: 0.7734 - classification_loss: 0.0993
 400/1000 [===========>..................] - ETA: 22:20 - loss: 0.8726 - regression_loss: 0.7734 - classification_loss: 0.0992
 401/1000 [===========>..................] - ETA: 22:17 - loss: 0.8723 - regression_loss: 0.7731 - classification_loss: 0.0992
 402/1000 [===========>..................] - ETA: 22:16 - loss: 0.8726 - regression_loss: 0.7734 - classification_loss: 0.0992
 403/1000 [===========>..................] - ETA: 22:14 - loss: 0.8727 - regression_loss: 0.7735 - classification_loss: 0.0992
 404/1000 [===========>..................] - ETA: 22:12 - loss: 0.8726 - regression_loss: 0.7734 - classification_loss: 0.0992
 405/1000 [===========>..................] - ETA: 22:10 - loss: 0.8726 - regression_loss: 0.7734 - classification_loss: 0.0992
 406/1000 [===========>..................] - ETA: 22:07 - loss: 0.8719 - regression_loss: 0.7728 - classification_loss: 0.0992
 407/1000 [===========>..................] - ETA: 22:04 - loss: 0.8705 - regression_loss: 0.7714 - classification_loss: 0.0991
 408/1000 [===========>..................] - ETA: 22:02 - loss: 0.8708 - regression_loss: 0.7717 - classification_loss: 0.0991
 409/1000 [===========>..................] - ETA: 21:59 - loss: 0.8701 - regression_loss: 0.7711 - classification_loss: 0.0991
 410/1000 [===========>..................] - ETA: 21:57 - loss: 0.8700 - regression_loss: 0.7709 - classification_loss: 0.0991
 411/1000 [===========>..................] - ETA: 21:56 - loss: 0.8703 - regression_loss: 0.7713 - classification_loss: 0.0991
 412/1000 [===========>..................] - ETA: 21:54 - loss: 0.8706 - regression_loss: 0.7715 - classification_loss: 0.0991
 413/1000 [===========>..................] - ETA: 21:52 - loss: 0.8706 - regression_loss: 0.7715 - classification_loss: 0.0991
 414/1000 [===========>..................] - ETA: 21:48 - loss: 0.8691 - regression_loss: 0.7702 - classification_loss: 0.0989
 415/1000 [===========>..................] - ETA: 21:47 - loss: 0.8692 - regression_loss: 0.7703 - classification_loss: 0.0989
 416/1000 [===========>..................] - ETA: 21:45 - loss: 0.8695 - regression_loss: 0.7706 - classification_loss: 0.0989
 417/1000 [===========>..................] - ETA: 21:43 - loss: 0.8698 - regression_loss: 0.7709 - classification_loss: 0.0989
 418/1000 [===========>..................] - ETA: 21:40 - loss: 0.8692 - regression_loss: 0.7703 - classification_loss: 0.0989
 419/1000 [===========>..................] - ETA: 21:38 - loss: 0.8690 - regression_loss: 0.7702 - classification_loss: 0.0989
 420/1000 [===========>..................] - ETA: 21:36 - loss: 0.8690 - regression_loss: 0.7702 - classification_loss: 0.0988
 421/1000 [===========>..................] - ETA: 21:34 - loss: 0.8691 - regression_loss: 0.7703 - classification_loss: 0.0988
 422/1000 [===========>..................] - ETA: 21:31 - loss: 0.8677 - regression_loss: 0.7690 - classification_loss: 0.0987
 423/1000 [===========>..................] - ETA: 21:29 - loss: 0.8680 - regression_loss: 0.7693 - classification_loss: 0.0987
 424/1000 [===========>..................] - ETA: 21:28 - loss: 0.8683 - regression_loss: 0.7696 - classification_loss: 0.0987
 425/1000 [===========>..................] - ETA: 21:26 - loss: 0.8684 - regression_loss: 0.7697 - classification_loss: 0.0987
 426/1000 [===========>..................] - ETA: 21:23 - loss: 0.8678 - regression_loss: 0.7691 - classification_loss: 0.0986
 427/1000 [===========>..................] - ETA: 21:21 - loss: 0.8676 - regression_loss: 0.7690 - classification_loss: 0.0986
 428/1000 [===========>..................] - ETA: 21:18 - loss: 0.8676 - regression_loss: 0.7690 - classification_loss: 0.0986
 429/1000 [===========>..................] - ETA: 21:15 - loss: 0.8662 - regression_loss: 0.7677 - classification_loss: 0.0985
 430/1000 [===========>..................] - ETA: 21:13 - loss: 0.8663 - regression_loss: 0.7678 - classification_loss: 0.0985
 431/1000 [===========>..................] - ETA: 21:11 - loss: 0.8663 - regression_loss: 0.7678 - classification_loss: 0.0985
 432/1000 [===========>..................] - ETA: 21:10 - loss: 0.8666 - regression_loss: 0.7681 - classification_loss: 0.0985
 433/1000 [===========>..................] - ETA: 21:08 - loss: 0.8667 - regression_loss: 0.7682 - classification_loss: 0.0985
 434/1000 [============>.................] - ETA: 21:06 - loss: 0.8670 - regression_loss: 0.7685 - classification_loss: 0.0985
 435/1000 [============>.................] - ETA: 21:03 - loss: 0.8657 - regression_loss: 0.7674 - classification_loss: 0.0984
 436/1000 [============>.................] - ETA: 21:00 - loss: 0.8651 - regression_loss: 0.7668 - classification_loss: 0.0983
 437/1000 [============>.................] - ETA: 20:57 - loss: 0.8645 - regression_loss: 0.7662 - classification_loss: 0.0983
 438/1000 [============>.................] - ETA: 20:54 - loss: 0.8633 - regression_loss: 0.7651 - classification_loss: 0.0982
 439/1000 [============>.................] - ETA: 20:52 - loss: 0.8636 - regression_loss: 0.7654 - classification_loss: 0.0982
 440/1000 [============>.................] - ETA: 20:50 - loss: 0.8640 - regression_loss: 0.7657 - classification_loss: 0.0982
 441/1000 [============>.................] - ETA: 20:48 - loss: 0.8641 - regression_loss: 0.7658 - classification_loss: 0.0982
 442/1000 [============>.................] - ETA: 20:47 - loss: 0.8644 - regression_loss: 0.7662 - classification_loss: 0.0982
 443/1000 [============>.................] - ETA: 20:45 - loss: 0.8643 - regression_loss: 0.7661 - classification_loss: 0.0982
 444/1000 [============>.................] - ETA: 20:42 - loss: 0.8634 - regression_loss: 0.7653 - classification_loss: 0.0981
 445/1000 [============>.................] - ETA: 20:40 - loss: 0.8634 - regression_loss: 0.7652 - classification_loss: 0.0981
 446/1000 [============>.................] - ETA: 20:38 - loss: 0.8636 - regression_loss: 0.7654 - classification_loss: 0.0982
 447/1000 [============>.................] - ETA: 20:35 - loss: 0.8634 - regression_loss: 0.7653 - classification_loss: 0.0982
 448/1000 [============>.................] - ETA: 20:33 - loss: 0.8638 - regression_loss: 0.7657 - classification_loss: 0.0982
 449/1000 [============>.................] - ETA: 20:31 - loss: 0.8639 - regression_loss: 0.7657 - classification_loss: 0.0981
 450/1000 [============>.................] - ETA: 20:29 - loss: 0.8642 - regression_loss: 0.7660 - classification_loss: 0.0981
 451/1000 [============>.................] - ETA: 20:27 - loss: 0.8643 - regression_loss: 0.7662 - classification_loss: 0.0981
 452/1000 [============>.................] - ETA: 20:24 - loss: 0.8629 - regression_loss: 0.7649 - classification_loss: 0.0980
 453/1000 [============>.................] - ETA: 20:22 - loss: 0.8629 - regression_loss: 0.7648 - classification_loss: 0.0980
 454/1000 [============>.................] - ETA: 20:20 - loss: 0.8632 - regression_loss: 0.7652 - classification_loss: 0.0980
 455/1000 [============>.................] - ETA: 20:18 - loss: 0.8627 - regression_loss: 0.7647 - classification_loss: 0.0980
 456/1000 [============>.................] - ETA: 20:15 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 457/1000 [============>.................] - ETA: 20:13 - loss: 0.8633 - regression_loss: 0.7653 - classification_loss: 0.0980
 458/1000 [============>.................] - ETA: 20:11 - loss: 0.8634 - regression_loss: 0.7653 - classification_loss: 0.0980
 459/1000 [============>.................] - ETA: 20:10 - loss: 0.8637 - regression_loss: 0.7657 - classification_loss: 0.0980
 460/1000 [============>.................] - ETA: 20:08 - loss: 0.8637 - regression_loss: 0.7657 - classification_loss: 0.0980
 461/1000 [============>.................] - ETA: 20:05 - loss: 0.8626 - regression_loss: 0.7647 - classification_loss: 0.0979
 462/1000 [============>.................] - ETA: 20:02 - loss: 0.8622 - regression_loss: 0.7643 - classification_loss: 0.0979
 463/1000 [============>.................] - ETA: 20:00 - loss: 0.8627 - regression_loss: 0.7648 - classification_loss: 0.0979
 464/1000 [============>.................] - ETA: 19:58 - loss: 0.8634 - regression_loss: 0.7654 - classification_loss: 0.0980
 465/1000 [============>.................] - ETA: 19:55 - loss: 0.8638 - regression_loss: 0.7658 - classification_loss: 0.0980
 466/1000 [============>.................] - ETA: 19:52 - loss: 0.8625 - regression_loss: 0.7645 - classification_loss: 0.0980
 467/1000 [=============>................] - ETA: 19:50 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 468/1000 [=============>................] - ETA: 19:48 - loss: 0.8627 - regression_loss: 0.7647 - classification_loss: 0.0980
 469/1000 [=============>................] - ETA: 19:46 - loss: 0.8624 - regression_loss: 0.7644 - classification_loss: 0.0980
 470/1000 [=============>................] - ETA: 19:44 - loss: 0.8627 - regression_loss: 0.7647 - classification_loss: 0.0980
 471/1000 [=============>................] - ETA: 19:42 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 472/1000 [=============>................] - ETA: 19:40 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 473/1000 [=============>................] - ETA: 19:38 - loss: 0.8629 - regression_loss: 0.7649 - classification_loss: 0.0980
 474/1000 [=============>................] - ETA: 19:36 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 475/1000 [=============>................] - ETA: 19:34 - loss: 0.8631 - regression_loss: 0.7651 - classification_loss: 0.0980
 476/1000 [=============>................] - ETA: 19:31 - loss: 0.8625 - regression_loss: 0.7645 - classification_loss: 0.0980
 477/1000 [=============>................] - ETA: 19:29 - loss: 0.8628 - regression_loss: 0.7648 - classification_loss: 0.0980
 478/1000 [=============>................] - ETA: 19:26 - loss: 0.8616 - regression_loss: 0.7637 - classification_loss: 0.0979
 479/1000 [=============>................] - ETA: 19:24 - loss: 0.8618 - regression_loss: 0.7640 - classification_loss: 0.0979
 480/1000 [=============>................] - ETA: 19:22 - loss: 0.8617 - regression_loss: 0.7639 - classification_loss: 0.0979
 481/1000 [=============>................] - ETA: 19:20 - loss: 0.8614 - regression_loss: 0.7635 - classification_loss: 0.0978
 482/1000 [=============>................] - ETA: 19:18 - loss: 0.8614 - regression_loss: 0.7636 - classification_loss: 0.0978
 483/1000 [=============>................] - ETA: 19:15 - loss: 0.8601 - regression_loss: 0.7624 - classification_loss: 0.0977
 484/1000 [=============>................] - ETA: 19:12 - loss: 0.8601 - regression_loss: 0.7624 - classification_loss: 0.0977
 485/1000 [=============>................] - ETA: 19:11 - loss: 0.8603 - regression_loss: 0.7626 - classification_loss: 0.0977
 486/1000 [=============>................] - ETA: 19:09 - loss: 0.8602 - regression_loss: 0.7625 - classification_loss: 0.0977
 487/1000 [=============>................] - ETA: 19:06 - loss: 0.8596 - regression_loss: 0.7620 - classification_loss: 0.0976
 488/1000 [=============>................] - ETA: 19:04 - loss: 0.8597 - regression_loss: 0.7621 - classification_loss: 0.0976
 489/1000 [=============>................] - ETA: 19:02 - loss: 0.8599 - regression_loss: 0.7623 - classification_loss: 0.0976
 490/1000 [=============>................] - ETA: 18:59 - loss: 0.8586 - regression_loss: 0.7611 - classification_loss: 0.0975
 491/1000 [=============>................] - ETA: 18:57 - loss: 0.8586 - regression_loss: 0.7611 - classification_loss: 0.0975
 492/1000 [=============>................] - ETA: 18:55 - loss: 0.8590 - regression_loss: 0.7615 - classification_loss: 0.0975
 493/1000 [=============>................] - ETA: 18:53 - loss: 0.8592 - regression_loss: 0.7617 - classification_loss: 0.0975
 494/1000 [=============>................] - ETA: 18:51 - loss: 0.8590 - regression_loss: 0.7616 - classification_loss: 0.0975
 495/1000 [=============>................] - ETA: 18:48 - loss: 0.8588 - regression_loss: 0.7613 - classification_loss: 0.0975
 496/1000 [=============>................] - ETA: 18:45 - loss: 0.8577 - regression_loss: 0.7603 - classification_loss: 0.0974
 497/1000 [=============>................] - ETA: 18:43 - loss: 0.8580 - regression_loss: 0.7607 - classification_loss: 0.0974
 498/1000 [=============>................] - ETA: 18:41 - loss: 0.8586 - regression_loss: 0.7612 - classification_loss: 0.0974
 499/1000 [=============>................] - ETA: 18:39 - loss: 0.8590 - regression_loss: 0.7616 - classification_loss: 0.0974
 500/1000 [==============>...............] - ETA: 18:37 - loss: 0.8592 - regression_loss: 0.7618 - classification_loss: 0.0974
 501/1000 [==============>...............] - ETA: 18:35 - loss: 0.8593 - regression_loss: 0.7619 - classification_loss: 0.0974
 502/1000 [==============>...............] - ETA: 18:33 - loss: 0.8594 - regression_loss: 0.7620 - classification_loss: 0.0974
 503/1000 [==============>...............] - ETA: 18:31 - loss: 0.8592 - regression_loss: 0.7618 - classification_loss: 0.0974
 504/1000 [==============>...............] - ETA: 18:29 - loss: 0.8595 - regression_loss: 0.7621 - classification_loss: 0.0974
 505/1000 [==============>...............] - ETA: 18:26 - loss: 0.8584 - regression_loss: 0.7611 - classification_loss: 0.0973
 506/1000 [==============>...............] - ETA: 18:24 - loss: 0.8586 - regression_loss: 0.7613 - classification_loss: 0.0973
 507/1000 [==============>...............] - ETA: 18:22 - loss: 0.8590 - regression_loss: 0.7616 - classification_loss: 0.0973
 508/1000 [==============>...............] - ETA: 18:20 - loss: 0.8591 - regression_loss: 0.7618 - classification_loss: 0.0973
 509/1000 [==============>...............] - ETA: 18:18 - loss: 0.8590 - regression_loss: 0.7617 - classification_loss: 0.0973
 510/1000 [==============>...............] - ETA: 18:15 - loss: 0.8577 - regression_loss: 0.7605 - classification_loss: 0.0972
 511/1000 [==============>...............] - ETA: 18:13 - loss: 0.8581 - regression_loss: 0.7609 - classification_loss: 0.0972
 512/1000 [==============>...............] - ETA: 18:10 - loss: 0.8575 - regression_loss: 0.7604 - classification_loss: 0.0972
 513/1000 [==============>...............] - ETA: 18:08 - loss: 0.8577 - regression_loss: 0.7606 - classification_loss: 0.0972
 514/1000 [==============>...............] - ETA: 18:06 - loss: 0.8582 - regression_loss: 0.7610 - classification_loss: 0.0972
 515/1000 [==============>...............] - ETA: 18:04 - loss: 0.8583 - regression_loss: 0.7611 - classification_loss: 0.0972
 516/1000 [==============>...............] - ETA: 18:01 - loss: 0.8578 - regression_loss: 0.7607 - classification_loss: 0.0971
 517/1000 [==============>...............] - ETA: 18:00 - loss: 0.8581 - regression_loss: 0.7610 - classification_loss: 0.0971
 518/1000 [==============>...............] - ETA: 17:58 - loss: 0.8580 - regression_loss: 0.7609 - classification_loss: 0.0971
 519/1000 [==============>...............] - ETA: 17:55 - loss: 0.8569 - regression_loss: 0.7599 - classification_loss: 0.0970
 520/1000 [==============>...............] - ETA: 17:53 - loss: 0.8569 - regression_loss: 0.7599 - classification_loss: 0.0970
 521/1000 [==============>...............] - ETA: 17:50 - loss: 0.8567 - regression_loss: 0.7597 - classification_loss: 0.0970
 522/1000 [==============>...............] - ETA: 17:48 - loss: 0.8570 - regression_loss: 0.7600 - classification_loss: 0.0970
 523/1000 [==============>...............] - ETA: 17:46 - loss: 0.8570 - regression_loss: 0.7600 - classification_loss: 0.0970
 524/1000 [==============>...............] - ETA: 17:44 - loss: 0.8570 - regression_loss: 0.7601 - classification_loss: 0.0970
 525/1000 [==============>...............] - ETA: 17:42 - loss: 0.8572 - regression_loss: 0.7603 - classification_loss: 0.0969
 526/1000 [==============>...............] - ETA: 17:40 - loss: 0.8573 - regression_loss: 0.7604 - classification_loss: 0.0969
 527/1000 [==============>...............] - ETA: 17:37 - loss: 0.8561 - regression_loss: 0.7593 - classification_loss: 0.0968
 528/1000 [==============>...............] - ETA: 17:35 - loss: 0.8562 - regression_loss: 0.7594 - classification_loss: 0.0968
 529/1000 [==============>...............] - ETA: 17:33 - loss: 0.8561 - regression_loss: 0.7593 - classification_loss: 0.0968
 530/1000 [==============>...............] - ETA: 17:31 - loss: 0.8563 - regression_loss: 0.7595 - classification_loss: 0.0968
 531/1000 [==============>...............] - ETA: 17:28 - loss: 0.8552 - regression_loss: 0.7585 - classification_loss: 0.0967
 532/1000 [==============>...............] - ETA: 17:26 - loss: 0.8555 - regression_loss: 0.7588 - classification_loss: 0.0967
 533/1000 [==============>...............] - ETA: 17:24 - loss: 0.8549 - regression_loss: 0.7583 - classification_loss: 0.0967
 534/1000 [===============>..............] - ETA: 17:22 - loss: 0.8550 - regression_loss: 0.7584 - classification_loss: 0.0966
 535/1000 [===============>..............] - ETA: 17:20 - loss: 0.8550 - regression_loss: 0.7584 - classification_loss: 0.0966
 536/1000 [===============>..............] - ETA: 17:18 - loss: 0.8553 - regression_loss: 0.7587 - classification_loss: 0.0966
 537/1000 [===============>..............] - ETA: 17:16 - loss: 0.8551 - regression_loss: 0.7585 - classification_loss: 0.0966
 538/1000 [===============>..............] - ETA: 17:13 - loss: 0.8547 - regression_loss: 0.7581 - classification_loss: 0.0966
 539/1000 [===============>..............] - ETA: 17:10 - loss: 0.8535 - regression_loss: 0.7570 - classification_loss: 0.0965
 540/1000 [===============>..............] - ETA: 17:08 - loss: 0.8539 - regression_loss: 0.7575 - classification_loss: 0.0965
 541/1000 [===============>..............] - ETA: 17:06 - loss: 0.8541 - regression_loss: 0.7577 - classification_loss: 0.0965
 542/1000 [===============>..............] - ETA: 17:04 - loss: 0.8545 - regression_loss: 0.7581 - classification_loss: 0.0965
 543/1000 [===============>..............] - ETA: 17:01 - loss: 0.8534 - regression_loss: 0.7571 - classification_loss: 0.0963
 544/1000 [===============>..............] - ETA: 16:59 - loss: 0.8534 - regression_loss: 0.7571 - classification_loss: 0.0963
 545/1000 [===============>..............] - ETA: 16:57 - loss: 0.8535 - regression_loss: 0.7572 - classification_loss: 0.0963
 546/1000 [===============>..............] - ETA: 16:55 - loss: 0.8534 - regression_loss: 0.7570 - classification_loss: 0.0963
 547/1000 [===============>..............] - ETA: 16:53 - loss: 0.8536 - regression_loss: 0.7573 - classification_loss: 0.0963
 548/1000 [===============>..............] - ETA: 16:51 - loss: 0.8531 - regression_loss: 0.7569 - classification_loss: 0.0963
 549/1000 [===============>..............] - ETA: 16:48 - loss: 0.8521 - regression_loss: 0.7559 - classification_loss: 0.0962
 550/1000 [===============>..............] - ETA: 16:46 - loss: 0.8523 - regression_loss: 0.7562 - classification_loss: 0.0962
 551/1000 [===============>..............] - ETA: 16:44 - loss: 0.8524 - regression_loss: 0.7562 - classification_loss: 0.0962
 552/1000 [===============>..............] - ETA: 16:42 - loss: 0.8524 - regression_loss: 0.7563 - classification_loss: 0.0962
 553/1000 [===============>..............] - ETA: 16:39 - loss: 0.8521 - regression_loss: 0.7559 - classification_loss: 0.0961
 554/1000 [===============>..............] - ETA: 16:37 - loss: 0.8520 - regression_loss: 0.7558 - classification_loss: 0.0961
 555/1000 [===============>..............] - ETA: 16:35 - loss: 0.8522 - regression_loss: 0.7561 - classification_loss: 0.0961
 556/1000 [===============>..............] - ETA: 16:33 - loss: 0.8521 - regression_loss: 0.7559 - classification_loss: 0.0961
 557/1000 [===============>..............] - ETA: 16:30 - loss: 0.8516 - regression_loss: 0.7554 - classification_loss: 0.0961
 558/1000 [===============>..............] - ETA: 16:29 - loss: 0.8518 - regression_loss: 0.7557 - classification_loss: 0.0961
 559/1000 [===============>..............] - ETA: 16:27 - loss: 0.8520 - regression_loss: 0.7559 - classification_loss: 0.0961
 560/1000 [===============>..............] - ETA: 16:25 - loss: 0.8520 - regression_loss: 0.7559 - classification_loss: 0.0961
 561/1000 [===============>..............] - ETA: 16:22 - loss: 0.8510 - regression_loss: 0.7551 - classification_loss: 0.0960
 562/1000 [===============>..............] - ETA: 16:20 - loss: 0.8511 - regression_loss: 0.7551 - classification_loss: 0.0960
 563/1000 [===============>..............] - ETA: 16:18 - loss: 0.8509 - regression_loss: 0.7550 - classification_loss: 0.0959
 564/1000 [===============>..............] - ETA: 16:16 - loss: 0.8509 - regression_loss: 0.7550 - classification_loss: 0.0959
 565/1000 [===============>..............] - ETA: 16:14 - loss: 0.8509 - regression_loss: 0.7550 - classification_loss: 0.0959
 566/1000 [===============>..............] - ETA: 16:12 - loss: 0.8511 - regression_loss: 0.7552 - classification_loss: 0.0959
 567/1000 [================>.............] - ETA: 16:10 - loss: 0.8513 - regression_loss: 0.7554 - classification_loss: 0.0959
 568/1000 [================>.............] - ETA: 16:07 - loss: 0.8509 - regression_loss: 0.7550 - classification_loss: 0.0959
 569/1000 [================>.............] - ETA: 16:04 - loss: 0.8498 - regression_loss: 0.7540 - classification_loss: 0.0958
 570/1000 [================>.............] - ETA: 16:02 - loss: 0.8497 - regression_loss: 0.7540 - classification_loss: 0.0958
 571/1000 [================>.............] - ETA: 16:00 - loss: 0.8500 - regression_loss: 0.7542 - classification_loss: 0.0958
 572/1000 [================>.............] - ETA: 15:58 - loss: 0.8500 - regression_loss: 0.7543 - classification_loss: 0.0957
 573/1000 [================>.............] - ETA: 15:56 - loss: 0.8496 - regression_loss: 0.7538 - classification_loss: 0.0957
 574/1000 [================>.............] - ETA: 15:53 - loss: 0.8485 - regression_loss: 0.7528 - classification_loss: 0.0956
 575/1000 [================>.............] - ETA: 15:51 - loss: 0.8488 - regression_loss: 0.7532 - classification_loss: 0.0956
 576/1000 [================>.............] - ETA: 15:49 - loss: 0.8493 - regression_loss: 0.7537 - classification_loss: 0.0956
 577/1000 [================>.............] - ETA: 15:47 - loss: 0.8494 - regression_loss: 0.7538 - classification_loss: 0.0956
 578/1000 [================>.............] - ETA: 15:45 - loss: 0.8495 - regression_loss: 0.7539 - classification_loss: 0.0956
 579/1000 [================>.............] - ETA: 15:42 - loss: 0.8484 - regression_loss: 0.7529 - classification_loss: 0.0955
 580/1000 [================>.............] - ETA: 15:39 - loss: 0.8481 - regression_loss: 0.7526 - classification_loss: 0.0955
 581/1000 [================>.............] - ETA: 15:38 - loss: 0.8485 - regression_loss: 0.7529 - classification_loss: 0.0955
 582/1000 [================>.............] - ETA: 15:35 - loss: 0.8488 - regression_loss: 0.7533 - classification_loss: 0.0955
 583/1000 [================>.............] - ETA: 15:34 - loss: 0.8491 - regression_loss: 0.7536 - classification_loss: 0.0955
 584/1000 [================>.............] - ETA: 15:32 - loss: 0.8494 - regression_loss: 0.7539 - classification_loss: 0.0955
 585/1000 [================>.............] - ETA: 15:29 - loss: 0.8486 - regression_loss: 0.7532 - classification_loss: 0.0955
 586/1000 [================>.............] - ETA: 15:27 - loss: 0.8484 - regression_loss: 0.7529 - classification_loss: 0.0954
 587/1000 [================>.............] - ETA: 15:26 - loss: 0.8487 - regression_loss: 0.7533 - classification_loss: 0.0954
 588/1000 [================>.............] - ETA: 15:24 - loss: 0.8489 - regression_loss: 0.7534 - classification_loss: 0.0954
 589/1000 [================>.............] - ETA: 15:22 - loss: 0.8487 - regression_loss: 0.7533 - classification_loss: 0.0954
 590/1000 [================>.............] - ETA: 15:20 - loss: 0.8487 - regression_loss: 0.7533 - classification_loss: 0.0954
 591/1000 [================>.............] - ETA: 15:19 - loss: 0.8489 - regression_loss: 0.7535 - classification_loss: 0.0954
 592/1000 [================>.............] - ETA: 15:17 - loss: 0.8489 - regression_loss: 0.7535 - classification_loss: 0.0954
 593/1000 [================>.............] - ETA: 15:15 - loss: 0.8488 - regression_loss: 0.7534 - classification_loss: 0.0954
 594/1000 [================>.............] - ETA: 15:12 - loss: 0.8477 - regression_loss: 0.7524 - classification_loss: 0.0953
 595/1000 [================>.............] - ETA: 15:11 - loss: 0.8476 - regression_loss: 0.7523 - classification_loss: 0.0953
 596/1000 [================>.............] - ETA: 15:09 - loss: 0.8476 - regression_loss: 0.7524 - classification_loss: 0.0952
 597/1000 [================>.............] - ETA: 15:08 - loss: 0.8479 - regression_loss: 0.7526 - classification_loss: 0.0952
 598/1000 [================>.............] - ETA: 15:06 - loss: 0.8479 - regression_loss: 0.7527 - classification_loss: 0.0952
 599/1000 [================>.............] - ETA: 15:04 - loss: 0.8479 - regression_loss: 0.7527 - classification_loss: 0.0952
 600/1000 [=================>............] - ETA: 15:03 - loss: 0.8481 - regression_loss: 0.7529 - classification_loss: 0.0952
 601/1000 [=================>............] - ETA: 15:01 - loss: 0.8483 - regression_loss: 0.7531 - classification_loss: 0.0952
 602/1000 [=================>............] - ETA: 14:59 - loss: 0.8473 - regression_loss: 0.7522 - classification_loss: 0.0951
 603/1000 [=================>............] - ETA: 14:57 - loss: 0.8472 - regression_loss: 0.7521 - classification_loss: 0.0951
 604/1000 [=================>............] - ETA: 14:55 - loss: 0.8469 - regression_loss: 0.7518 - classification_loss: 0.0951
 605/1000 [=================>............] - ETA: 14:53 - loss: 0.8472 - regression_loss: 0.7521 - classification_loss: 0.0951
 606/1000 [=================>............] - ETA: 14:50 - loss: 0.8462 - regression_loss: 0.7511 - classification_loss: 0.0950
 607/1000 [=================>............] - ETA: 14:49 - loss: 0.8462 - regression_loss: 0.7512 - classification_loss: 0.0950
 608/1000 [=================>............] - ETA: 14:46 - loss: 0.8457 - regression_loss: 0.7507 - classification_loss: 0.0950
 609/1000 [=================>............] - ETA: 14:45 - loss: 0.8459 - regression_loss: 0.7509 - classification_loss: 0.0950
 610/1000 [=================>............] - ETA: 14:43 - loss: 0.8458 - regression_loss: 0.7508 - classification_loss: 0.0950
 611/1000 [=================>............] - ETA: 14:41 - loss: 0.8458 - regression_loss: 0.7508 - classification_loss: 0.0950
 612/1000 [=================>............] - ETA: 14:39 - loss: 0.8458 - regression_loss: 0.7508 - classification_loss: 0.0950
 613/1000 [=================>............] - ETA: 14:37 - loss: 0.8457 - regression_loss: 0.7507 - classification_loss: 0.0950
 614/1000 [=================>............] - ETA: 14:35 - loss: 0.8459 - regression_loss: 0.7509 - classification_loss: 0.0950
 615/1000 [=================>............] - ETA: 14:33 - loss: 0.8456 - regression_loss: 0.7507 - classification_loss: 0.0949
 616/1000 [=================>............] - ETA: 14:31 - loss: 0.8456 - regression_loss: 0.7507 - classification_loss: 0.0949
 617/1000 [=================>............] - ETA: 14:29 - loss: 0.8458 - regression_loss: 0.7509 - classification_loss: 0.0949
 618/1000 [=================>............] - ETA: 14:27 - loss: 0.8449 - regression_loss: 0.7501 - classification_loss: 0.0948
 619/1000 [=================>............] - ETA: 14:25 - loss: 0.8449 - regression_loss: 0.7501 - classification_loss: 0.0948
 620/1000 [=================>............] - ETA: 14:24 - loss: 0.8451 - regression_loss: 0.7503 - classification_loss: 0.0948
 621/1000 [=================>............] - ETA: 14:22 - loss: 0.8451 - regression_loss: 0.7503 - classification_loss: 0.0948
 622/1000 [=================>............] - ETA: 14:20 - loss: 0.8447 - regression_loss: 0.7499 - classification_loss: 0.0948
 623/1000 [=================>............] - ETA: 14:18 - loss: 0.8450 - regression_loss: 0.7502 - classification_loss: 0.0948
 624/1000 [=================>............] - ETA: 14:16 - loss: 0.8444 - regression_loss: 0.7497 - classification_loss: 0.0947
 625/1000 [=================>............] - ETA: 14:14 - loss: 0.8444 - regression_loss: 0.7497 - classification_loss: 0.0947
 626/1000 [=================>............] - ETA: 14:12 - loss: 0.8446 - regression_loss: 0.7500 - classification_loss: 0.0947
 627/1000 [=================>............] - ETA: 14:10 - loss: 0.8442 - regression_loss: 0.7496 - classification_loss: 0.0946
 628/1000 [=================>............] - ETA: 14:07 - loss: 0.8442 - regression_loss: 0.7496 - classification_loss: 0.0946
 629/1000 [=================>............] - ETA: 14:06 - loss: 0.8444 - regression_loss: 0.7498 - classification_loss: 0.0946
 630/1000 [=================>............] - ETA: 14:04 - loss: 0.8447 - regression_loss: 0.7501 - classification_loss: 0.0946
 631/1000 [=================>............] - ETA: 14:02 - loss: 0.8447 - regression_loss: 0.7501 - classification_loss: 0.0946
 632/1000 [=================>............] - ETA: 13:59 - loss: 0.8437 - regression_loss: 0.7492 - classification_loss: 0.0945
 633/1000 [=================>............] - ETA: 13:58 - loss: 0.8438 - regression_loss: 0.7492 - classification_loss: 0.0945
 634/1000 [==================>...........] - ETA: 13:56 - loss: 0.8440 - regression_loss: 0.7495 - classification_loss: 0.0945
 635/1000 [==================>...........] - ETA: 13:54 - loss: 0.8436 - regression_loss: 0.7491 - classification_loss: 0.0945
 636/1000 [==================>...........] - ETA: 13:51 - loss: 0.8436 - regression_loss: 0.7492 - classification_loss: 0.0945
 637/1000 [==================>...........] - ETA: 13:49 - loss: 0.8428 - regression_loss: 0.7484 - classification_loss: 0.0944
 638/1000 [==================>...........] - ETA: 13:47 - loss: 0.8427 - regression_loss: 0.7484 - classification_loss: 0.0944
 639/1000 [==================>...........] - ETA: 13:45 - loss: 0.8429 - regression_loss: 0.7486 - classification_loss: 0.0944
 640/1000 [==================>...........] - ETA: 13:43 - loss: 0.8428 - regression_loss: 0.7484 - classification_loss: 0.0944
 641/1000 [==================>...........] - ETA: 13:41 - loss: 0.8428 - regression_loss: 0.7484 - classification_loss: 0.0943
 642/1000 [==================>...........] - ETA: 13:39 - loss: 0.8425 - regression_loss: 0.7481 - classification_loss: 0.0943
 643/1000 [==================>...........] - ETA: 13:37 - loss: 0.8426 - regression_loss: 0.7483 - classification_loss: 0.0943
 644/1000 [==================>...........] - ETA: 13:35 - loss: 0.8426 - regression_loss: 0.7483 - classification_loss: 0.0943
 645/1000 [==================>...........] - ETA: 13:33 - loss: 0.8418 - regression_loss: 0.7476 - classification_loss: 0.0942
 646/1000 [==================>...........] - ETA: 13:31 - loss: 0.8419 - regression_loss: 0.7477 - classification_loss: 0.0942
 647/1000 [==================>...........] - ETA: 13:28 - loss: 0.8410 - regression_loss: 0.7468 - classification_loss: 0.0941
 648/1000 [==================>...........] - ETA: 13:26 - loss: 0.8409 - regression_loss: 0.7468 - classification_loss: 0.0941
 649/1000 [==================>...........] - ETA: 13:24 - loss: 0.8409 - regression_loss: 0.7468 - classification_loss: 0.0941
 650/1000 [==================>...........] - ETA: 13:22 - loss: 0.8408 - regression_loss: 0.7467 - classification_loss: 0.0941
 651/1000 [==================>...........] - ETA: 13:20 - loss: 0.8405 - regression_loss: 0.7464 - classification_loss: 0.0941
 652/1000 [==================>...........] - ETA: 13:18 - loss: 0.8406 - regression_loss: 0.7466 - classification_loss: 0.0941
 653/1000 [==================>...........] - ETA: 13:16 - loss: 0.8408 - regression_loss: 0.7468 - classification_loss: 0.0940
 654/1000 [==================>...........] - ETA: 13:14 - loss: 0.8398 - regression_loss: 0.7459 - classification_loss: 0.0940
 655/1000 [==================>...........] - ETA: 13:12 - loss: 0.8398 - regression_loss: 0.7458 - classification_loss: 0.0939
 656/1000 [==================>...........] - ETA: 13:10 - loss: 0.8394 - regression_loss: 0.7455 - classification_loss: 0.0939
 657/1000 [==================>...........] - ETA: 13:08 - loss: 0.8394 - regression_loss: 0.7455 - classification_loss: 0.0939
 658/1000 [==================>...........] - ETA: 13:06 - loss: 0.8396 - regression_loss: 0.7457 - classification_loss: 0.0939
 659/1000 [==================>...........] - ETA: 13:04 - loss: 0.8398 - regression_loss: 0.7459 - classification_loss: 0.0939
 660/1000 [==================>...........] - ETA: 13:02 - loss: 0.8397 - regression_loss: 0.7458 - classification_loss: 0.0939
 661/1000 [==================>...........] - ETA: 13:00 - loss: 0.8397 - regression_loss: 0.7458 - classification_loss: 0.0939
 662/1000 [==================>...........] - ETA: 12:58 - loss: 0.8397 - regression_loss: 0.7458 - classification_loss: 0.0939
 663/1000 [==================>...........] - ETA: 12:55 - loss: 0.8388 - regression_loss: 0.7449 - classification_loss: 0.0938
 664/1000 [==================>...........] - ETA: 12:53 - loss: 0.8387 - regression_loss: 0.7448 - classification_loss: 0.0938
 665/1000 [==================>...........] - ETA: 12:51 - loss: 0.8388 - regression_loss: 0.7450 - classification_loss: 0.0938
 666/1000 [==================>...........] - ETA: 12:49 - loss: 0.8390 - regression_loss: 0.7452 - classification_loss: 0.0938
 667/1000 [===================>..........] - ETA: 12:47 - loss: 0.8386 - regression_loss: 0.7448 - classification_loss: 0.0938
 668/1000 [===================>..........] - ETA: 12:45 - loss: 0.8387 - regression_loss: 0.7448 - classification_loss: 0.0938
 669/1000 [===================>..........] - ETA: 12:43 - loss: 0.8386 - regression_loss: 0.7448 - classification_loss: 0.0938
 670/1000 [===================>..........] - ETA: 12:41 - loss: 0.8388 - regression_loss: 0.7449 - classification_loss: 0.0938
 671/1000 [===================>..........] - ETA: 12:39 - loss: 0.8388 - regression_loss: 0.7450 - classification_loss: 0.0938
 672/1000 [===================>..........] - ETA: 12:37 - loss: 0.8384 - regression_loss: 0.7446 - classification_loss: 0.0938
 673/1000 [===================>..........] - ETA: 12:35 - loss: 0.8386 - regression_loss: 0.7448 - classification_loss: 0.0938
 674/1000 [===================>..........] - ETA: 12:32 - loss: 0.8377 - regression_loss: 0.7440 - classification_loss: 0.0937
 675/1000 [===================>..........] - ETA: 12:30 - loss: 0.8377 - regression_loss: 0.7439 - classification_loss: 0.0937
 676/1000 [===================>..........] - ETA: 12:28 - loss: 0.8377 - regression_loss: 0.7440 - classification_loss: 0.0937
 677/1000 [===================>..........] - ETA: 12:26 - loss: 0.8367 - regression_loss: 0.7431 - classification_loss: 0.0937
 678/1000 [===================>..........] - ETA: 12:24 - loss: 0.8369 - regression_loss: 0.7433 - classification_loss: 0.0937
 679/1000 [===================>..........] - ETA: 12:22 - loss: 0.8371 - regression_loss: 0.7434 - classification_loss: 0.0937
 680/1000 [===================>..........] - ETA: 12:20 - loss: 0.8367 - regression_loss: 0.7431 - classification_loss: 0.0936
 681/1000 [===================>..........] - ETA: 12:17 - loss: 0.8367 - regression_loss: 0.7431 - classification_loss: 0.0936
 682/1000 [===================>..........] - ETA: 12:16 - loss: 0.8369 - regression_loss: 0.7433 - classification_loss: 0.0936
 683/1000 [===================>..........] - ETA: 12:14 - loss: 0.8369 - regression_loss: 0.7433 - classification_loss: 0.0936
 684/1000 [===================>..........] - ETA: 12:12 - loss: 0.8368 - regression_loss: 0.7432 - classification_loss: 0.0936
 685/1000 [===================>..........] - ETA: 12:10 - loss: 0.8370 - regression_loss: 0.7433 - classification_loss: 0.0936
 686/1000 [===================>..........] - ETA: 12:08 - loss: 0.8369 - regression_loss: 0.7433 - classification_loss: 0.0936
 687/1000 [===================>..........] - ETA: 12:05 - loss: 0.8361 - regression_loss: 0.7426 - classification_loss: 0.0935
 688/1000 [===================>..........] - ETA: 12:03 - loss: 0.8358 - regression_loss: 0.7423 - classification_loss: 0.0935
 689/1000 [===================>..........] - ETA: 12:01 - loss: 0.8360 - regression_loss: 0.7425 - classification_loss: 0.0935
 690/1000 [===================>..........] - ETA: 11:59 - loss: 0.8360 - regression_loss: 0.7425 - classification_loss: 0.0935
 691/1000 [===================>..........] - ETA: 11:56 - loss: 0.8356 - regression_loss: 0.7421 - classification_loss: 0.0935
 692/1000 [===================>..........] - ETA: 11:54 - loss: 0.8355 - regression_loss: 0.7421 - classification_loss: 0.0934
 693/1000 [===================>..........] - ETA: 11:52 - loss: 0.8354 - regression_loss: 0.7420 - classification_loss: 0.0934
 694/1000 [===================>..........] - ETA: 11:50 - loss: 0.8356 - regression_loss: 0.7421 - classification_loss: 0.0934
 695/1000 [===================>..........] - ETA: 11:48 - loss: 0.8351 - regression_loss: 0.7417 - classification_loss: 0.0934
 696/1000 [===================>..........] - ETA: 11:46 - loss: 0.8352 - regression_loss: 0.7418 - classification_loss: 0.0934
 697/1000 [===================>..........] - ETA: 11:44 - loss: 0.8354 - regression_loss: 0.7420 - classification_loss: 0.0934
 698/1000 [===================>..........] - ETA: 11:42 - loss: 0.8353 - regression_loss: 0.7420 - classification_loss: 0.0933
 699/1000 [===================>..........] - ETA: 11:40 - loss: 0.8352 - regression_loss: 0.7419 - classification_loss: 0.0933
 700/1000 [====================>.........] - ETA: 11:37 - loss: 0.8350 - regression_loss: 0.7417 - classification_loss: 0.0933
 701/1000 [====================>.........] - ETA: 11:35 - loss: 0.8341 - regression_loss: 0.7409 - classification_loss: 0.0932
 702/1000 [====================>.........] - ETA: 11:33 - loss: 0.8342 - regression_loss: 0.7410 - classification_loss: 0.0932
 703/1000 [====================>.........] - ETA: 11:31 - loss: 0.8343 - regression_loss: 0.7411 - classification_loss: 0.0932
 704/1000 [====================>.........] - ETA: 11:28 - loss: 0.8335 - regression_loss: 0.7404 - classification_loss: 0.0931
 705/1000 [====================>.........] - ETA: 11:26 - loss: 0.8331 - regression_loss: 0.7400 - classification_loss: 0.0931
 706/1000 [====================>.........] - ETA: 11:23 - loss: 0.8331 - regression_loss: 0.7400 - classification_loss: 0.0931
 707/1000 [====================>.........] - ETA: 11:21 - loss: 0.8330 - regression_loss: 0.7399 - classification_loss: 0.0931
 708/1000 [====================>.........] - ETA: 11:19 - loss: 0.8332 - regression_loss: 0.7401 - classification_loss: 0.0931
 709/1000 [====================>.........] - ETA: 11:17 - loss: 0.8333 - regression_loss: 0.7402 - classification_loss: 0.0931
 710/1000 [====================>.........] - ETA: 11:15 - loss: 0.8329 - regression_loss: 0.7398 - classification_loss: 0.0930
 711/1000 [====================>.........] - ETA: 11:13 - loss: 0.8330 - regression_loss: 0.7400 - classification_loss: 0.0930
 712/1000 [====================>.........] - ETA: 11:11 - loss: 0.8330 - regression_loss: 0.7400 - classification_loss: 0.0930
 713/1000 [====================>.........] - ETA: 11:09 - loss: 0.8332 - regression_loss: 0.7402 - classification_loss: 0.0930
 714/1000 [====================>.........] - ETA: 11:07 - loss: 0.8332 - regression_loss: 0.7402 - classification_loss: 0.0930
 715/1000 [====================>.........] - ETA: 11:04 - loss: 0.8334 - regression_loss: 0.7404 - classification_loss: 0.0930
 716/1000 [====================>.........] - ETA: 11:02 - loss: 0.8325 - regression_loss: 0.7396 - classification_loss: 0.0929
 717/1000 [====================>.........] - ETA: 10:59 - loss: 0.8319 - regression_loss: 0.7391 - classification_loss: 0.0928
 718/1000 [====================>.........] - ETA: 10:57 - loss: 0.8321 - regression_loss: 0.7393 - classification_loss: 0.0928
 719/1000 [====================>.........] - ETA: 10:55 - loss: 0.8323 - regression_loss: 0.7395 - classification_loss: 0.0928
 720/1000 [====================>.........] - ETA: 10:53 - loss: 0.8320 - regression_loss: 0.7392 - classification_loss: 0.0928
 721/1000 [====================>.........] - ETA: 10:51 - loss: 0.8322 - regression_loss: 0.7394 - classification_loss: 0.0928
 722/1000 [====================>.........] - ETA: 10:49 - loss: 0.8321 - regression_loss: 0.7393 - classification_loss: 0.0928
 723/1000 [====================>.........] - ETA: 10:47 - loss: 0.8321 - regression_loss: 0.7393 - classification_loss: 0.0928
 724/1000 [====================>.........] - ETA: 10:45 - loss: 0.8321 - regression_loss: 0.7394 - classification_loss: 0.0928
 725/1000 [====================>.........] - ETA: 10:43 - loss: 0.8320 - regression_loss: 0.7392 - classification_loss: 0.0928
 726/1000 [====================>.........] - ETA: 10:40 - loss: 0.8312 - regression_loss: 0.7385 - classification_loss: 0.0927
 727/1000 [====================>.........] - ETA: 10:38 - loss: 0.8313 - regression_loss: 0.7386 - classification_loss: 0.0927
 728/1000 [====================>.........] - ETA: 10:36 - loss: 0.8310 - regression_loss: 0.7384 - classification_loss: 0.0927
 729/1000 [====================>.........] - ETA: 10:34 - loss: 0.8313 - regression_loss: 0.7386 - classification_loss: 0.0927
 730/1000 [====================>.........] - ETA: 10:32 - loss: 0.8315 - regression_loss: 0.7388 - classification_loss: 0.0926
 731/1000 [====================>.........] - ETA: 10:30 - loss: 0.8317 - regression_loss: 0.7390 - classification_loss: 0.0926
 732/1000 [====================>.........] - ETA: 10:27 - loss: 0.8308 - regression_loss: 0.7383 - classification_loss: 0.0926
 733/1000 [====================>.........] - ETA: 10:25 - loss: 0.8309 - regression_loss: 0.7383 - classification_loss: 0.0926
 734/1000 [=====================>........] - ETA: 10:23 - loss: 0.8309 - regression_loss: 0.7384 - classification_loss: 0.0925
 735/1000 [=====================>........] - ETA: 10:21 - loss: 0.8308 - regression_loss: 0.7383 - classification_loss: 0.0926
 736/1000 [=====================>........] - ETA: 10:18 - loss: 0.8305 - regression_loss: 0.7380 - classification_loss: 0.0925
 737/1000 [=====================>........] - ETA: 10:16 - loss: 0.8307 - regression_loss: 0.7382 - classification_loss: 0.0925
 738/1000 [=====================>........] - ETA: 10:14 - loss: 0.8308 - regression_loss: 0.7382 - classification_loss: 0.0925
 739/1000 [=====================>........] - ETA: 10:12 - loss: 0.8303 - regression_loss: 0.7379 - classification_loss: 0.0925
 740/1000 [=====================>........] - ETA: 10:09 - loss: 0.8297 - regression_loss: 0.7373 - classification_loss: 0.0924
 741/1000 [=====================>........] - ETA: 10:07 - loss: 0.8299 - regression_loss: 0.7375 - classification_loss: 0.0924
 742/1000 [=====================>........] - ETA: 10:05 - loss: 0.8298 - regression_loss: 0.7374 - classification_loss: 0.0924
 743/1000 [=====================>........] - ETA: 10:03 - loss: 0.8298 - regression_loss: 0.7374 - classification_loss: 0.0924
 744/1000 [=====================>........] - ETA: 10:00 - loss: 0.8299 - regression_loss: 0.7375 - classification_loss: 0.0924
 745/1000 [=====================>........] - ETA: 9:58 - loss: 0.8301 - regression_loss: 0.7376 - classification_loss: 0.0924 
 746/1000 [=====================>........] - ETA: 9:56 - loss: 0.8301 - regression_loss: 0.7377 - classification_loss: 0.0924
 747/1000 [=====================>........] - ETA: 9:54 - loss: 0.8301 - regression_loss: 0.7377 - classification_loss: 0.0924
 748/1000 [=====================>........] - ETA: 9:51 - loss: 0.8293 - regression_loss: 0.7370 - classification_loss: 0.0923
 749/1000 [=====================>........] - ETA: 9:49 - loss: 0.8295 - regression_loss: 0.7371 - classification_loss: 0.0923
 750/1000 [=====================>........] - ETA: 9:47 - loss: 0.8295 - regression_loss: 0.7372 - classification_loss: 0.0923
 751/1000 [=====================>........] - ETA: 9:45 - loss: 0.8296 - regression_loss: 0.7373 - classification_loss: 0.0923
 752/1000 [=====================>........] - ETA: 9:42 - loss: 0.8295 - regression_loss: 0.7372 - classification_loss: 0.0923
 753/1000 [=====================>........] - ETA: 9:40 - loss: 0.8295 - regression_loss: 0.7373 - classification_loss: 0.0923
 754/1000 [=====================>........] - ETA: 9:38 - loss: 0.8296 - regression_loss: 0.7373 - classification_loss: 0.0923
 755/1000 [=====================>........] - ETA: 9:36 - loss: 0.8296 - regression_loss: 0.7373 - classification_loss: 0.0923
 756/1000 [=====================>........] - ETA: 9:33 - loss: 0.8288 - regression_loss: 0.7366 - classification_loss: 0.0922
 757/1000 [=====================>........] - ETA: 9:31 - loss: 0.8290 - regression_loss: 0.7368 - classification_loss: 0.0922
 758/1000 [=====================>........] - ETA: 9:29 - loss: 0.8291 - regression_loss: 0.7369 - classification_loss: 0.0922
 759/1000 [=====================>........] - ETA: 9:27 - loss: 0.8291 - regression_loss: 0.7369 - classification_loss: 0.0922
 760/1000 [=====================>........] - ETA: 9:25 - loss: 0.8289 - regression_loss: 0.7367 - classification_loss: 0.0922
 761/1000 [=====================>........] - ETA: 9:22 - loss: 0.8280 - regression_loss: 0.7360 - classification_loss: 0.0921
 762/1000 [=====================>........] - ETA: 9:20 - loss: 0.8282 - regression_loss: 0.7361 - classification_loss: 0.0921
 763/1000 [=====================>........] - ETA: 9:18 - loss: 0.8283 - regression_loss: 0.7362 - classification_loss: 0.0921
 764/1000 [=====================>........] - ETA: 9:16 - loss: 0.8283 - regression_loss: 0.7363 - classification_loss: 0.0921
 765/1000 [=====================>........] - ETA: 9:14 - loss: 0.8282 - regression_loss: 0.7362 - classification_loss: 0.0921
 766/1000 [=====================>........] - ETA: 9:11 - loss: 0.8278 - regression_loss: 0.7358 - classification_loss: 0.0920
 767/1000 [======================>.......] - ETA: 9:09 - loss: 0.8270 - regression_loss: 0.7351 - classification_loss: 0.0920
 768/1000 [======================>.......] - ETA: 9:07 - loss: 0.8272 - regression_loss: 0.7352 - classification_loss: 0.0920
 769/1000 [======================>.......] - ETA: 9:05 - loss: 0.8273 - regression_loss: 0.7354 - classification_loss: 0.0919
 770/1000 [======================>.......] - ETA: 9:02 - loss: 0.8273 - regression_loss: 0.7354 - classification_loss: 0.0919
 771/1000 [======================>.......] - ETA: 9:00 - loss: 0.8272 - regression_loss: 0.7353 - classification_loss: 0.0919
 772/1000 [======================>.......] - ETA: 8:58 - loss: 0.8271 - regression_loss: 0.7352 - classification_loss: 0.0919
 773/1000 [======================>.......] - ETA: 8:55 - loss: 0.8268 - regression_loss: 0.7349 - classification_loss: 0.0919
 774/1000 [======================>.......] - ETA: 8:53 - loss: 0.8268 - regression_loss: 0.7349 - classification_loss: 0.0919
 775/1000 [======================>.......] - ETA: 8:51 - loss: 0.8271 - regression_loss: 0.7352 - classification_loss: 0.0919
 776/1000 [======================>.......] - ETA: 8:49 - loss: 0.8271 - regression_loss: 0.7353 - classification_loss: 0.0919
 777/1000 [======================>.......] - ETA: 8:47 - loss: 0.8273 - regression_loss: 0.7354 - classification_loss: 0.0919
 778/1000 [======================>.......] - ETA: 8:44 - loss: 0.8266 - regression_loss: 0.7348 - classification_loss: 0.0918
 779/1000 [======================>.......] - ETA: 8:42 - loss: 0.8267 - regression_loss: 0.7349 - classification_loss: 0.0918
 780/1000 [======================>.......] - ETA: 8:40 - loss: 0.8267 - regression_loss: 0.7350 - classification_loss: 0.0918
 781/1000 [======================>.......] - ETA: 8:38 - loss: 0.8269 - regression_loss: 0.7351 - classification_loss: 0.0918
 782/1000 [======================>.......] - ETA: 8:36 - loss: 0.8268 - regression_loss: 0.7351 - classification_loss: 0.0918
 783/1000 [======================>.......] - ETA: 8:33 - loss: 0.8261 - regression_loss: 0.7344 - classification_loss: 0.0917
 784/1000 [======================>.......] - ETA: 8:31 - loss: 0.8259 - regression_loss: 0.7342 - classification_loss: 0.0917
 785/1000 [======================>.......] - ETA: 8:28 - loss: 0.8261 - regression_loss: 0.7344 - classification_loss: 0.0917
 786/1000 [======================>.......] - ETA: 8:26 - loss: 0.8264 - regression_loss: 0.7347 - classification_loss: 0.0917
 787/1000 [======================>.......] - ETA: 8:24 - loss: 0.8260 - regression_loss: 0.7344 - classification_loss: 0.0917
 788/1000 [======================>.......] - ETA: 8:22 - loss: 0.8262 - regression_loss: 0.7345 - classification_loss: 0.0917
 789/1000 [======================>.......] - ETA: 8:19 - loss: 0.8262 - regression_loss: 0.7346 - classification_loss: 0.0916
 790/1000 [======================>.......] - ETA: 8:17 - loss: 0.8264 - regression_loss: 0.7348 - classification_loss: 0.0916
 791/1000 [======================>.......] - ETA: 8:15 - loss: 0.8257 - regression_loss: 0.7341 - classification_loss: 0.0916
 792/1000 [======================>.......] - ETA: 8:13 - loss: 0.8258 - regression_loss: 0.7342 - classification_loss: 0.0916
 793/1000 [======================>.......] - ETA: 8:10 - loss: 0.8256 - regression_loss: 0.7341 - classification_loss: 0.0916
 794/1000 [======================>.......] - ETA: 8:08 - loss: 0.8255 - regression_loss: 0.7340 - classification_loss: 0.0916
 795/1000 [======================>.......] - ETA: 8:06 - loss: 0.8256 - regression_loss: 0.7341 - classification_loss: 0.0915
 796/1000 [======================>.......] - ETA: 8:04 - loss: 0.8255 - regression_loss: 0.7340 - classification_loss: 0.0915
 797/1000 [======================>.......] - ETA: 8:01 - loss: 0.8257 - regression_loss: 0.7342 - classification_loss: 0.0915
 798/1000 [======================>.......] - ETA: 7:59 - loss: 0.8258 - regression_loss: 0.7343 - classification_loss: 0.0915
 799/1000 [======================>.......] - ETA: 7:57 - loss: 0.8259 - regression_loss: 0.7344 - classification_loss: 0.0915
 800/1000 [=======================>......] - ETA: 7:54 - loss: 0.8252 - regression_loss: 0.7337 - classification_loss: 0.0914
 801/1000 [=======================>......] - ETA: 7:52 - loss: 0.8253 - regression_loss: 0.7339 - classification_loss: 0.0914
 802/1000 [=======================>......] - ETA: 7:50 - loss: 0.8256 - regression_loss: 0.7342 - classification_loss: 0.0914
 803/1000 [=======================>......] - ETA: 7:48 - loss: 0.8259 - regression_loss: 0.7345 - classification_loss: 0.0914
 804/1000 [=======================>......] - ETA: 7:46 - loss: 0.8259 - regression_loss: 0.7345 - classification_loss: 0.0914
 805/1000 [=======================>......] - ETA: 7:43 - loss: 0.8251 - regression_loss: 0.7338 - classification_loss: 0.0913
 806/1000 [=======================>......] - ETA: 7:41 - loss: 0.8252 - regression_loss: 0.7338 - classification_loss: 0.0913
 807/1000 [=======================>......] - ETA: 7:38 - loss: 0.8248 - regression_loss: 0.7335 - classification_loss: 0.0913
 808/1000 [=======================>......] - ETA: 7:36 - loss: 0.8250 - regression_loss: 0.7337 - classification_loss: 0.0913
 809/1000 [=======================>......] - ETA: 7:34 - loss: 0.8243 - regression_loss: 0.7331 - classification_loss: 0.0912
 810/1000 [=======================>......] - ETA: 7:32 - loss: 0.8243 - regression_loss: 0.7331 - classification_loss: 0.0912
 811/1000 [=======================>......] - ETA: 7:29 - loss: 0.8241 - regression_loss: 0.7329 - classification_loss: 0.0912
 812/1000 [=======================>......] - ETA: 7:27 - loss: 0.8242 - regression_loss: 0.7330 - classification_loss: 0.0912
 813/1000 [=======================>......] - ETA: 7:25 - loss: 0.8242 - regression_loss: 0.7330 - classification_loss: 0.0912
 814/1000 [=======================>......] - ETA: 7:22 - loss: 0.8240 - regression_loss: 0.7329 - classification_loss: 0.0912
 815/1000 [=======================>......] - ETA: 7:20 - loss: 0.8236 - regression_loss: 0.7325 - classification_loss: 0.0912
 816/1000 [=======================>......] - ETA: 7:17 - loss: 0.8229 - regression_loss: 0.7318 - classification_loss: 0.0911
 817/1000 [=======================>......] - ETA: 7:15 - loss: 0.8232 - regression_loss: 0.7321 - classification_loss: 0.0911
 818/1000 [=======================>......] - ETA: 7:13 - loss: 0.8233 - regression_loss: 0.7322 - classification_loss: 0.0911
 819/1000 [=======================>......] - ETA: 7:11 - loss: 0.8235 - regression_loss: 0.7324 - classification_loss: 0.0911
 820/1000 [=======================>......] - ETA: 7:08 - loss: 0.8235 - regression_loss: 0.7324 - classification_loss: 0.0911
 821/1000 [=======================>......] - ETA: 7:06 - loss: 0.8235 - regression_loss: 0.7325 - classification_loss: 0.0910
 822/1000 [=======================>......] - ETA: 7:04 - loss: 0.8233 - regression_loss: 0.7323 - classification_loss: 0.0910
 823/1000 [=======================>......] - ETA: 7:01 - loss: 0.8235 - regression_loss: 0.7325 - classification_loss: 0.0910
 824/1000 [=======================>......] - ETA: 6:59 - loss: 0.8237 - regression_loss: 0.7328 - classification_loss: 0.0910
 825/1000 [=======================>......] - ETA: 6:57 - loss: 0.8238 - regression_loss: 0.7328 - classification_loss: 0.0910
 826/1000 [=======================>......] - ETA: 6:55 - loss: 0.8237 - regression_loss: 0.7328 - classification_loss: 0.0910
 827/1000 [=======================>......] - ETA: 6:52 - loss: 0.8236 - regression_loss: 0.7327 - classification_loss: 0.0909
 828/1000 [=======================>......] - ETA: 6:50 - loss: 0.8238 - regression_loss: 0.7328 - classification_loss: 0.0909
 829/1000 [=======================>......] - ETA: 6:48 - loss: 0.8239 - regression_loss: 0.7330 - classification_loss: 0.0909
 830/1000 [=======================>......] - ETA: 6:45 - loss: 0.8239 - regression_loss: 0.7329 - classification_loss: 0.0909
 831/1000 [=======================>......] - ETA: 6:43 - loss: 0.8231 - regression_loss: 0.7323 - classification_loss: 0.0909
 832/1000 [=======================>......] - ETA: 6:41 - loss: 0.8231 - regression_loss: 0.7322 - classification_loss: 0.0909
 833/1000 [=======================>......] - ETA: 6:38 - loss: 0.8232 - regression_loss: 0.7324 - classification_loss: 0.0908
 834/1000 [========================>.....] - ETA: 6:36 - loss: 0.8232 - regression_loss: 0.7324 - classification_loss: 0.0908
 835/1000 [========================>.....] - ETA: 6:34 - loss: 0.8229 - regression_loss: 0.7321 - classification_loss: 0.0908
 836/1000 [========================>.....] - ETA: 6:31 - loss: 0.8225 - regression_loss: 0.7318 - classification_loss: 0.0908
 837/1000 [========================>.....] - ETA: 6:29 - loss: 0.8227 - regression_loss: 0.7319 - classification_loss: 0.0908
 838/1000 [========================>.....] - ETA: 6:27 - loss: 0.8227 - regression_loss: 0.7319 - classification_loss: 0.0908
 839/1000 [========================>.....] - ETA: 6:25 - loss: 0.8227 - regression_loss: 0.7319 - classification_loss: 0.0908
 840/1000 [========================>.....] - ETA: 6:22 - loss: 0.8219 - regression_loss: 0.7312 - classification_loss: 0.0907
 841/1000 [========================>.....] - ETA: 6:20 - loss: 0.8218 - regression_loss: 0.7311 - classification_loss: 0.0907
 842/1000 [========================>.....] - ETA: 6:18 - loss: 0.8219 - regression_loss: 0.7313 - classification_loss: 0.0907
 843/1000 [========================>.....] - ETA: 6:15 - loss: 0.8219 - regression_loss: 0.7312 - classification_loss: 0.0907
 844/1000 [========================>.....] - ETA: 6:13 - loss: 0.8219 - regression_loss: 0.7312 - classification_loss: 0.0907
 845/1000 [========================>.....] - ETA: 6:11 - loss: 0.8218 - regression_loss: 0.7311 - classification_loss: 0.0907
 846/1000 [========================>.....] - ETA: 6:08 - loss: 0.8219 - regression_loss: 0.7312 - classification_loss: 0.0907
 847/1000 [========================>.....] - ETA: 6:06 - loss: 0.8212 - regression_loss: 0.7306 - classification_loss: 0.0906
 848/1000 [========================>.....] - ETA: 6:04 - loss: 0.8211 - regression_loss: 0.7306 - classification_loss: 0.0906
 849/1000 [========================>.....] - ETA: 6:01 - loss: 0.8213 - regression_loss: 0.7307 - classification_loss: 0.0906
 850/1000 [========================>.....] - ETA: 5:59 - loss: 0.8214 - regression_loss: 0.7308 - classification_loss: 0.0906
 851/1000 [========================>.....] - ETA: 5:57 - loss: 0.8213 - regression_loss: 0.7308 - classification_loss: 0.0906
 852/1000 [========================>.....] - ETA: 5:54 - loss: 0.8212 - regression_loss: 0.7307 - classification_loss: 0.0905
 853/1000 [========================>.....] - ETA: 5:52 - loss: 0.8212 - regression_loss: 0.7307 - classification_loss: 0.0905
 854/1000 [========================>.....] - ETA: 5:50 - loss: 0.8210 - regression_loss: 0.7304 - classification_loss: 0.0905
 855/1000 [========================>.....] - ETA: 5:47 - loss: 0.8211 - regression_loss: 0.7306 - classification_loss: 0.0905
 856/1000 [========================>.....] - ETA: 5:45 - loss: 0.8204 - regression_loss: 0.7300 - classification_loss: 0.0904
 857/1000 [========================>.....] - ETA: 5:43 - loss: 0.8205 - regression_loss: 0.7301 - classification_loss: 0.0904
 858/1000 [========================>.....] - ETA: 5:40 - loss: 0.8206 - regression_loss: 0.7302 - classification_loss: 0.0904
 859/1000 [========================>.....] - ETA: 5:38 - loss: 0.8203 - regression_loss: 0.7299 - classification_loss: 0.0904
 860/1000 [========================>.....] - ETA: 5:36 - loss: 0.8203 - regression_loss: 0.7299 - classification_loss: 0.0904
 861/1000 [========================>.....] - ETA: 5:33 - loss: 0.8201 - regression_loss: 0.7297 - classification_loss: 0.0904
 862/1000 [========================>.....] - ETA: 5:31 - loss: 0.8201 - regression_loss: 0.7297 - classification_loss: 0.0904
 863/1000 [========================>.....] - ETA: 5:28 - loss: 0.8195 - regression_loss: 0.7291 - classification_loss: 0.0903
 864/1000 [========================>.....] - ETA: 5:26 - loss: 0.8196 - regression_loss: 0.7293 - classification_loss: 0.0903
 865/1000 [========================>.....] - ETA: 5:24 - loss: 0.8195 - regression_loss: 0.7292 - classification_loss: 0.0903
 866/1000 [========================>.....] - ETA: 5:22 - loss: 0.8196 - regression_loss: 0.7293 - classification_loss: 0.0903
 867/1000 [=========================>....] - ETA: 5:19 - loss: 0.8195 - regression_loss: 0.7293 - classification_loss: 0.0903
 868/1000 [=========================>....] - ETA: 5:17 - loss: 0.8195 - regression_loss: 0.7292 - classification_loss: 0.0903
 869/1000 [=========================>....] - ETA: 5:15 - loss: 0.8188 - regression_loss: 0.7286 - classification_loss: 0.0902
 870/1000 [=========================>....] - ETA: 5:12 - loss: 0.8186 - regression_loss: 0.7284 - classification_loss: 0.0902
 871/1000 [=========================>....] - ETA: 5:10 - loss: 0.8187 - regression_loss: 0.7285 - classification_loss: 0.0902
 872/1000 [=========================>....] - ETA: 5:07 - loss: 0.8187 - regression_loss: 0.7285 - classification_loss: 0.0901
 873/1000 [=========================>....] - ETA: 5:05 - loss: 0.8183 - regression_loss: 0.7282 - classification_loss: 0.0901
 874/1000 [=========================>....] - ETA: 5:03 - loss: 0.8184 - regression_loss: 0.7283 - classification_loss: 0.0901
 875/1000 [=========================>....] - ETA: 5:00 - loss: 0.8185 - regression_loss: 0.7284 - classification_loss: 0.0901
 876/1000 [=========================>....] - ETA: 4:58 - loss: 0.8181 - regression_loss: 0.7281 - classification_loss: 0.0901
 877/1000 [=========================>....] - ETA: 4:56 - loss: 0.8181 - regression_loss: 0.7281 - classification_loss: 0.0900
 878/1000 [=========================>....] - ETA: 4:53 - loss: 0.8175 - regression_loss: 0.7275 - classification_loss: 0.0900
 879/1000 [=========================>....] - ETA: 4:51 - loss: 0.8176 - regression_loss: 0.7277 - classification_loss: 0.0900
 880/1000 [=========================>....] - ETA: 4:49 - loss: 0.8177 - regression_loss: 0.7277 - classification_loss: 0.0900
 881/1000 [=========================>....] - ETA: 4:46 - loss: 0.8176 - regression_loss: 0.7276 - classification_loss: 0.0900
 882/1000 [=========================>....] - ETA: 4:44 - loss: 0.8177 - regression_loss: 0.7277 - classification_loss: 0.0900
 883/1000 [=========================>....] - ETA: 4:41 - loss: 0.8175 - regression_loss: 0.7276 - classification_loss: 0.0900
 884/1000 [=========================>....] - ETA: 4:39 - loss: 0.8176 - regression_loss: 0.7276 - classification_loss: 0.0900
 885/1000 [=========================>....] - ETA: 4:37 - loss: 0.8176 - regression_loss: 0.7277 - classification_loss: 0.0899
 886/1000 [=========================>....] - ETA: 4:34 - loss: 0.8173 - regression_loss: 0.7274 - classification_loss: 0.0899
 887/1000 [=========================>....] - ETA: 4:32 - loss: 0.8174 - regression_loss: 0.7275 - classification_loss: 0.0899
 888/1000 [=========================>....] - ETA: 4:30 - loss: 0.8174 - regression_loss: 0.7275 - classification_loss: 0.0899
 889/1000 [=========================>....] - ETA: 4:27 - loss: 0.8169 - regression_loss: 0.7271 - classification_loss: 0.0898
 890/1000 [=========================>....] - ETA: 4:25 - loss: 0.8170 - regression_loss: 0.7272 - classification_loss: 0.0898
 891/1000 [=========================>....] - ETA: 4:22 - loss: 0.8169 - regression_loss: 0.7271 - classification_loss: 0.0898
 892/1000 [=========================>....] - ETA: 4:20 - loss: 0.8168 - regression_loss: 0.7270 - classification_loss: 0.0898
 893/1000 [=========================>....] - ETA: 4:18 - loss: 0.8169 - regression_loss: 0.7271 - classification_loss: 0.0898
 894/1000 [=========================>....] - ETA: 4:15 - loss: 0.8169 - regression_loss: 0.7271 - classification_loss: 0.0898
 895/1000 [=========================>....] - ETA: 4:13 - loss: 0.8169 - regression_loss: 0.7271 - classification_loss: 0.0898
 896/1000 [=========================>....] - ETA: 4:10 - loss: 0.8161 - regression_loss: 0.7264 - classification_loss: 0.0897
 897/1000 [=========================>....] - ETA: 4:08 - loss: 0.8159 - regression_loss: 0.7262 - classification_loss: 0.0897
 898/1000 [=========================>....] - ETA: 4:06 - loss: 0.8161 - regression_loss: 0.7264 - classification_loss: 0.0897
 899/1000 [=========================>....] - ETA: 4:03 - loss: 0.8158 - regression_loss: 0.7261 - classification_loss: 0.0897
 900/1000 [==========================>...] - ETA: 4:01 - loss: 0.8157 - regression_loss: 0.7260 - classification_loss: 0.0897
 901/1000 [==========================>...] - ETA: 3:59 - loss: 0.8158 - regression_loss: 0.7261 - classification_loss: 0.0897
 902/1000 [==========================>...] - ETA: 3:56 - loss: 0.8160 - regression_loss: 0.7263 - classification_loss: 0.0897
 903/1000 [==========================>...] - ETA: 3:54 - loss: 0.8161 - regression_loss: 0.7264 - classification_loss: 0.0897
 904/1000 [==========================>...] - ETA: 3:52 - loss: 0.8154 - regression_loss: 0.7258 - classification_loss: 0.0896
 905/1000 [==========================>...] - ETA: 3:49 - loss: 0.8154 - regression_loss: 0.7258 - classification_loss: 0.0896
 906/1000 [==========================>...] - ETA: 3:47 - loss: 0.8155 - regression_loss: 0.7258 - classification_loss: 0.0896
 907/1000 [==========================>...] - ETA: 3:44 - loss: 0.8157 - regression_loss: 0.7260 - classification_loss: 0.0896
 908/1000 [==========================>...] - ETA: 3:42 - loss: 0.8156 - regression_loss: 0.7260 - classification_loss: 0.0896
 909/1000 [==========================>...] - ETA: 3:40 - loss: 0.8157 - regression_loss: 0.7261 - classification_loss: 0.0896
 910/1000 [==========================>...] - ETA: 3:37 - loss: 0.8157 - regression_loss: 0.7261 - classification_loss: 0.0896
 911/1000 [==========================>...] - ETA: 3:35 - loss: 0.8157 - regression_loss: 0.7261 - classification_loss: 0.0897
 912/1000 [==========================>...] - ETA: 3:32 - loss: 0.8154 - regression_loss: 0.7258 - classification_loss: 0.0896
 913/1000 [==========================>...] - ETA: 3:30 - loss: 0.8153 - regression_loss: 0.7257 - classification_loss: 0.0896
 914/1000 [==========================>...] - ETA: 3:27 - loss: 0.8146 - regression_loss: 0.7251 - classification_loss: 0.0895
 915/1000 [==========================>...] - ETA: 3:25 - loss: 0.8147 - regression_loss: 0.7252 - classification_loss: 0.0895
 916/1000 [==========================>...] - ETA: 3:23 - loss: 0.8147 - regression_loss: 0.7252 - classification_loss: 0.0895
 917/1000 [==========================>...] - ETA: 3:20 - loss: 0.8147 - regression_loss: 0.7252 - classification_loss: 0.0895
 918/1000 [==========================>...] - ETA: 3:18 - loss: 0.8144 - regression_loss: 0.7249 - classification_loss: 0.0895
 919/1000 [==========================>...] - ETA: 3:15 - loss: 0.8146 - regression_loss: 0.7251 - classification_loss: 0.0895
 920/1000 [==========================>...] - ETA: 3:13 - loss: 0.8143 - regression_loss: 0.7248 - classification_loss: 0.0895
 921/1000 [==========================>...] - ETA: 3:10 - loss: 0.8144 - regression_loss: 0.7249 - classification_loss: 0.0894
 922/1000 [==========================>...] - ETA: 3:08 - loss: 0.8145 - regression_loss: 0.7251 - classification_loss: 0.0894
 923/1000 [==========================>...] - ETA: 3:06 - loss: 0.8145 - regression_loss: 0.7251 - classification_loss: 0.0894
 924/1000 [==========================>...] - ETA: 3:03 - loss: 0.8147 - regression_loss: 0.7252 - classification_loss: 0.0894
 925/1000 [==========================>...] - ETA: 3:01 - loss: 0.8146 - regression_loss: 0.7252 - classification_loss: 0.0894
 926/1000 [==========================>...] - ETA: 2:58 - loss: 0.8142 - regression_loss: 0.7248 - classification_loss: 0.0894
 927/1000 [==========================>...] - ETA: 2:56 - loss: 0.8141 - regression_loss: 0.7247 - classification_loss: 0.0894
 928/1000 [==========================>...] - ETA: 2:53 - loss: 0.8141 - regression_loss: 0.7248 - classification_loss: 0.0894
 929/1000 [==========================>...] - ETA: 2:51 - loss: 0.8143 - regression_loss: 0.7249 - classification_loss: 0.0894
 930/1000 [==========================>...] - ETA: 2:49 - loss: 0.8143 - regression_loss: 0.7249 - classification_loss: 0.0893
 931/1000 [==========================>...] - ETA: 2:46 - loss: 0.8141 - regression_loss: 0.7248 - classification_loss: 0.0893
 932/1000 [==========================>...] - ETA: 2:44 - loss: 0.8135 - regression_loss: 0.7242 - classification_loss: 0.0893
 933/1000 [==========================>...] - ETA: 2:41 - loss: 0.8136 - regression_loss: 0.7243 - classification_loss: 0.0893
 934/1000 [===========================>..] - ETA: 2:39 - loss: 0.8133 - regression_loss: 0.7241 - classification_loss: 0.0892
 935/1000 [===========================>..] - ETA: 2:36 - loss: 0.8134 - regression_loss: 0.7242 - classification_loss: 0.0892
 936/1000 [===========================>..] - ETA: 2:34 - loss: 0.8134 - regression_loss: 0.7242 - classification_loss: 0.0892
 937/1000 [===========================>..] - ETA: 2:32 - loss: 0.8135 - regression_loss: 0.7243 - classification_loss: 0.0892
 938/1000 [===========================>..] - ETA: 2:29 - loss: 0.8134 - regression_loss: 0.7242 - classification_loss: 0.0892
 939/1000 [===========================>..] - ETA: 2:27 - loss: 0.8133 - regression_loss: 0.7241 - classification_loss: 0.0892
 940/1000 [===========================>..] - ETA: 2:24 - loss: 0.8127 - regression_loss: 0.7235 - classification_loss: 0.0891
 941/1000 [===========================>..] - ETA: 2:22 - loss: 0.8127 - regression_loss: 0.7236 - classification_loss: 0.0891
 942/1000 [===========================>..] - ETA: 2:20 - loss: 0.8129 - regression_loss: 0.7238 - classification_loss: 0.0891
 943/1000 [===========================>..] - ETA: 2:17 - loss: 0.8128 - regression_loss: 0.7237 - classification_loss: 0.0891
 944/1000 [===========================>..] - ETA: 2:15 - loss: 0.8129 - regression_loss: 0.7238 - classification_loss: 0.0891
 945/1000 [===========================>..] - ETA: 2:12 - loss: 0.8130 - regression_loss: 0.7239 - classification_loss: 0.0891
 946/1000 [===========================>..] - ETA: 2:10 - loss: 0.8130 - regression_loss: 0.7239 - classification_loss: 0.0891
 947/1000 [===========================>..] - ETA: 2:07 - loss: 0.8123 - regression_loss: 0.7233 - classification_loss: 0.0890
 948/1000 [===========================>..] - ETA: 2:05 - loss: 0.8123 - regression_loss: 0.7233 - classification_loss: 0.0890
 949/1000 [===========================>..] - ETA: 2:03 - loss: 0.8122 - regression_loss: 0.7232 - classification_loss: 0.0890
 950/1000 [===========================>..] - ETA: 2:00 - loss: 0.8122 - regression_loss: 0.7232 - classification_loss: 0.0890
 951/1000 [===========================>..] - ETA: 1:58 - loss: 0.8123 - regression_loss: 0.7233 - classification_loss: 0.0890
 952/1000 [===========================>..] - ETA: 1:55 - loss: 0.8120 - regression_loss: 0.7230 - classification_loss: 0.0890
 953/1000 [===========================>..] - ETA: 1:53 - loss: 0.8121 - regression_loss: 0.7231 - classification_loss: 0.0890
 954/1000 [===========================>..] - ETA: 1:50 - loss: 0.8115 - regression_loss: 0.7226 - classification_loss: 0.0889
 955/1000 [===========================>..] - ETA: 1:48 - loss: 0.8114 - regression_loss: 0.7225 - classification_loss: 0.0889
 956/1000 [===========================>..] - ETA: 1:46 - loss: 0.8115 - regression_loss: 0.7226 - classification_loss: 0.0889
 957/1000 [===========================>..] - ETA: 1:43 - loss: 0.8115 - regression_loss: 0.7226 - classification_loss: 0.0889
 958/1000 [===========================>..] - ETA: 1:41 - loss: 0.8114 - regression_loss: 0.7225 - classification_loss: 0.0889
 959/1000 [===========================>..] - ETA: 1:38 - loss: 0.8107 - regression_loss: 0.7219 - classification_loss: 0.0888
 960/1000 [===========================>..] - ETA: 1:36 - loss: 0.8109 - regression_loss: 0.7221 - classification_loss: 0.0888
 961/1000 [===========================>..] - ETA: 1:34 - loss: 0.8108 - regression_loss: 0.7220 - classification_loss: 0.0888
 962/1000 [===========================>..] - ETA: 1:31 - loss: 0.8109 - regression_loss: 0.7221 - classification_loss: 0.0888
 963/1000 [===========================>..] - ETA: 1:29 - loss: 0.8109 - regression_loss: 0.7221 - classification_loss: 0.0888
 964/1000 [===========================>..] - ETA: 1:26 - loss: 0.8110 - regression_loss: 0.7222 - classification_loss: 0.0888
 965/1000 [===========================>..] - ETA: 1:24 - loss: 0.8106 - regression_loss: 0.7219 - classification_loss: 0.0888
 966/1000 [===========================>..] - ETA: 1:21 - loss: 0.8106 - regression_loss: 0.7219 - classification_loss: 0.0888
 967/1000 [============================>.] - ETA: 1:19 - loss: 0.8100 - regression_loss: 0.7213 - classification_loss: 0.0887
 968/1000 [============================>.] - ETA: 1:17 - loss: 0.8099 - regression_loss: 0.7212 - classification_loss: 0.0887
 969/1000 [============================>.] - ETA: 1:14 - loss: 0.8098 - regression_loss: 0.7211 - classification_loss: 0.0887
 970/1000 [============================>.] - ETA: 1:12 - loss: 0.8099 - regression_loss: 0.7212 - classification_loss: 0.0887
 971/1000 [============================>.] - ETA: 1:09 - loss: 0.8096 - regression_loss: 0.7210 - classification_loss: 0.0886
 972/1000 [============================>.] - ETA: 1:07 - loss: 0.8090 - regression_loss: 0.7205 - classification_loss: 0.0886
 973/1000 [============================>.] - ETA: 1:05 - loss: 0.8091 - regression_loss: 0.7205 - classification_loss: 0.0886
 974/1000 [============================>.] - ETA: 1:02 - loss: 0.8090 - regression_loss: 0.7204 - classification_loss: 0.0886
 975/1000 [============================>.] - ETA: 1:00 - loss: 0.8091 - regression_loss: 0.7205 - classification_loss: 0.0886
 976/1000 [============================>.] - ETA: 57s - loss: 0.8085 - regression_loss: 0.7199 - classification_loss: 0.0885 
 977/1000 [============================>.] - ETA: 55s - loss: 0.8085 - regression_loss: 0.7200 - classification_loss: 0.0885
 978/1000 [============================>.] - ETA: 52s - loss: 0.8085 - regression_loss: 0.7200 - classification_loss: 0.0885
 979/1000 [============================>.] - ETA: 50s - loss: 0.8085 - regression_loss: 0.7200 - classification_loss: 0.0885
 980/1000 [============================>.] - ETA: 48s - loss: 0.8083 - regression_loss: 0.7199 - classification_loss: 0.0885
 981/1000 [============================>.] - ETA: 45s - loss: 0.8085 - regression_loss: 0.7200 - classification_loss: 0.0885
 982/1000 [============================>.] - ETA: 43s - loss: 0.8082 - regression_loss: 0.7197 - classification_loss: 0.0884
 983/1000 [============================>.] - ETA: 40s - loss: 0.8082 - regression_loss: 0.7198 - classification_loss: 0.0884
 984/1000 [============================>.] - ETA: 38s - loss: 0.8082 - regression_loss: 0.7198 - classification_loss: 0.0884
 985/1000 [============================>.] - ETA: 36s - loss: 0.8083 - regression_loss: 0.7199 - classification_loss: 0.0884
 986/1000 [============================>.] - ETA: 33s - loss: 0.8082 - regression_loss: 0.7198 - classification_loss: 0.0884
 987/1000 [============================>.] - ETA: 31s - loss: 0.8081 - regression_loss: 0.7198 - classification_loss: 0.0884
 988/1000 [============================>.] - ETA: 28s - loss: 0.8077 - regression_loss: 0.7193 - classification_loss: 0.0883
 989/1000 [============================>.] - ETA: 26s - loss: 0.8074 - regression_loss: 0.7191 - classification_loss: 0.0883
 990/1000 [============================>.] - ETA: 24s - loss: 0.8068 - regression_loss: 0.7185 - classification_loss: 0.0883
 991/1000 [============================>.] - ETA: 21s - loss: 0.8068 - regression_loss: 0.7185 - classification_loss: 0.0882
 992/1000 [============================>.] - ETA: 19s - loss: 0.8067 - regression_loss: 0.7185 - classification_loss: 0.0882
 993/1000 [============================>.] - ETA: 16s - loss: 0.8064 - regression_loss: 0.7182 - classification_loss: 0.0882
 994/1000 [============================>.] - ETA: 14s - loss: 0.8064 - regression_loss: 0.7182 - classification_loss: 0.0882
 995/1000 [============================>.] - ETA: 12s - loss: 0.8065 - regression_loss: 0.7183 - classification_loss: 0.0882
 996/1000 [============================>.] - ETA: 9s - loss: 0.8066 - regression_loss: 0.7184 - classification_loss: 0.0882 
 997/1000 [============================>.] - ETA: 7s - loss: 0.8062 - regression_loss: 0.7181 - classification_loss: 0.0882
 998/1000 [============================>.] - ETA: 4s - loss: 0.8063 - regression_loss: 0.7182 - classification_loss: 0.0882
 999/1000 [============================>.] - ETA: 2s - loss: 0.8063 - regression_loss: 0.7181 - classification_loss: 0.0881
1000/1000 [==============================] - 2405s 2s/step - loss: 0.8063 - regression_loss: 0.7182 - classification_loss: 0.0881

Epoch 00003: saving model to ./snapshots/resnet50_csv_03.h5
Epoch 4/10

   1/1000 [..............................] - ETA: 12:52 - loss: 0.3948 - regression_loss: 0.3650 - classification_loss: 0.0298
   2/1000 [..............................] - ETA: 27:21 - loss: 0.5932 - regression_loss: 0.5408 - classification_loss: 0.0524
   3/1000 [..............................] - ETA: 31:44 - loss: 0.6351 - regression_loss: 0.5729 - classification_loss: 0.0622
   4/1000 [..............................] - ETA: 35:00 - loss: 0.6964 - regression_loss: 0.6291 - classification_loss: 0.0672
   5/1000 [..............................] - ETA: 36:22 - loss: 0.7144 - regression_loss: 0.6458 - classification_loss: 0.0687
   6/1000 [..............................] - ETA: 34:36 - loss: 0.6237 - regression_loss: 0.5612 - classification_loss: 0.0624
   7/1000 [..............................] - ETA: 35:47 - loss: 0.6597 - regression_loss: 0.5949 - classification_loss: 0.0648
   8/1000 [..............................] - ETA: 34:51 - loss: 0.6586 - regression_loss: 0.5939 - classification_loss: 0.0648
   9/1000 [..............................] - ETA: 34:53 - loss: 0.6696 - regression_loss: 0.6037 - classification_loss: 0.0659
  10/1000 [..............................] - ETA: 35:24 - loss: 0.6728 - regression_loss: 0.6053 - classification_loss: 0.0675
  11/1000 [..............................] - ETA: 35:23 - loss: 0.6781 - regression_loss: 0.6102 - classification_loss: 0.0679
  12/1000 [..............................] - ETA: 35:51 - loss: 0.6887 - regression_loss: 0.6202 - classification_loss: 0.0686
  13/1000 [..............................] - ETA: 36:31 - loss: 0.7072 - regression_loss: 0.6374 - classification_loss: 0.0698
  14/1000 [..............................] - ETA: 35:42 - loss: 0.6752 - regression_loss: 0.6081 - classification_loss: 0.0670
  15/1000 [..............................] - ETA: 35:12 - loss: 0.6631 - regression_loss: 0.5962 - classification_loss: 0.0669
  16/1000 [..............................] - ETA: 35:28 - loss: 0.6646 - regression_loss: 0.5968 - classification_loss: 0.0678
  17/1000 [..............................] - ETA: 35:52 - loss: 0.6791 - regression_loss: 0.6106 - classification_loss: 0.0685
  18/1000 [..............................] - ETA: 36:09 - loss: 0.6859 - regression_loss: 0.6171 - classification_loss: 0.0689
  19/1000 [..............................] - ETA: 36:05 - loss: 0.6881 - regression_loss: 0.6191 - classification_loss: 0.0691
  20/1000 [..............................] - ETA: 36:22 - loss: 0.6985 - regression_loss: 0.6289 - classification_loss: 0.0696
  21/1000 [..............................] - ETA: 36:45 - loss: 0.7098 - regression_loss: 0.6394 - classification_loss: 0.0704
  22/1000 [..............................] - ETA: 36:20 - loss: 0.7025 - regression_loss: 0.6323 - classification_loss: 0.0702
  23/1000 [..............................] - ETA: 36:27 - loss: 0.7038 - regression_loss: 0.6330 - classification_loss: 0.0708
  24/1000 [..............................] - ETA: 35:58 - loss: 0.6815 - regression_loss: 0.6124 - classification_loss: 0.0690
  25/1000 [..............................] - ETA: 36:05 - loss: 0.6823 - regression_loss: 0.6127 - classification_loss: 0.0696
  26/1000 [..............................] - ETA: 35:45 - loss: 0.6741 - regression_loss: 0.6047 - classification_loss: 0.0694
  27/1000 [..............................] - ETA: 35:55 - loss: 0.6784 - regression_loss: 0.6088 - classification_loss: 0.0696
  28/1000 [..............................] - ETA: 35:30 - loss: 0.6621 - regression_loss: 0.5939 - classification_loss: 0.0682
  29/1000 [..............................] - ETA: 35:46 - loss: 0.6705 - regression_loss: 0.6018 - classification_loss: 0.0687
  30/1000 [..............................] - ETA: 35:56 - loss: 0.6776 - regression_loss: 0.6085 - classification_loss: 0.0691
  31/1000 [..............................] - ETA: 35:52 - loss: 0.6808 - regression_loss: 0.6115 - classification_loss: 0.0693
  32/1000 [..............................] - ETA: 35:56 - loss: 0.6813 - regression_loss: 0.6116 - classification_loss: 0.0697
  33/1000 [..............................] - ETA: 35:34 - loss: 0.6667 - regression_loss: 0.5982 - classification_loss: 0.0685
  34/1000 [>.............................] - ETA: 35:41 - loss: 0.6710 - regression_loss: 0.6022 - classification_loss: 0.0687
  35/1000 [>.............................] - ETA: 35:53 - loss: 0.6773 - regression_loss: 0.6082 - classification_loss: 0.0691
  36/1000 [>.............................] - ETA: 35:37 - loss: 0.6767 - regression_loss: 0.6074 - classification_loss: 0.0693
  37/1000 [>.............................] - ETA: 35:45 - loss: 0.6832 - regression_loss: 0.6136 - classification_loss: 0.0696
  38/1000 [>.............................] - ETA: 35:41 - loss: 0.6861 - regression_loss: 0.6164 - classification_loss: 0.0697
  39/1000 [>.............................] - ETA: 35:26 - loss: 0.6811 - regression_loss: 0.6114 - classification_loss: 0.0697
  40/1000 [>.............................] - ETA: 35:32 - loss: 0.6847 - regression_loss: 0.6149 - classification_loss: 0.0698
  41/1000 [>.............................] - ETA: 35:14 - loss: 0.6738 - regression_loss: 0.6048 - classification_loss: 0.0689
  42/1000 [>.............................] - ETA: 35:24 - loss: 0.6789 - regression_loss: 0.6096 - classification_loss: 0.0693
  43/1000 [>.............................] - ETA: 35:20 - loss: 0.6808 - regression_loss: 0.6114 - classification_loss: 0.0694
  44/1000 [>.............................] - ETA: 35:23 - loss: 0.6810 - regression_loss: 0.6113 - classification_loss: 0.0697
  45/1000 [>.............................] - ETA: 35:28 - loss: 0.6854 - regression_loss: 0.6156 - classification_loss: 0.0699
  46/1000 [>.............................] - ETA: 35:12 - loss: 0.6743 - regression_loss: 0.6053 - classification_loss: 0.0690
  47/1000 [>.............................] - ETA: 35:16 - loss: 0.6757 - regression_loss: 0.6064 - classification_loss: 0.0693
  48/1000 [>.............................] - ETA: 35:21 - loss: 0.6810 - regression_loss: 0.6113 - classification_loss: 0.0696
  49/1000 [>.............................] - ETA: 35:09 - loss: 0.6808 - regression_loss: 0.6107 - classification_loss: 0.0701
  50/1000 [>.............................] - ETA: 35:17 - loss: 0.6848 - regression_loss: 0.6144 - classification_loss: 0.0704
  51/1000 [>.............................] - ETA: 35:14 - loss: 0.6860 - regression_loss: 0.6156 - classification_loss: 0.0704
  52/1000 [>.............................] - ETA: 35:17 - loss: 0.6883 - regression_loss: 0.6178 - classification_loss: 0.0705
  53/1000 [>.............................] - ETA: 35:21 - loss: 0.6922 - regression_loss: 0.6216 - classification_loss: 0.0707
  54/1000 [>.............................] - ETA: 35:24 - loss: 0.6939 - regression_loss: 0.6232 - classification_loss: 0.0707
  55/1000 [>.............................] - ETA: 35:21 - loss: 0.6950 - regression_loss: 0.6242 - classification_loss: 0.0708
  56/1000 [>.............................] - ETA: 35:22 - loss: 0.6952 - regression_loss: 0.6242 - classification_loss: 0.0710
  57/1000 [>.............................] - ETA: 35:08 - loss: 0.6879 - regression_loss: 0.6175 - classification_loss: 0.0703
  58/1000 [>.............................] - ETA: 35:14 - loss: 0.6910 - regression_loss: 0.6205 - classification_loss: 0.0705
  59/1000 [>.............................] - ETA: 35:04 - loss: 0.6878 - regression_loss: 0.6173 - classification_loss: 0.0704
  60/1000 [>.............................] - ETA: 34:51 - loss: 0.6793 - regression_loss: 0.6095 - classification_loss: 0.0698
  61/1000 [>.............................] - ETA: 34:53 - loss: 0.6794 - regression_loss: 0.6094 - classification_loss: 0.0699
  62/1000 [>.............................] - ETA: 34:43 - loss: 0.6764 - regression_loss: 0.6065 - classification_loss: 0.0699
  63/1000 [>.............................] - ETA: 34:40 - loss: 0.6774 - regression_loss: 0.6075 - classification_loss: 0.0699
  64/1000 [>.............................] - ETA: 34:45 - loss: 0.6805 - regression_loss: 0.6104 - classification_loss: 0.0701
  65/1000 [>.............................] - ETA: 34:48 - loss: 0.6835 - regression_loss: 0.6132 - classification_loss: 0.0702
  66/1000 [>.............................] - ETA: 34:50 - loss: 0.6848 - regression_loss: 0.6146 - classification_loss: 0.0703
  67/1000 [=>............................] - ETA: 34:53 - loss: 0.6875 - regression_loss: 0.6171 - classification_loss: 0.0704
  68/1000 [=>............................] - ETA: 34:57 - loss: 0.6900 - regression_loss: 0.6195 - classification_loss: 0.0705
  69/1000 [=>............................] - ETA: 34:59 - loss: 0.6911 - regression_loss: 0.6205 - classification_loss: 0.0706
  70/1000 [=>............................] - ETA: 34:55 - loss: 0.6917 - regression_loss: 0.6212 - classification_loss: 0.0706
  71/1000 [=>............................] - ETA: 34:46 - loss: 0.6889 - regression_loss: 0.6185 - classification_loss: 0.0705
  72/1000 [=>............................] - ETA: 34:47 - loss: 0.6888 - regression_loss: 0.6182 - classification_loss: 0.0706
  73/1000 [=>............................] - ETA: 34:36 - loss: 0.6841 - regression_loss: 0.6140 - classification_loss: 0.0700
  74/1000 [=>............................] - ETA: 34:33 - loss: 0.6848 - regression_loss: 0.6148 - classification_loss: 0.0700
  75/1000 [=>............................] - ETA: 34:25 - loss: 0.6817 - regression_loss: 0.6118 - classification_loss: 0.0699
  76/1000 [=>............................] - ETA: 34:28 - loss: 0.6845 - regression_loss: 0.6144 - classification_loss: 0.0701
  77/1000 [=>............................] - ETA: 34:31 - loss: 0.6876 - regression_loss: 0.6173 - classification_loss: 0.0702
  78/1000 [=>............................] - ETA: 34:32 - loss: 0.6888 - regression_loss: 0.6185 - classification_loss: 0.0703
  79/1000 [=>............................] - ETA: 34:22 - loss: 0.6829 - regression_loss: 0.6131 - classification_loss: 0.0697
  80/1000 [=>............................] - ETA: 34:22 - loss: 0.6828 - regression_loss: 0.6130 - classification_loss: 0.0699
  81/1000 [=>............................] - ETA: 34:19 - loss: 0.6834 - regression_loss: 0.6135 - classification_loss: 0.0699
  82/1000 [=>............................] - ETA: 34:19 - loss: 0.6832 - regression_loss: 0.6132 - classification_loss: 0.0700
  83/1000 [=>............................] - ETA: 34:20 - loss: 0.6843 - regression_loss: 0.6142 - classification_loss: 0.0700
  84/1000 [=>............................] - ETA: 34:23 - loss: 0.6863 - regression_loss: 0.6161 - classification_loss: 0.0702
  85/1000 [=>............................] - ETA: 34:14 - loss: 0.6799 - regression_loss: 0.6102 - classification_loss: 0.0697
  86/1000 [=>............................] - ETA: 34:06 - loss: 0.6781 - regression_loss: 0.6084 - classification_loss: 0.0696
  87/1000 [=>............................] - ETA: 34:08 - loss: 0.6802 - regression_loss: 0.6104 - classification_loss: 0.0697
  88/1000 [=>............................] - ETA: 34:11 - loss: 0.6821 - regression_loss: 0.6123 - classification_loss: 0.0699
  89/1000 [=>............................] - ETA: 34:12 - loss: 0.6831 - regression_loss: 0.6132 - classification_loss: 0.0699
  90/1000 [=>............................] - ETA: 34:13 - loss: 0.6849 - regression_loss: 0.6150 - classification_loss: 0.0700
  91/1000 [=>............................] - ETA: 34:05 - loss: 0.6831 - regression_loss: 0.6133 - classification_loss: 0.0699
  92/1000 [=>............................] - ETA: 34:02 - loss: 0.6836 - regression_loss: 0.6138 - classification_loss: 0.0699
  93/1000 [=>............................] - ETA: 34:02 - loss: 0.6835 - regression_loss: 0.6135 - classification_loss: 0.0700
  94/1000 [=>............................] - ETA: 33:53 - loss: 0.6779 - regression_loss: 0.6084 - classification_loss: 0.0696
  95/1000 [=>............................] - ETA: 33:55 - loss: 0.6799 - regression_loss: 0.6102 - classification_loss: 0.0697
  96/1000 [=>............................] - ETA: 33:52 - loss: 0.6805 - regression_loss: 0.6107 - classification_loss: 0.0697
  97/1000 [=>............................] - ETA: 33:51 - loss: 0.6803 - regression_loss: 0.6104 - classification_loss: 0.0698
  98/1000 [=>............................] - ETA: 33:53 - loss: 0.6820 - regression_loss: 0.6121 - classification_loss: 0.0699
  99/1000 [=>............................] - ETA: 33:45 - loss: 0.6768 - regression_loss: 0.6073 - classification_loss: 0.0695
 100/1000 [==>...........................] - ETA: 33:38 - loss: 0.6750 - regression_loss: 0.6055 - classification_loss: 0.0695
 101/1000 [==>...........................] - ETA: 33:38 - loss: 0.6763 - regression_loss: 0.6067 - classification_loss: 0.0696
 102/1000 [==>...........................] - ETA: 33:31 - loss: 0.6741 - regression_loss: 0.6046 - classification_loss: 0.0696
 103/1000 [==>...........................] - ETA: 33:32 - loss: 0.6751 - regression_loss: 0.6055 - classification_loss: 0.0696
 104/1000 [==>...........................] - ETA: 33:29 - loss: 0.6755 - regression_loss: 0.6059 - classification_loss: 0.0696
 105/1000 [==>...........................] - ETA: 33:21 - loss: 0.6707 - regression_loss: 0.6015 - classification_loss: 0.0692
 106/1000 [==>...........................] - ETA: 33:22 - loss: 0.6726 - regression_loss: 0.6033 - classification_loss: 0.0693
 107/1000 [==>...........................] - ETA: 33:24 - loss: 0.6744 - regression_loss: 0.6050 - classification_loss: 0.0695
 108/1000 [==>...........................] - ETA: 33:23 - loss: 0.6742 - regression_loss: 0.6047 - classification_loss: 0.0695
 109/1000 [==>...........................] - ETA: 33:20 - loss: 0.6746 - regression_loss: 0.6050 - classification_loss: 0.0695
 110/1000 [==>...........................] - ETA: 33:14 - loss: 0.6736 - regression_loss: 0.6039 - classification_loss: 0.0697
 111/1000 [==>...........................] - ETA: 33:06 - loss: 0.6695 - regression_loss: 0.6002 - classification_loss: 0.0693
 112/1000 [==>...........................] - ETA: 33:07 - loss: 0.6704 - regression_loss: 0.6010 - classification_loss: 0.0695
 113/1000 [==>...........................] - ETA: 33:07 - loss: 0.6724 - regression_loss: 0.6027 - classification_loss: 0.0697
 114/1000 [==>...........................] - ETA: 33:09 - loss: 0.6741 - regression_loss: 0.6042 - classification_loss: 0.0698
 115/1000 [==>...........................] - ETA: 33:08 - loss: 0.6740 - regression_loss: 0.6041 - classification_loss: 0.0699
 116/1000 [==>...........................] - ETA: 33:08 - loss: 0.6740 - regression_loss: 0.6040 - classification_loss: 0.0700
 117/1000 [==>...........................] - ETA: 33:01 - loss: 0.6728 - regression_loss: 0.6028 - classification_loss: 0.0700
 118/1000 [==>...........................] - ETA: 33:03 - loss: 0.6746 - regression_loss: 0.6045 - classification_loss: 0.0701
 119/1000 [==>...........................] - ETA: 33:00 - loss: 0.6751 - regression_loss: 0.6049 - classification_loss: 0.0702
 120/1000 [==>...........................] - ETA: 33:00 - loss: 0.6768 - regression_loss: 0.6065 - classification_loss: 0.0703
 121/1000 [==>...........................] - ETA: 33:00 - loss: 0.6775 - regression_loss: 0.6072 - classification_loss: 0.0703
 122/1000 [==>...........................] - ETA: 32:53 - loss: 0.6735 - regression_loss: 0.6035 - classification_loss: 0.0700
 123/1000 [==>...........................] - ETA: 32:53 - loss: 0.6752 - regression_loss: 0.6051 - classification_loss: 0.0701
 124/1000 [==>...........................] - ETA: 32:47 - loss: 0.6741 - regression_loss: 0.6040 - classification_loss: 0.0701
 125/1000 [==>...........................] - ETA: 32:47 - loss: 0.6742 - regression_loss: 0.6039 - classification_loss: 0.0703
 126/1000 [==>...........................] - ETA: 32:44 - loss: 0.6747 - regression_loss: 0.6044 - classification_loss: 0.0703
 127/1000 [==>...........................] - ETA: 32:43 - loss: 0.6755 - regression_loss: 0.6052 - classification_loss: 0.0703
 128/1000 [==>...........................] - ETA: 32:44 - loss: 0.6774 - regression_loss: 0.6070 - classification_loss: 0.0704
 129/1000 [==>...........................] - ETA: 32:37 - loss: 0.6738 - regression_loss: 0.6034 - classification_loss: 0.0703
 130/1000 [==>...........................] - ETA: 32:37 - loss: 0.6753 - regression_loss: 0.6049 - classification_loss: 0.0704
 131/1000 [==>...........................] - ETA: 32:36 - loss: 0.6752 - regression_loss: 0.6047 - classification_loss: 0.0704
 132/1000 [==>...........................] - ETA: 32:34 - loss: 0.6756 - regression_loss: 0.6052 - classification_loss: 0.0705
 133/1000 [==>...........................] - ETA: 32:34 - loss: 0.6770 - regression_loss: 0.6064 - classification_loss: 0.0705
 134/1000 [===>..........................] - ETA: 32:28 - loss: 0.6735 - regression_loss: 0.6032 - classification_loss: 0.0702
 135/1000 [===>..........................] - ETA: 32:27 - loss: 0.6742 - regression_loss: 0.6040 - classification_loss: 0.0702
 136/1000 [===>..........................] - ETA: 32:22 - loss: 0.6733 - regression_loss: 0.6031 - classification_loss: 0.0702
 137/1000 [===>..........................] - ETA: 32:22 - loss: 0.6747 - regression_loss: 0.6044 - classification_loss: 0.0703
 138/1000 [===>..........................] - ETA: 32:22 - loss: 0.6761 - regression_loss: 0.6057 - classification_loss: 0.0704
 139/1000 [===>..........................] - ETA: 32:19 - loss: 0.6764 - regression_loss: 0.6060 - classification_loss: 0.0704
 140/1000 [===>..........................] - ETA: 32:18 - loss: 0.6762 - regression_loss: 0.6058 - classification_loss: 0.0704
 141/1000 [===>..........................] - ETA: 32:12 - loss: 0.6724 - regression_loss: 0.6023 - classification_loss: 0.0701
 142/1000 [===>..........................] - ETA: 32:11 - loss: 0.6731 - regression_loss: 0.6030 - classification_loss: 0.0701
 143/1000 [===>..........................] - ETA: 32:06 - loss: 0.6716 - regression_loss: 0.6015 - classification_loss: 0.0701
 144/1000 [===>..........................] - ETA: 32:05 - loss: 0.6718 - regression_loss: 0.6016 - classification_loss: 0.0702
 145/1000 [===>..........................] - ETA: 31:58 - loss: 0.6688 - regression_loss: 0.5989 - classification_loss: 0.0699
 146/1000 [===>..........................] - ETA: 31:56 - loss: 0.6695 - regression_loss: 0.5995 - classification_loss: 0.0699
 147/1000 [===>..........................] - ETA: 31:56 - loss: 0.6706 - regression_loss: 0.6006 - classification_loss: 0.0700
 148/1000 [===>..........................] - ETA: 31:56 - loss: 0.6714 - regression_loss: 0.6014 - classification_loss: 0.0700
 149/1000 [===>..........................] - ETA: 31:56 - loss: 0.6728 - regression_loss: 0.6028 - classification_loss: 0.0700
 150/1000 [===>..........................] - ETA: 31:51 - loss: 0.6716 - regression_loss: 0.6016 - classification_loss: 0.0700
 151/1000 [===>..........................] - ETA: 31:45 - loss: 0.6702 - regression_loss: 0.6002 - classification_loss: 0.0700
 152/1000 [===>..........................] - ETA: 31:43 - loss: 0.6704 - regression_loss: 0.6005 - classification_loss: 0.0700
 153/1000 [===>..........................] - ETA: 31:37 - loss: 0.6673 - regression_loss: 0.5976 - classification_loss: 0.0697
 154/1000 [===>..........................] - ETA: 31:37 - loss: 0.6687 - regression_loss: 0.5990 - classification_loss: 0.0698
 155/1000 [===>..........................] - ETA: 31:37 - loss: 0.6697 - regression_loss: 0.5999 - classification_loss: 0.0698
 156/1000 [===>..........................] - ETA: 31:35 - loss: 0.6697 - regression_loss: 0.5998 - classification_loss: 0.0699
 157/1000 [===>..........................] - ETA: 31:35 - loss: 0.6709 - regression_loss: 0.6010 - classification_loss: 0.0699
 158/1000 [===>..........................] - ETA: 31:34 - loss: 0.6709 - regression_loss: 0.6009 - classification_loss: 0.0700
 159/1000 [===>..........................] - ETA: 31:34 - loss: 0.6720 - regression_loss: 0.6020 - classification_loss: 0.0700
 160/1000 [===>..........................] - ETA: 31:29 - loss: 0.6718 - regression_loss: 0.6018 - classification_loss: 0.0700
 161/1000 [===>..........................] - ETA: 31:24 - loss: 0.6685 - regression_loss: 0.5988 - classification_loss: 0.0697
 162/1000 [===>..........................] - ETA: 31:23 - loss: 0.6698 - regression_loss: 0.6000 - classification_loss: 0.0698
 163/1000 [===>..........................] - ETA: 31:23 - loss: 0.6704 - regression_loss: 0.6006 - classification_loss: 0.0698
 164/1000 [===>..........................] - ETA: 31:20 - loss: 0.6711 - regression_loss: 0.6012 - classification_loss: 0.0699
 165/1000 [===>..........................] - ETA: 31:15 - loss: 0.6700 - regression_loss: 0.6001 - classification_loss: 0.0699
 166/1000 [===>..........................] - ETA: 31:16 - loss: 0.6713 - regression_loss: 0.6013 - classification_loss: 0.0700
 167/1000 [====>.........................] - ETA: 31:10 - loss: 0.6693 - regression_loss: 0.5995 - classification_loss: 0.0698
 168/1000 [====>.........................] - ETA: 31:09 - loss: 0.6692 - regression_loss: 0.5994 - classification_loss: 0.0698
 169/1000 [====>.........................] - ETA: 31:08 - loss: 0.6702 - regression_loss: 0.6004 - classification_loss: 0.0698
 170/1000 [====>.........................] - ETA: 31:06 - loss: 0.6704 - regression_loss: 0.6006 - classification_loss: 0.0698
 171/1000 [====>.........................] - ETA: 31:05 - loss: 0.6709 - regression_loss: 0.6011 - classification_loss: 0.0699
 172/1000 [====>.........................] - ETA: 31:04 - loss: 0.6719 - regression_loss: 0.6020 - classification_loss: 0.0699
 173/1000 [====>.........................] - ETA: 30:59 - loss: 0.6689 - regression_loss: 0.5993 - classification_loss: 0.0696
 174/1000 [====>.........................] - ETA: 30:59 - loss: 0.6701 - regression_loss: 0.6004 - classification_loss: 0.0697
 175/1000 [====>.........................] - ETA: 30:56 - loss: 0.6706 - regression_loss: 0.6009 - classification_loss: 0.0697
 176/1000 [====>.........................] - ETA: 30:55 - loss: 0.6705 - regression_loss: 0.6008 - classification_loss: 0.0697
 177/1000 [====>.........................] - ETA: 30:54 - loss: 0.6710 - regression_loss: 0.6012 - classification_loss: 0.0697
 178/1000 [====>.........................] - ETA: 30:49 - loss: 0.6700 - regression_loss: 0.6003 - classification_loss: 0.0698
 179/1000 [====>.........................] - ETA: 30:45 - loss: 0.6688 - regression_loss: 0.5991 - classification_loss: 0.0697
 180/1000 [====>.........................] - ETA: 30:42 - loss: 0.6694 - regression_loss: 0.5997 - classification_loss: 0.0697
 181/1000 [====>.........................] - ETA: 30:41 - loss: 0.6702 - regression_loss: 0.6004 - classification_loss: 0.0698
 182/1000 [====>.........................] - ETA: 30:40 - loss: 0.6703 - regression_loss: 0.6005 - classification_loss: 0.0698
 183/1000 [====>.........................] - ETA: 30:35 - loss: 0.6680 - regression_loss: 0.5984 - classification_loss: 0.0696
 184/1000 [====>.........................] - ETA: 30:34 - loss: 0.6693 - regression_loss: 0.5996 - classification_loss: 0.0696
 185/1000 [====>.........................] - ETA: 30:34 - loss: 0.6705 - regression_loss: 0.6008 - classification_loss: 0.0697
 186/1000 [====>.........................] - ETA: 30:33 - loss: 0.6718 - regression_loss: 0.6020 - classification_loss: 0.0698
 187/1000 [====>.........................] - ETA: 30:33 - loss: 0.6728 - regression_loss: 0.6029 - classification_loss: 0.0698
 188/1000 [====>.........................] - ETA: 30:28 - loss: 0.6701 - regression_loss: 0.6005 - classification_loss: 0.0696
 189/1000 [====>.........................] - ETA: 30:26 - loss: 0.6703 - regression_loss: 0.6007 - classification_loss: 0.0696
 190/1000 [====>.........................] - ETA: 30:24 - loss: 0.6703 - regression_loss: 0.6006 - classification_loss: 0.0697
 191/1000 [====>.........................] - ETA: 30:20 - loss: 0.6707 - regression_loss: 0.6009 - classification_loss: 0.0698
 192/1000 [====>.........................] - ETA: 30:19 - loss: 0.6712 - regression_loss: 0.6014 - classification_loss: 0.0699
 193/1000 [====>.........................] - ETA: 30:14 - loss: 0.6712 - regression_loss: 0.6013 - classification_loss: 0.0699
 194/1000 [====>.........................] - ETA: 30:12 - loss: 0.6714 - regression_loss: 0.6015 - classification_loss: 0.0699
 195/1000 [====>.........................] - ETA: 30:10 - loss: 0.6714 - regression_loss: 0.6014 - classification_loss: 0.0700
 196/1000 [====>.........................] - ETA: 30:09 - loss: 0.6723 - regression_loss: 0.6023 - classification_loss: 0.0700
 197/1000 [====>.........................] - ETA: 30:08 - loss: 0.6728 - regression_loss: 0.6027 - classification_loss: 0.0700
 198/1000 [====>.........................] - ETA: 30:03 - loss: 0.6706 - regression_loss: 0.6007 - classification_loss: 0.0699
 199/1000 [====>.........................] - ETA: 30:03 - loss: 0.6716 - regression_loss: 0.6016 - classification_loss: 0.0699
 200/1000 [=====>........................] - ETA: 29:58 - loss: 0.6689 - regression_loss: 0.5992 - classification_loss: 0.0697
 201/1000 [=====>........................] - ETA: 29:57 - loss: 0.6694 - regression_loss: 0.5997 - classification_loss: 0.0697
 202/1000 [=====>........................] - ETA: 29:54 - loss: 0.6696 - regression_loss: 0.5999 - classification_loss: 0.0697
 203/1000 [=====>........................] - ETA: 29:54 - loss: 0.6706 - regression_loss: 0.6008 - classification_loss: 0.0698
 204/1000 [=====>........................] - ETA: 29:53 - loss: 0.6715 - regression_loss: 0.6017 - classification_loss: 0.0698
 205/1000 [=====>........................] - ETA: 29:49 - loss: 0.6714 - regression_loss: 0.6015 - classification_loss: 0.0699
 206/1000 [=====>........................] - ETA: 29:47 - loss: 0.6713 - regression_loss: 0.6014 - classification_loss: 0.0699
 207/1000 [=====>........................] - ETA: 29:46 - loss: 0.6717 - regression_loss: 0.6018 - classification_loss: 0.0699
 208/1000 [=====>........................] - ETA: 29:43 - loss: 0.6718 - regression_loss: 0.6019 - classification_loss: 0.0699
 209/1000 [=====>........................] - ETA: 29:42 - loss: 0.6716 - regression_loss: 0.6017 - classification_loss: 0.0699
 210/1000 [=====>........................] - ETA: 29:37 - loss: 0.6694 - regression_loss: 0.5996 - classification_loss: 0.0697
 211/1000 [=====>........................] - ETA: 29:37 - loss: 0.6702 - regression_loss: 0.6004 - classification_loss: 0.0698
 212/1000 [=====>........................] - ETA: 29:36 - loss: 0.6710 - regression_loss: 0.6012 - classification_loss: 0.0698
 213/1000 [=====>........................] - ETA: 29:32 - loss: 0.6700 - regression_loss: 0.6002 - classification_loss: 0.0698
 214/1000 [=====>........................] - ETA: 29:30 - loss: 0.6699 - regression_loss: 0.6001 - classification_loss: 0.0698
 215/1000 [=====>........................] - ETA: 29:29 - loss: 0.6702 - regression_loss: 0.6004 - classification_loss: 0.0698
 216/1000 [=====>........................] - ETA: 29:26 - loss: 0.6703 - regression_loss: 0.6005 - classification_loss: 0.0698
 217/1000 [=====>........................] - ETA: 29:26 - loss: 0.6711 - regression_loss: 0.6013 - classification_loss: 0.0698
 218/1000 [=====>........................] - ETA: 29:22 - loss: 0.6702 - regression_loss: 0.6005 - classification_loss: 0.0698
 219/1000 [=====>........................] - ETA: 29:21 - loss: 0.6710 - regression_loss: 0.6012 - classification_loss: 0.0698
 220/1000 [=====>........................] - ETA: 29:16 - loss: 0.6686 - regression_loss: 0.5990 - classification_loss: 0.0696
 221/1000 [=====>........................] - ETA: 29:15 - loss: 0.6684 - regression_loss: 0.5988 - classification_loss: 0.0697
 222/1000 [=====>........................] - ETA: 29:14 - loss: 0.6692 - regression_loss: 0.5996 - classification_loss: 0.0697
 223/1000 [=====>........................] - ETA: 29:13 - loss: 0.6696 - regression_loss: 0.5999 - classification_loss: 0.0697
 224/1000 [=====>........................] - ETA: 29:08 - loss: 0.6673 - regression_loss: 0.5979 - classification_loss: 0.0695
 225/1000 [=====>........................] - ETA: 29:04 - loss: 0.6663 - regression_loss: 0.5968 - classification_loss: 0.0694
 226/1000 [=====>........................] - ETA: 29:03 - loss: 0.6671 - regression_loss: 0.5976 - classification_loss: 0.0695
 227/1000 [=====>........................] - ETA: 29:01 - loss: 0.6672 - regression_loss: 0.5977 - classification_loss: 0.0695
 228/1000 [=====>........................] - ETA: 28:58 - loss: 0.6673 - regression_loss: 0.5978 - classification_loss: 0.0695
 229/1000 [=====>........................] - ETA: 28:57 - loss: 0.6681 - regression_loss: 0.5985 - classification_loss: 0.0695
 230/1000 [=====>........................] - ETA: 28:56 - loss: 0.6688 - regression_loss: 0.5992 - classification_loss: 0.0696
 231/1000 [=====>........................] - ETA: 28:55 - loss: 0.6690 - regression_loss: 0.5995 - classification_loss: 0.0696
 232/1000 [=====>........................] - ETA: 28:51 - loss: 0.6683 - regression_loss: 0.5988 - classification_loss: 0.0695
 233/1000 [=====>........................] - ETA: 28:49 - loss: 0.6683 - regression_loss: 0.5987 - classification_loss: 0.0696
 234/1000 [======>.......................] - ETA: 28:45 - loss: 0.6665 - regression_loss: 0.5971 - classification_loss: 0.0694
 235/1000 [======>.......................] - ETA: 28:42 - loss: 0.6667 - regression_loss: 0.5972 - classification_loss: 0.0694
 236/1000 [======>.......................] - ETA: 28:41 - loss: 0.6674 - regression_loss: 0.5980 - classification_loss: 0.0694
 237/1000 [======>.......................] - ETA: 28:37 - loss: 0.6653 - regression_loss: 0.5961 - classification_loss: 0.0693
 238/1000 [======>.......................] - ETA: 28:33 - loss: 0.6647 - regression_loss: 0.5954 - classification_loss: 0.0693
 239/1000 [======>.......................] - ETA: 28:32 - loss: 0.6655 - regression_loss: 0.5962 - classification_loss: 0.0693
 240/1000 [======>.......................] - ETA: 28:31 - loss: 0.6656 - regression_loss: 0.5962 - classification_loss: 0.0694
 241/1000 [======>.......................] - ETA: 28:29 - loss: 0.6659 - regression_loss: 0.5966 - classification_loss: 0.0694
 242/1000 [======>.......................] - ETA: 28:28 - loss: 0.6663 - regression_loss: 0.5969 - classification_loss: 0.0694
 243/1000 [======>.......................] - ETA: 28:27 - loss: 0.6670 - regression_loss: 0.5976 - classification_loss: 0.0694
 244/1000 [======>.......................] - ETA: 28:25 - loss: 0.6668 - regression_loss: 0.5974 - classification_loss: 0.0694
 245/1000 [======>.......................] - ETA: 28:21 - loss: 0.6647 - regression_loss: 0.5955 - classification_loss: 0.0692
 246/1000 [======>.......................] - ETA: 28:20 - loss: 0.6658 - regression_loss: 0.5965 - classification_loss: 0.0693
 247/1000 [======>.......................] - ETA: 28:16 - loss: 0.6660 - regression_loss: 0.5967 - classification_loss: 0.0693
 248/1000 [======>.......................] - ETA: 28:14 - loss: 0.6669 - regression_loss: 0.5976 - classification_loss: 0.0693
 249/1000 [======>.......................] - ETA: 28:11 - loss: 0.6676 - regression_loss: 0.5983 - classification_loss: 0.0693
 250/1000 [======>.......................] - ETA: 28:07 - loss: 0.6668 - regression_loss: 0.5975 - classification_loss: 0.0693
 251/1000 [======>.......................] - ETA: 28:06 - loss: 0.6671 - regression_loss: 0.5979 - classification_loss: 0.0693
 252/1000 [======>.......................] - ETA: 28:05 - loss: 0.6680 - regression_loss: 0.5987 - classification_loss: 0.0693
 253/1000 [======>.......................] - ETA: 28:03 - loss: 0.6681 - regression_loss: 0.5987 - classification_loss: 0.0693
 254/1000 [======>.......................] - ETA: 28:02 - loss: 0.6688 - regression_loss: 0.5994 - classification_loss: 0.0694
 255/1000 [======>.......................] - ETA: 27:58 - loss: 0.6668 - regression_loss: 0.5977 - classification_loss: 0.0692
 256/1000 [======>.......................] - ETA: 27:56 - loss: 0.6668 - regression_loss: 0.5976 - classification_loss: 0.0692
 257/1000 [======>.......................] - ETA: 27:53 - loss: 0.6666 - regression_loss: 0.5974 - classification_loss: 0.0692
 258/1000 [======>.......................] - ETA: 27:52 - loss: 0.6675 - regression_loss: 0.5982 - classification_loss: 0.0692
 259/1000 [======>.......................] - ETA: 27:50 - loss: 0.6683 - regression_loss: 0.5991 - classification_loss: 0.0693
 260/1000 [======>.......................] - ETA: 27:49 - loss: 0.6686 - regression_loss: 0.5994 - classification_loss: 0.0693
 261/1000 [======>.......................] - ETA: 27:46 - loss: 0.6689 - regression_loss: 0.5997 - classification_loss: 0.0693
 262/1000 [======>.......................] - ETA: 27:42 - loss: 0.6677 - regression_loss: 0.5986 - classification_loss: 0.0691
 263/1000 [======>.......................] - ETA: 27:40 - loss: 0.6681 - regression_loss: 0.5990 - classification_loss: 0.0691
 264/1000 [======>.......................] - ETA: 27:38 - loss: 0.6684 - regression_loss: 0.5993 - classification_loss: 0.0691
 265/1000 [======>.......................] - ETA: 27:37 - loss: 0.6691 - regression_loss: 0.6000 - classification_loss: 0.0691
 266/1000 [======>.......................] - ETA: 27:33 - loss: 0.6671 - regression_loss: 0.5982 - classification_loss: 0.0690
 267/1000 [=======>......................] - ETA: 27:30 - loss: 0.6668 - regression_loss: 0.5979 - classification_loss: 0.0690
 268/1000 [=======>......................] - ETA: 27:28 - loss: 0.6678 - regression_loss: 0.5988 - classification_loss: 0.0690
 269/1000 [=======>......................] - ETA: 27:27 - loss: 0.6677 - regression_loss: 0.5987 - classification_loss: 0.0690
 270/1000 [=======>......................] - ETA: 27:24 - loss: 0.6679 - regression_loss: 0.5989 - classification_loss: 0.0690
 271/1000 [=======>......................] - ETA: 27:21 - loss: 0.6673 - regression_loss: 0.5983 - classification_loss: 0.0690
 272/1000 [=======>......................] - ETA: 27:20 - loss: 0.6682 - regression_loss: 0.5991 - classification_loss: 0.0690
 273/1000 [=======>......................] - ETA: 27:18 - loss: 0.6683 - regression_loss: 0.5993 - classification_loss: 0.0691
 274/1000 [=======>......................] - ETA: 27:16 - loss: 0.6687 - regression_loss: 0.5996 - classification_loss: 0.0691
 275/1000 [=======>......................] - ETA: 27:15 - loss: 0.6693 - regression_loss: 0.6002 - classification_loss: 0.0691
 276/1000 [=======>......................] - ETA: 27:11 - loss: 0.6676 - regression_loss: 0.5987 - classification_loss: 0.0689
 277/1000 [=======>......................] - ETA: 27:10 - loss: 0.6676 - regression_loss: 0.5987 - classification_loss: 0.0690
 278/1000 [=======>......................] - ETA: 27:06 - loss: 0.6670 - regression_loss: 0.5981 - classification_loss: 0.0689
 279/1000 [=======>......................] - ETA: 27:02 - loss: 0.6651 - regression_loss: 0.5963 - classification_loss: 0.0688
 280/1000 [=======>......................] - ETA: 27:00 - loss: 0.6654 - regression_loss: 0.5966 - classification_loss: 0.0688
 281/1000 [=======>......................] - ETA: 26:59 - loss: 0.6661 - regression_loss: 0.5972 - classification_loss: 0.0688
 282/1000 [=======>......................] - ETA: 26:57 - loss: 0.6662 - regression_loss: 0.5974 - classification_loss: 0.0688
 283/1000 [=======>......................] - ETA: 26:56 - loss: 0.6668 - regression_loss: 0.5979 - classification_loss: 0.0688
 284/1000 [=======>......................] - ETA: 26:54 - loss: 0.6667 - regression_loss: 0.5978 - classification_loss: 0.0689
 285/1000 [=======>......................] - ETA: 26:51 - loss: 0.6668 - regression_loss: 0.5980 - classification_loss: 0.0689
 286/1000 [=======>......................] - ETA: 26:50 - loss: 0.6675 - regression_loss: 0.5986 - classification_loss: 0.0689
 287/1000 [=======>......................] - ETA: 26:46 - loss: 0.6668 - regression_loss: 0.5979 - classification_loss: 0.0688
 288/1000 [=======>......................] - ETA: 26:45 - loss: 0.6671 - regression_loss: 0.5982 - classification_loss: 0.0689
 289/1000 [=======>......................] - ETA: 26:41 - loss: 0.6659 - regression_loss: 0.5971 - classification_loss: 0.0688
 290/1000 [=======>......................] - ETA: 26:40 - loss: 0.6665 - regression_loss: 0.5977 - classification_loss: 0.0688
 291/1000 [=======>......................] - ETA: 26:39 - loss: 0.6671 - regression_loss: 0.5982 - classification_loss: 0.0688
 292/1000 [=======>......................] - ETA: 26:37 - loss: 0.6676 - regression_loss: 0.5987 - classification_loss: 0.0688
 293/1000 [=======>......................] - ETA: 26:34 - loss: 0.6667 - regression_loss: 0.5979 - classification_loss: 0.0688
 294/1000 [=======>......................] - ETA: 26:30 - loss: 0.6651 - regression_loss: 0.5964 - classification_loss: 0.0687
 295/1000 [=======>......................] - ETA: 26:27 - loss: 0.6653 - regression_loss: 0.5966 - classification_loss: 0.0687
 296/1000 [=======>......................] - ETA: 26:26 - loss: 0.6653 - regression_loss: 0.5965 - classification_loss: 0.0688
 297/1000 [=======>......................] - ETA: 26:24 - loss: 0.6656 - regression_loss: 0.5968 - classification_loss: 0.0688
 298/1000 [=======>......................] - ETA: 26:20 - loss: 0.6639 - regression_loss: 0.5952 - classification_loss: 0.0687
 299/1000 [=======>......................] - ETA: 26:18 - loss: 0.6642 - regression_loss: 0.5955 - classification_loss: 0.0687
 300/1000 [========>.....................] - ETA: 26:17 - loss: 0.6648 - regression_loss: 0.5961 - classification_loss: 0.0687
 301/1000 [========>.....................] - ETA: 26:15 - loss: 0.6650 - regression_loss: 0.5962 - classification_loss: 0.0688
 302/1000 [========>.....................] - ETA: 26:13 - loss: 0.6649 - regression_loss: 0.5961 - classification_loss: 0.0688
 303/1000 [========>.....................] - ETA: 26:12 - loss: 0.6655 - regression_loss: 0.5967 - classification_loss: 0.0688
 304/1000 [========>.....................] - ETA: 26:08 - loss: 0.6648 - regression_loss: 0.5960 - classification_loss: 0.0688
 305/1000 [========>.....................] - ETA: 26:04 - loss: 0.6634 - regression_loss: 0.5948 - classification_loss: 0.0687
 306/1000 [========>.....................] - ETA: 26:03 - loss: 0.6641 - regression_loss: 0.5954 - classification_loss: 0.0687
 307/1000 [========>.....................] - ETA: 25:59 - loss: 0.6634 - regression_loss: 0.5947 - classification_loss: 0.0687
 308/1000 [========>.....................] - ETA: 25:57 - loss: 0.6636 - regression_loss: 0.5949 - classification_loss: 0.0687
 309/1000 [========>.....................] - ETA: 25:56 - loss: 0.6641 - regression_loss: 0.5954 - classification_loss: 0.0687
 310/1000 [========>.....................] - ETA: 25:54 - loss: 0.6644 - regression_loss: 0.5956 - classification_loss: 0.0687
 311/1000 [========>.....................] - ETA: 25:52 - loss: 0.6642 - regression_loss: 0.5955 - classification_loss: 0.0688
 312/1000 [========>.....................] - ETA: 25:51 - loss: 0.6648 - regression_loss: 0.5960 - classification_loss: 0.0688
 313/1000 [========>.....................] - ETA: 25:47 - loss: 0.6632 - regression_loss: 0.5946 - classification_loss: 0.0687
 314/1000 [========>.....................] - ETA: 25:45 - loss: 0.6635 - regression_loss: 0.5948 - classification_loss: 0.0687
 315/1000 [========>.....................] - ETA: 25:44 - loss: 0.6640 - regression_loss: 0.5953 - classification_loss: 0.0687
 316/1000 [========>.....................] - ETA: 25:42 - loss: 0.6638 - regression_loss: 0.5951 - classification_loss: 0.0687
 317/1000 [========>.....................] - ETA: 25:40 - loss: 0.6639 - regression_loss: 0.5952 - classification_loss: 0.0687
 318/1000 [========>.....................] - ETA: 25:37 - loss: 0.6632 - regression_loss: 0.5946 - classification_loss: 0.0687
 319/1000 [========>.....................] - ETA: 25:35 - loss: 0.6639 - regression_loss: 0.5952 - classification_loss: 0.0687
 320/1000 [========>.....................] - ETA: 25:33 - loss: 0.6638 - regression_loss: 0.5951 - classification_loss: 0.0687
 321/1000 [========>.....................] - ETA: 25:31 - loss: 0.6640 - regression_loss: 0.5953 - classification_loss: 0.0687
 322/1000 [========>.....................] - ETA: 25:27 - loss: 0.6634 - regression_loss: 0.5947 - classification_loss: 0.0687
 323/1000 [========>.....................] - ETA: 25:26 - loss: 0.6639 - regression_loss: 0.5952 - classification_loss: 0.0687
 324/1000 [========>.....................] - ETA: 25:25 - loss: 0.6641 - regression_loss: 0.5954 - classification_loss: 0.0687
 325/1000 [========>.....................] - ETA: 25:21 - loss: 0.6627 - regression_loss: 0.5941 - classification_loss: 0.0686
 326/1000 [========>.....................] - ETA: 25:19 - loss: 0.6632 - regression_loss: 0.5946 - classification_loss: 0.0686
 327/1000 [========>.....................] - ETA: 25:18 - loss: 0.6632 - regression_loss: 0.5946 - classification_loss: 0.0687
 328/1000 [========>.....................] - ETA: 25:15 - loss: 0.6634 - regression_loss: 0.5947 - classification_loss: 0.0687
 329/1000 [========>.....................] - ETA: 25:12 - loss: 0.6629 - regression_loss: 0.5942 - classification_loss: 0.0686
 330/1000 [========>.....................] - ETA: 25:08 - loss: 0.6614 - regression_loss: 0.5929 - classification_loss: 0.0685
 331/1000 [========>.....................] - ETA: 25:06 - loss: 0.6616 - regression_loss: 0.5931 - classification_loss: 0.0685
 332/1000 [========>.....................] - ETA: 25:05 - loss: 0.6622 - regression_loss: 0.5936 - classification_loss: 0.0685
 333/1000 [========>.....................] - ETA: 25:04 - loss: 0.6626 - regression_loss: 0.5941 - classification_loss: 0.0685
 334/1000 [=========>....................] - ETA: 25:02 - loss: 0.6626 - regression_loss: 0.5940 - classification_loss: 0.0686
 335/1000 [=========>....................] - ETA: 25:00 - loss: 0.6630 - regression_loss: 0.5944 - classification_loss: 0.0686
 336/1000 [=========>....................] - ETA: 24:59 - loss: 0.6632 - regression_loss: 0.5946 - classification_loss: 0.0686
 337/1000 [=========>....................] - ETA: 24:56 - loss: 0.6632 - regression_loss: 0.5947 - classification_loss: 0.0686
 338/1000 [=========>....................] - ETA: 24:53 - loss: 0.6627 - regression_loss: 0.5941 - classification_loss: 0.0685
 339/1000 [=========>....................] - ETA: 24:49 - loss: 0.6613 - regression_loss: 0.5929 - classification_loss: 0.0684
 340/1000 [=========>....................] - ETA: 24:48 - loss: 0.6620 - regression_loss: 0.5936 - classification_loss: 0.0685
 341/1000 [=========>....................] - ETA: 24:45 - loss: 0.6614 - regression_loss: 0.5930 - classification_loss: 0.0684
 342/1000 [=========>....................] - ETA: 24:43 - loss: 0.6617 - regression_loss: 0.5933 - classification_loss: 0.0684
 343/1000 [=========>....................] - ETA: 24:41 - loss: 0.6618 - regression_loss: 0.5934 - classification_loss: 0.0684
 344/1000 [=========>....................] - ETA: 24:39 - loss: 0.6619 - regression_loss: 0.5935 - classification_loss: 0.0684
 345/1000 [=========>....................] - ETA: 24:37 - loss: 0.6627 - regression_loss: 0.5942 - classification_loss: 0.0685
 346/1000 [=========>....................] - ETA: 24:34 - loss: 0.6620 - regression_loss: 0.5937 - classification_loss: 0.0683
 347/1000 [=========>....................] - ETA: 24:32 - loss: 0.6623 - regression_loss: 0.5940 - classification_loss: 0.0683
 348/1000 [=========>....................] - ETA: 24:30 - loss: 0.6628 - regression_loss: 0.5944 - classification_loss: 0.0683
 349/1000 [=========>....................] - ETA: 24:29 - loss: 0.6633 - regression_loss: 0.5949 - classification_loss: 0.0684
 350/1000 [=========>....................] - ETA: 24:26 - loss: 0.6627 - regression_loss: 0.5943 - classification_loss: 0.0683
 351/1000 [=========>....................] - ETA: 24:24 - loss: 0.6627 - regression_loss: 0.5943 - classification_loss: 0.0683
 352/1000 [=========>....................] - ETA: 24:21 - loss: 0.6613 - regression_loss: 0.5931 - classification_loss: 0.0682
 353/1000 [=========>....................] - ETA: 24:18 - loss: 0.6616 - regression_loss: 0.5933 - classification_loss: 0.0683
 354/1000 [=========>....................] - ETA: 24:16 - loss: 0.6615 - regression_loss: 0.5932 - classification_loss: 0.0683
 355/1000 [=========>....................] - ETA: 24:15 - loss: 0.6618 - regression_loss: 0.5934 - classification_loss: 0.0683
 356/1000 [=========>....................] - ETA: 24:11 - loss: 0.6605 - regression_loss: 0.5923 - classification_loss: 0.0682
 357/1000 [=========>....................] - ETA: 24:09 - loss: 0.6609 - regression_loss: 0.5927 - classification_loss: 0.0682
 358/1000 [=========>....................] - ETA: 24:06 - loss: 0.6608 - regression_loss: 0.5926 - classification_loss: 0.0682
 359/1000 [=========>....................] - ETA: 24:04 - loss: 0.6609 - regression_loss: 0.5926 - classification_loss: 0.0682
 360/1000 [=========>....................] - ETA: 24:02 - loss: 0.6614 - regression_loss: 0.5931 - classification_loss: 0.0683
 361/1000 [=========>....................] - ETA: 24:01 - loss: 0.6618 - regression_loss: 0.5936 - classification_loss: 0.0683
 362/1000 [=========>....................] - ETA: 23:58 - loss: 0.6614 - regression_loss: 0.5931 - classification_loss: 0.0682
 363/1000 [=========>....................] - ETA: 23:56 - loss: 0.6616 - regression_loss: 0.5933 - classification_loss: 0.0682
 364/1000 [=========>....................] - ETA: 23:54 - loss: 0.6621 - regression_loss: 0.5938 - classification_loss: 0.0683
 365/1000 [=========>....................] - ETA: 23:52 - loss: 0.6621 - regression_loss: 0.5938 - classification_loss: 0.0683
 366/1000 [=========>....................] - ETA: 23:48 - loss: 0.6608 - regression_loss: 0.5927 - classification_loss: 0.0682
 367/1000 [==========>...................] - ETA: 23:47 - loss: 0.6608 - regression_loss: 0.5926 - classification_loss: 0.0682
 368/1000 [==========>...................] - ETA: 23:45 - loss: 0.6612 - regression_loss: 0.5930 - classification_loss: 0.0682
 369/1000 [==========>...................] - ETA: 23:41 - loss: 0.6601 - regression_loss: 0.5920 - classification_loss: 0.0681
 370/1000 [==========>...................] - ETA: 23:39 - loss: 0.6602 - regression_loss: 0.5921 - classification_loss: 0.0681
 371/1000 [==========>...................] - ETA: 23:36 - loss: 0.6597 - regression_loss: 0.5917 - classification_loss: 0.0681
 372/1000 [==========>...................] - ETA: 23:34 - loss: 0.6604 - regression_loss: 0.5923 - classification_loss: 0.0681
 373/1000 [==========>...................] - ETA: 23:33 - loss: 0.6605 - regression_loss: 0.5924 - classification_loss: 0.0681
 374/1000 [==========>...................] - ETA: 23:31 - loss: 0.6607 - regression_loss: 0.5926 - classification_loss: 0.0681
 375/1000 [==========>...................] - ETA: 23:27 - loss: 0.6597 - regression_loss: 0.5917 - classification_loss: 0.0680
 376/1000 [==========>...................] - ETA: 23:26 - loss: 0.6601 - regression_loss: 0.5921 - classification_loss: 0.0680
 377/1000 [==========>...................] - ETA: 23:24 - loss: 0.6603 - regression_loss: 0.5922 - classification_loss: 0.0681
 378/1000 [==========>...................] - ETA: 23:21 - loss: 0.6597 - regression_loss: 0.5917 - classification_loss: 0.0680
 379/1000 [==========>...................] - ETA: 23:18 - loss: 0.6600 - regression_loss: 0.5920 - classification_loss: 0.0680
 380/1000 [==========>...................] - ETA: 23:17 - loss: 0.6605 - regression_loss: 0.5924 - classification_loss: 0.0680
 381/1000 [==========>...................] - ETA: 23:15 - loss: 0.6609 - regression_loss: 0.5928 - classification_loss: 0.0681
 382/1000 [==========>...................] - ETA: 23:14 - loss: 0.6614 - regression_loss: 0.5933 - classification_loss: 0.0681
 383/1000 [==========>...................] - ETA: 23:11 - loss: 0.6616 - regression_loss: 0.5935 - classification_loss: 0.0681
 384/1000 [==========>...................] - ETA: 23:09 - loss: 0.6616 - regression_loss: 0.5935 - classification_loss: 0.0681
 385/1000 [==========>...................] - ETA: 23:06 - loss: 0.6612 - regression_loss: 0.5931 - classification_loss: 0.0681
 386/1000 [==========>...................] - ETA: 23:03 - loss: 0.6599 - regression_loss: 0.5919 - classification_loss: 0.0680
 387/1000 [==========>...................] - ETA: 23:01 - loss: 0.6606 - regression_loss: 0.5926 - classification_loss: 0.0680
 388/1000 [==========>...................] - ETA: 22:59 - loss: 0.6615 - regression_loss: 0.5935 - classification_loss: 0.0680
 389/1000 [==========>...................] - ETA: 22:56 - loss: 0.6610 - regression_loss: 0.5931 - classification_loss: 0.0680
 390/1000 [==========>...................] - ETA: 22:55 - loss: 0.6617 - regression_loss: 0.5937 - classification_loss: 0.0680
 391/1000 [==========>...................] - ETA: 22:52 - loss: 0.6617 - regression_loss: 0.5937 - classification_loss: 0.0680
 392/1000 [==========>...................] - ETA: 22:49 - loss: 0.6609 - regression_loss: 0.5930 - classification_loss: 0.0679
 393/1000 [==========>...................] - ETA: 22:47 - loss: 0.6612 - regression_loss: 0.5933 - classification_loss: 0.0679
 394/1000 [==========>...................] - ETA: 22:45 - loss: 0.6619 - regression_loss: 0.5940 - classification_loss: 0.0679
 395/1000 [==========>...................] - ETA: 22:43 - loss: 0.6624 - regression_loss: 0.5944 - classification_loss: 0.0679
 396/1000 [==========>...................] - ETA: 22:42 - loss: 0.6626 - regression_loss: 0.5947 - classification_loss: 0.0679
 397/1000 [==========>...................] - ETA: 22:39 - loss: 0.6622 - regression_loss: 0.5943 - classification_loss: 0.0679
 398/1000 [==========>...................] - ETA: 22:37 - loss: 0.6627 - regression_loss: 0.5948 - classification_loss: 0.0679
 399/1000 [==========>...................] - ETA: 22:34 - loss: 0.6617 - regression_loss: 0.5939 - classification_loss: 0.0678
 400/1000 [===========>..................] - ETA: 22:31 - loss: 0.6621 - regression_loss: 0.5943 - classification_loss: 0.0678
 401/1000 [===========>..................] - ETA: 22:29 - loss: 0.6623 - regression_loss: 0.5944 - classification_loss: 0.0678
 402/1000 [===========>..................] - ETA: 22:28 - loss: 0.6628 - regression_loss: 0.5949 - classification_loss: 0.0679
 403/1000 [===========>..................] - ETA: 22:24 - loss: 0.6615 - regression_loss: 0.5937 - classification_loss: 0.0677
 404/1000 [===========>..................] - ETA: 22:23 - loss: 0.6617 - regression_loss: 0.5939 - classification_loss: 0.0677
 405/1000 [===========>..................] - ETA: 22:20 - loss: 0.6618 - regression_loss: 0.5940 - classification_loss: 0.0677
 406/1000 [===========>..................] - ETA: 22:19 - loss: 0.6622 - regression_loss: 0.5944 - classification_loss: 0.0677
 407/1000 [===========>..................] - ETA: 22:17 - loss: 0.6621 - regression_loss: 0.5944 - classification_loss: 0.0678
 408/1000 [===========>..................] - ETA: 22:15 - loss: 0.6626 - regression_loss: 0.5948 - classification_loss: 0.0678
 409/1000 [===========>..................] - ETA: 22:12 - loss: 0.6624 - regression_loss: 0.5947 - classification_loss: 0.0677
 410/1000 [===========>..................] - ETA: 22:10 - loss: 0.6625 - regression_loss: 0.5948 - classification_loss: 0.0677
 411/1000 [===========>..................] - ETA: 22:06 - loss: 0.6613 - regression_loss: 0.5937 - classification_loss: 0.0676
 412/1000 [===========>..................] - ETA: 22:05 - loss: 0.6617 - regression_loss: 0.5941 - classification_loss: 0.0676
 413/1000 [===========>..................] - ETA: 22:03 - loss: 0.6619 - regression_loss: 0.5942 - classification_loss: 0.0676
 414/1000 [===========>..................] - ETA: 22:00 - loss: 0.6613 - regression_loss: 0.5937 - classification_loss: 0.0676
 415/1000 [===========>..................] - ETA: 21:58 - loss: 0.6614 - regression_loss: 0.5937 - classification_loss: 0.0676
 416/1000 [===========>..................] - ETA: 21:56 - loss: 0.6618 - regression_loss: 0.5942 - classification_loss: 0.0677
 417/1000 [===========>..................] - ETA: 21:54 - loss: 0.6620 - regression_loss: 0.5943 - classification_loss: 0.0677
 418/1000 [===========>..................] - ETA: 21:51 - loss: 0.6611 - regression_loss: 0.5936 - classification_loss: 0.0676
 419/1000 [===========>..................] - ETA: 21:49 - loss: 0.6615 - regression_loss: 0.5939 - classification_loss: 0.0676
 420/1000 [===========>..................] - ETA: 21:47 - loss: 0.6619 - regression_loss: 0.5944 - classification_loss: 0.0676
 421/1000 [===========>..................] - ETA: 21:44 - loss: 0.6615 - regression_loss: 0.5939 - classification_loss: 0.0676
 422/1000 [===========>..................] - ETA: 21:43 - loss: 0.6615 - regression_loss: 0.5939 - classification_loss: 0.0676
 423/1000 [===========>..................] - ETA: 21:41 - loss: 0.6617 - regression_loss: 0.5941 - classification_loss: 0.0676
 424/1000 [===========>..................] - ETA: 21:39 - loss: 0.6621 - regression_loss: 0.5945 - classification_loss: 0.0676
 425/1000 [===========>..................] - ETA: 21:36 - loss: 0.6609 - regression_loss: 0.5934 - classification_loss: 0.0675
 426/1000 [===========>..................] - ETA: 21:33 - loss: 0.6603 - regression_loss: 0.5928 - classification_loss: 0.0675
 427/1000 [===========>..................] - ETA: 21:31 - loss: 0.6603 - regression_loss: 0.5928 - classification_loss: 0.0675
 428/1000 [===========>..................] - ETA: 21:29 - loss: 0.6604 - regression_loss: 0.5929 - classification_loss: 0.0675
 429/1000 [===========>..................] - ETA: 21:27 - loss: 0.6607 - regression_loss: 0.5932 - classification_loss: 0.0675
 430/1000 [===========>..................] - ETA: 21:25 - loss: 0.6610 - regression_loss: 0.5935 - classification_loss: 0.0675
 431/1000 [===========>..................] - ETA: 21:23 - loss: 0.6614 - regression_loss: 0.5939 - classification_loss: 0.0675
 432/1000 [===========>..................] - ETA: 21:20 - loss: 0.6612 - regression_loss: 0.5937 - classification_loss: 0.0675
 433/1000 [===========>..................] - ETA: 21:17 - loss: 0.6601 - regression_loss: 0.5928 - classification_loss: 0.0674
 434/1000 [============>.................] - ETA: 21:15 - loss: 0.6604 - regression_loss: 0.5930 - classification_loss: 0.0674
 435/1000 [============>.................] - ETA: 21:13 - loss: 0.6605 - regression_loss: 0.5931 - classification_loss: 0.0674
 436/1000 [============>.................] - ETA: 21:11 - loss: 0.6609 - regression_loss: 0.5935 - classification_loss: 0.0674
 437/1000 [============>.................] - ETA: 21:09 - loss: 0.6608 - regression_loss: 0.5934 - classification_loss: 0.0674
 438/1000 [============>.................] - ETA: 21:07 - loss: 0.6611 - regression_loss: 0.5936 - classification_loss: 0.0674
 439/1000 [============>.................] - ETA: 21:04 - loss: 0.6611 - regression_loss: 0.5937 - classification_loss: 0.0674
 440/1000 [============>.................] - ETA: 21:01 - loss: 0.6601 - regression_loss: 0.5927 - classification_loss: 0.0674
 441/1000 [============>.................] - ETA: 21:00 - loss: 0.6606 - regression_loss: 0.5932 - classification_loss: 0.0674
 442/1000 [============>.................] - ETA: 20:57 - loss: 0.6608 - regression_loss: 0.5934 - classification_loss: 0.0674
 443/1000 [============>.................] - ETA: 20:56 - loss: 0.6612 - regression_loss: 0.5938 - classification_loss: 0.0674
 444/1000 [============>.................] - ETA: 20:53 - loss: 0.6613 - regression_loss: 0.5939 - classification_loss: 0.0674
 445/1000 [============>.................] - ETA: 20:52 - loss: 0.6617 - regression_loss: 0.5942 - classification_loss: 0.0675
 446/1000 [============>.................] - ETA: 20:50 - loss: 0.6616 - regression_loss: 0.5941 - classification_loss: 0.0675
 447/1000 [============>.................] - ETA: 20:48 - loss: 0.6620 - regression_loss: 0.5945 - classification_loss: 0.0675
 448/1000 [============>.................] - ETA: 20:45 - loss: 0.6615 - regression_loss: 0.5940 - classification_loss: 0.0675
 449/1000 [============>.................] - ETA: 20:43 - loss: 0.6615 - regression_loss: 0.5940 - classification_loss: 0.0675
 450/1000 [============>.................] - ETA: 20:40 - loss: 0.6604 - regression_loss: 0.5930 - classification_loss: 0.0674
 451/1000 [============>.................] - ETA: 20:38 - loss: 0.6606 - regression_loss: 0.5932 - classification_loss: 0.0674
 452/1000 [============>.................] - ETA: 20:36 - loss: 0.6610 - regression_loss: 0.5936 - classification_loss: 0.0674
 453/1000 [============>.................] - ETA: 20:34 - loss: 0.6611 - regression_loss: 0.5937 - classification_loss: 0.0674
 454/1000 [============>.................] - ETA: 20:32 - loss: 0.6614 - regression_loss: 0.5940 - classification_loss: 0.0674
 455/1000 [============>.................] - ETA: 20:29 - loss: 0.6609 - regression_loss: 0.5935 - classification_loss: 0.0674
 456/1000 [============>.................] - ETA: 20:26 - loss: 0.6598 - regression_loss: 0.5925 - classification_loss: 0.0673
 457/1000 [============>.................] - ETA: 20:24 - loss: 0.6600 - regression_loss: 0.5927 - classification_loss: 0.0673
 458/1000 [============>.................] - ETA: 20:22 - loss: 0.6599 - regression_loss: 0.5926 - classification_loss: 0.0673
 459/1000 [============>.................] - ETA: 20:20 - loss: 0.6599 - regression_loss: 0.5926 - classification_loss: 0.0673
 460/1000 [============>.................] - ETA: 20:18 - loss: 0.6602 - regression_loss: 0.5929 - classification_loss: 0.0673
 461/1000 [============>.................] - ETA: 20:16 - loss: 0.6603 - regression_loss: 0.5929 - classification_loss: 0.0673
 462/1000 [============>.................] - ETA: 20:14 - loss: 0.6604 - regression_loss: 0.5931 - classification_loss: 0.0673
 463/1000 [============>.................] - ETA: 20:11 - loss: 0.6599 - regression_loss: 0.5926 - classification_loss: 0.0673
 464/1000 [============>.................] - ETA: 20:09 - loss: 0.6603 - regression_loss: 0.5930 - classification_loss: 0.0673
 465/1000 [============>.................] - ETA: 20:06 - loss: 0.6592 - regression_loss: 0.5919 - classification_loss: 0.0672
 466/1000 [============>.................] - ETA: 20:04 - loss: 0.6596 - regression_loss: 0.5924 - classification_loss: 0.0672
 467/1000 [=============>................] - ETA: 20:01 - loss: 0.6585 - regression_loss: 0.5914 - classification_loss: 0.0671
 468/1000 [=============>................] - ETA: 19:59 - loss: 0.6585 - regression_loss: 0.5913 - classification_loss: 0.0671
 469/1000 [=============>................] - ETA: 19:58 - loss: 0.6588 - regression_loss: 0.5916 - classification_loss: 0.0672
 470/1000 [=============>................] - ETA: 19:55 - loss: 0.6588 - regression_loss: 0.5917 - classification_loss: 0.0672
 471/1000 [=============>................] - ETA: 19:52 - loss: 0.6584 - regression_loss: 0.5912 - classification_loss: 0.0671
 472/1000 [=============>................] - ETA: 19:50 - loss: 0.6586 - regression_loss: 0.5914 - classification_loss: 0.0671
 473/1000 [=============>................] - ETA: 19:49 - loss: 0.6588 - regression_loss: 0.5916 - classification_loss: 0.0671
 474/1000 [=============>................] - ETA: 19:46 - loss: 0.6588 - regression_loss: 0.5917 - classification_loss: 0.0672
 475/1000 [=============>................] - ETA: 19:44 - loss: 0.6588 - regression_loss: 0.5917 - classification_loss: 0.0671
 476/1000 [=============>................] - ETA: 19:42 - loss: 0.6592 - regression_loss: 0.5920 - classification_loss: 0.0672
 477/1000 [=============>................] - ETA: 19:40 - loss: 0.6587 - regression_loss: 0.5916 - classification_loss: 0.0672
 478/1000 [=============>................] - ETA: 19:37 - loss: 0.6578 - regression_loss: 0.5907 - classification_loss: 0.0671
 479/1000 [=============>................] - ETA: 19:35 - loss: 0.6581 - regression_loss: 0.5911 - classification_loss: 0.0671
 480/1000 [=============>................] - ETA: 19:32 - loss: 0.6582 - regression_loss: 0.5911 - classification_loss: 0.0671
 481/1000 [=============>................] - ETA: 19:30 - loss: 0.6585 - regression_loss: 0.5914 - classification_loss: 0.0671
 482/1000 [=============>................] - ETA: 19:28 - loss: 0.6575 - regression_loss: 0.5905 - classification_loss: 0.0670
 483/1000 [=============>................] - ETA: 19:26 - loss: 0.6579 - regression_loss: 0.5908 - classification_loss: 0.0670
 484/1000 [=============>................] - ETA: 19:24 - loss: 0.6579 - regression_loss: 0.5908 - classification_loss: 0.0670
 485/1000 [=============>................] - ETA: 19:21 - loss: 0.6576 - regression_loss: 0.5905 - classification_loss: 0.0670
 486/1000 [=============>................] - ETA: 19:19 - loss: 0.6578 - regression_loss: 0.5908 - classification_loss: 0.0670
 487/1000 [=============>................] - ETA: 19:17 - loss: 0.6581 - regression_loss: 0.5911 - classification_loss: 0.0671
 488/1000 [=============>................] - ETA: 19:15 - loss: 0.6583 - regression_loss: 0.5912 - classification_loss: 0.0671
 489/1000 [=============>................] - ETA: 19:13 - loss: 0.6582 - regression_loss: 0.5912 - classification_loss: 0.0671
 490/1000 [=============>................] - ETA: 19:11 - loss: 0.6583 - regression_loss: 0.5913 - classification_loss: 0.0671
 491/1000 [=============>................] - ETA: 19:08 - loss: 0.6573 - regression_loss: 0.5903 - classification_loss: 0.0670
 492/1000 [=============>................] - ETA: 19:06 - loss: 0.6576 - regression_loss: 0.5906 - classification_loss: 0.0670
 493/1000 [=============>................] - ETA: 19:03 - loss: 0.6572 - regression_loss: 0.5902 - classification_loss: 0.0670
 494/1000 [=============>................] - ETA: 19:01 - loss: 0.6571 - regression_loss: 0.5901 - classification_loss: 0.0670
 495/1000 [=============>................] - ETA: 18:59 - loss: 0.6573 - regression_loss: 0.5904 - classification_loss: 0.0670
 496/1000 [=============>................] - ETA: 18:57 - loss: 0.6576 - regression_loss: 0.5906 - classification_loss: 0.0670
 497/1000 [=============>................] - ETA: 18:55 - loss: 0.6576 - regression_loss: 0.5906 - classification_loss: 0.0670
 498/1000 [=============>................] - ETA: 18:53 - loss: 0.6580 - regression_loss: 0.5910 - classification_loss: 0.0670
 499/1000 [=============>................] - ETA: 18:50 - loss: 0.6575 - regression_loss: 0.5905 - classification_loss: 0.0670
 500/1000 [==============>...............] - ETA: 18:47 - loss: 0.6565 - regression_loss: 0.5896 - classification_loss: 0.0669
 501/1000 [==============>...............] - ETA: 18:45 - loss: 0.6567 - regression_loss: 0.5898 - classification_loss: 0.0669
 502/1000 [==============>...............] - ETA: 18:43 - loss: 0.6562 - regression_loss: 0.5893 - classification_loss: 0.0669
 503/1000 [==============>...............] - ETA: 18:41 - loss: 0.6565 - regression_loss: 0.5896 - classification_loss: 0.0669
 504/1000 [==============>...............] - ETA: 18:38 - loss: 0.6556 - regression_loss: 0.5887 - classification_loss: 0.0668
 505/1000 [==============>...............] - ETA: 18:36 - loss: 0.6556 - regression_loss: 0.5888 - classification_loss: 0.0668
 506/1000 [==============>...............] - ETA: 18:33 - loss: 0.6555 - regression_loss: 0.5887 - classification_loss: 0.0668
 507/1000 [==============>...............] - ETA: 18:32 - loss: 0.6558 - regression_loss: 0.5890 - classification_loss: 0.0669
 508/1000 [==============>...............] - ETA: 18:29 - loss: 0.6555 - regression_loss: 0.5887 - classification_loss: 0.0669
 509/1000 [==============>...............] - ETA: 18:27 - loss: 0.6558 - regression_loss: 0.5889 - classification_loss: 0.0669
 510/1000 [==============>...............] - ETA: 18:24 - loss: 0.6548 - regression_loss: 0.5880 - classification_loss: 0.0668
 511/1000 [==============>...............] - ETA: 18:22 - loss: 0.6550 - regression_loss: 0.5882 - classification_loss: 0.0668
 512/1000 [==============>...............] - ETA: 18:20 - loss: 0.6553 - regression_loss: 0.5884 - classification_loss: 0.0668
 513/1000 [==============>...............] - ETA: 18:18 - loss: 0.6552 - regression_loss: 0.5884 - classification_loss: 0.0668
 514/1000 [==============>...............] - ETA: 18:16 - loss: 0.6551 - regression_loss: 0.5883 - classification_loss: 0.0668
 515/1000 [==============>...............] - ETA: 18:13 - loss: 0.6546 - regression_loss: 0.5878 - classification_loss: 0.0668
 516/1000 [==============>...............] - ETA: 18:11 - loss: 0.6550 - regression_loss: 0.5881 - classification_loss: 0.0668
 517/1000 [==============>...............] - ETA: 18:09 - loss: 0.6552 - regression_loss: 0.5884 - classification_loss: 0.0668
 518/1000 [==============>...............] - ETA: 18:07 - loss: 0.6553 - regression_loss: 0.5884 - classification_loss: 0.0668
 519/1000 [==============>...............] - ETA: 18:05 - loss: 0.6552 - regression_loss: 0.5883 - classification_loss: 0.0668
 520/1000 [==============>...............] - ETA: 18:02 - loss: 0.6542 - regression_loss: 0.5874 - classification_loss: 0.0667
 521/1000 [==============>...............] - ETA: 18:00 - loss: 0.6543 - regression_loss: 0.5875 - classification_loss: 0.0668
 522/1000 [==============>...............] - ETA: 17:57 - loss: 0.6539 - regression_loss: 0.5872 - classification_loss: 0.0667
 523/1000 [==============>...............] - ETA: 17:55 - loss: 0.6538 - regression_loss: 0.5871 - classification_loss: 0.0668
 524/1000 [==============>...............] - ETA: 17:53 - loss: 0.6538 - regression_loss: 0.5870 - classification_loss: 0.0667
 525/1000 [==============>...............] - ETA: 17:50 - loss: 0.6528 - regression_loss: 0.5861 - classification_loss: 0.0667
 526/1000 [==============>...............] - ETA: 17:48 - loss: 0.6530 - regression_loss: 0.5863 - classification_loss: 0.0667
 527/1000 [==============>...............] - ETA: 17:46 - loss: 0.6534 - regression_loss: 0.5867 - classification_loss: 0.0667
 528/1000 [==============>...............] - ETA: 17:44 - loss: 0.6537 - regression_loss: 0.5869 - classification_loss: 0.0667
 529/1000 [==============>...............] - ETA: 17:42 - loss: 0.6532 - regression_loss: 0.5865 - classification_loss: 0.0667
 530/1000 [==============>...............] - ETA: 17:39 - loss: 0.6523 - regression_loss: 0.5857 - classification_loss: 0.0666
 531/1000 [==============>...............] - ETA: 17:37 - loss: 0.6526 - regression_loss: 0.5860 - classification_loss: 0.0666
 532/1000 [==============>...............] - ETA: 17:35 - loss: 0.6526 - regression_loss: 0.5860 - classification_loss: 0.0666
 533/1000 [==============>...............] - ETA: 17:33 - loss: 0.6528 - regression_loss: 0.5861 - classification_loss: 0.0666
 534/1000 [===============>..............] - ETA: 17:31 - loss: 0.6530 - regression_loss: 0.5864 - classification_loss: 0.0667
 535/1000 [===============>..............] - ETA: 17:29 - loss: 0.6530 - regression_loss: 0.5863 - classification_loss: 0.0667
 536/1000 [===============>..............] - ETA: 17:26 - loss: 0.6526 - regression_loss: 0.5860 - classification_loss: 0.0666
 537/1000 [===============>..............] - ETA: 17:24 - loss: 0.6526 - regression_loss: 0.5859 - classification_loss: 0.0666
 538/1000 [===============>..............] - ETA: 17:22 - loss: 0.6529 - regression_loss: 0.5863 - classification_loss: 0.0667
 539/1000 [===============>..............] - ETA: 17:19 - loss: 0.6520 - regression_loss: 0.5854 - classification_loss: 0.0666
 540/1000 [===============>..............] - ETA: 17:17 - loss: 0.6520 - regression_loss: 0.5854 - classification_loss: 0.0666
 541/1000 [===============>..............] - ETA: 17:15 - loss: 0.6521 - regression_loss: 0.5855 - classification_loss: 0.0666
 542/1000 [===============>..............] - ETA: 17:13 - loss: 0.6524 - regression_loss: 0.5858 - classification_loss: 0.0666
 543/1000 [===============>..............] - ETA: 17:11 - loss: 0.6524 - regression_loss: 0.5858 - classification_loss: 0.0666
 544/1000 [===============>..............] - ETA: 17:09 - loss: 0.6527 - regression_loss: 0.5861 - classification_loss: 0.0666
 545/1000 [===============>..............] - ETA: 17:07 - loss: 0.6528 - regression_loss: 0.5862 - classification_loss: 0.0666
 546/1000 [===============>..............] - ETA: 17:05 - loss: 0.6527 - regression_loss: 0.5861 - classification_loss: 0.0666
 547/1000 [===============>..............] - ETA: 17:02 - loss: 0.6517 - regression_loss: 0.5852 - classification_loss: 0.0665
 548/1000 [===============>..............] - ETA: 17:00 - loss: 0.6520 - regression_loss: 0.5855 - classification_loss: 0.0665
 549/1000 [===============>..............] - ETA: 16:57 - loss: 0.6516 - regression_loss: 0.5851 - classification_loss: 0.0665
 550/1000 [===============>..............] - ETA: 16:55 - loss: 0.6517 - regression_loss: 0.5852 - classification_loss: 0.0665
 551/1000 [===============>..............] - ETA: 16:53 - loss: 0.6516 - regression_loss: 0.5851 - classification_loss: 0.0665
 552/1000 [===============>..............] - ETA: 16:51 - loss: 0.6519 - regression_loss: 0.5854 - classification_loss: 0.0665
 553/1000 [===============>..............] - ETA: 16:49 - loss: 0.6518 - regression_loss: 0.5853 - classification_loss: 0.0665
 554/1000 [===============>..............] - ETA: 16:46 - loss: 0.6514 - regression_loss: 0.5849 - classification_loss: 0.0665
 555/1000 [===============>..............] - ETA: 16:43 - loss: 0.6505 - regression_loss: 0.5841 - classification_loss: 0.0664
 556/1000 [===============>..............] - ETA: 16:41 - loss: 0.6509 - regression_loss: 0.5844 - classification_loss: 0.0664
 557/1000 [===============>..............] - ETA: 16:39 - loss: 0.6509 - regression_loss: 0.5844 - classification_loss: 0.0665
 558/1000 [===============>..............] - ETA: 16:37 - loss: 0.6509 - regression_loss: 0.5845 - classification_loss: 0.0665
 559/1000 [===============>..............] - ETA: 16:35 - loss: 0.6512 - regression_loss: 0.5847 - classification_loss: 0.0665
 560/1000 [===============>..............] - ETA: 16:32 - loss: 0.6504 - regression_loss: 0.5840 - classification_loss: 0.0664
 561/1000 [===============>..............] - ETA: 16:30 - loss: 0.6505 - regression_loss: 0.5841 - classification_loss: 0.0664
 562/1000 [===============>..............] - ETA: 16:28 - loss: 0.6501 - regression_loss: 0.5838 - classification_loss: 0.0664
 563/1000 [===============>..............] - ETA: 16:26 - loss: 0.6504 - regression_loss: 0.5840 - classification_loss: 0.0664
 564/1000 [===============>..............] - ETA: 16:23 - loss: 0.6500 - regression_loss: 0.5836 - classification_loss: 0.0664
 565/1000 [===============>..............] - ETA: 16:21 - loss: 0.6503 - regression_loss: 0.5839 - classification_loss: 0.0664
 566/1000 [===============>..............] - ETA: 16:19 - loss: 0.6506 - regression_loss: 0.5841 - classification_loss: 0.0664
 567/1000 [================>.............] - ETA: 16:17 - loss: 0.6505 - regression_loss: 0.5841 - classification_loss: 0.0664
 568/1000 [================>.............] - ETA: 16:15 - loss: 0.6506 - regression_loss: 0.5842 - classification_loss: 0.0664
 569/1000 [================>.............] - ETA: 16:13 - loss: 0.6506 - regression_loss: 0.5841 - classification_loss: 0.0664
 570/1000 [================>.............] - ETA: 16:10 - loss: 0.6497 - regression_loss: 0.5834 - classification_loss: 0.0664
 571/1000 [================>.............] - ETA: 16:08 - loss: 0.6500 - regression_loss: 0.5836 - classification_loss: 0.0664
 572/1000 [================>.............] - ETA: 16:06 - loss: 0.6501 - regression_loss: 0.5837 - classification_loss: 0.0664
 573/1000 [================>.............] - ETA: 16:04 - loss: 0.6503 - regression_loss: 0.5840 - classification_loss: 0.0664
 574/1000 [================>.............] - ETA: 16:01 - loss: 0.6500 - regression_loss: 0.5836 - classification_loss: 0.0664
 575/1000 [================>.............] - ETA: 15:59 - loss: 0.6501 - regression_loss: 0.5837 - classification_loss: 0.0664
 576/1000 [================>.............] - ETA: 15:57 - loss: 0.6500 - regression_loss: 0.5836 - classification_loss: 0.0664
 577/1000 [================>.............] - ETA: 15:54 - loss: 0.6492 - regression_loss: 0.5829 - classification_loss: 0.0663
 578/1000 [================>.............] - ETA: 15:52 - loss: 0.6493 - regression_loss: 0.5830 - classification_loss: 0.0663
 579/1000 [================>.............] - ETA: 15:49 - loss: 0.6484 - regression_loss: 0.5822 - classification_loss: 0.0662
 580/1000 [================>.............] - ETA: 15:47 - loss: 0.6486 - regression_loss: 0.5824 - classification_loss: 0.0662
 581/1000 [================>.............] - ETA: 15:45 - loss: 0.6489 - regression_loss: 0.5826 - classification_loss: 0.0662
 582/1000 [================>.............] - ETA: 15:43 - loss: 0.6488 - regression_loss: 0.5826 - classification_loss: 0.0662
 583/1000 [================>.............] - ETA: 15:41 - loss: 0.6489 - regression_loss: 0.5827 - classification_loss: 0.0662
 584/1000 [================>.............] - ETA: 15:38 - loss: 0.6485 - regression_loss: 0.5823 - classification_loss: 0.0662
 585/1000 [================>.............] - ETA: 15:36 - loss: 0.6481 - regression_loss: 0.5819 - classification_loss: 0.0662
 586/1000 [================>.............] - ETA: 15:34 - loss: 0.6481 - regression_loss: 0.5819 - classification_loss: 0.0662
 587/1000 [================>.............] - ETA: 15:32 - loss: 0.6485 - regression_loss: 0.5823 - classification_loss: 0.0662
 588/1000 [================>.............] - ETA: 15:29 - loss: 0.6478 - regression_loss: 0.5817 - classification_loss: 0.0661
 589/1000 [================>.............] - ETA: 15:27 - loss: 0.6479 - regression_loss: 0.5818 - classification_loss: 0.0661
 590/1000 [================>.............] - ETA: 15:25 - loss: 0.6481 - regression_loss: 0.5819 - classification_loss: 0.0661
 591/1000 [================>.............] - ETA: 15:23 - loss: 0.6483 - regression_loss: 0.5822 - classification_loss: 0.0662
 592/1000 [================>.............] - ETA: 15:20 - loss: 0.6482 - regression_loss: 0.5821 - classification_loss: 0.0662
 593/1000 [================>.............] - ETA: 15:18 - loss: 0.6480 - regression_loss: 0.5819 - classification_loss: 0.0662
 594/1000 [================>.............] - ETA: 15:16 - loss: 0.6482 - regression_loss: 0.5820 - classification_loss: 0.0662
 595/1000 [================>.............] - ETA: 15:13 - loss: 0.6476 - regression_loss: 0.5815 - classification_loss: 0.0661
 596/1000 [================>.............] - ETA: 15:11 - loss: 0.6479 - regression_loss: 0.5818 - classification_loss: 0.0661
 597/1000 [================>.............] - ETA: 15:09 - loss: 0.6480 - regression_loss: 0.5818 - classification_loss: 0.0661
 598/1000 [================>.............] - ETA: 15:07 - loss: 0.6482 - regression_loss: 0.5821 - classification_loss: 0.0662
 599/1000 [================>.............] - ETA: 15:04 - loss: 0.6474 - regression_loss: 0.5813 - classification_loss: 0.0661
 600/1000 [=================>............] - ETA: 15:02 - loss: 0.6476 - regression_loss: 0.5815 - classification_loss: 0.0661
 601/1000 [=================>............] - ETA: 14:59 - loss: 0.6474 - regression_loss: 0.5813 - classification_loss: 0.0661
 602/1000 [=================>............] - ETA: 14:57 - loss: 0.6477 - regression_loss: 0.5816 - classification_loss: 0.0661
 603/1000 [=================>............] - ETA: 14:55 - loss: 0.6477 - regression_loss: 0.5816 - classification_loss: 0.0661
 604/1000 [=================>............] - ETA: 14:53 - loss: 0.6478 - regression_loss: 0.5817 - classification_loss: 0.0661
 605/1000 [=================>............] - ETA: 14:51 - loss: 0.6477 - regression_loss: 0.5816 - classification_loss: 0.0661
 606/1000 [=================>............] - ETA: 14:49 - loss: 0.6477 - regression_loss: 0.5816 - classification_loss: 0.0661
 607/1000 [=================>............] - ETA: 14:47 - loss: 0.6478 - regression_loss: 0.5817 - classification_loss: 0.0661
 608/1000 [=================>............] - ETA: 14:44 - loss: 0.6476 - regression_loss: 0.5815 - classification_loss: 0.0661
 609/1000 [=================>............] - ETA: 14:42 - loss: 0.6479 - regression_loss: 0.5818 - classification_loss: 0.0661
 610/1000 [=================>............] - ETA: 14:40 - loss: 0.6482 - regression_loss: 0.5821 - classification_loss: 0.0661
 611/1000 [=================>............] - ETA: 14:38 - loss: 0.6481 - regression_loss: 0.5820 - classification_loss: 0.0661
 612/1000 [=================>............] - ETA: 14:35 - loss: 0.6474 - regression_loss: 0.5813 - classification_loss: 0.0661
 613/1000 [=================>............] - ETA: 14:33 - loss: 0.6476 - regression_loss: 0.5816 - classification_loss: 0.0661
 614/1000 [=================>............] - ETA: 14:31 - loss: 0.6472 - regression_loss: 0.5812 - classification_loss: 0.0660
 615/1000 [=================>............] - ETA: 14:28 - loss: 0.6472 - regression_loss: 0.5812 - classification_loss: 0.0660
 616/1000 [=================>............] - ETA: 14:26 - loss: 0.6473 - regression_loss: 0.5812 - classification_loss: 0.0660
 617/1000 [=================>............] - ETA: 14:24 - loss: 0.6472 - regression_loss: 0.5812 - classification_loss: 0.0660
 618/1000 [=================>............] - ETA: 14:21 - loss: 0.6465 - regression_loss: 0.5805 - classification_loss: 0.0660
 619/1000 [=================>............] - ETA: 14:19 - loss: 0.6467 - regression_loss: 0.5807 - classification_loss: 0.0660
 620/1000 [=================>............] - ETA: 14:17 - loss: 0.6468 - regression_loss: 0.5809 - classification_loss: 0.0660
 621/1000 [=================>............] - ETA: 14:15 - loss: 0.6467 - regression_loss: 0.5807 - classification_loss: 0.0660
 622/1000 [=================>............] - ETA: 14:13 - loss: 0.6467 - regression_loss: 0.5808 - classification_loss: 0.0660
 623/1000 [=================>............] - ETA: 14:11 - loss: 0.6466 - regression_loss: 0.5807 - classification_loss: 0.0660
 624/1000 [=================>............] - ETA: 14:08 - loss: 0.6466 - regression_loss: 0.5807 - classification_loss: 0.0660
 625/1000 [=================>............] - ETA: 14:06 - loss: 0.6468 - regression_loss: 0.5809 - classification_loss: 0.0660
 626/1000 [=================>............] - ETA: 14:04 - loss: 0.6460 - regression_loss: 0.5801 - classification_loss: 0.0659
 627/1000 [=================>............] - ETA: 14:01 - loss: 0.6456 - regression_loss: 0.5797 - classification_loss: 0.0659
 628/1000 [=================>............] - ETA: 13:59 - loss: 0.6456 - regression_loss: 0.5797 - classification_loss: 0.0659
 629/1000 [=================>............] - ETA: 13:56 - loss: 0.6449 - regression_loss: 0.5791 - classification_loss: 0.0658
 630/1000 [=================>............] - ETA: 13:54 - loss: 0.6452 - regression_loss: 0.5794 - classification_loss: 0.0658
 631/1000 [=================>............] - ETA: 13:52 - loss: 0.6451 - regression_loss: 0.5793 - classification_loss: 0.0658
 632/1000 [=================>............] - ETA: 13:50 - loss: 0.6452 - regression_loss: 0.5793 - classification_loss: 0.0658
 633/1000 [=================>............] - ETA: 13:48 - loss: 0.6454 - regression_loss: 0.5795 - classification_loss: 0.0658
 634/1000 [==================>...........] - ETA: 13:46 - loss: 0.6455 - regression_loss: 0.5797 - classification_loss: 0.0658
 635/1000 [==================>...........] - ETA: 13:44 - loss: 0.6455 - regression_loss: 0.5797 - classification_loss: 0.0658
 636/1000 [==================>...........] - ETA: 13:41 - loss: 0.6456 - regression_loss: 0.5798 - classification_loss: 0.0658
 637/1000 [==================>...........] - ETA: 13:39 - loss: 0.6458 - regression_loss: 0.5799 - classification_loss: 0.0658
 638/1000 [==================>...........] - ETA: 13:37 - loss: 0.6456 - regression_loss: 0.5798 - classification_loss: 0.0658
 639/1000 [==================>...........] - ETA: 13:35 - loss: 0.6454 - regression_loss: 0.5796 - classification_loss: 0.0658
 640/1000 [==================>...........] - ETA: 13:32 - loss: 0.6445 - regression_loss: 0.5788 - classification_loss: 0.0657
 641/1000 [==================>...........] - ETA: 13:30 - loss: 0.6448 - regression_loss: 0.5790 - classification_loss: 0.0657
 642/1000 [==================>...........] - ETA: 13:27 - loss: 0.6439 - regression_loss: 0.5783 - classification_loss: 0.0657
 643/1000 [==================>...........] - ETA: 13:25 - loss: 0.6440 - regression_loss: 0.5783 - classification_loss: 0.0657
 644/1000 [==================>...........] - ETA: 13:23 - loss: 0.6439 - regression_loss: 0.5782 - classification_loss: 0.0657
 645/1000 [==================>...........] - ETA: 13:21 - loss: 0.6441 - regression_loss: 0.5785 - classification_loss: 0.0657
 646/1000 [==================>...........] - ETA: 13:19 - loss: 0.6438 - regression_loss: 0.5781 - classification_loss: 0.0657
 647/1000 [==================>...........] - ETA: 13:16 - loss: 0.6437 - regression_loss: 0.5781 - classification_loss: 0.0656
 648/1000 [==================>...........] - ETA: 13:14 - loss: 0.6438 - regression_loss: 0.5781 - classification_loss: 0.0656
 649/1000 [==================>...........] - ETA: 13:12 - loss: 0.6431 - regression_loss: 0.5775 - classification_loss: 0.0656
 650/1000 [==================>...........] - ETA: 13:09 - loss: 0.6427 - regression_loss: 0.5772 - classification_loss: 0.0655
 651/1000 [==================>...........] - ETA: 13:07 - loss: 0.6427 - regression_loss: 0.5772 - classification_loss: 0.0656
 652/1000 [==================>...........] - ETA: 13:05 - loss: 0.6430 - regression_loss: 0.5774 - classification_loss: 0.0656
 653/1000 [==================>...........] - ETA: 13:03 - loss: 0.6428 - regression_loss: 0.5773 - classification_loss: 0.0656
 654/1000 [==================>...........] - ETA: 13:01 - loss: 0.6430 - regression_loss: 0.5775 - classification_loss: 0.0656
 655/1000 [==================>...........] - ETA: 12:58 - loss: 0.6427 - regression_loss: 0.5771 - classification_loss: 0.0655
 656/1000 [==================>...........] - ETA: 12:56 - loss: 0.6426 - regression_loss: 0.5770 - classification_loss: 0.0656
 657/1000 [==================>...........] - ETA: 12:54 - loss: 0.6427 - regression_loss: 0.5771 - classification_loss: 0.0656
 658/1000 [==================>...........] - ETA: 12:52 - loss: 0.6429 - regression_loss: 0.5773 - classification_loss: 0.0656
 659/1000 [==================>...........] - ETA: 12:49 - loss: 0.6429 - regression_loss: 0.5773 - classification_loss: 0.0656
 660/1000 [==================>...........] - ETA: 12:47 - loss: 0.6422 - regression_loss: 0.5767 - classification_loss: 0.0655
 661/1000 [==================>...........] - ETA: 12:45 - loss: 0.6424 - regression_loss: 0.5769 - classification_loss: 0.0655
 662/1000 [==================>...........] - ETA: 12:42 - loss: 0.6416 - regression_loss: 0.5762 - classification_loss: 0.0654
 663/1000 [==================>...........] - ETA: 12:40 - loss: 0.6417 - regression_loss: 0.5763 - classification_loss: 0.0654
 664/1000 [==================>...........] - ETA: 12:38 - loss: 0.6415 - regression_loss: 0.5761 - classification_loss: 0.0654
 665/1000 [==================>...........] - ETA: 12:35 - loss: 0.6414 - regression_loss: 0.5760 - classification_loss: 0.0654
 666/1000 [==================>...........] - ETA: 12:33 - loss: 0.6414 - regression_loss: 0.5760 - classification_loss: 0.0654
 667/1000 [===================>..........] - ETA: 12:31 - loss: 0.6416 - regression_loss: 0.5762 - classification_loss: 0.0654
 668/1000 [===================>..........] - ETA: 12:29 - loss: 0.6418 - regression_loss: 0.5764 - classification_loss: 0.0654
 669/1000 [===================>..........] - ETA: 12:27 - loss: 0.6418 - regression_loss: 0.5764 - classification_loss: 0.0654
 670/1000 [===================>..........] - ETA: 12:25 - loss: 0.6417 - regression_loss: 0.5762 - classification_loss: 0.0654
 671/1000 [===================>..........] - ETA: 12:22 - loss: 0.6410 - regression_loss: 0.5756 - classification_loss: 0.0654
 672/1000 [===================>..........] - ETA: 12:20 - loss: 0.6410 - regression_loss: 0.5757 - classification_loss: 0.0653
 673/1000 [===================>..........] - ETA: 12:18 - loss: 0.6413 - regression_loss: 0.5759 - classification_loss: 0.0654
 674/1000 [===================>..........] - ETA: 12:16 - loss: 0.6415 - regression_loss: 0.5761 - classification_loss: 0.0654
 675/1000 [===================>..........] - ETA: 12:13 - loss: 0.6411 - regression_loss: 0.5758 - classification_loss: 0.0653
 676/1000 [===================>..........] - ETA: 12:11 - loss: 0.6404 - regression_loss: 0.5751 - classification_loss: 0.0653
 677/1000 [===================>..........] - ETA: 12:08 - loss: 0.6406 - regression_loss: 0.5753 - classification_loss: 0.0653
 678/1000 [===================>..........] - ETA: 12:06 - loss: 0.6406 - regression_loss: 0.5754 - classification_loss: 0.0653
 679/1000 [===================>..........] - ETA: 12:04 - loss: 0.6409 - regression_loss: 0.5756 - classification_loss: 0.0653
 680/1000 [===================>..........] - ETA: 12:02 - loss: 0.6406 - regression_loss: 0.5753 - classification_loss: 0.0653
 681/1000 [===================>..........] - ETA: 11:59 - loss: 0.6406 - regression_loss: 0.5753 - classification_loss: 0.0653
 682/1000 [===================>..........] - ETA: 11:57 - loss: 0.6410 - regression_loss: 0.5757 - classification_loss: 0.0653
 683/1000 [===================>..........] - ETA: 11:55 - loss: 0.6407 - regression_loss: 0.5754 - classification_loss: 0.0653
 684/1000 [===================>..........] - ETA: 11:52 - loss: 0.6408 - regression_loss: 0.5756 - classification_loss: 0.0653
 685/1000 [===================>..........] - ETA: 11:50 - loss: 0.6407 - regression_loss: 0.5754 - classification_loss: 0.0653
 686/1000 [===================>..........] - ETA: 11:48 - loss: 0.6400 - regression_loss: 0.5748 - classification_loss: 0.0652
 687/1000 [===================>..........] - ETA: 11:46 - loss: 0.6403 - regression_loss: 0.5751 - classification_loss: 0.0652
 688/1000 [===================>..........] - ETA: 11:44 - loss: 0.6408 - regression_loss: 0.5756 - classification_loss: 0.0652
 689/1000 [===================>..........] - ETA: 11:41 - loss: 0.6413 - regression_loss: 0.5761 - classification_loss: 0.0652
 690/1000 [===================>..........] - ETA: 11:39 - loss: 0.6415 - regression_loss: 0.5763 - classification_loss: 0.0653
 691/1000 [===================>..........] - ETA: 11:37 - loss: 0.6418 - regression_loss: 0.5766 - classification_loss: 0.0653
 692/1000 [===================>..........] - ETA: 11:35 - loss: 0.6419 - regression_loss: 0.5767 - classification_loss: 0.0653
 693/1000 [===================>..........] - ETA: 11:33 - loss: 0.6422 - regression_loss: 0.5769 - classification_loss: 0.0653
 694/1000 [===================>..........] - ETA: 11:30 - loss: 0.6415 - regression_loss: 0.5763 - classification_loss: 0.0652
 695/1000 [===================>..........] - ETA: 11:28 - loss: 0.6413 - regression_loss: 0.5761 - classification_loss: 0.0652
 696/1000 [===================>..........] - ETA: 11:26 - loss: 0.6417 - regression_loss: 0.5765 - classification_loss: 0.0652
 697/1000 [===================>..........] - ETA: 11:23 - loss: 0.6412 - regression_loss: 0.5760 - classification_loss: 0.0652
 698/1000 [===================>..........] - ETA: 11:21 - loss: 0.6418 - regression_loss: 0.5766 - classification_loss: 0.0652
 699/1000 [===================>..........] - ETA: 11:19 - loss: 0.6425 - regression_loss: 0.5773 - classification_loss: 0.0652
 700/1000 [====================>.........] - ETA: 11:17 - loss: 0.6427 - regression_loss: 0.5775 - classification_loss: 0.0652
 701/1000 [====================>.........] - ETA: 11:15 - loss: 0.6430 - regression_loss: 0.5778 - classification_loss: 0.0652
 702/1000 [====================>.........] - ETA: 11:12 - loss: 0.6431 - regression_loss: 0.5778 - classification_loss: 0.0653
 703/1000 [====================>.........] - ETA: 11:10 - loss: 0.6431 - regression_loss: 0.5779 - classification_loss: 0.0653
 704/1000 [====================>.........] - ETA: 11:07 - loss: 0.6428 - regression_loss: 0.5776 - classification_loss: 0.0652
 705/1000 [====================>.........] - ETA: 11:05 - loss: 0.6432 - regression_loss: 0.5780 - classification_loss: 0.0652
 706/1000 [====================>.........] - ETA: 11:03 - loss: 0.6433 - regression_loss: 0.5780 - classification_loss: 0.0652
 707/1000 [====================>.........] - ETA: 11:00 - loss: 0.6431 - regression_loss: 0.5779 - classification_loss: 0.0652
 708/1000 [====================>.........] - ETA: 10:58 - loss: 0.6434 - regression_loss: 0.5782 - classification_loss: 0.0652
 709/1000 [====================>.........] - ETA: 10:56 - loss: 0.6436 - regression_loss: 0.5784 - classification_loss: 0.0652
 710/1000 [====================>.........] - ETA: 10:54 - loss: 0.6438 - regression_loss: 0.5785 - classification_loss: 0.0652
 711/1000 [====================>.........] - ETA: 10:52 - loss: 0.6438 - regression_loss: 0.5786 - classification_loss: 0.0652
 712/1000 [====================>.........] - ETA: 10:50 - loss: 0.6441 - regression_loss: 0.5789 - classification_loss: 0.0652
 713/1000 [====================>.........] - ETA: 10:47 - loss: 0.6440 - regression_loss: 0.5788 - classification_loss: 0.0652
 714/1000 [====================>.........] - ETA: 10:45 - loss: 0.6442 - regression_loss: 0.5790 - classification_loss: 0.0652
 715/1000 [====================>.........] - ETA: 10:43 - loss: 0.6443 - regression_loss: 0.5791 - classification_loss: 0.0652
 716/1000 [====================>.........] - ETA: 10:40 - loss: 0.6437 - regression_loss: 0.5785 - classification_loss: 0.0652
 717/1000 [====================>.........] - ETA: 10:38 - loss: 0.6439 - regression_loss: 0.5787 - classification_loss: 0.0652
 718/1000 [====================>.........] - ETA: 10:36 - loss: 0.6432 - regression_loss: 0.5781 - classification_loss: 0.0651
 719/1000 [====================>.........] - ETA: 10:34 - loss: 0.6432 - regression_loss: 0.5780 - classification_loss: 0.0651
 720/1000 [====================>.........] - ETA: 10:32 - loss: 0.6434 - regression_loss: 0.5782 - classification_loss: 0.0651
 721/1000 [====================>.........] - ETA: 10:29 - loss: 0.6435 - regression_loss: 0.5783 - classification_loss: 0.0651
 722/1000 [====================>.........] - ETA: 10:27 - loss: 0.6435 - regression_loss: 0.5783 - classification_loss: 0.0651
 723/1000 [====================>.........] - ETA: 10:25 - loss: 0.6437 - regression_loss: 0.5785 - classification_loss: 0.0652
 724/1000 [====================>.........] - ETA: 10:23 - loss: 0.6434 - regression_loss: 0.5783 - classification_loss: 0.0651
 725/1000 [====================>.........] - ETA: 10:20 - loss: 0.6436 - regression_loss: 0.5785 - classification_loss: 0.0652
 726/1000 [====================>.........] - ETA: 10:18 - loss: 0.6429 - regression_loss: 0.5779 - classification_loss: 0.0651
 727/1000 [====================>.........] - ETA: 10:16 - loss: 0.6430 - regression_loss: 0.5779 - classification_loss: 0.0651
 728/1000 [====================>.........] - ETA: 10:14 - loss: 0.6429 - regression_loss: 0.5778 - classification_loss: 0.0651
 729/1000 [====================>.........] - ETA: 10:11 - loss: 0.6431 - regression_loss: 0.5780 - classification_loss: 0.0651
 730/1000 [====================>.........] - ETA: 10:09 - loss: 0.6431 - regression_loss: 0.5780 - classification_loss: 0.0651
 731/1000 [====================>.........] - ETA: 10:07 - loss: 0.6428 - regression_loss: 0.5777 - classification_loss: 0.0651
 732/1000 [====================>.........] - ETA: 10:04 - loss: 0.6424 - regression_loss: 0.5774 - classification_loss: 0.0651
 733/1000 [====================>.........] - ETA: 10:02 - loss: 0.6425 - regression_loss: 0.5775 - classification_loss: 0.0651
 734/1000 [=====================>........] - ETA: 10:00 - loss: 0.6419 - regression_loss: 0.5769 - classification_loss: 0.0650
 735/1000 [=====================>........] - ETA: 9:57 - loss: 0.6420 - regression_loss: 0.5769 - classification_loss: 0.0650 
 736/1000 [=====================>........] - ETA: 9:55 - loss: 0.6422 - regression_loss: 0.5771 - classification_loss: 0.0650
 737/1000 [=====================>........] - ETA: 9:53 - loss: 0.6421 - regression_loss: 0.5770 - classification_loss: 0.0650
 738/1000 [=====================>........] - ETA: 9:51 - loss: 0.6422 - regression_loss: 0.5772 - classification_loss: 0.0650
 739/1000 [=====================>........] - ETA: 9:49 - loss: 0.6424 - regression_loss: 0.5774 - classification_loss: 0.0650
 740/1000 [=====================>........] - ETA: 9:47 - loss: 0.6424 - regression_loss: 0.5774 - classification_loss: 0.0650
 741/1000 [=====================>........] - ETA: 9:44 - loss: 0.6423 - regression_loss: 0.5772 - classification_loss: 0.0650
 742/1000 [=====================>........] - ETA: 9:42 - loss: 0.6416 - regression_loss: 0.5766 - classification_loss: 0.0650
 743/1000 [=====================>........] - ETA: 9:39 - loss: 0.6415 - regression_loss: 0.5765 - classification_loss: 0.0650
 744/1000 [=====================>........] - ETA: 9:37 - loss: 0.6416 - regression_loss: 0.5766 - classification_loss: 0.0650
 745/1000 [=====================>........] - ETA: 9:35 - loss: 0.6418 - regression_loss: 0.5768 - classification_loss: 0.0650
 746/1000 [=====================>........] - ETA: 9:33 - loss: 0.6412 - regression_loss: 0.5762 - classification_loss: 0.0649
 747/1000 [=====================>........] - ETA: 9:31 - loss: 0.6412 - regression_loss: 0.5763 - classification_loss: 0.0649
 748/1000 [=====================>........] - ETA: 9:28 - loss: 0.6411 - regression_loss: 0.5762 - classification_loss: 0.0649
 749/1000 [=====================>........] - ETA: 9:26 - loss: 0.6411 - regression_loss: 0.5762 - classification_loss: 0.0649
 750/1000 [=====================>........] - ETA: 9:24 - loss: 0.6413 - regression_loss: 0.5764 - classification_loss: 0.0649
 751/1000 [=====================>........] - ETA: 9:22 - loss: 0.6411 - regression_loss: 0.5762 - classification_loss: 0.0649
 752/1000 [=====================>........] - ETA: 9:19 - loss: 0.6412 - regression_loss: 0.5763 - classification_loss: 0.0649
 753/1000 [=====================>........] - ETA: 9:17 - loss: 0.6413 - regression_loss: 0.5764 - classification_loss: 0.0649
 754/1000 [=====================>........] - ETA: 9:15 - loss: 0.6415 - regression_loss: 0.5766 - classification_loss: 0.0649
 755/1000 [=====================>........] - ETA: 9:13 - loss: 0.6413 - regression_loss: 0.5764 - classification_loss: 0.0649
 756/1000 [=====================>........] - ETA: 9:11 - loss: 0.6415 - regression_loss: 0.5766 - classification_loss: 0.0649
 757/1000 [=====================>........] - ETA: 9:08 - loss: 0.6408 - regression_loss: 0.5760 - classification_loss: 0.0649
 758/1000 [=====================>........] - ETA: 9:06 - loss: 0.6405 - regression_loss: 0.5757 - classification_loss: 0.0648
 759/1000 [=====================>........] - ETA: 9:04 - loss: 0.6406 - regression_loss: 0.5757 - classification_loss: 0.0649
 760/1000 [=====================>........] - ETA: 9:01 - loss: 0.6407 - regression_loss: 0.5759 - classification_loss: 0.0649
 761/1000 [=====================>........] - ETA: 8:59 - loss: 0.6404 - regression_loss: 0.5755 - classification_loss: 0.0649
 762/1000 [=====================>........] - ETA: 8:57 - loss: 0.6403 - regression_loss: 0.5754 - classification_loss: 0.0649
 763/1000 [=====================>........] - ETA: 8:55 - loss: 0.6403 - regression_loss: 0.5754 - classification_loss: 0.0649
 764/1000 [=====================>........] - ETA: 8:52 - loss: 0.6404 - regression_loss: 0.5756 - classification_loss: 0.0649
 765/1000 [=====================>........] - ETA: 8:50 - loss: 0.6398 - regression_loss: 0.5750 - classification_loss: 0.0648
 766/1000 [=====================>........] - ETA: 8:48 - loss: 0.6398 - regression_loss: 0.5750 - classification_loss: 0.0648
 767/1000 [======================>.......] - ETA: 8:46 - loss: 0.6400 - regression_loss: 0.5752 - classification_loss: 0.0648
 768/1000 [======================>.......] - ETA: 8:43 - loss: 0.6394 - regression_loss: 0.5747 - classification_loss: 0.0648
 769/1000 [======================>.......] - ETA: 8:41 - loss: 0.6392 - regression_loss: 0.5745 - classification_loss: 0.0647
 770/1000 [======================>.......] - ETA: 8:39 - loss: 0.6391 - regression_loss: 0.5744 - classification_loss: 0.0647
 771/1000 [======================>.......] - ETA: 8:36 - loss: 0.6393 - regression_loss: 0.5745 - classification_loss: 0.0647
 772/1000 [======================>.......] - ETA: 8:34 - loss: 0.6392 - regression_loss: 0.5745 - classification_loss: 0.0647
 773/1000 [======================>.......] - ETA: 8:32 - loss: 0.6393 - regression_loss: 0.5745 - classification_loss: 0.0647
 774/1000 [======================>.......] - ETA: 8:30 - loss: 0.6392 - regression_loss: 0.5745 - classification_loss: 0.0647
 775/1000 [======================>.......] - ETA: 8:28 - loss: 0.6393 - regression_loss: 0.5746 - classification_loss: 0.0647
 776/1000 [======================>.......] - ETA: 8:25 - loss: 0.6392 - regression_loss: 0.5745 - classification_loss: 0.0647
 777/1000 [======================>.......] - ETA: 8:23 - loss: 0.6389 - regression_loss: 0.5742 - classification_loss: 0.0647
 778/1000 [======================>.......] - ETA: 8:21 - loss: 0.6391 - regression_loss: 0.5744 - classification_loss: 0.0647
 779/1000 [======================>.......] - ETA: 8:19 - loss: 0.6391 - regression_loss: 0.5744 - classification_loss: 0.0647
 780/1000 [======================>.......] - ETA: 8:16 - loss: 0.6385 - regression_loss: 0.5738 - classification_loss: 0.0647
 781/1000 [======================>.......] - ETA: 8:14 - loss: 0.6387 - regression_loss: 0.5740 - classification_loss: 0.0647
 782/1000 [======================>.......] - ETA: 8:12 - loss: 0.6380 - regression_loss: 0.5734 - classification_loss: 0.0646
 783/1000 [======================>.......] - ETA: 8:09 - loss: 0.6379 - regression_loss: 0.5733 - classification_loss: 0.0646
 784/1000 [======================>.......] - ETA: 8:07 - loss: 0.6380 - regression_loss: 0.5734 - classification_loss: 0.0646
 785/1000 [======================>.......] - ETA: 8:05 - loss: 0.6379 - regression_loss: 0.5733 - classification_loss: 0.0646
 786/1000 [======================>.......] - ETA: 8:03 - loss: 0.6378 - regression_loss: 0.5733 - classification_loss: 0.0646
 787/1000 [======================>.......] - ETA: 8:00 - loss: 0.6380 - regression_loss: 0.5734 - classification_loss: 0.0646
 788/1000 [======================>.......] - ETA: 7:58 - loss: 0.6381 - regression_loss: 0.5736 - classification_loss: 0.0646
 789/1000 [======================>.......] - ETA: 7:56 - loss: 0.6380 - regression_loss: 0.5734 - classification_loss: 0.0646
 790/1000 [======================>.......] - ETA: 7:54 - loss: 0.6382 - regression_loss: 0.5736 - classification_loss: 0.0646
 791/1000 [======================>.......] - ETA: 7:51 - loss: 0.6376 - regression_loss: 0.5731 - classification_loss: 0.0645
 792/1000 [======================>.......] - ETA: 7:49 - loss: 0.6376 - regression_loss: 0.5731 - classification_loss: 0.0645
 793/1000 [======================>.......] - ETA: 7:47 - loss: 0.6373 - regression_loss: 0.5728 - classification_loss: 0.0645
 794/1000 [======================>.......] - ETA: 7:45 - loss: 0.6373 - regression_loss: 0.5728 - classification_loss: 0.0645
 795/1000 [======================>.......] - ETA: 7:42 - loss: 0.6373 - regression_loss: 0.5728 - classification_loss: 0.0645
 796/1000 [======================>.......] - ETA: 7:40 - loss: 0.6372 - regression_loss: 0.5727 - classification_loss: 0.0645
 797/1000 [======================>.......] - ETA: 7:38 - loss: 0.6372 - regression_loss: 0.5727 - classification_loss: 0.0645
 798/1000 [======================>.......] - ETA: 7:36 - loss: 0.6374 - regression_loss: 0.5729 - classification_loss: 0.0645
 799/1000 [======================>.......] - ETA: 7:33 - loss: 0.6372 - regression_loss: 0.5727 - classification_loss: 0.0645
 800/1000 [=======================>......] - ETA: 7:31 - loss: 0.6365 - regression_loss: 0.5721 - classification_loss: 0.0644
 801/1000 [=======================>......] - ETA: 7:29 - loss: 0.6367 - regression_loss: 0.5723 - classification_loss: 0.0644
 802/1000 [=======================>......] - ETA: 7:27 - loss: 0.6367 - regression_loss: 0.5722 - classification_loss: 0.0644
 803/1000 [=======================>......] - ETA: 7:24 - loss: 0.6368 - regression_loss: 0.5724 - classification_loss: 0.0644
 804/1000 [=======================>......] - ETA: 7:22 - loss: 0.6369 - regression_loss: 0.5725 - classification_loss: 0.0644
 805/1000 [=======================>......] - ETA: 7:20 - loss: 0.6363 - regression_loss: 0.5720 - classification_loss: 0.0644
 806/1000 [=======================>......] - ETA: 7:18 - loss: 0.6362 - regression_loss: 0.5718 - classification_loss: 0.0644
 807/1000 [=======================>......] - ETA: 7:15 - loss: 0.6363 - regression_loss: 0.5719 - classification_loss: 0.0644
 808/1000 [=======================>......] - ETA: 7:13 - loss: 0.6359 - regression_loss: 0.5716 - classification_loss: 0.0644
 809/1000 [=======================>......] - ETA: 7:11 - loss: 0.6359 - regression_loss: 0.5716 - classification_loss: 0.0643
 810/1000 [=======================>......] - ETA: 7:08 - loss: 0.6354 - regression_loss: 0.5711 - classification_loss: 0.0643
 811/1000 [=======================>......] - ETA: 7:06 - loss: 0.6351 - regression_loss: 0.5708 - classification_loss: 0.0643
 812/1000 [=======================>......] - ETA: 7:04 - loss: 0.6352 - regression_loss: 0.5710 - classification_loss: 0.0643
 813/1000 [=======================>......] - ETA: 7:02 - loss: 0.6354 - regression_loss: 0.5711 - classification_loss: 0.0643
 814/1000 [=======================>......] - ETA: 6:59 - loss: 0.6353 - regression_loss: 0.5710 - classification_loss: 0.0643
 815/1000 [=======================>......] - ETA: 6:57 - loss: 0.6353 - regression_loss: 0.5710 - classification_loss: 0.0643
 816/1000 [=======================>......] - ETA: 6:55 - loss: 0.6350 - regression_loss: 0.5707 - classification_loss: 0.0643
 817/1000 [=======================>......] - ETA: 6:53 - loss: 0.6351 - regression_loss: 0.5709 - classification_loss: 0.0643
 818/1000 [=======================>......] - ETA: 6:50 - loss: 0.6352 - regression_loss: 0.5709 - classification_loss: 0.0643
 819/1000 [=======================>......] - ETA: 6:48 - loss: 0.6345 - regression_loss: 0.5703 - classification_loss: 0.0642
 820/1000 [=======================>......] - ETA: 6:46 - loss: 0.6344 - regression_loss: 0.5702 - classification_loss: 0.0642
 821/1000 [=======================>......] - ETA: 6:44 - loss: 0.6344 - regression_loss: 0.5702 - classification_loss: 0.0642
 822/1000 [=======================>......] - ETA: 6:41 - loss: 0.6345 - regression_loss: 0.5703 - classification_loss: 0.0642
 823/1000 [=======================>......] - ETA: 6:39 - loss: 0.6347 - regression_loss: 0.5705 - classification_loss: 0.0642
 824/1000 [=======================>......] - ETA: 6:37 - loss: 0.6346 - regression_loss: 0.5704 - classification_loss: 0.0642
 825/1000 [=======================>......] - ETA: 6:35 - loss: 0.6346 - regression_loss: 0.5704 - classification_loss: 0.0642
 826/1000 [=======================>......] - ETA: 6:33 - loss: 0.6347 - regression_loss: 0.5705 - classification_loss: 0.0642
 827/1000 [=======================>......] - ETA: 6:30 - loss: 0.6344 - regression_loss: 0.5702 - classification_loss: 0.0642
 828/1000 [=======================>......] - ETA: 6:28 - loss: 0.6339 - regression_loss: 0.5697 - classification_loss: 0.0641
 829/1000 [=======================>......] - ETA: 6:26 - loss: 0.6339 - regression_loss: 0.5698 - classification_loss: 0.0641
 830/1000 [=======================>......] - ETA: 6:23 - loss: 0.6339 - regression_loss: 0.5698 - classification_loss: 0.0641
 831/1000 [=======================>......] - ETA: 6:21 - loss: 0.6340 - regression_loss: 0.5698 - classification_loss: 0.0641
 832/1000 [=======================>......] - ETA: 6:19 - loss: 0.6339 - regression_loss: 0.5697 - classification_loss: 0.0641
 833/1000 [=======================>......] - ETA: 6:17 - loss: 0.6340 - regression_loss: 0.5699 - classification_loss: 0.0641
 834/1000 [========================>.....] - ETA: 6:15 - loss: 0.6342 - regression_loss: 0.5700 - classification_loss: 0.0642
 835/1000 [========================>.....] - ETA: 6:12 - loss: 0.6336 - regression_loss: 0.5695 - classification_loss: 0.0641
 836/1000 [========================>.....] - ETA: 6:10 - loss: 0.6333 - regression_loss: 0.5692 - classification_loss: 0.0641
 837/1000 [========================>.....] - ETA: 6:08 - loss: 0.6334 - regression_loss: 0.5693 - classification_loss: 0.0641
 838/1000 [========================>.....] - ETA: 6:05 - loss: 0.6333 - regression_loss: 0.5692 - classification_loss: 0.0641
 839/1000 [========================>.....] - ETA: 6:03 - loss: 0.6333 - regression_loss: 0.5692 - classification_loss: 0.0641
 840/1000 [========================>.....] - ETA: 6:01 - loss: 0.6330 - regression_loss: 0.5690 - classification_loss: 0.0641
 841/1000 [========================>.....] - ETA: 5:58 - loss: 0.6325 - regression_loss: 0.5685 - classification_loss: 0.0640
 842/1000 [========================>.....] - ETA: 5:56 - loss: 0.6327 - regression_loss: 0.5687 - classification_loss: 0.0640
 843/1000 [========================>.....] - ETA: 5:54 - loss: 0.6328 - regression_loss: 0.5688 - classification_loss: 0.0640
 844/1000 [========================>.....] - ETA: 5:52 - loss: 0.6325 - regression_loss: 0.5685 - classification_loss: 0.0640
 845/1000 [========================>.....] - ETA: 5:49 - loss: 0.6325 - regression_loss: 0.5685 - classification_loss: 0.0640
 846/1000 [========================>.....] - ETA: 5:47 - loss: 0.6324 - regression_loss: 0.5684 - classification_loss: 0.0640
 847/1000 [========================>.....] - ETA: 5:45 - loss: 0.6318 - regression_loss: 0.5679 - classification_loss: 0.0639
 848/1000 [========================>.....] - ETA: 5:43 - loss: 0.6320 - regression_loss: 0.5680 - classification_loss: 0.0640
 849/1000 [========================>.....] - ETA: 5:40 - loss: 0.6321 - regression_loss: 0.5682 - classification_loss: 0.0640
 850/1000 [========================>.....] - ETA: 5:38 - loss: 0.6321 - regression_loss: 0.5682 - classification_loss: 0.0640
 851/1000 [========================>.....] - ETA: 5:36 - loss: 0.6316 - regression_loss: 0.5677 - classification_loss: 0.0639
 852/1000 [========================>.....] - ETA: 5:34 - loss: 0.6317 - regression_loss: 0.5678 - classification_loss: 0.0639
 853/1000 [========================>.....] - ETA: 5:31 - loss: 0.6318 - regression_loss: 0.5679 - classification_loss: 0.0639
 854/1000 [========================>.....] - ETA: 5:29 - loss: 0.6319 - regression_loss: 0.5680 - classification_loss: 0.0639
 855/1000 [========================>.....] - ETA: 5:27 - loss: 0.6316 - regression_loss: 0.5677 - classification_loss: 0.0639
 856/1000 [========================>.....] - ETA: 5:25 - loss: 0.6316 - regression_loss: 0.5677 - classification_loss: 0.0639
 857/1000 [========================>.....] - ETA: 5:22 - loss: 0.6314 - regression_loss: 0.5676 - classification_loss: 0.0639
 858/1000 [========================>.....] - ETA: 5:20 - loss: 0.6308 - regression_loss: 0.5670 - classification_loss: 0.0638
 859/1000 [========================>.....] - ETA: 5:18 - loss: 0.6309 - regression_loss: 0.5671 - classification_loss: 0.0638
 860/1000 [========================>.....] - ETA: 5:16 - loss: 0.6311 - regression_loss: 0.5672 - classification_loss: 0.0638
 861/1000 [========================>.....] - ETA: 5:13 - loss: 0.6312 - regression_loss: 0.5674 - classification_loss: 0.0638
 862/1000 [========================>.....] - ETA: 5:11 - loss: 0.6311 - regression_loss: 0.5673 - classification_loss: 0.0638
 863/1000 [========================>.....] - ETA: 5:09 - loss: 0.6311 - regression_loss: 0.5673 - classification_loss: 0.0638
 864/1000 [========================>.....] - ETA: 5:07 - loss: 0.6308 - regression_loss: 0.5670 - classification_loss: 0.0638
 865/1000 [========================>.....] - ETA: 5:04 - loss: 0.6310 - regression_loss: 0.5672 - classification_loss: 0.0638
 866/1000 [========================>.....] - ETA: 5:02 - loss: 0.6311 - regression_loss: 0.5673 - classification_loss: 0.0638
 867/1000 [=========================>....] - ETA: 5:00 - loss: 0.6311 - regression_loss: 0.5672 - classification_loss: 0.0638
 868/1000 [=========================>....] - ETA: 4:58 - loss: 0.6310 - regression_loss: 0.5672 - classification_loss: 0.0638
 869/1000 [=========================>....] - ETA: 4:55 - loss: 0.6311 - regression_loss: 0.5673 - classification_loss: 0.0638
 870/1000 [=========================>....] - ETA: 4:53 - loss: 0.6306 - regression_loss: 0.5668 - classification_loss: 0.0638
 871/1000 [=========================>....] - ETA: 4:51 - loss: 0.6304 - regression_loss: 0.5666 - classification_loss: 0.0637
 872/1000 [=========================>....] - ETA: 4:49 - loss: 0.6305 - regression_loss: 0.5668 - classification_loss: 0.0638
 873/1000 [=========================>....] - ETA: 4:46 - loss: 0.6306 - regression_loss: 0.5668 - classification_loss: 0.0638
 874/1000 [=========================>....] - ETA: 4:44 - loss: 0.6307 - regression_loss: 0.5669 - classification_loss: 0.0638
 875/1000 [=========================>....] - ETA: 4:42 - loss: 0.6301 - regression_loss: 0.5664 - classification_loss: 0.0637
 876/1000 [=========================>....] - ETA: 4:40 - loss: 0.6303 - regression_loss: 0.5666 - classification_loss: 0.0637
 877/1000 [=========================>....] - ETA: 4:37 - loss: 0.6302 - regression_loss: 0.5665 - classification_loss: 0.0637
 878/1000 [=========================>....] - ETA: 4:35 - loss: 0.6300 - regression_loss: 0.5662 - classification_loss: 0.0637
 879/1000 [=========================>....] - ETA: 4:33 - loss: 0.6300 - regression_loss: 0.5663 - classification_loss: 0.0637
 880/1000 [=========================>....] - ETA: 4:30 - loss: 0.6299 - regression_loss: 0.5662 - classification_loss: 0.0637
 881/1000 [=========================>....] - ETA: 4:28 - loss: 0.6300 - regression_loss: 0.5663 - classification_loss: 0.0637
 882/1000 [=========================>....] - ETA: 4:26 - loss: 0.6301 - regression_loss: 0.5664 - classification_loss: 0.0637
 883/1000 [=========================>....] - ETA: 4:24 - loss: 0.6301 - regression_loss: 0.5664 - classification_loss: 0.0637
 884/1000 [=========================>....] - ETA: 4:21 - loss: 0.6295 - regression_loss: 0.5658 - classification_loss: 0.0637
 885/1000 [=========================>....] - ETA: 4:19 - loss: 0.6292 - regression_loss: 0.5656 - classification_loss: 0.0637
 886/1000 [=========================>....] - ETA: 4:17 - loss: 0.6294 - regression_loss: 0.5657 - classification_loss: 0.0637
 887/1000 [=========================>....] - ETA: 4:15 - loss: 0.6293 - regression_loss: 0.5656 - classification_loss: 0.0637
 888/1000 [=========================>....] - ETA: 4:12 - loss: 0.6288 - regression_loss: 0.5652 - classification_loss: 0.0636
 889/1000 [=========================>....] - ETA: 4:10 - loss: 0.6289 - regression_loss: 0.5652 - classification_loss: 0.0636
 890/1000 [=========================>....] - ETA: 4:08 - loss: 0.6288 - regression_loss: 0.5652 - classification_loss: 0.0636
 891/1000 [=========================>....] - ETA: 4:06 - loss: 0.6285 - regression_loss: 0.5649 - classification_loss: 0.0636
 892/1000 [=========================>....] - ETA: 4:03 - loss: 0.6287 - regression_loss: 0.5651 - classification_loss: 0.0636
 893/1000 [=========================>....] - ETA: 4:01 - loss: 0.6286 - regression_loss: 0.5650 - classification_loss: 0.0636
 894/1000 [=========================>....] - ETA: 3:59 - loss: 0.6287 - regression_loss: 0.5651 - classification_loss: 0.0636
 895/1000 [=========================>....] - ETA: 3:57 - loss: 0.6287 - regression_loss: 0.5651 - classification_loss: 0.0636
 896/1000 [=========================>....] - ETA: 3:54 - loss: 0.6288 - regression_loss: 0.5652 - classification_loss: 0.0636
 897/1000 [=========================>....] - ETA: 3:52 - loss: 0.6282 - regression_loss: 0.5647 - classification_loss: 0.0636
 898/1000 [=========================>....] - ETA: 3:50 - loss: 0.6283 - regression_loss: 0.5647 - classification_loss: 0.0636
 899/1000 [=========================>....] - ETA: 3:48 - loss: 0.6281 - regression_loss: 0.5645 - classification_loss: 0.0636
 900/1000 [==========================>...] - ETA: 3:45 - loss: 0.6278 - regression_loss: 0.5643 - classification_loss: 0.0636
 901/1000 [==========================>...] - ETA: 3:43 - loss: 0.6279 - regression_loss: 0.5644 - classification_loss: 0.0636
 902/1000 [==========================>...] - ETA: 3:41 - loss: 0.6281 - regression_loss: 0.5645 - classification_loss: 0.0636
 903/1000 [==========================>...] - ETA: 3:39 - loss: 0.6281 - regression_loss: 0.5645 - classification_loss: 0.0635
 904/1000 [==========================>...] - ETA: 3:36 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0635
 905/1000 [==========================>...] - ETA: 3:34 - loss: 0.6277 - regression_loss: 0.5642 - classification_loss: 0.0635
 906/1000 [==========================>...] - ETA: 3:32 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0635
 907/1000 [==========================>...] - ETA: 3:29 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0635
 908/1000 [==========================>...] - ETA: 3:27 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0635
 909/1000 [==========================>...] - ETA: 3:25 - loss: 0.6277 - regression_loss: 0.5642 - classification_loss: 0.0635
 910/1000 [==========================>...] - ETA: 3:23 - loss: 0.6272 - regression_loss: 0.5637 - classification_loss: 0.0635
 911/1000 [==========================>...] - ETA: 3:20 - loss: 0.6274 - regression_loss: 0.5639 - classification_loss: 0.0635
 912/1000 [==========================>...] - ETA: 3:18 - loss: 0.6275 - regression_loss: 0.5641 - classification_loss: 0.0635
 913/1000 [==========================>...] - ETA: 3:16 - loss: 0.6277 - regression_loss: 0.5642 - classification_loss: 0.0635
 914/1000 [==========================>...] - ETA: 3:14 - loss: 0.6275 - regression_loss: 0.5641 - classification_loss: 0.0635
 915/1000 [==========================>...] - ETA: 3:11 - loss: 0.6271 - regression_loss: 0.5637 - classification_loss: 0.0634
 916/1000 [==========================>...] - ETA: 3:09 - loss: 0.6272 - regression_loss: 0.5638 - classification_loss: 0.0634
 917/1000 [==========================>...] - ETA: 3:07 - loss: 0.6274 - regression_loss: 0.5639 - classification_loss: 0.0634
 918/1000 [==========================>...] - ETA: 3:05 - loss: 0.6273 - regression_loss: 0.5638 - classification_loss: 0.0634
 919/1000 [==========================>...] - ETA: 3:02 - loss: 0.6273 - regression_loss: 0.5639 - classification_loss: 0.0634
 920/1000 [==========================>...] - ETA: 3:00 - loss: 0.6275 - regression_loss: 0.5640 - classification_loss: 0.0634
 921/1000 [==========================>...] - ETA: 2:58 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0635
 922/1000 [==========================>...] - ETA: 2:56 - loss: 0.6275 - regression_loss: 0.5640 - classification_loss: 0.0635
 923/1000 [==========================>...] - ETA: 2:53 - loss: 0.6273 - regression_loss: 0.5639 - classification_loss: 0.0634
 924/1000 [==========================>...] - ETA: 2:51 - loss: 0.6276 - regression_loss: 0.5641 - classification_loss: 0.0634
 925/1000 [==========================>...] - ETA: 2:49 - loss: 0.6271 - regression_loss: 0.5637 - classification_loss: 0.0634
 926/1000 [==========================>...] - ETA: 2:47 - loss: 0.6274 - regression_loss: 0.5639 - classification_loss: 0.0634
 927/1000 [==========================>...] - ETA: 2:44 - loss: 0.6276 - regression_loss: 0.5642 - classification_loss: 0.0634
 928/1000 [==========================>...] - ETA: 2:42 - loss: 0.6278 - regression_loss: 0.5644 - classification_loss: 0.0634
 929/1000 [==========================>...] - ETA: 2:40 - loss: 0.6276 - regression_loss: 0.5642 - classification_loss: 0.0634
 930/1000 [==========================>...] - ETA: 2:38 - loss: 0.6277 - regression_loss: 0.5643 - classification_loss: 0.0634
 931/1000 [==========================>...] - ETA: 2:35 - loss: 0.6279 - regression_loss: 0.5645 - classification_loss: 0.0634
 932/1000 [==========================>...] - ETA: 2:33 - loss: 0.6279 - regression_loss: 0.5645 - classification_loss: 0.0634
 933/1000 [==========================>...] - ETA: 2:31 - loss: 0.6274 - regression_loss: 0.5641 - classification_loss: 0.0634
 934/1000 [===========================>..] - ETA: 2:29 - loss: 0.6274 - regression_loss: 0.5640 - classification_loss: 0.0634
 935/1000 [===========================>..] - ETA: 2:26 - loss: 0.6273 - regression_loss: 0.5640 - classification_loss: 0.0634
 936/1000 [===========================>..] - ETA: 2:24 - loss: 0.6269 - regression_loss: 0.5636 - classification_loss: 0.0633
 937/1000 [===========================>..] - ETA: 2:22 - loss: 0.6271 - regression_loss: 0.5637 - classification_loss: 0.0633
 938/1000 [===========================>..] - ETA: 2:19 - loss: 0.6269 - regression_loss: 0.5636 - classification_loss: 0.0633
 939/1000 [===========================>..] - ETA: 2:17 - loss: 0.6270 - regression_loss: 0.5637 - classification_loss: 0.0633
 940/1000 [===========================>..] - ETA: 2:15 - loss: 0.6272 - regression_loss: 0.5638 - classification_loss: 0.0633
 941/1000 [===========================>..] - ETA: 2:13 - loss: 0.6272 - regression_loss: 0.5639 - classification_loss: 0.0633
 942/1000 [===========================>..] - ETA: 2:10 - loss: 0.6271 - regression_loss: 0.5638 - classification_loss: 0.0633
 943/1000 [===========================>..] - ETA: 2:08 - loss: 0.6266 - regression_loss: 0.5633 - classification_loss: 0.0633
 944/1000 [===========================>..] - ETA: 2:06 - loss: 0.6265 - regression_loss: 0.5633 - classification_loss: 0.0633
 945/1000 [===========================>..] - ETA: 2:04 - loss: 0.6267 - regression_loss: 0.5634 - classification_loss: 0.0633
 946/1000 [===========================>..] - ETA: 2:01 - loss: 0.6268 - regression_loss: 0.5635 - classification_loss: 0.0633
 947/1000 [===========================>..] - ETA: 1:59 - loss: 0.6268 - regression_loss: 0.5636 - classification_loss: 0.0633
 948/1000 [===========================>..] - ETA: 1:57 - loss: 0.6266 - regression_loss: 0.5633 - classification_loss: 0.0633
 949/1000 [===========================>..] - ETA: 1:55 - loss: 0.6267 - regression_loss: 0.5635 - classification_loss: 0.0633
 950/1000 [===========================>..] - ETA: 1:52 - loss: 0.6268 - regression_loss: 0.5635 - classification_loss: 0.0633
 951/1000 [===========================>..] - ETA: 1:50 - loss: 0.6269 - regression_loss: 0.5636 - classification_loss: 0.0633
 952/1000 [===========================>..] - ETA: 1:48 - loss: 0.6264 - regression_loss: 0.5632 - classification_loss: 0.0632
 953/1000 [===========================>..] - ETA: 1:46 - loss: 0.6265 - regression_loss: 0.5632 - classification_loss: 0.0632
 954/1000 [===========================>..] - ETA: 1:43 - loss: 0.6263 - regression_loss: 0.5631 - classification_loss: 0.0632
 955/1000 [===========================>..] - ETA: 1:41 - loss: 0.6262 - regression_loss: 0.5630 - classification_loss: 0.0632
 956/1000 [===========================>..] - ETA: 1:39 - loss: 0.6263 - regression_loss: 0.5631 - classification_loss: 0.0632
 957/1000 [===========================>..] - ETA: 1:37 - loss: 0.6260 - regression_loss: 0.5629 - classification_loss: 0.0632
 958/1000 [===========================>..] - ETA: 1:34 - loss: 0.6262 - regression_loss: 0.5630 - classification_loss: 0.0632
 959/1000 [===========================>..] - ETA: 1:32 - loss: 0.6261 - regression_loss: 0.5629 - classification_loss: 0.0632
 960/1000 [===========================>..] - ETA: 1:30 - loss: 0.6256 - regression_loss: 0.5624 - classification_loss: 0.0631
 961/1000 [===========================>..] - ETA: 1:28 - loss: 0.6256 - regression_loss: 0.5624 - classification_loss: 0.0631
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.6257 - regression_loss: 0.5626 - classification_loss: 0.0631
 963/1000 [===========================>..] - ETA: 1:23 - loss: 0.6258 - regression_loss: 0.5627 - classification_loss: 0.0631
 964/1000 [===========================>..] - ETA: 1:21 - loss: 0.6258 - regression_loss: 0.5627 - classification_loss: 0.0631
 965/1000 [===========================>..] - ETA: 1:19 - loss: 0.6258 - regression_loss: 0.5627 - classification_loss: 0.0631
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.6253 - regression_loss: 0.5622 - classification_loss: 0.0631
 967/1000 [============================>.] - ETA: 1:14 - loss: 0.6254 - regression_loss: 0.5623 - classification_loss: 0.0631
 968/1000 [============================>.] - ETA: 1:12 - loss: 0.6252 - regression_loss: 0.5622 - classification_loss: 0.0631
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.6251 - regression_loss: 0.5621 - classification_loss: 0.0631
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.6249 - regression_loss: 0.5618 - classification_loss: 0.0631
 971/1000 [============================>.] - ETA: 1:05 - loss: 0.6244 - regression_loss: 0.5614 - classification_loss: 0.0630
 972/1000 [============================>.] - ETA: 1:03 - loss: 0.6246 - regression_loss: 0.5615 - classification_loss: 0.0630
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.6245 - regression_loss: 0.5615 - classification_loss: 0.0630
 974/1000 [============================>.] - ETA: 58s - loss: 0.6246 - regression_loss: 0.5616 - classification_loss: 0.0630 
 975/1000 [============================>.] - ETA: 56s - loss: 0.6246 - regression_loss: 0.5616 - classification_loss: 0.0630
 976/1000 [============================>.] - ETA: 54s - loss: 0.6246 - regression_loss: 0.5616 - classification_loss: 0.0630
 977/1000 [============================>.] - ETA: 51s - loss: 0.6244 - regression_loss: 0.5614 - classification_loss: 0.0630
 978/1000 [============================>.] - ETA: 49s - loss: 0.6245 - regression_loss: 0.5615 - classification_loss: 0.0630
 979/1000 [============================>.] - ETA: 47s - loss: 0.6245 - regression_loss: 0.5615 - classification_loss: 0.0630
 980/1000 [============================>.] - ETA: 45s - loss: 0.6245 - regression_loss: 0.5615 - classification_loss: 0.0630
 981/1000 [============================>.] - ETA: 42s - loss: 0.6240 - regression_loss: 0.5610 - classification_loss: 0.0629
 982/1000 [============================>.] - ETA: 40s - loss: 0.6242 - regression_loss: 0.5612 - classification_loss: 0.0630
 983/1000 [============================>.] - ETA: 38s - loss: 0.6241 - regression_loss: 0.5611 - classification_loss: 0.0630
 984/1000 [============================>.] - ETA: 36s - loss: 0.6241 - regression_loss: 0.5611 - classification_loss: 0.0630
 985/1000 [============================>.] - ETA: 33s - loss: 0.6242 - regression_loss: 0.5612 - classification_loss: 0.0630
 986/1000 [============================>.] - ETA: 31s - loss: 0.6242 - regression_loss: 0.5612 - classification_loss: 0.0630
 987/1000 [============================>.] - ETA: 29s - loss: 0.6243 - regression_loss: 0.5613 - classification_loss: 0.0630
 988/1000 [============================>.] - ETA: 27s - loss: 0.6238 - regression_loss: 0.5609 - classification_loss: 0.0630
 989/1000 [============================>.] - ETA: 24s - loss: 0.6238 - regression_loss: 0.5609 - classification_loss: 0.0630
 990/1000 [============================>.] - ETA: 22s - loss: 0.6236 - regression_loss: 0.5606 - classification_loss: 0.0630
 991/1000 [============================>.] - ETA: 20s - loss: 0.6237 - regression_loss: 0.5607 - classification_loss: 0.0630
 992/1000 [============================>.] - ETA: 18s - loss: 0.6234 - regression_loss: 0.5605 - classification_loss: 0.0629
 993/1000 [============================>.] - ETA: 15s - loss: 0.6235 - regression_loss: 0.5606 - classification_loss: 0.0629
 994/1000 [============================>.] - ETA: 13s - loss: 0.6235 - regression_loss: 0.5605 - classification_loss: 0.0629
 995/1000 [============================>.] - ETA: 11s - loss: 0.6234 - regression_loss: 0.5605 - classification_loss: 0.0629
 996/1000 [============================>.] - ETA: 9s - loss: 0.6234 - regression_loss: 0.5605 - classification_loss: 0.0629 
 997/1000 [============================>.] - ETA: 6s - loss: 0.6229 - regression_loss: 0.5600 - classification_loss: 0.0629
 998/1000 [============================>.] - ETA: 4s - loss: 0.6229 - regression_loss: 0.5600 - classification_loss: 0.0629
 999/1000 [============================>.] - ETA: 2s - loss: 0.6228 - regression_loss: 0.5600 - classification_loss: 0.0629
1000/1000 [==============================] - 2258s 2s/step - loss: 0.6229 - regression_loss: 0.5601 - classification_loss: 0.0629

Epoch 00004: saving model to ./snapshots/resnet50_csv_04.h5
Epoch 5/10

   1/1000 [..............................] - ETA: 31:11 - loss: 0.6330 - regression_loss: 0.5751 - classification_loss: 0.0579
   2/1000 [..............................] - ETA: 37:06 - loss: 0.6870 - regression_loss: 0.6259 - classification_loss: 0.0611
   3/1000 [..............................] - ETA: 34:19 - loss: 0.6119 - regression_loss: 0.5559 - classification_loss: 0.0560
   4/1000 [..............................] - ETA: 32:10 - loss: 0.4900 - regression_loss: 0.4435 - classification_loss: 0.0465
   5/1000 [..............................] - ETA: 34:43 - loss: 0.5400 - regression_loss: 0.4883 - classification_loss: 0.0517
   6/1000 [..............................] - ETA: 36:04 - loss: 0.5747 - regression_loss: 0.5198 - classification_loss: 0.0549
   7/1000 [..............................] - ETA: 36:50 - loss: 0.5831 - regression_loss: 0.5275 - classification_loss: 0.0556
   8/1000 [..............................] - ETA: 37:15 - loss: 0.5761 - regression_loss: 0.5198 - classification_loss: 0.0563
   9/1000 [..............................] - ETA: 37:00 - loss: 0.5781 - regression_loss: 0.5216 - classification_loss: 0.0565
  10/1000 [..............................] - ETA: 35:51 - loss: 0.5418 - regression_loss: 0.4887 - classification_loss: 0.0532
  11/1000 [..............................] - ETA: 35:10 - loss: 0.5263 - regression_loss: 0.4737 - classification_loss: 0.0526
  12/1000 [..............................] - ETA: 35:53 - loss: 0.5443 - regression_loss: 0.4903 - classification_loss: 0.0540
  13/1000 [..............................] - ETA: 35:04 - loss: 0.5122 - regression_loss: 0.4609 - classification_loss: 0.0513
  14/1000 [..............................] - ETA: 35:28 - loss: 0.5216 - regression_loss: 0.4698 - classification_loss: 0.0519
  15/1000 [..............................] - ETA: 35:25 - loss: 0.5283 - regression_loss: 0.4758 - classification_loss: 0.0525
  16/1000 [..............................] - ETA: 35:38 - loss: 0.5295 - regression_loss: 0.4764 - classification_loss: 0.0531
  17/1000 [..............................] - ETA: 36:00 - loss: 0.5394 - regression_loss: 0.4859 - classification_loss: 0.0536
  18/1000 [..............................] - ETA: 35:31 - loss: 0.5300 - regression_loss: 0.4768 - classification_loss: 0.0532
  19/1000 [..............................] - ETA: 35:57 - loss: 0.5419 - regression_loss: 0.4879 - classification_loss: 0.0540
  20/1000 [..............................] - ETA: 36:13 - loss: 0.5517 - regression_loss: 0.4971 - classification_loss: 0.0546
  21/1000 [..............................] - ETA: 36:24 - loss: 0.5568 - regression_loss: 0.5020 - classification_loss: 0.0548
  22/1000 [..............................] - ETA: 35:52 - loss: 0.5391 - regression_loss: 0.4859 - classification_loss: 0.0532
  23/1000 [..............................] - ETA: 35:29 - loss: 0.5313 - regression_loss: 0.4785 - classification_loss: 0.0528
  24/1000 [..............................] - ETA: 35:37 - loss: 0.5313 - regression_loss: 0.4778 - classification_loss: 0.0535
  25/1000 [..............................] - ETA: 35:33 - loss: 0.5343 - regression_loss: 0.4803 - classification_loss: 0.0540
  26/1000 [..............................] - ETA: 35:13 - loss: 0.5270 - regression_loss: 0.4733 - classification_loss: 0.0537
  27/1000 [..............................] - ETA: 34:49 - loss: 0.5139 - regression_loss: 0.4615 - classification_loss: 0.0524
  28/1000 [..............................] - ETA: 35:02 - loss: 0.5212 - regression_loss: 0.4684 - classification_loss: 0.0528
  29/1000 [..............................] - ETA: 34:59 - loss: 0.5234 - regression_loss: 0.4704 - classification_loss: 0.0529
  30/1000 [..............................] - ETA: 35:14 - loss: 0.5296 - regression_loss: 0.4763 - classification_loss: 0.0533
  31/1000 [..............................] - ETA: 35:22 - loss: 0.5335 - regression_loss: 0.4800 - classification_loss: 0.0535
  32/1000 [..............................] - ETA: 35:27 - loss: 0.5337 - regression_loss: 0.4800 - classification_loss: 0.0537
  33/1000 [..............................] - ETA: 35:07 - loss: 0.5219 - regression_loss: 0.4693 - classification_loss: 0.0527
  34/1000 [>.............................] - ETA: 35:16 - loss: 0.5294 - regression_loss: 0.4764 - classification_loss: 0.0530
  35/1000 [>.............................] - ETA: 35:23 - loss: 0.5343 - regression_loss: 0.4811 - classification_loss: 0.0532
  36/1000 [>.............................] - ETA: 35:27 - loss: 0.5357 - regression_loss: 0.4823 - classification_loss: 0.0534
  37/1000 [>.............................] - ETA: 35:38 - loss: 0.5417 - regression_loss: 0.4879 - classification_loss: 0.0538
  38/1000 [>.............................] - ETA: 35:23 - loss: 0.5409 - regression_loss: 0.4872 - classification_loss: 0.0537
  39/1000 [>.............................] - ETA: 35:20 - loss: 0.5438 - regression_loss: 0.4900 - classification_loss: 0.0538
  40/1000 [>.............................] - ETA: 35:06 - loss: 0.5419 - regression_loss: 0.4882 - classification_loss: 0.0537
  41/1000 [>.............................] - ETA: 35:10 - loss: 0.5438 - regression_loss: 0.4899 - classification_loss: 0.0540
  42/1000 [>.............................] - ETA: 35:06 - loss: 0.5460 - regression_loss: 0.4918 - classification_loss: 0.0542
  43/1000 [>.............................] - ETA: 35:13 - loss: 0.5509 - regression_loss: 0.4964 - classification_loss: 0.0545
  44/1000 [>.............................] - ETA: 35:17 - loss: 0.5532 - regression_loss: 0.4987 - classification_loss: 0.0545
  45/1000 [>.............................] - ETA: 35:01 - loss: 0.5458 - regression_loss: 0.4920 - classification_loss: 0.0538
  46/1000 [>.............................] - ETA: 35:09 - loss: 0.5516 - regression_loss: 0.4976 - classification_loss: 0.0541
  47/1000 [>.............................] - ETA: 35:14 - loss: 0.5561 - regression_loss: 0.5017 - classification_loss: 0.0544
  48/1000 [>.............................] - ETA: 35:18 - loss: 0.5582 - regression_loss: 0.5036 - classification_loss: 0.0546
  49/1000 [>.............................] - ETA: 35:14 - loss: 0.5587 - regression_loss: 0.5041 - classification_loss: 0.0546
  50/1000 [>.............................] - ETA: 35:15 - loss: 0.5593 - regression_loss: 0.5046 - classification_loss: 0.0548
  51/1000 [>.............................] - ETA: 35:04 - loss: 0.5577 - regression_loss: 0.5031 - classification_loss: 0.0546
  52/1000 [>.............................] - ETA: 35:12 - loss: 0.5634 - regression_loss: 0.5086 - classification_loss: 0.0548
  53/1000 [>.............................] - ETA: 34:57 - loss: 0.5578 - regression_loss: 0.5036 - classification_loss: 0.0542
  54/1000 [>.............................] - ETA: 34:59 - loss: 0.5584 - regression_loss: 0.5037 - classification_loss: 0.0547
  55/1000 [>.............................] - ETA: 35:05 - loss: 0.5615 - regression_loss: 0.5065 - classification_loss: 0.0550
  56/1000 [>.............................] - ETA: 35:08 - loss: 0.5641 - regression_loss: 0.5091 - classification_loss: 0.0551
  57/1000 [>.............................] - ETA: 34:55 - loss: 0.5568 - regression_loss: 0.5023 - classification_loss: 0.0544
  58/1000 [>.............................] - ETA: 34:45 - loss: 0.5559 - regression_loss: 0.5016 - classification_loss: 0.0543
  59/1000 [>.............................] - ETA: 34:41 - loss: 0.5575 - regression_loss: 0.5029 - classification_loss: 0.0546
  60/1000 [>.............................] - ETA: 34:44 - loss: 0.5604 - regression_loss: 0.5056 - classification_loss: 0.0548
  61/1000 [>.............................] - ETA: 34:32 - loss: 0.5536 - regression_loss: 0.4994 - classification_loss: 0.0542
  62/1000 [>.............................] - ETA: 34:34 - loss: 0.5558 - regression_loss: 0.5014 - classification_loss: 0.0544
  63/1000 [>.............................] - ETA: 34:25 - loss: 0.5525 - regression_loss: 0.4983 - classification_loss: 0.0542
  64/1000 [>.............................] - ETA: 34:29 - loss: 0.5551 - regression_loss: 0.5007 - classification_loss: 0.0544
  65/1000 [>.............................] - ETA: 34:34 - loss: 0.5579 - regression_loss: 0.5033 - classification_loss: 0.0545
  66/1000 [>.............................] - ETA: 34:35 - loss: 0.5587 - regression_loss: 0.5039 - classification_loss: 0.0547
  67/1000 [=>............................] - ETA: 34:32 - loss: 0.5605 - regression_loss: 0.5054 - classification_loss: 0.0551
  68/1000 [=>............................] - ETA: 34:34 - loss: 0.5620 - regression_loss: 0.5068 - classification_loss: 0.0552
  69/1000 [=>............................] - ETA: 34:30 - loss: 0.5626 - regression_loss: 0.5073 - classification_loss: 0.0553
  70/1000 [=>............................] - ETA: 34:31 - loss: 0.5623 - regression_loss: 0.5069 - classification_loss: 0.0554
  71/1000 [=>............................] - ETA: 34:33 - loss: 0.5649 - regression_loss: 0.5093 - classification_loss: 0.0556
  72/1000 [=>............................] - ETA: 34:22 - loss: 0.5590 - regression_loss: 0.5039 - classification_loss: 0.0551
  73/1000 [=>............................] - ETA: 34:26 - loss: 0.5615 - regression_loss: 0.5062 - classification_loss: 0.0553
  74/1000 [=>............................] - ETA: 34:18 - loss: 0.5596 - regression_loss: 0.5044 - classification_loss: 0.0552
  75/1000 [=>............................] - ETA: 34:21 - loss: 0.5622 - regression_loss: 0.5068 - classification_loss: 0.0554
  76/1000 [=>............................] - ETA: 34:13 - loss: 0.5595 - regression_loss: 0.5043 - classification_loss: 0.0552
  77/1000 [=>............................] - ETA: 34:10 - loss: 0.5599 - regression_loss: 0.5047 - classification_loss: 0.0552
  78/1000 [=>............................] - ETA: 34:11 - loss: 0.5609 - regression_loss: 0.5057 - classification_loss: 0.0553
  79/1000 [=>............................] - ETA: 34:11 - loss: 0.5603 - regression_loss: 0.5050 - classification_loss: 0.0553
  80/1000 [=>............................] - ETA: 34:13 - loss: 0.5620 - regression_loss: 0.5066 - classification_loss: 0.0554
  81/1000 [=>............................] - ETA: 34:03 - loss: 0.5578 - regression_loss: 0.5029 - classification_loss: 0.0550
  82/1000 [=>............................] - ETA: 34:00 - loss: 0.5585 - regression_loss: 0.5035 - classification_loss: 0.0550
  83/1000 [=>............................] - ETA: 34:01 - loss: 0.5610 - regression_loss: 0.5059 - classification_loss: 0.0551
  84/1000 [=>............................] - ETA: 34:01 - loss: 0.5608 - regression_loss: 0.5056 - classification_loss: 0.0552
  85/1000 [=>............................] - ETA: 34:02 - loss: 0.5618 - regression_loss: 0.5065 - classification_loss: 0.0552
  86/1000 [=>............................] - ETA: 33:53 - loss: 0.5579 - regression_loss: 0.5031 - classification_loss: 0.0548
  87/1000 [=>............................] - ETA: 33:56 - loss: 0.5602 - regression_loss: 0.5053 - classification_loss: 0.0549
  88/1000 [=>............................] - ETA: 33:49 - loss: 0.5592 - regression_loss: 0.5043 - classification_loss: 0.0549
  89/1000 [=>............................] - ETA: 33:40 - loss: 0.5542 - regression_loss: 0.4997 - classification_loss: 0.0545
  90/1000 [=>............................] - ETA: 33:37 - loss: 0.5546 - regression_loss: 0.5001 - classification_loss: 0.0545
  91/1000 [=>............................] - ETA: 33:39 - loss: 0.5574 - regression_loss: 0.5028 - classification_loss: 0.0546
  92/1000 [=>............................] - ETA: 33:32 - loss: 0.5560 - regression_loss: 0.5016 - classification_loss: 0.0545
  93/1000 [=>............................] - ETA: 33:33 - loss: 0.5580 - regression_loss: 0.5034 - classification_loss: 0.0546
  94/1000 [=>............................] - ETA: 33:34 - loss: 0.5587 - regression_loss: 0.5041 - classification_loss: 0.0546
  95/1000 [=>............................] - ETA: 33:34 - loss: 0.5583 - regression_loss: 0.5036 - classification_loss: 0.0547
  96/1000 [=>............................] - ETA: 33:27 - loss: 0.5562 - regression_loss: 0.5016 - classification_loss: 0.0546
  97/1000 [=>............................] - ETA: 33:24 - loss: 0.5568 - regression_loss: 0.5022 - classification_loss: 0.0546
  98/1000 [=>............................] - ETA: 33:25 - loss: 0.5581 - regression_loss: 0.5035 - classification_loss: 0.0546
  99/1000 [=>............................] - ETA: 33:25 - loss: 0.5590 - regression_loss: 0.5043 - classification_loss: 0.0547
 100/1000 [==>...........................] - ETA: 33:25 - loss: 0.5590 - regression_loss: 0.5043 - classification_loss: 0.0548
 101/1000 [==>...........................] - ETA: 33:17 - loss: 0.5552 - regression_loss: 0.5008 - classification_loss: 0.0544
 102/1000 [==>...........................] - ETA: 33:19 - loss: 0.5571 - regression_loss: 0.5026 - classification_loss: 0.0545
 103/1000 [==>...........................] - ETA: 33:11 - loss: 0.5527 - regression_loss: 0.4986 - classification_loss: 0.0541
 104/1000 [==>...........................] - ETA: 33:05 - loss: 0.5519 - regression_loss: 0.4978 - classification_loss: 0.0541
 105/1000 [==>...........................] - ETA: 33:05 - loss: 0.5533 - regression_loss: 0.4992 - classification_loss: 0.0542
 106/1000 [==>...........................] - ETA: 33:05 - loss: 0.5535 - regression_loss: 0.4993 - classification_loss: 0.0542
 107/1000 [==>...........................] - ETA: 33:07 - loss: 0.5553 - regression_loss: 0.5010 - classification_loss: 0.0543
 108/1000 [==>...........................] - ETA: 33:08 - loss: 0.5570 - regression_loss: 0.5025 - classification_loss: 0.0544
 109/1000 [==>...........................] - ETA: 33:05 - loss: 0.5575 - regression_loss: 0.5031 - classification_loss: 0.0544
 110/1000 [==>...........................] - ETA: 33:05 - loss: 0.5585 - regression_loss: 0.5040 - classification_loss: 0.0545
 111/1000 [==>...........................] - ETA: 33:04 - loss: 0.5583 - regression_loss: 0.5038 - classification_loss: 0.0545
 112/1000 [==>...........................] - ETA: 33:01 - loss: 0.5588 - regression_loss: 0.5043 - classification_loss: 0.0545
 113/1000 [==>...........................] - ETA: 33:02 - loss: 0.5608 - regression_loss: 0.5062 - classification_loss: 0.0546
 114/1000 [==>...........................] - ETA: 32:56 - loss: 0.5590 - regression_loss: 0.5045 - classification_loss: 0.0545
 115/1000 [==>...........................] - ETA: 32:57 - loss: 0.5607 - regression_loss: 0.5061 - classification_loss: 0.0546
 116/1000 [==>...........................] - ETA: 32:50 - loss: 0.5586 - regression_loss: 0.5043 - classification_loss: 0.0543
 117/1000 [==>...........................] - ETA: 32:52 - loss: 0.5601 - regression_loss: 0.5057 - classification_loss: 0.0544
 118/1000 [==>...........................] - ETA: 32:46 - loss: 0.5592 - regression_loss: 0.5049 - classification_loss: 0.0543
 119/1000 [==>...........................] - ETA: 32:46 - loss: 0.5606 - regression_loss: 0.5062 - classification_loss: 0.0544
 120/1000 [==>...........................] - ETA: 32:45 - loss: 0.5603 - regression_loss: 0.5059 - classification_loss: 0.0545
 121/1000 [==>...........................] - ETA: 32:38 - loss: 0.5572 - regression_loss: 0.5030 - classification_loss: 0.0542
 122/1000 [==>...........................] - ETA: 32:38 - loss: 0.5582 - regression_loss: 0.5040 - classification_loss: 0.0542
 123/1000 [==>...........................] - ETA: 32:35 - loss: 0.5588 - regression_loss: 0.5046 - classification_loss: 0.0542
 124/1000 [==>...........................] - ETA: 32:36 - loss: 0.5604 - regression_loss: 0.5060 - classification_loss: 0.0543
 125/1000 [==>...........................] - ETA: 32:35 - loss: 0.5603 - regression_loss: 0.5059 - classification_loss: 0.0544
 126/1000 [==>...........................] - ETA: 32:34 - loss: 0.5610 - regression_loss: 0.5066 - classification_loss: 0.0544
 127/1000 [==>...........................] - ETA: 32:35 - loss: 0.5622 - regression_loss: 0.5077 - classification_loss: 0.0545
 128/1000 [==>...........................] - ETA: 32:33 - loss: 0.5623 - regression_loss: 0.5077 - classification_loss: 0.0545
 129/1000 [==>...........................] - ETA: 32:26 - loss: 0.5593 - regression_loss: 0.5051 - classification_loss: 0.0542
 130/1000 [==>...........................] - ETA: 32:20 - loss: 0.5581 - regression_loss: 0.5040 - classification_loss: 0.0541
 131/1000 [==>...........................] - ETA: 32:19 - loss: 0.5578 - regression_loss: 0.5036 - classification_loss: 0.0542
 132/1000 [==>...........................] - ETA: 32:16 - loss: 0.5581 - regression_loss: 0.5039 - classification_loss: 0.0542
 133/1000 [==>...........................] - ETA: 32:11 - loss: 0.5564 - regression_loss: 0.5023 - classification_loss: 0.0541
 134/1000 [===>..........................] - ETA: 32:12 - loss: 0.5575 - regression_loss: 0.5033 - classification_loss: 0.0542
 135/1000 [===>..........................] - ETA: 32:11 - loss: 0.5579 - regression_loss: 0.5037 - classification_loss: 0.0542
 136/1000 [===>..........................] - ETA: 32:11 - loss: 0.5588 - regression_loss: 0.5046 - classification_loss: 0.0542
 137/1000 [===>..........................] - ETA: 32:05 - loss: 0.5560 - regression_loss: 0.5021 - classification_loss: 0.0539
 138/1000 [===>..........................] - ETA: 32:05 - loss: 0.5571 - regression_loss: 0.5031 - classification_loss: 0.0540
 139/1000 [===>..........................] - ETA: 31:59 - loss: 0.5538 - regression_loss: 0.5001 - classification_loss: 0.0537
 140/1000 [===>..........................] - ETA: 31:56 - loss: 0.5540 - regression_loss: 0.5002 - classification_loss: 0.0537
 141/1000 [===>..........................] - ETA: 31:57 - loss: 0.5550 - regression_loss: 0.5012 - classification_loss: 0.0538
 142/1000 [===>..........................] - ETA: 31:56 - loss: 0.5556 - regression_loss: 0.5017 - classification_loss: 0.0538
 143/1000 [===>..........................] - ETA: 31:51 - loss: 0.5541 - regression_loss: 0.5003 - classification_loss: 0.0538
 144/1000 [===>..........................] - ETA: 31:50 - loss: 0.5539 - regression_loss: 0.5001 - classification_loss: 0.0538
 145/1000 [===>..........................] - ETA: 31:44 - loss: 0.5509 - regression_loss: 0.4973 - classification_loss: 0.0536
 146/1000 [===>..........................] - ETA: 31:43 - loss: 0.5517 - regression_loss: 0.4980 - classification_loss: 0.0536
 147/1000 [===>..........................] - ETA: 31:43 - loss: 0.5528 - regression_loss: 0.4991 - classification_loss: 0.0537
 148/1000 [===>..........................] - ETA: 31:40 - loss: 0.5531 - regression_loss: 0.4993 - classification_loss: 0.0537
 149/1000 [===>..........................] - ETA: 31:41 - loss: 0.5540 - regression_loss: 0.5002 - classification_loss: 0.0538
 150/1000 [===>..........................] - ETA: 31:36 - loss: 0.5528 - regression_loss: 0.4991 - classification_loss: 0.0537
 151/1000 [===>..........................] - ETA: 31:35 - loss: 0.5529 - regression_loss: 0.4991 - classification_loss: 0.0538
 152/1000 [===>..........................] - ETA: 31:34 - loss: 0.5529 - regression_loss: 0.4990 - classification_loss: 0.0539
 153/1000 [===>..........................] - ETA: 31:33 - loss: 0.5541 - regression_loss: 0.5002 - classification_loss: 0.0540
 154/1000 [===>..........................] - ETA: 31:28 - loss: 0.5528 - regression_loss: 0.4989 - classification_loss: 0.0539
 155/1000 [===>..........................] - ETA: 31:22 - loss: 0.5517 - regression_loss: 0.4980 - classification_loss: 0.0537
 156/1000 [===>..........................] - ETA: 31:23 - loss: 0.5530 - regression_loss: 0.4992 - classification_loss: 0.0538
 157/1000 [===>..........................] - ETA: 31:20 - loss: 0.5533 - regression_loss: 0.4995 - classification_loss: 0.0538
 158/1000 [===>..........................] - ETA: 31:19 - loss: 0.5539 - regression_loss: 0.5000 - classification_loss: 0.0539
 159/1000 [===>..........................] - ETA: 31:19 - loss: 0.5543 - regression_loss: 0.5003 - classification_loss: 0.0540
 160/1000 [===>..........................] - ETA: 31:13 - loss: 0.5520 - regression_loss: 0.4983 - classification_loss: 0.0537
 161/1000 [===>..........................] - ETA: 31:11 - loss: 0.5524 - regression_loss: 0.4986 - classification_loss: 0.0538
 162/1000 [===>..........................] - ETA: 31:06 - loss: 0.5514 - regression_loss: 0.4977 - classification_loss: 0.0537
 163/1000 [===>..........................] - ETA: 31:05 - loss: 0.5514 - regression_loss: 0.4976 - classification_loss: 0.0538
 164/1000 [===>..........................] - ETA: 31:04 - loss: 0.5526 - regression_loss: 0.4987 - classification_loss: 0.0539
 165/1000 [===>..........................] - ETA: 31:05 - loss: 0.5537 - regression_loss: 0.4998 - classification_loss: 0.0539
 166/1000 [===>..........................] - ETA: 31:00 - loss: 0.5524 - regression_loss: 0.4986 - classification_loss: 0.0539
 167/1000 [====>.........................] - ETA: 30:54 - loss: 0.5507 - regression_loss: 0.4970 - classification_loss: 0.0537
 168/1000 [====>.........................] - ETA: 30:54 - loss: 0.5519 - regression_loss: 0.4982 - classification_loss: 0.0537
 169/1000 [====>.........................] - ETA: 30:51 - loss: 0.5524 - regression_loss: 0.4986 - classification_loss: 0.0538
 170/1000 [====>.........................] - ETA: 30:50 - loss: 0.5529 - regression_loss: 0.4991 - classification_loss: 0.0538
 171/1000 [====>.........................] - ETA: 30:49 - loss: 0.5528 - regression_loss: 0.4989 - classification_loss: 0.0539
 172/1000 [====>.........................] - ETA: 30:49 - loss: 0.5539 - regression_loss: 0.4999 - classification_loss: 0.0540
 173/1000 [====>.........................] - ETA: 30:48 - loss: 0.5537 - regression_loss: 0.4996 - classification_loss: 0.0540
 174/1000 [====>.........................] - ETA: 30:45 - loss: 0.5540 - regression_loss: 0.4999 - classification_loss: 0.0541
 175/1000 [====>.........................] - ETA: 30:45 - loss: 0.5551 - regression_loss: 0.5010 - classification_loss: 0.0542
 176/1000 [====>.........................] - ETA: 30:40 - loss: 0.5542 - regression_loss: 0.5001 - classification_loss: 0.0541
 177/1000 [====>.........................] - ETA: 30:39 - loss: 0.5545 - regression_loss: 0.5004 - classification_loss: 0.0541
 178/1000 [====>.........................] - ETA: 30:34 - loss: 0.5521 - regression_loss: 0.4982 - classification_loss: 0.0539
 179/1000 [====>.........................] - ETA: 30:33 - loss: 0.5530 - regression_loss: 0.4990 - classification_loss: 0.0540
 180/1000 [====>.........................] - ETA: 30:28 - loss: 0.5506 - regression_loss: 0.4968 - classification_loss: 0.0538
 181/1000 [====>.........................] - ETA: 30:27 - loss: 0.5511 - regression_loss: 0.4972 - classification_loss: 0.0538
 182/1000 [====>.........................] - ETA: 30:25 - loss: 0.5512 - regression_loss: 0.4974 - classification_loss: 0.0538
 183/1000 [====>.........................] - ETA: 30:24 - loss: 0.5520 - regression_loss: 0.4981 - classification_loss: 0.0539
 184/1000 [====>.........................] - ETA: 30:23 - loss: 0.5518 - regression_loss: 0.4979 - classification_loss: 0.0539
 185/1000 [====>.........................] - ETA: 30:19 - loss: 0.5507 - regression_loss: 0.4969 - classification_loss: 0.0538
 186/1000 [====>.........................] - ETA: 30:18 - loss: 0.5519 - regression_loss: 0.4979 - classification_loss: 0.0539
 187/1000 [====>.........................] - ETA: 30:15 - loss: 0.5522 - regression_loss: 0.4983 - classification_loss: 0.0540
 188/1000 [====>.........................] - ETA: 30:15 - loss: 0.5530 - regression_loss: 0.4990 - classification_loss: 0.0540
 189/1000 [====>.........................] - ETA: 30:14 - loss: 0.5538 - regression_loss: 0.4998 - classification_loss: 0.0540
 190/1000 [====>.........................] - ETA: 30:10 - loss: 0.5534 - regression_loss: 0.4994 - classification_loss: 0.0540
 191/1000 [====>.........................] - ETA: 30:05 - loss: 0.5514 - regression_loss: 0.4976 - classification_loss: 0.0538
 192/1000 [====>.........................] - ETA: 30:04 - loss: 0.5520 - regression_loss: 0.4981 - classification_loss: 0.0539
 193/1000 [====>.........................] - ETA: 30:02 - loss: 0.5523 - regression_loss: 0.4982 - classification_loss: 0.0541
 194/1000 [====>.........................] - ETA: 30:02 - loss: 0.5532 - regression_loss: 0.4990 - classification_loss: 0.0542
 195/1000 [====>.........................] - ETA: 30:01 - loss: 0.5535 - regression_loss: 0.4993 - classification_loss: 0.0542
 196/1000 [====>.........................] - ETA: 30:00 - loss: 0.5543 - regression_loss: 0.5001 - classification_loss: 0.0543
 197/1000 [====>.........................] - ETA: 29:56 - loss: 0.5532 - regression_loss: 0.4990 - classification_loss: 0.0542
 198/1000 [====>.........................] - ETA: 29:51 - loss: 0.5519 - regression_loss: 0.4978 - classification_loss: 0.0541
 199/1000 [====>.........................] - ETA: 29:48 - loss: 0.5523 - regression_loss: 0.4981 - classification_loss: 0.0542
 200/1000 [=====>........................] - ETA: 29:47 - loss: 0.5523 - regression_loss: 0.4980 - classification_loss: 0.0543
 201/1000 [=====>........................] - ETA: 29:44 - loss: 0.5525 - regression_loss: 0.4982 - classification_loss: 0.0543
 202/1000 [=====>........................] - ETA: 29:43 - loss: 0.5523 - regression_loss: 0.4979 - classification_loss: 0.0543
 203/1000 [=====>........................] - ETA: 29:38 - loss: 0.5501 - regression_loss: 0.4960 - classification_loss: 0.0541
 204/1000 [=====>........................] - ETA: 29:34 - loss: 0.5495 - regression_loss: 0.4953 - classification_loss: 0.0541
 205/1000 [=====>........................] - ETA: 29:33 - loss: 0.5504 - regression_loss: 0.4962 - classification_loss: 0.0542
 206/1000 [=====>........................] - ETA: 29:32 - loss: 0.5509 - regression_loss: 0.4966 - classification_loss: 0.0543
 207/1000 [=====>........................] - ETA: 29:32 - loss: 0.5517 - regression_loss: 0.4973 - classification_loss: 0.0543
 208/1000 [=====>........................] - ETA: 29:27 - loss: 0.5496 - regression_loss: 0.4955 - classification_loss: 0.0542
 209/1000 [=====>........................] - ETA: 29:26 - loss: 0.5500 - regression_loss: 0.4958 - classification_loss: 0.0542
 210/1000 [=====>........................] - ETA: 29:22 - loss: 0.5490 - regression_loss: 0.4948 - classification_loss: 0.0541
 211/1000 [=====>........................] - ETA: 29:19 - loss: 0.5490 - regression_loss: 0.4949 - classification_loss: 0.0541
 212/1000 [=====>........................] - ETA: 29:18 - loss: 0.5498 - regression_loss: 0.4957 - classification_loss: 0.0542
 213/1000 [=====>........................] - ETA: 29:18 - loss: 0.5506 - regression_loss: 0.4964 - classification_loss: 0.0542
 214/1000 [=====>........................] - ETA: 29:16 - loss: 0.5504 - regression_loss: 0.4962 - classification_loss: 0.0542
 215/1000 [=====>........................] - ETA: 29:15 - loss: 0.5510 - regression_loss: 0.4968 - classification_loss: 0.0542
 216/1000 [=====>........................] - ETA: 29:10 - loss: 0.5490 - regression_loss: 0.4949 - classification_loss: 0.0541
 217/1000 [=====>........................] - ETA: 29:08 - loss: 0.5491 - regression_loss: 0.4950 - classification_loss: 0.0541
 218/1000 [=====>........................] - ETA: 29:07 - loss: 0.5494 - regression_loss: 0.4953 - classification_loss: 0.0541
 219/1000 [=====>........................] - ETA: 29:06 - loss: 0.5501 - regression_loss: 0.4960 - classification_loss: 0.0541
 220/1000 [=====>........................] - ETA: 29:02 - loss: 0.5496 - regression_loss: 0.4955 - classification_loss: 0.0541
 221/1000 [=====>........................] - ETA: 29:01 - loss: 0.5494 - regression_loss: 0.4953 - classification_loss: 0.0541
 222/1000 [=====>........................] - ETA: 29:00 - loss: 0.5497 - regression_loss: 0.4956 - classification_loss: 0.0541
 223/1000 [=====>........................] - ETA: 28:56 - loss: 0.5487 - regression_loss: 0.4947 - classification_loss: 0.0540
 224/1000 [=====>........................] - ETA: 28:55 - loss: 0.5495 - regression_loss: 0.4954 - classification_loss: 0.0541
 225/1000 [=====>........................] - ETA: 28:54 - loss: 0.5503 - regression_loss: 0.4961 - classification_loss: 0.0541
 226/1000 [=====>........................] - ETA: 28:50 - loss: 0.5484 - regression_loss: 0.4944 - classification_loss: 0.0540
 227/1000 [=====>........................] - ETA: 28:48 - loss: 0.5482 - regression_loss: 0.4942 - classification_loss: 0.0540
 228/1000 [=====>........................] - ETA: 28:45 - loss: 0.5483 - regression_loss: 0.4944 - classification_loss: 0.0540
 229/1000 [=====>........................] - ETA: 28:43 - loss: 0.5485 - regression_loss: 0.4945 - classification_loss: 0.0540
 230/1000 [=====>........................] - ETA: 28:41 - loss: 0.5482 - regression_loss: 0.4943 - classification_loss: 0.0540
 231/1000 [=====>........................] - ETA: 28:40 - loss: 0.5488 - regression_loss: 0.4948 - classification_loss: 0.0540
 232/1000 [=====>........................] - ETA: 28:39 - loss: 0.5490 - regression_loss: 0.4950 - classification_loss: 0.0540
 233/1000 [=====>........................] - ETA: 28:38 - loss: 0.5497 - regression_loss: 0.4957 - classification_loss: 0.0540
 234/1000 [======>.......................] - ETA: 28:34 - loss: 0.5479 - regression_loss: 0.4940 - classification_loss: 0.0539
 235/1000 [======>.......................] - ETA: 28:30 - loss: 0.5473 - regression_loss: 0.4935 - classification_loss: 0.0538
 236/1000 [======>.......................] - ETA: 28:29 - loss: 0.5481 - regression_loss: 0.4942 - classification_loss: 0.0539
 237/1000 [======>.......................] - ETA: 28:27 - loss: 0.5484 - regression_loss: 0.4945 - classification_loss: 0.0539
 238/1000 [======>.......................] - ETA: 28:25 - loss: 0.5485 - regression_loss: 0.4946 - classification_loss: 0.0539
 239/1000 [======>.......................] - ETA: 28:21 - loss: 0.5476 - regression_loss: 0.4938 - classification_loss: 0.0538
 240/1000 [======>.......................] - ETA: 28:19 - loss: 0.5475 - regression_loss: 0.4936 - classification_loss: 0.0539
 241/1000 [======>.......................] - ETA: 28:15 - loss: 0.5458 - regression_loss: 0.4921 - classification_loss: 0.0537
 242/1000 [======>.......................] - ETA: 28:14 - loss: 0.5465 - regression_loss: 0.4928 - classification_loss: 0.0538
 243/1000 [======>.......................] - ETA: 28:10 - loss: 0.5448 - regression_loss: 0.4912 - classification_loss: 0.0536
 244/1000 [======>.......................] - ETA: 28:06 - loss: 0.5444 - regression_loss: 0.4908 - classification_loss: 0.0536
 245/1000 [======>.......................] - ETA: 28:04 - loss: 0.5443 - regression_loss: 0.4907 - classification_loss: 0.0536
 246/1000 [======>.......................] - ETA: 28:03 - loss: 0.5446 - regression_loss: 0.4910 - classification_loss: 0.0536
 247/1000 [======>.......................] - ETA: 28:02 - loss: 0.5452 - regression_loss: 0.4915 - classification_loss: 0.0537
 248/1000 [======>.......................] - ETA: 28:01 - loss: 0.5458 - regression_loss: 0.4921 - classification_loss: 0.0537
 249/1000 [======>.......................] - ETA: 27:59 - loss: 0.5458 - regression_loss: 0.4922 - classification_loss: 0.0537
 250/1000 [======>.......................] - ETA: 27:57 - loss: 0.5465 - regression_loss: 0.4927 - classification_loss: 0.0537
 251/1000 [======>.......................] - ETA: 27:57 - loss: 0.5471 - regression_loss: 0.4933 - classification_loss: 0.0538
 252/1000 [======>.......................] - ETA: 27:53 - loss: 0.5463 - regression_loss: 0.4926 - classification_loss: 0.0537
 253/1000 [======>.......................] - ETA: 27:51 - loss: 0.5461 - regression_loss: 0.4924 - classification_loss: 0.0537
 254/1000 [======>.......................] - ETA: 27:49 - loss: 0.5462 - regression_loss: 0.4925 - classification_loss: 0.0537
 255/1000 [======>.......................] - ETA: 27:45 - loss: 0.5445 - regression_loss: 0.4909 - classification_loss: 0.0536
 256/1000 [======>.......................] - ETA: 27:43 - loss: 0.5448 - regression_loss: 0.4912 - classification_loss: 0.0536
 257/1000 [======>.......................] - ETA: 27:42 - loss: 0.5452 - regression_loss: 0.4915 - classification_loss: 0.0536
 258/1000 [======>.......................] - ETA: 27:41 - loss: 0.5457 - regression_loss: 0.4921 - classification_loss: 0.0536
 259/1000 [======>.......................] - ETA: 27:40 - loss: 0.5463 - regression_loss: 0.4926 - classification_loss: 0.0537
 260/1000 [======>.......................] - ETA: 27:38 - loss: 0.5461 - regression_loss: 0.4924 - classification_loss: 0.0537
 261/1000 [======>.......................] - ETA: 27:34 - loss: 0.5444 - regression_loss: 0.4909 - classification_loss: 0.0535
 262/1000 [======>.......................] - ETA: 27:30 - loss: 0.5439 - regression_loss: 0.4904 - classification_loss: 0.0535
 263/1000 [======>.......................] - ETA: 27:28 - loss: 0.5439 - regression_loss: 0.4904 - classification_loss: 0.0535
 264/1000 [======>.......................] - ETA: 27:26 - loss: 0.5445 - regression_loss: 0.4910 - classification_loss: 0.0535
 265/1000 [======>.......................] - ETA: 27:25 - loss: 0.5443 - regression_loss: 0.4908 - classification_loss: 0.0535
 266/1000 [======>.......................] - ETA: 27:21 - loss: 0.5426 - regression_loss: 0.4892 - classification_loss: 0.0534
 267/1000 [=======>......................] - ETA: 27:19 - loss: 0.5430 - regression_loss: 0.4896 - classification_loss: 0.0534
 268/1000 [=======>......................] - ETA: 27:18 - loss: 0.5437 - regression_loss: 0.4903 - classification_loss: 0.0534
 269/1000 [=======>......................] - ETA: 27:16 - loss: 0.5440 - regression_loss: 0.4905 - classification_loss: 0.0534
 270/1000 [=======>......................] - ETA: 27:12 - loss: 0.5433 - regression_loss: 0.4899 - classification_loss: 0.0534
 271/1000 [=======>......................] - ETA: 27:08 - loss: 0.5424 - regression_loss: 0.4891 - classification_loss: 0.0532
 272/1000 [=======>......................] - ETA: 27:07 - loss: 0.5438 - regression_loss: 0.4905 - classification_loss: 0.0533
 273/1000 [=======>......................] - ETA: 27:05 - loss: 0.5445 - regression_loss: 0.4912 - classification_loss: 0.0533
 274/1000 [=======>......................] - ETA: 27:03 - loss: 0.5452 - regression_loss: 0.4918 - classification_loss: 0.0534
 275/1000 [=======>......................] - ETA: 26:59 - loss: 0.5446 - regression_loss: 0.4912 - classification_loss: 0.0533
 276/1000 [=======>......................] - ETA: 26:58 - loss: 0.5451 - regression_loss: 0.4918 - classification_loss: 0.0534
 277/1000 [=======>......................] - ETA: 26:56 - loss: 0.5455 - regression_loss: 0.4921 - classification_loss: 0.0534
 278/1000 [=======>......................] - ETA: 26:53 - loss: 0.5450 - regression_loss: 0.4916 - classification_loss: 0.0533
 279/1000 [=======>......................] - ETA: 26:49 - loss: 0.5436 - regression_loss: 0.4905 - classification_loss: 0.0532
 280/1000 [=======>......................] - ETA: 26:47 - loss: 0.5436 - regression_loss: 0.4904 - classification_loss: 0.0532
 281/1000 [=======>......................] - ETA: 26:46 - loss: 0.5441 - regression_loss: 0.4909 - classification_loss: 0.0532
 282/1000 [=======>......................] - ETA: 26:45 - loss: 0.5444 - regression_loss: 0.4911 - classification_loss: 0.0532
 283/1000 [=======>......................] - ETA: 26:42 - loss: 0.5446 - regression_loss: 0.4913 - classification_loss: 0.0532
 284/1000 [=======>......................] - ETA: 26:41 - loss: 0.5450 - regression_loss: 0.4918 - classification_loss: 0.0533
 285/1000 [=======>......................] - ETA: 26:39 - loss: 0.5450 - regression_loss: 0.4918 - classification_loss: 0.0533
 286/1000 [=======>......................] - ETA: 26:37 - loss: 0.5456 - regression_loss: 0.4924 - classification_loss: 0.0533
 287/1000 [=======>......................] - ETA: 26:33 - loss: 0.5441 - regression_loss: 0.4910 - classification_loss: 0.0532
 288/1000 [=======>......................] - ETA: 26:32 - loss: 0.5444 - regression_loss: 0.4913 - classification_loss: 0.0532
 289/1000 [=======>......................] - ETA: 26:30 - loss: 0.5442 - regression_loss: 0.4911 - classification_loss: 0.0532
 290/1000 [=======>......................] - ETA: 26:29 - loss: 0.5447 - regression_loss: 0.4915 - classification_loss: 0.0532
 291/1000 [=======>......................] - ETA: 26:25 - loss: 0.5441 - regression_loss: 0.4909 - classification_loss: 0.0532
 292/1000 [=======>......................] - ETA: 26:24 - loss: 0.5444 - regression_loss: 0.4912 - classification_loss: 0.0532
 293/1000 [=======>......................] - ETA: 26:20 - loss: 0.5431 - regression_loss: 0.4901 - classification_loss: 0.0530
 294/1000 [=======>......................] - ETA: 26:17 - loss: 0.5425 - regression_loss: 0.4895 - classification_loss: 0.0530
 295/1000 [=======>......................] - ETA: 26:15 - loss: 0.5424 - regression_loss: 0.4894 - classification_loss: 0.0530
 296/1000 [=======>......................] - ETA: 26:14 - loss: 0.5430 - regression_loss: 0.4900 - classification_loss: 0.0530
 297/1000 [=======>......................] - ETA: 26:13 - loss: 0.5435 - regression_loss: 0.4905 - classification_loss: 0.0531
 298/1000 [=======>......................] - ETA: 26:10 - loss: 0.5435 - regression_loss: 0.4905 - classification_loss: 0.0531
 299/1000 [=======>......................] - ETA: 26:07 - loss: 0.5430 - regression_loss: 0.4899 - classification_loss: 0.0530
 300/1000 [========>.....................] - ETA: 26:05 - loss: 0.5443 - regression_loss: 0.4912 - classification_loss: 0.0531
 301/1000 [========>.....................] - ETA: 26:04 - loss: 0.5455 - regression_loss: 0.4923 - classification_loss: 0.0532
 302/1000 [========>.....................] - ETA: 26:01 - loss: 0.5462 - regression_loss: 0.4930 - classification_loss: 0.0532
 303/1000 [========>.....................] - ETA: 25:57 - loss: 0.5449 - regression_loss: 0.4917 - classification_loss: 0.0532
 304/1000 [========>.....................] - ETA: 25:56 - loss: 0.5450 - regression_loss: 0.4918 - classification_loss: 0.0532
 305/1000 [========>.....................] - ETA: 25:55 - loss: 0.5455 - regression_loss: 0.4923 - classification_loss: 0.0533
 306/1000 [========>.....................] - ETA: 25:51 - loss: 0.5455 - regression_loss: 0.4923 - classification_loss: 0.0532
 307/1000 [========>.....................] - ETA: 25:47 - loss: 0.5443 - regression_loss: 0.4912 - classification_loss: 0.0531
 308/1000 [========>.....................] - ETA: 25:46 - loss: 0.5450 - regression_loss: 0.4918 - classification_loss: 0.0532
 309/1000 [========>.....................] - ETA: 25:44 - loss: 0.5451 - regression_loss: 0.4918 - classification_loss: 0.0533
 310/1000 [========>.....................] - ETA: 25:42 - loss: 0.5456 - regression_loss: 0.4922 - classification_loss: 0.0534
 311/1000 [========>.....................] - ETA: 25:40 - loss: 0.5461 - regression_loss: 0.4926 - classification_loss: 0.0535
 312/1000 [========>.....................] - ETA: 25:39 - loss: 0.5467 - regression_loss: 0.4932 - classification_loss: 0.0535
 313/1000 [========>.....................] - ETA: 25:37 - loss: 0.5470 - regression_loss: 0.4935 - classification_loss: 0.0535
 314/1000 [========>.....................] - ETA: 25:36 - loss: 0.5475 - regression_loss: 0.4940 - classification_loss: 0.0536
 315/1000 [========>.....................] - ETA: 25:33 - loss: 0.5469 - regression_loss: 0.4934 - classification_loss: 0.0535
 316/1000 [========>.....................] - ETA: 25:31 - loss: 0.5469 - regression_loss: 0.4933 - classification_loss: 0.0536
 317/1000 [========>.....................] - ETA: 25:27 - loss: 0.5455 - regression_loss: 0.4920 - classification_loss: 0.0535
 318/1000 [========>.....................] - ETA: 25:26 - loss: 0.5460 - regression_loss: 0.4925 - classification_loss: 0.0535
 319/1000 [========>.....................] - ETA: 25:24 - loss: 0.5461 - regression_loss: 0.4926 - classification_loss: 0.0535
 320/1000 [========>.....................] - ETA: 25:20 - loss: 0.5460 - regression_loss: 0.4925 - classification_loss: 0.0535
 321/1000 [========>.....................] - ETA: 25:19 - loss: 0.5461 - regression_loss: 0.4926 - classification_loss: 0.0535
 322/1000 [========>.....................] - ETA: 25:17 - loss: 0.5468 - regression_loss: 0.4931 - classification_loss: 0.0537
 323/1000 [========>.....................] - ETA: 25:13 - loss: 0.5462 - regression_loss: 0.4927 - classification_loss: 0.0535
 324/1000 [========>.....................] - ETA: 25:12 - loss: 0.5465 - regression_loss: 0.4929 - classification_loss: 0.0536
 325/1000 [========>.....................] - ETA: 25:09 - loss: 0.5466 - regression_loss: 0.4930 - classification_loss: 0.0536
 326/1000 [========>.....................] - ETA: 25:08 - loss: 0.5470 - regression_loss: 0.4934 - classification_loss: 0.0536
 327/1000 [========>.....................] - ETA: 25:04 - loss: 0.5458 - regression_loss: 0.4923 - classification_loss: 0.0535
 328/1000 [========>.....................] - ETA: 25:03 - loss: 0.5461 - regression_loss: 0.4926 - classification_loss: 0.0535
 329/1000 [========>.....................] - ETA: 25:01 - loss: 0.5465 - regression_loss: 0.4930 - classification_loss: 0.0535
 330/1000 [========>.....................] - ETA: 24:59 - loss: 0.5466 - regression_loss: 0.4930 - classification_loss: 0.0536
 331/1000 [========>.....................] - ETA: 24:56 - loss: 0.5461 - regression_loss: 0.4926 - classification_loss: 0.0535
 332/1000 [========>.....................] - ETA: 24:54 - loss: 0.5466 - regression_loss: 0.4930 - classification_loss: 0.0536
 333/1000 [========>.....................] - ETA: 24:52 - loss: 0.5464 - regression_loss: 0.4928 - classification_loss: 0.0536
 334/1000 [=========>....................] - ETA: 24:49 - loss: 0.5455 - regression_loss: 0.4920 - classification_loss: 0.0535
 335/1000 [=========>....................] - ETA: 24:47 - loss: 0.5459 - regression_loss: 0.4924 - classification_loss: 0.0535
 336/1000 [=========>....................] - ETA: 24:44 - loss: 0.5454 - regression_loss: 0.4919 - classification_loss: 0.0535
 337/1000 [=========>....................] - ETA: 24:42 - loss: 0.5456 - regression_loss: 0.4920 - classification_loss: 0.0535
 338/1000 [=========>....................] - ETA: 24:40 - loss: 0.5456 - regression_loss: 0.4920 - classification_loss: 0.0535
 339/1000 [=========>....................] - ETA: 24:38 - loss: 0.5454 - regression_loss: 0.4918 - classification_loss: 0.0536
 340/1000 [=========>....................] - ETA: 24:37 - loss: 0.5458 - regression_loss: 0.4922 - classification_loss: 0.0536
 341/1000 [=========>....................] - ETA: 24:34 - loss: 0.5458 - regression_loss: 0.4923 - classification_loss: 0.0536
 342/1000 [=========>....................] - ETA: 24:31 - loss: 0.5447 - regression_loss: 0.4912 - classification_loss: 0.0535
 343/1000 [=========>....................] - ETA: 24:29 - loss: 0.5449 - regression_loss: 0.4914 - classification_loss: 0.0535
 344/1000 [=========>....................] - ETA: 24:27 - loss: 0.5448 - regression_loss: 0.4913 - classification_loss: 0.0535
 345/1000 [=========>....................] - ETA: 24:24 - loss: 0.5442 - regression_loss: 0.4908 - classification_loss: 0.0535
 346/1000 [=========>....................] - ETA: 24:23 - loss: 0.5446 - regression_loss: 0.4911 - classification_loss: 0.0535
 347/1000 [=========>....................] - ETA: 24:21 - loss: 0.5450 - regression_loss: 0.4915 - classification_loss: 0.0535
 348/1000 [=========>....................] - ETA: 24:19 - loss: 0.5449 - regression_loss: 0.4914 - classification_loss: 0.0535
 349/1000 [=========>....................] - ETA: 24:18 - loss: 0.5453 - regression_loss: 0.4917 - classification_loss: 0.0535
 350/1000 [=========>....................] - ETA: 24:16 - loss: 0.5454 - regression_loss: 0.4919 - classification_loss: 0.0535
 351/1000 [=========>....................] - ETA: 24:14 - loss: 0.5458 - regression_loss: 0.4923 - classification_loss: 0.0535
 352/1000 [=========>....................] - ETA: 24:11 - loss: 0.5447 - regression_loss: 0.4913 - classification_loss: 0.0534
 353/1000 [=========>....................] - ETA: 24:08 - loss: 0.5442 - regression_loss: 0.4908 - classification_loss: 0.0534
 354/1000 [=========>....................] - ETA: 24:05 - loss: 0.5444 - regression_loss: 0.4910 - classification_loss: 0.0534
 355/1000 [=========>....................] - ETA: 24:04 - loss: 0.5447 - regression_loss: 0.4912 - classification_loss: 0.0534
 356/1000 [=========>....................] - ETA: 24:02 - loss: 0.5451 - regression_loss: 0.4917 - classification_loss: 0.0535
 357/1000 [=========>....................] - ETA: 24:01 - loss: 0.5456 - regression_loss: 0.4921 - classification_loss: 0.0535
 358/1000 [=========>....................] - ETA: 23:57 - loss: 0.5444 - regression_loss: 0.4910 - classification_loss: 0.0534
 359/1000 [=========>....................] - ETA: 23:54 - loss: 0.5439 - regression_loss: 0.4905 - classification_loss: 0.0534
 360/1000 [=========>....................] - ETA: 23:52 - loss: 0.5440 - regression_loss: 0.4906 - classification_loss: 0.0534
 361/1000 [=========>....................] - ETA: 23:50 - loss: 0.5439 - regression_loss: 0.4905 - classification_loss: 0.0534
 362/1000 [=========>....................] - ETA: 23:48 - loss: 0.5438 - regression_loss: 0.4904 - classification_loss: 0.0534
 363/1000 [=========>....................] - ETA: 23:46 - loss: 0.5444 - regression_loss: 0.4910 - classification_loss: 0.0534
 364/1000 [=========>....................] - ETA: 23:45 - loss: 0.5448 - regression_loss: 0.4913 - classification_loss: 0.0534
 365/1000 [=========>....................] - ETA: 23:42 - loss: 0.5442 - regression_loss: 0.4908 - classification_loss: 0.0534
 366/1000 [=========>....................] - ETA: 23:39 - loss: 0.5444 - regression_loss: 0.4910 - classification_loss: 0.0534
 367/1000 [==========>...................] - ETA: 23:37 - loss: 0.5449 - regression_loss: 0.4914 - classification_loss: 0.0534
 368/1000 [==========>...................] - ETA: 23:34 - loss: 0.5438 - regression_loss: 0.4904 - classification_loss: 0.0533
 369/1000 [==========>...................] - ETA: 23:31 - loss: 0.5426 - regression_loss: 0.4893 - classification_loss: 0.0532
 370/1000 [==========>...................] - ETA: 23:29 - loss: 0.5432 - regression_loss: 0.4899 - classification_loss: 0.0532
 371/1000 [==========>...................] - ETA: 23:27 - loss: 0.5441 - regression_loss: 0.4908 - classification_loss: 0.0533
 372/1000 [==========>...................] - ETA: 23:26 - loss: 0.5446 - regression_loss: 0.4913 - classification_loss: 0.0533
 373/1000 [==========>...................] - ETA: 23:23 - loss: 0.5447 - regression_loss: 0.4915 - classification_loss: 0.0533
 374/1000 [==========>...................] - ETA: 23:22 - loss: 0.5447 - regression_loss: 0.4914 - classification_loss: 0.0533
 375/1000 [==========>...................] - ETA: 23:18 - loss: 0.5445 - regression_loss: 0.4912 - classification_loss: 0.0533
 376/1000 [==========>...................] - ETA: 23:17 - loss: 0.5451 - regression_loss: 0.4918 - classification_loss: 0.0533
 377/1000 [==========>...................] - ETA: 23:13 - loss: 0.5443 - regression_loss: 0.4910 - classification_loss: 0.0533
 378/1000 [==========>...................] - ETA: 23:12 - loss: 0.5449 - regression_loss: 0.4916 - classification_loss: 0.0533
 379/1000 [==========>...................] - ETA: 23:10 - loss: 0.5449 - regression_loss: 0.4916 - classification_loss: 0.0533
 380/1000 [==========>...................] - ETA: 23:08 - loss: 0.5451 - regression_loss: 0.4918 - classification_loss: 0.0533
 381/1000 [==========>...................] - ETA: 23:05 - loss: 0.5446 - regression_loss: 0.4913 - classification_loss: 0.0533
 382/1000 [==========>...................] - ETA: 23:03 - loss: 0.5449 - regression_loss: 0.4915 - classification_loss: 0.0533
 383/1000 [==========>...................] - ETA: 23:01 - loss: 0.5452 - regression_loss: 0.4919 - classification_loss: 0.0534
 384/1000 [==========>...................] - ETA: 22:58 - loss: 0.5447 - regression_loss: 0.4914 - classification_loss: 0.0533
 385/1000 [==========>...................] - ETA: 22:56 - loss: 0.5446 - regression_loss: 0.4913 - classification_loss: 0.0533
 386/1000 [==========>...................] - ETA: 22:54 - loss: 0.5448 - regression_loss: 0.4914 - classification_loss: 0.0534
 387/1000 [==========>...................] - ETA: 22:51 - loss: 0.5437 - regression_loss: 0.4904 - classification_loss: 0.0533
 388/1000 [==========>...................] - ETA: 22:49 - loss: 0.5441 - regression_loss: 0.4908 - classification_loss: 0.0533
 389/1000 [==========>...................] - ETA: 22:47 - loss: 0.5445 - regression_loss: 0.4912 - classification_loss: 0.0533
 390/1000 [==========>...................] - ETA: 22:45 - loss: 0.5444 - regression_loss: 0.4911 - classification_loss: 0.0533
 391/1000 [==========>...................] - ETA: 22:44 - loss: 0.5448 - regression_loss: 0.4914 - classification_loss: 0.0533
 392/1000 [==========>...................] - ETA: 22:41 - loss: 0.5448 - regression_loss: 0.4914 - classification_loss: 0.0533
 393/1000 [==========>...................] - ETA: 22:40 - loss: 0.5449 - regression_loss: 0.4916 - classification_loss: 0.0533
 394/1000 [==========>...................] - ETA: 22:36 - loss: 0.5439 - regression_loss: 0.4907 - classification_loss: 0.0532
 395/1000 [==========>...................] - ETA: 22:35 - loss: 0.5444 - regression_loss: 0.4911 - classification_loss: 0.0533
 396/1000 [==========>...................] - ETA: 22:32 - loss: 0.5441 - regression_loss: 0.4909 - classification_loss: 0.0532
 397/1000 [==========>...................] - ETA: 22:30 - loss: 0.5443 - regression_loss: 0.4910 - classification_loss: 0.0532
 398/1000 [==========>...................] - ETA: 22:27 - loss: 0.5437 - regression_loss: 0.4905 - classification_loss: 0.0532
 399/1000 [==========>...................] - ETA: 22:25 - loss: 0.5438 - regression_loss: 0.4906 - classification_loss: 0.0532
 400/1000 [===========>..................] - ETA: 22:21 - loss: 0.5429 - regression_loss: 0.4898 - classification_loss: 0.0531
 401/1000 [===========>..................] - ETA: 22:20 - loss: 0.5434 - regression_loss: 0.4902 - classification_loss: 0.0531
 402/1000 [===========>..................] - ETA: 22:17 - loss: 0.5434 - regression_loss: 0.4903 - classification_loss: 0.0531
 403/1000 [===========>..................] - ETA: 22:16 - loss: 0.5438 - regression_loss: 0.4906 - classification_loss: 0.0531
 404/1000 [===========>..................] - ETA: 22:14 - loss: 0.5442 - regression_loss: 0.4910 - classification_loss: 0.0532
 405/1000 [===========>..................] - ETA: 22:11 - loss: 0.5437 - regression_loss: 0.4906 - classification_loss: 0.0531
 406/1000 [===========>..................] - ETA: 22:09 - loss: 0.5440 - regression_loss: 0.4909 - classification_loss: 0.0531
 407/1000 [===========>..................] - ETA: 22:07 - loss: 0.5442 - regression_loss: 0.4911 - classification_loss: 0.0531
 408/1000 [===========>..................] - ETA: 22:04 - loss: 0.5433 - regression_loss: 0.4903 - classification_loss: 0.0530
 409/1000 [===========>..................] - ETA: 22:02 - loss: 0.5433 - regression_loss: 0.4903 - classification_loss: 0.0531
 410/1000 [===========>..................] - ETA: 22:00 - loss: 0.5440 - regression_loss: 0.4909 - classification_loss: 0.0531
 411/1000 [===========>..................] - ETA: 21:57 - loss: 0.5440 - regression_loss: 0.4909 - classification_loss: 0.0531
 412/1000 [===========>..................] - ETA: 21:54 - loss: 0.5429 - regression_loss: 0.4900 - classification_loss: 0.0530
 413/1000 [===========>..................] - ETA: 21:52 - loss: 0.5429 - regression_loss: 0.4899 - classification_loss: 0.0530
 414/1000 [===========>..................] - ETA: 21:50 - loss: 0.5433 - regression_loss: 0.4903 - classification_loss: 0.0530
 415/1000 [===========>..................] - ETA: 21:48 - loss: 0.5434 - regression_loss: 0.4904 - classification_loss: 0.0530
 416/1000 [===========>..................] - ETA: 21:46 - loss: 0.5439 - regression_loss: 0.4909 - classification_loss: 0.0530
 417/1000 [===========>..................] - ETA: 21:45 - loss: 0.5441 - regression_loss: 0.4911 - classification_loss: 0.0530
 418/1000 [===========>..................] - ETA: 21:42 - loss: 0.5442 - regression_loss: 0.4912 - classification_loss: 0.0530
 419/1000 [===========>..................] - ETA: 21:39 - loss: 0.5433 - regression_loss: 0.4904 - classification_loss: 0.0529
 420/1000 [===========>..................] - ETA: 21:37 - loss: 0.5437 - regression_loss: 0.4907 - classification_loss: 0.0530
 421/1000 [===========>..................] - ETA: 21:35 - loss: 0.5439 - regression_loss: 0.4910 - classification_loss: 0.0530
 422/1000 [===========>..................] - ETA: 21:33 - loss: 0.5439 - regression_loss: 0.4909 - classification_loss: 0.0530
 423/1000 [===========>..................] - ETA: 21:30 - loss: 0.5434 - regression_loss: 0.4905 - classification_loss: 0.0529
 424/1000 [===========>..................] - ETA: 21:29 - loss: 0.5438 - regression_loss: 0.4908 - classification_loss: 0.0529
 425/1000 [===========>..................] - ETA: 21:27 - loss: 0.5437 - regression_loss: 0.4907 - classification_loss: 0.0530
 426/1000 [===========>..................] - ETA: 21:24 - loss: 0.5437 - regression_loss: 0.4907 - classification_loss: 0.0530
 427/1000 [===========>..................] - ETA: 21:21 - loss: 0.5427 - regression_loss: 0.4898 - classification_loss: 0.0529
 428/1000 [===========>..................] - ETA: 21:20 - loss: 0.5431 - regression_loss: 0.4902 - classification_loss: 0.0529
 429/1000 [===========>..................] - ETA: 21:18 - loss: 0.5435 - regression_loss: 0.4906 - classification_loss: 0.0529
 430/1000 [===========>..................] - ETA: 21:15 - loss: 0.5431 - regression_loss: 0.4902 - classification_loss: 0.0529
 431/1000 [===========>..................] - ETA: 21:13 - loss: 0.5432 - regression_loss: 0.4903 - classification_loss: 0.0529
 432/1000 [===========>..................] - ETA: 21:11 - loss: 0.5436 - regression_loss: 0.4907 - classification_loss: 0.0529
 433/1000 [===========>..................] - ETA: 21:10 - loss: 0.5439 - regression_loss: 0.4910 - classification_loss: 0.0529
 434/1000 [============>.................] - ETA: 21:08 - loss: 0.5437 - regression_loss: 0.4908 - classification_loss: 0.0529
 435/1000 [============>.................] - ETA: 21:06 - loss: 0.5438 - regression_loss: 0.4909 - classification_loss: 0.0529
 436/1000 [============>.................] - ETA: 21:03 - loss: 0.5429 - regression_loss: 0.4900 - classification_loss: 0.0528
 437/1000 [============>.................] - ETA: 21:00 - loss: 0.5429 - regression_loss: 0.4900 - classification_loss: 0.0528
 438/1000 [============>.................] - ETA: 20:58 - loss: 0.5424 - regression_loss: 0.4896 - classification_loss: 0.0528
 439/1000 [============>.................] - ETA: 20:55 - loss: 0.5424 - regression_loss: 0.4896 - classification_loss: 0.0528
 440/1000 [============>.................] - ETA: 20:52 - loss: 0.5418 - regression_loss: 0.4891 - classification_loss: 0.0528
 441/1000 [============>.................] - ETA: 20:50 - loss: 0.5420 - regression_loss: 0.4893 - classification_loss: 0.0528
 442/1000 [============>.................] - ETA: 20:49 - loss: 0.5424 - regression_loss: 0.4896 - classification_loss: 0.0528
 443/1000 [============>.................] - ETA: 20:46 - loss: 0.5413 - regression_loss: 0.4886 - classification_loss: 0.0527
 444/1000 [============>.................] - ETA: 20:44 - loss: 0.5417 - regression_loss: 0.4890 - classification_loss: 0.0527
 445/1000 [============>.................] - ETA: 20:42 - loss: 0.5416 - regression_loss: 0.4889 - classification_loss: 0.0527
 446/1000 [============>.................] - ETA: 20:40 - loss: 0.5417 - regression_loss: 0.4890 - classification_loss: 0.0527
 447/1000 [============>.................] - ETA: 20:37 - loss: 0.5414 - regression_loss: 0.4887 - classification_loss: 0.0527
 448/1000 [============>.................] - ETA: 20:35 - loss: 0.5414 - regression_loss: 0.4887 - classification_loss: 0.0527
 449/1000 [============>.................] - ETA: 20:33 - loss: 0.5419 - regression_loss: 0.4892 - classification_loss: 0.0527
 450/1000 [============>.................] - ETA: 20:31 - loss: 0.5422 - regression_loss: 0.4895 - classification_loss: 0.0527
 451/1000 [============>.................] - ETA: 20:28 - loss: 0.5414 - regression_loss: 0.4888 - classification_loss: 0.0526
 452/1000 [============>.................] - ETA: 20:26 - loss: 0.5418 - regression_loss: 0.4892 - classification_loss: 0.0526
 453/1000 [============>.................] - ETA: 20:24 - loss: 0.5419 - regression_loss: 0.4893 - classification_loss: 0.0526
 454/1000 [============>.................] - ETA: 20:21 - loss: 0.5409 - regression_loss: 0.4884 - classification_loss: 0.0526
 455/1000 [============>.................] - ETA: 20:19 - loss: 0.5413 - regression_loss: 0.4887 - classification_loss: 0.0526
 456/1000 [============>.................] - ETA: 20:16 - loss: 0.5409 - regression_loss: 0.4883 - classification_loss: 0.0525
 457/1000 [============>.................] - ETA: 20:14 - loss: 0.5408 - regression_loss: 0.4882 - classification_loss: 0.0526
 458/1000 [============>.................] - ETA: 20:13 - loss: 0.5410 - regression_loss: 0.4884 - classification_loss: 0.0526
 459/1000 [============>.................] - ETA: 20:11 - loss: 0.5413 - regression_loss: 0.4887 - classification_loss: 0.0526
 460/1000 [============>.................] - ETA: 20:09 - loss: 0.5412 - regression_loss: 0.4886 - classification_loss: 0.0526
 461/1000 [============>.................] - ETA: 20:07 - loss: 0.5415 - regression_loss: 0.4889 - classification_loss: 0.0526
 462/1000 [============>.................] - ETA: 20:05 - loss: 0.5416 - regression_loss: 0.4890 - classification_loss: 0.0526
 463/1000 [============>.................] - ETA: 20:03 - loss: 0.5416 - regression_loss: 0.4890 - classification_loss: 0.0526
 464/1000 [============>.................] - ETA: 20:00 - loss: 0.5407 - regression_loss: 0.4882 - classification_loss: 0.0525
 465/1000 [============>.................] - ETA: 19:57 - loss: 0.5403 - regression_loss: 0.4878 - classification_loss: 0.0525
 466/1000 [============>.................] - ETA: 19:55 - loss: 0.5406 - regression_loss: 0.4881 - classification_loss: 0.0525
 467/1000 [=============>................] - ETA: 19:53 - loss: 0.5409 - regression_loss: 0.4884 - classification_loss: 0.0525
 468/1000 [=============>................] - ETA: 19:51 - loss: 0.5409 - regression_loss: 0.4884 - classification_loss: 0.0525
 469/1000 [=============>................] - ETA: 19:49 - loss: 0.5411 - regression_loss: 0.4886 - classification_loss: 0.0525
 470/1000 [=============>................] - ETA: 19:47 - loss: 0.5410 - regression_loss: 0.4885 - classification_loss: 0.0525
 471/1000 [=============>................] - ETA: 19:44 - loss: 0.5405 - regression_loss: 0.4881 - classification_loss: 0.0525
 472/1000 [=============>................] - ETA: 19:41 - loss: 0.5398 - regression_loss: 0.4874 - classification_loss: 0.0524
 473/1000 [=============>................] - ETA: 19:39 - loss: 0.5404 - regression_loss: 0.4879 - classification_loss: 0.0524
 474/1000 [=============>................] - ETA: 19:37 - loss: 0.5409 - regression_loss: 0.4884 - classification_loss: 0.0524
 475/1000 [=============>................] - ETA: 19:36 - loss: 0.5413 - regression_loss: 0.4889 - classification_loss: 0.0525
 476/1000 [=============>................] - ETA: 19:33 - loss: 0.5404 - regression_loss: 0.4880 - classification_loss: 0.0524
 477/1000 [=============>................] - ETA: 19:31 - loss: 0.5403 - regression_loss: 0.4879 - classification_loss: 0.0524
 478/1000 [=============>................] - ETA: 19:29 - loss: 0.5406 - regression_loss: 0.4882 - classification_loss: 0.0524
 479/1000 [=============>................] - ETA: 19:26 - loss: 0.5406 - regression_loss: 0.4882 - classification_loss: 0.0524
 480/1000 [=============>................] - ETA: 19:24 - loss: 0.5402 - regression_loss: 0.4878 - classification_loss: 0.0524
 481/1000 [=============>................] - ETA: 19:22 - loss: 0.5405 - regression_loss: 0.4882 - classification_loss: 0.0524
 482/1000 [=============>................] - ETA: 19:19 - loss: 0.5405 - regression_loss: 0.4881 - classification_loss: 0.0524
 483/1000 [=============>................] - ETA: 19:18 - loss: 0.5406 - regression_loss: 0.4883 - classification_loss: 0.0524
 484/1000 [=============>................] - ETA: 19:16 - loss: 0.5409 - regression_loss: 0.4885 - classification_loss: 0.0524
 485/1000 [=============>................] - ETA: 19:14 - loss: 0.5407 - regression_loss: 0.4883 - classification_loss: 0.0524
 486/1000 [=============>................] - ETA: 19:11 - loss: 0.5403 - regression_loss: 0.4879 - classification_loss: 0.0524
 487/1000 [=============>................] - ETA: 19:08 - loss: 0.5395 - regression_loss: 0.4873 - classification_loss: 0.0523
 488/1000 [=============>................] - ETA: 19:06 - loss: 0.5398 - regression_loss: 0.4875 - classification_loss: 0.0523
 489/1000 [=============>................] - ETA: 19:04 - loss: 0.5401 - regression_loss: 0.4878 - classification_loss: 0.0523
 490/1000 [=============>................] - ETA: 19:02 - loss: 0.5402 - regression_loss: 0.4878 - classification_loss: 0.0523
 491/1000 [=============>................] - ETA: 19:00 - loss: 0.5403 - regression_loss: 0.4879 - classification_loss: 0.0523
 492/1000 [=============>................] - ETA: 18:58 - loss: 0.5402 - regression_loss: 0.4878 - classification_loss: 0.0524
 493/1000 [=============>................] - ETA: 18:55 - loss: 0.5395 - regression_loss: 0.4872 - classification_loss: 0.0523
 494/1000 [=============>................] - ETA: 18:52 - loss: 0.5392 - regression_loss: 0.4869 - classification_loss: 0.0523
 495/1000 [=============>................] - ETA: 18:50 - loss: 0.5393 - regression_loss: 0.4871 - classification_loss: 0.0523
 496/1000 [=============>................] - ETA: 18:47 - loss: 0.5388 - regression_loss: 0.4866 - classification_loss: 0.0522
 497/1000 [=============>................] - ETA: 18:45 - loss: 0.5391 - regression_loss: 0.4868 - classification_loss: 0.0522
 498/1000 [=============>................] - ETA: 18:43 - loss: 0.5382 - regression_loss: 0.4860 - classification_loss: 0.0522
 499/1000 [=============>................] - ETA: 18:40 - loss: 0.5383 - regression_loss: 0.4861 - classification_loss: 0.0522
 500/1000 [==============>...............] - ETA: 18:39 - loss: 0.5386 - regression_loss: 0.4864 - classification_loss: 0.0522
 501/1000 [==============>...............] - ETA: 18:37 - loss: 0.5387 - regression_loss: 0.4865 - classification_loss: 0.0522
 502/1000 [==============>...............] - ETA: 18:34 - loss: 0.5380 - regression_loss: 0.4858 - classification_loss: 0.0521
 503/1000 [==============>...............] - ETA: 18:32 - loss: 0.5381 - regression_loss: 0.4860 - classification_loss: 0.0521
 504/1000 [==============>...............] - ETA: 18:30 - loss: 0.5384 - regression_loss: 0.4862 - classification_loss: 0.0522
 505/1000 [==============>...............] - ETA: 18:28 - loss: 0.5387 - regression_loss: 0.4866 - classification_loss: 0.0522
 506/1000 [==============>...............] - ETA: 18:25 - loss: 0.5391 - regression_loss: 0.4869 - classification_loss: 0.0522
 507/1000 [==============>...............] - ETA: 18:23 - loss: 0.5390 - regression_loss: 0.4868 - classification_loss: 0.0522
 508/1000 [==============>...............] - ETA: 18:21 - loss: 0.5393 - regression_loss: 0.4870 - classification_loss: 0.0522
 509/1000 [==============>...............] - ETA: 18:19 - loss: 0.5395 - regression_loss: 0.4872 - classification_loss: 0.0522
 510/1000 [==============>...............] - ETA: 18:16 - loss: 0.5386 - regression_loss: 0.4864 - classification_loss: 0.0522
 511/1000 [==============>...............] - ETA: 18:14 - loss: 0.5387 - regression_loss: 0.4865 - classification_loss: 0.0522
 512/1000 [==============>...............] - ETA: 18:12 - loss: 0.5386 - regression_loss: 0.4864 - classification_loss: 0.0522
 513/1000 [==============>...............] - ETA: 18:10 - loss: 0.5386 - regression_loss: 0.4864 - classification_loss: 0.0522
 514/1000 [==============>...............] - ETA: 18:08 - loss: 0.5389 - regression_loss: 0.4867 - classification_loss: 0.0522
 515/1000 [==============>...............] - ETA: 18:05 - loss: 0.5387 - regression_loss: 0.4865 - classification_loss: 0.0522
 516/1000 [==============>...............] - ETA: 18:03 - loss: 0.5386 - regression_loss: 0.4864 - classification_loss: 0.0522
 517/1000 [==============>...............] - ETA: 18:01 - loss: 0.5382 - regression_loss: 0.4861 - classification_loss: 0.0522
 518/1000 [==============>...............] - ETA: 17:58 - loss: 0.5383 - regression_loss: 0.4861 - classification_loss: 0.0522
 519/1000 [==============>...............] - ETA: 17:56 - loss: 0.5385 - regression_loss: 0.4864 - classification_loss: 0.0522
 520/1000 [==============>...............] - ETA: 17:55 - loss: 0.5388 - regression_loss: 0.4867 - classification_loss: 0.0522
 521/1000 [==============>...............] - ETA: 17:53 - loss: 0.5389 - regression_loss: 0.4868 - classification_loss: 0.0522
 522/1000 [==============>...............] - ETA: 17:50 - loss: 0.5382 - regression_loss: 0.4861 - classification_loss: 0.0521
 523/1000 [==============>...............] - ETA: 17:48 - loss: 0.5383 - regression_loss: 0.4862 - classification_loss: 0.0521
 524/1000 [==============>...............] - ETA: 17:45 - loss: 0.5376 - regression_loss: 0.4856 - classification_loss: 0.0521
 525/1000 [==============>...............] - ETA: 17:43 - loss: 0.5377 - regression_loss: 0.4856 - classification_loss: 0.0521
 526/1000 [==============>...............] - ETA: 17:40 - loss: 0.5376 - regression_loss: 0.4855 - classification_loss: 0.0521
 527/1000 [==============>...............] - ETA: 17:38 - loss: 0.5381 - regression_loss: 0.4860 - classification_loss: 0.0521
 528/1000 [==============>...............] - ETA: 17:36 - loss: 0.5384 - regression_loss: 0.4863 - classification_loss: 0.0521
 529/1000 [==============>...............] - ETA: 17:34 - loss: 0.5387 - regression_loss: 0.4866 - classification_loss: 0.0521
 530/1000 [==============>...............] - ETA: 17:32 - loss: 0.5390 - regression_loss: 0.4869 - classification_loss: 0.0522
 531/1000 [==============>...............] - ETA: 17:30 - loss: 0.5393 - regression_loss: 0.4871 - classification_loss: 0.0522
 532/1000 [==============>...............] - ETA: 17:28 - loss: 0.5392 - regression_loss: 0.4870 - classification_loss: 0.0522
 533/1000 [==============>...............] - ETA: 17:25 - loss: 0.5385 - regression_loss: 0.4864 - classification_loss: 0.0521
 534/1000 [===============>..............] - ETA: 17:23 - loss: 0.5384 - regression_loss: 0.4863 - classification_loss: 0.0521
 535/1000 [===============>..............] - ETA: 17:21 - loss: 0.5386 - regression_loss: 0.4865 - classification_loss: 0.0521
 536/1000 [===============>..............] - ETA: 17:18 - loss: 0.5387 - regression_loss: 0.4866 - classification_loss: 0.0522
 537/1000 [===============>..............] - ETA: 17:17 - loss: 0.5391 - regression_loss: 0.4869 - classification_loss: 0.0522
 538/1000 [===============>..............] - ETA: 17:14 - loss: 0.5392 - regression_loss: 0.4870 - classification_loss: 0.0522
 539/1000 [===============>..............] - ETA: 17:12 - loss: 0.5388 - regression_loss: 0.4867 - classification_loss: 0.0522
 540/1000 [===============>..............] - ETA: 17:10 - loss: 0.5387 - regression_loss: 0.4865 - classification_loss: 0.0522
 541/1000 [===============>..............] - ETA: 17:08 - loss: 0.5389 - regression_loss: 0.4868 - classification_loss: 0.0522
 542/1000 [===============>..............] - ETA: 17:05 - loss: 0.5383 - regression_loss: 0.4862 - classification_loss: 0.0521
 543/1000 [===============>..............] - ETA: 17:03 - loss: 0.5385 - regression_loss: 0.4863 - classification_loss: 0.0521
 544/1000 [===============>..............] - ETA: 17:01 - loss: 0.5387 - regression_loss: 0.4866 - classification_loss: 0.0521
 545/1000 [===============>..............] - ETA: 16:59 - loss: 0.5386 - regression_loss: 0.4865 - classification_loss: 0.0522
 546/1000 [===============>..............] - ETA: 16:57 - loss: 0.5388 - regression_loss: 0.4867 - classification_loss: 0.0522
 547/1000 [===============>..............] - ETA: 16:54 - loss: 0.5380 - regression_loss: 0.4859 - classification_loss: 0.0521
 548/1000 [===============>..............] - ETA: 16:52 - loss: 0.5380 - regression_loss: 0.4859 - classification_loss: 0.0521
 549/1000 [===============>..............] - ETA: 16:50 - loss: 0.5381 - regression_loss: 0.4860 - classification_loss: 0.0521
 550/1000 [===============>..............] - ETA: 16:47 - loss: 0.5379 - regression_loss: 0.4858 - classification_loss: 0.0521
 551/1000 [===============>..............] - ETA: 16:45 - loss: 0.5378 - regression_loss: 0.4857 - classification_loss: 0.0521
 552/1000 [===============>..............] - ETA: 16:43 - loss: 0.5381 - regression_loss: 0.4859 - classification_loss: 0.0521
 553/1000 [===============>..............] - ETA: 16:41 - loss: 0.5383 - regression_loss: 0.4862 - classification_loss: 0.0521
 554/1000 [===============>..............] - ETA: 16:38 - loss: 0.5376 - regression_loss: 0.4855 - classification_loss: 0.0521
 555/1000 [===============>..............] - ETA: 16:36 - loss: 0.5377 - regression_loss: 0.4856 - classification_loss: 0.0521
 556/1000 [===============>..............] - ETA: 16:34 - loss: 0.5379 - regression_loss: 0.4858 - classification_loss: 0.0521
 557/1000 [===============>..............] - ETA: 16:31 - loss: 0.5375 - regression_loss: 0.4854 - classification_loss: 0.0521
 558/1000 [===============>..............] - ETA: 16:30 - loss: 0.5378 - regression_loss: 0.4857 - classification_loss: 0.0521
 559/1000 [===============>..............] - ETA: 16:27 - loss: 0.5374 - regression_loss: 0.4853 - classification_loss: 0.0521
 560/1000 [===============>..............] - ETA: 16:24 - loss: 0.5366 - regression_loss: 0.4846 - classification_loss: 0.0520
 561/1000 [===============>..............] - ETA: 16:22 - loss: 0.5367 - regression_loss: 0.4847 - classification_loss: 0.0520
 562/1000 [===============>..............] - ETA: 16:20 - loss: 0.5367 - regression_loss: 0.4847 - classification_loss: 0.0520
 563/1000 [===============>..............] - ETA: 16:18 - loss: 0.5367 - regression_loss: 0.4846 - classification_loss: 0.0520
 564/1000 [===============>..............] - ETA: 16:16 - loss: 0.5369 - regression_loss: 0.4849 - classification_loss: 0.0520
 565/1000 [===============>..............] - ETA: 16:14 - loss: 0.5368 - regression_loss: 0.4848 - classification_loss: 0.0520
 566/1000 [===============>..............] - ETA: 16:11 - loss: 0.5368 - regression_loss: 0.4848 - classification_loss: 0.0520
 567/1000 [================>.............] - ETA: 16:09 - loss: 0.5369 - regression_loss: 0.4849 - classification_loss: 0.0520
 568/1000 [================>.............] - ETA: 16:07 - loss: 0.5363 - regression_loss: 0.4843 - classification_loss: 0.0520
 569/1000 [================>.............] - ETA: 16:05 - loss: 0.5366 - regression_loss: 0.4846 - classification_loss: 0.0520
 570/1000 [================>.............] - ETA: 16:03 - loss: 0.5368 - regression_loss: 0.4848 - classification_loss: 0.0520
 571/1000 [================>.............] - ETA: 16:00 - loss: 0.5365 - regression_loss: 0.4845 - classification_loss: 0.0520
 572/1000 [================>.............] - ETA: 15:58 - loss: 0.5365 - regression_loss: 0.4845 - classification_loss: 0.0520
 573/1000 [================>.............] - ETA: 15:56 - loss: 0.5367 - regression_loss: 0.4847 - classification_loss: 0.0520
 574/1000 [================>.............] - ETA: 15:54 - loss: 0.5366 - regression_loss: 0.4846 - classification_loss: 0.0520
 575/1000 [================>.............] - ETA: 15:52 - loss: 0.5368 - regression_loss: 0.4848 - classification_loss: 0.0520
 576/1000 [================>.............] - ETA: 15:49 - loss: 0.5365 - regression_loss: 0.4845 - classification_loss: 0.0520
 577/1000 [================>.............] - ETA: 15:47 - loss: 0.5367 - regression_loss: 0.4848 - classification_loss: 0.0520
 578/1000 [================>.............] - ETA: 15:44 - loss: 0.5360 - regression_loss: 0.4840 - classification_loss: 0.0519
 579/1000 [================>.............] - ETA: 15:42 - loss: 0.5361 - regression_loss: 0.4841 - classification_loss: 0.0519
 580/1000 [================>.............] - ETA: 15:40 - loss: 0.5362 - regression_loss: 0.4842 - classification_loss: 0.0519
 581/1000 [================>.............] - ETA: 15:38 - loss: 0.5364 - regression_loss: 0.4845 - classification_loss: 0.0519
 582/1000 [================>.............] - ETA: 15:36 - loss: 0.5367 - regression_loss: 0.4847 - classification_loss: 0.0520
 583/1000 [================>.............] - ETA: 15:34 - loss: 0.5366 - regression_loss: 0.4846 - classification_loss: 0.0520
 584/1000 [================>.............] - ETA: 15:31 - loss: 0.5359 - regression_loss: 0.4840 - classification_loss: 0.0519
 585/1000 [================>.............] - ETA: 15:29 - loss: 0.5357 - regression_loss: 0.4838 - classification_loss: 0.0519
 586/1000 [================>.............] - ETA: 15:27 - loss: 0.5360 - regression_loss: 0.4842 - classification_loss: 0.0519
 587/1000 [================>.............] - ETA: 15:25 - loss: 0.5362 - regression_loss: 0.4843 - classification_loss: 0.0519
 588/1000 [================>.............] - ETA: 15:22 - loss: 0.5355 - regression_loss: 0.4837 - classification_loss: 0.0518
 589/1000 [================>.............] - ETA: 15:20 - loss: 0.5355 - regression_loss: 0.4837 - classification_loss: 0.0518
 590/1000 [================>.............] - ETA: 15:18 - loss: 0.5356 - regression_loss: 0.4838 - classification_loss: 0.0518
 591/1000 [================>.............] - ETA: 15:15 - loss: 0.5353 - regression_loss: 0.4835 - classification_loss: 0.0518
 592/1000 [================>.............] - ETA: 15:13 - loss: 0.5356 - regression_loss: 0.4838 - classification_loss: 0.0518
 593/1000 [================>.............] - ETA: 15:11 - loss: 0.5359 - regression_loss: 0.4841 - classification_loss: 0.0518
 594/1000 [================>.............] - ETA: 15:09 - loss: 0.5360 - regression_loss: 0.4842 - classification_loss: 0.0518
 595/1000 [================>.............] - ETA: 15:07 - loss: 0.5360 - regression_loss: 0.4842 - classification_loss: 0.0518
 596/1000 [================>.............] - ETA: 15:05 - loss: 0.5363 - regression_loss: 0.4844 - classification_loss: 0.0518
 597/1000 [================>.............] - ETA: 15:02 - loss: 0.5356 - regression_loss: 0.4838 - classification_loss: 0.0518
 598/1000 [================>.............] - ETA: 15:00 - loss: 0.5357 - regression_loss: 0.4840 - classification_loss: 0.0518
 599/1000 [================>.............] - ETA: 14:58 - loss: 0.5357 - regression_loss: 0.4840 - classification_loss: 0.0518
 600/1000 [=================>............] - ETA: 14:55 - loss: 0.5359 - regression_loss: 0.4841 - classification_loss: 0.0518
 601/1000 [=================>............] - ETA: 14:54 - loss: 0.5363 - regression_loss: 0.4845 - classification_loss: 0.0518
 602/1000 [=================>............] - ETA: 14:51 - loss: 0.5356 - regression_loss: 0.4839 - classification_loss: 0.0517
 603/1000 [=================>............] - ETA: 14:49 - loss: 0.5357 - regression_loss: 0.4839 - classification_loss: 0.0517
 604/1000 [=================>............] - ETA: 14:47 - loss: 0.5358 - regression_loss: 0.4841 - classification_loss: 0.0517
 605/1000 [=================>............] - ETA: 14:44 - loss: 0.5355 - regression_loss: 0.4838 - classification_loss: 0.0517
 606/1000 [=================>............] - ETA: 14:42 - loss: 0.5358 - regression_loss: 0.4840 - classification_loss: 0.0517
 607/1000 [=================>............] - ETA: 14:40 - loss: 0.5359 - regression_loss: 0.4842 - classification_loss: 0.0517
 608/1000 [=================>............] - ETA: 14:38 - loss: 0.5358 - regression_loss: 0.4841 - classification_loss: 0.0517
 609/1000 [=================>............] - ETA: 14:36 - loss: 0.5360 - regression_loss: 0.4843 - classification_loss: 0.0517
 610/1000 [=================>............] - ETA: 14:33 - loss: 0.5357 - regression_loss: 0.4840 - classification_loss: 0.0517
 611/1000 [=================>............] - ETA: 14:30 - loss: 0.5350 - regression_loss: 0.4834 - classification_loss: 0.0516
 612/1000 [=================>............] - ETA: 14:28 - loss: 0.5349 - regression_loss: 0.4832 - classification_loss: 0.0517
 613/1000 [=================>............] - ETA: 14:26 - loss: 0.5351 - regression_loss: 0.4835 - classification_loss: 0.0517
 614/1000 [=================>............] - ETA: 14:24 - loss: 0.5351 - regression_loss: 0.4835 - classification_loss: 0.0517
 615/1000 [=================>............] - ETA: 14:21 - loss: 0.5345 - regression_loss: 0.4829 - classification_loss: 0.0516
 616/1000 [=================>............] - ETA: 14:19 - loss: 0.5341 - regression_loss: 0.4825 - classification_loss: 0.0516
 617/1000 [=================>............] - ETA: 14:17 - loss: 0.5344 - regression_loss: 0.4828 - classification_loss: 0.0516
 618/1000 [=================>............] - ETA: 14:15 - loss: 0.5348 - regression_loss: 0.4832 - classification_loss: 0.0517
 619/1000 [=================>............] - ETA: 14:13 - loss: 0.5349 - regression_loss: 0.4832 - classification_loss: 0.0517
 620/1000 [=================>............] - ETA: 14:11 - loss: 0.5351 - regression_loss: 0.4834 - classification_loss: 0.0517
 621/1000 [=================>............] - ETA: 14:08 - loss: 0.5347 - regression_loss: 0.4830 - classification_loss: 0.0517
 622/1000 [=================>............] - ETA: 14:06 - loss: 0.5350 - regression_loss: 0.4833 - classification_loss: 0.0517
 623/1000 [=================>............] - ETA: 14:04 - loss: 0.5352 - regression_loss: 0.4835 - classification_loss: 0.0517
 624/1000 [=================>............] - ETA: 14:02 - loss: 0.5352 - regression_loss: 0.4835 - classification_loss: 0.0517
 625/1000 [=================>............] - ETA: 14:00 - loss: 0.5354 - regression_loss: 0.4837 - classification_loss: 0.0517
 626/1000 [=================>............] - ETA: 13:58 - loss: 0.5355 - regression_loss: 0.4838 - classification_loss: 0.0517
 627/1000 [=================>............] - ETA: 13:55 - loss: 0.5349 - regression_loss: 0.4833 - classification_loss: 0.0517
 628/1000 [=================>............] - ETA: 13:53 - loss: 0.5350 - regression_loss: 0.4833 - classification_loss: 0.0517
 629/1000 [=================>............] - ETA: 13:50 - loss: 0.5349 - regression_loss: 0.4832 - classification_loss: 0.0517
 630/1000 [=================>............] - ETA: 13:48 - loss: 0.5352 - regression_loss: 0.4835 - classification_loss: 0.0517
 631/1000 [=================>............] - ETA: 13:46 - loss: 0.5353 - regression_loss: 0.4836 - classification_loss: 0.0517
 632/1000 [=================>............] - ETA: 13:44 - loss: 0.5356 - regression_loss: 0.4838 - classification_loss: 0.0517
 633/1000 [=================>............] - ETA: 13:42 - loss: 0.5355 - regression_loss: 0.4838 - classification_loss: 0.0517
 634/1000 [==================>...........] - ETA: 13:39 - loss: 0.5349 - regression_loss: 0.4832 - classification_loss: 0.0516
 635/1000 [==================>...........] - ETA: 13:37 - loss: 0.5349 - regression_loss: 0.4832 - classification_loss: 0.0517
 636/1000 [==================>...........] - ETA: 13:35 - loss: 0.5351 - regression_loss: 0.4835 - classification_loss: 0.0517
 637/1000 [==================>...........] - ETA: 13:32 - loss: 0.5345 - regression_loss: 0.4829 - classification_loss: 0.0516
 638/1000 [==================>...........] - ETA: 13:30 - loss: 0.5346 - regression_loss: 0.4830 - classification_loss: 0.0516
 639/1000 [==================>...........] - ETA: 13:28 - loss: 0.5349 - regression_loss: 0.4833 - classification_loss: 0.0516
 640/1000 [==================>...........] - ETA: 13:26 - loss: 0.5346 - regression_loss: 0.4830 - classification_loss: 0.0516
 641/1000 [==================>...........] - ETA: 13:24 - loss: 0.5349 - regression_loss: 0.4833 - classification_loss: 0.0516
 642/1000 [==================>...........] - ETA: 13:21 - loss: 0.5342 - regression_loss: 0.4827 - classification_loss: 0.0516
 643/1000 [==================>...........] - ETA: 13:18 - loss: 0.5339 - regression_loss: 0.4823 - classification_loss: 0.0515
 644/1000 [==================>...........] - ETA: 13:16 - loss: 0.5341 - regression_loss: 0.4825 - classification_loss: 0.0515
 645/1000 [==================>...........] - ETA: 13:14 - loss: 0.5340 - regression_loss: 0.4825 - classification_loss: 0.0515
 646/1000 [==================>...........] - ETA: 13:12 - loss: 0.5341 - regression_loss: 0.4826 - classification_loss: 0.0515
 647/1000 [==================>...........] - ETA: 13:10 - loss: 0.5340 - regression_loss: 0.4825 - classification_loss: 0.0515
 648/1000 [==================>...........] - ETA: 13:08 - loss: 0.5342 - regression_loss: 0.4826 - classification_loss: 0.0515
 649/1000 [==================>...........] - ETA: 13:06 - loss: 0.5342 - regression_loss: 0.4827 - classification_loss: 0.0515
 650/1000 [==================>...........] - ETA: 13:03 - loss: 0.5336 - regression_loss: 0.4821 - classification_loss: 0.0515
 651/1000 [==================>...........] - ETA: 13:01 - loss: 0.5338 - regression_loss: 0.4823 - classification_loss: 0.0515
 652/1000 [==================>...........] - ETA: 12:59 - loss: 0.5339 - regression_loss: 0.4824 - classification_loss: 0.0515
 653/1000 [==================>...........] - ETA: 12:57 - loss: 0.5338 - regression_loss: 0.4823 - classification_loss: 0.0515
 654/1000 [==================>...........] - ETA: 12:55 - loss: 0.5336 - regression_loss: 0.4821 - classification_loss: 0.0515
 655/1000 [==================>...........] - ETA: 12:52 - loss: 0.5336 - regression_loss: 0.4821 - classification_loss: 0.0515
 656/1000 [==================>...........] - ETA: 12:50 - loss: 0.5335 - regression_loss: 0.4820 - classification_loss: 0.0515
 657/1000 [==================>...........] - ETA: 12:48 - loss: 0.5335 - regression_loss: 0.4820 - classification_loss: 0.0515
 658/1000 [==================>...........] - ETA: 12:46 - loss: 0.5335 - regression_loss: 0.4820 - classification_loss: 0.0515
 659/1000 [==================>...........] - ETA: 12:44 - loss: 0.5337 - regression_loss: 0.4822 - classification_loss: 0.0515
 660/1000 [==================>...........] - ETA: 12:41 - loss: 0.5334 - regression_loss: 0.4819 - classification_loss: 0.0515
 661/1000 [==================>...........] - ETA: 12:39 - loss: 0.5336 - regression_loss: 0.4821 - classification_loss: 0.0515
 662/1000 [==================>...........] - ETA: 12:37 - loss: 0.5330 - regression_loss: 0.4815 - classification_loss: 0.0514
 663/1000 [==================>...........] - ETA: 12:35 - loss: 0.5331 - regression_loss: 0.4816 - classification_loss: 0.0514
 664/1000 [==================>...........] - ETA: 12:33 - loss: 0.5333 - regression_loss: 0.4818 - classification_loss: 0.0515
 665/1000 [==================>...........] - ETA: 12:30 - loss: 0.5326 - regression_loss: 0.4812 - classification_loss: 0.0514
 666/1000 [==================>...........] - ETA: 12:28 - loss: 0.5326 - regression_loss: 0.4813 - classification_loss: 0.0514
 667/1000 [===================>..........] - ETA: 12:26 - loss: 0.5326 - regression_loss: 0.4812 - classification_loss: 0.0514
 668/1000 [===================>..........] - ETA: 12:23 - loss: 0.5323 - regression_loss: 0.4809 - classification_loss: 0.0514
 669/1000 [===================>..........] - ETA: 12:21 - loss: 0.5325 - regression_loss: 0.4811 - classification_loss: 0.0514
 670/1000 [===================>..........] - ETA: 12:18 - loss: 0.5320 - regression_loss: 0.4806 - classification_loss: 0.0513
 671/1000 [===================>..........] - ETA: 12:16 - loss: 0.5319 - regression_loss: 0.4806 - classification_loss: 0.0513
 672/1000 [===================>..........] - ETA: 12:14 - loss: 0.5321 - regression_loss: 0.4808 - classification_loss: 0.0514
 673/1000 [===================>..........] - ETA: 12:12 - loss: 0.5323 - regression_loss: 0.4809 - classification_loss: 0.0514
 674/1000 [===================>..........] - ETA: 12:10 - loss: 0.5320 - regression_loss: 0.4806 - classification_loss: 0.0513
 675/1000 [===================>..........] - ETA: 12:08 - loss: 0.5320 - regression_loss: 0.4807 - classification_loss: 0.0513
 676/1000 [===================>..........] - ETA: 12:05 - loss: 0.5320 - regression_loss: 0.4807 - classification_loss: 0.0513
 677/1000 [===================>..........] - ETA: 12:03 - loss: 0.5320 - regression_loss: 0.4807 - classification_loss: 0.0513
 678/1000 [===================>..........] - ETA: 12:00 - loss: 0.5316 - regression_loss: 0.4803 - classification_loss: 0.0513
 679/1000 [===================>..........] - ETA: 11:58 - loss: 0.5316 - regression_loss: 0.4803 - classification_loss: 0.0513
 680/1000 [===================>..........] - ETA: 11:56 - loss: 0.5310 - regression_loss: 0.4797 - classification_loss: 0.0513
 681/1000 [===================>..........] - ETA: 11:54 - loss: 0.5312 - regression_loss: 0.4799 - classification_loss: 0.0513
 682/1000 [===================>..........] - ETA: 11:52 - loss: 0.5313 - regression_loss: 0.4800 - classification_loss: 0.0513
 683/1000 [===================>..........] - ETA: 11:50 - loss: 0.5314 - regression_loss: 0.4801 - classification_loss: 0.0513
 684/1000 [===================>..........] - ETA: 11:47 - loss: 0.5313 - regression_loss: 0.4801 - classification_loss: 0.0513
 685/1000 [===================>..........] - ETA: 11:45 - loss: 0.5308 - regression_loss: 0.4796 - classification_loss: 0.0512
 686/1000 [===================>..........] - ETA: 11:43 - loss: 0.5308 - regression_loss: 0.4796 - classification_loss: 0.0512
 687/1000 [===================>..........] - ETA: 11:40 - loss: 0.5305 - regression_loss: 0.4794 - classification_loss: 0.0512
 688/1000 [===================>..........] - ETA: 11:38 - loss: 0.5307 - regression_loss: 0.4795 - classification_loss: 0.0512
 689/1000 [===================>..........] - ETA: 11:36 - loss: 0.5309 - regression_loss: 0.4797 - classification_loss: 0.0512
 690/1000 [===================>..........] - ETA: 11:34 - loss: 0.5308 - regression_loss: 0.4796 - classification_loss: 0.0512
 691/1000 [===================>..........] - ETA: 11:32 - loss: 0.5307 - regression_loss: 0.4795 - classification_loss: 0.0512
 692/1000 [===================>..........] - ETA: 11:30 - loss: 0.5309 - regression_loss: 0.4796 - classification_loss: 0.0512
 693/1000 [===================>..........] - ETA: 11:28 - loss: 0.5309 - regression_loss: 0.4797 - classification_loss: 0.0512
 694/1000 [===================>..........] - ETA: 11:25 - loss: 0.5307 - regression_loss: 0.4795 - classification_loss: 0.0512
 695/1000 [===================>..........] - ETA: 11:23 - loss: 0.5301 - regression_loss: 0.4789 - classification_loss: 0.0511
 696/1000 [===================>..........] - ETA: 11:21 - loss: 0.5303 - regression_loss: 0.4791 - classification_loss: 0.0512
 697/1000 [===================>..........] - ETA: 11:18 - loss: 0.5303 - regression_loss: 0.4791 - classification_loss: 0.0511
 698/1000 [===================>..........] - ETA: 11:16 - loss: 0.5304 - regression_loss: 0.4793 - classification_loss: 0.0512
 699/1000 [===================>..........] - ETA: 11:14 - loss: 0.5299 - regression_loss: 0.4788 - classification_loss: 0.0511
 700/1000 [====================>.........] - ETA: 11:11 - loss: 0.5298 - regression_loss: 0.4787 - classification_loss: 0.0511
 701/1000 [====================>.........] - ETA: 11:09 - loss: 0.5299 - regression_loss: 0.4788 - classification_loss: 0.0511
 702/1000 [====================>.........] - ETA: 11:07 - loss: 0.5301 - regression_loss: 0.4790 - classification_loss: 0.0511
 703/1000 [====================>.........] - ETA: 11:05 - loss: 0.5298 - regression_loss: 0.4787 - classification_loss: 0.0511
 704/1000 [====================>.........] - ETA: 11:03 - loss: 0.5297 - regression_loss: 0.4786 - classification_loss: 0.0511
 705/1000 [====================>.........] - ETA: 11:00 - loss: 0.5297 - regression_loss: 0.4786 - classification_loss: 0.0511
 706/1000 [====================>.........] - ETA: 10:58 - loss: 0.5298 - regression_loss: 0.4787 - classification_loss: 0.0511
 707/1000 [====================>.........] - ETA: 10:56 - loss: 0.5295 - regression_loss: 0.4784 - classification_loss: 0.0511
 708/1000 [====================>.........] - ETA: 10:53 - loss: 0.5289 - regression_loss: 0.4779 - classification_loss: 0.0510
 709/1000 [====================>.........] - ETA: 10:51 - loss: 0.5290 - regression_loss: 0.4780 - classification_loss: 0.0510
 710/1000 [====================>.........] - ETA: 10:49 - loss: 0.5289 - regression_loss: 0.4779 - classification_loss: 0.0510
 711/1000 [====================>.........] - ETA: 10:47 - loss: 0.5291 - regression_loss: 0.4780 - classification_loss: 0.0510
 712/1000 [====================>.........] - ETA: 10:45 - loss: 0.5292 - regression_loss: 0.4782 - classification_loss: 0.0510
 713/1000 [====================>.........] - ETA: 10:43 - loss: 0.5294 - regression_loss: 0.4783 - classification_loss: 0.0511
 714/1000 [====================>.........] - ETA: 10:41 - loss: 0.5293 - regression_loss: 0.4783 - classification_loss: 0.0510
 715/1000 [====================>.........] - ETA: 10:38 - loss: 0.5287 - regression_loss: 0.4777 - classification_loss: 0.0510
 716/1000 [====================>.........] - ETA: 10:36 - loss: 0.5288 - regression_loss: 0.4778 - classification_loss: 0.0510
 717/1000 [====================>.........] - ETA: 10:34 - loss: 0.5286 - regression_loss: 0.4776 - classification_loss: 0.0510
 718/1000 [====================>.........] - ETA: 10:31 - loss: 0.5283 - regression_loss: 0.4773 - classification_loss: 0.0510
 719/1000 [====================>.........] - ETA: 10:29 - loss: 0.5285 - regression_loss: 0.4775 - classification_loss: 0.0510
 720/1000 [====================>.........] - ETA: 10:27 - loss: 0.5287 - regression_loss: 0.4777 - classification_loss: 0.0510
 721/1000 [====================>.........] - ETA: 10:25 - loss: 0.5285 - regression_loss: 0.4775 - classification_loss: 0.0510
 722/1000 [====================>.........] - ETA: 10:23 - loss: 0.5282 - regression_loss: 0.4772 - classification_loss: 0.0510
 723/1000 [====================>.........] - ETA: 10:20 - loss: 0.5284 - regression_loss: 0.4774 - classification_loss: 0.0510
 724/1000 [====================>.........] - ETA: 10:18 - loss: 0.5285 - regression_loss: 0.4775 - classification_loss: 0.0510
 725/1000 [====================>.........] - ETA: 10:16 - loss: 0.5282 - regression_loss: 0.4772 - classification_loss: 0.0509
 726/1000 [====================>.........] - ETA: 10:14 - loss: 0.5284 - regression_loss: 0.4774 - classification_loss: 0.0509
 727/1000 [====================>.........] - ETA: 10:11 - loss: 0.5283 - regression_loss: 0.4774 - classification_loss: 0.0509
 728/1000 [====================>.........] - ETA: 10:09 - loss: 0.5284 - regression_loss: 0.4774 - classification_loss: 0.0509
 729/1000 [====================>.........] - ETA: 10:07 - loss: 0.5285 - regression_loss: 0.4776 - classification_loss: 0.0509
 730/1000 [====================>.........] - ETA: 10:05 - loss: 0.5283 - regression_loss: 0.4774 - classification_loss: 0.0509
 731/1000 [====================>.........] - ETA: 10:02 - loss: 0.5277 - regression_loss: 0.4769 - classification_loss: 0.0509
 732/1000 [====================>.........] - ETA: 10:00 - loss: 0.5277 - regression_loss: 0.4769 - classification_loss: 0.0509
 733/1000 [====================>.........] - ETA: 9:58 - loss: 0.5278 - regression_loss: 0.4769 - classification_loss: 0.0509 
 734/1000 [=====================>........] - ETA: 9:56 - loss: 0.5277 - regression_loss: 0.4768 - classification_loss: 0.0509
 735/1000 [=====================>........] - ETA: 9:54 - loss: 0.5279 - regression_loss: 0.4770 - classification_loss: 0.0509
 736/1000 [=====================>........] - ETA: 9:51 - loss: 0.5274 - regression_loss: 0.4766 - classification_loss: 0.0508
 737/1000 [=====================>........] - ETA: 9:49 - loss: 0.5276 - regression_loss: 0.4767 - classification_loss: 0.0508
 738/1000 [=====================>........] - ETA: 9:47 - loss: 0.5276 - regression_loss: 0.4767 - classification_loss: 0.0508
 739/1000 [=====================>........] - ETA: 9:44 - loss: 0.5273 - regression_loss: 0.4765 - classification_loss: 0.0508
 740/1000 [=====================>........] - ETA: 9:42 - loss: 0.5277 - regression_loss: 0.4768 - classification_loss: 0.0508
 741/1000 [=====================>........] - ETA: 9:40 - loss: 0.5278 - regression_loss: 0.4769 - classification_loss: 0.0508
 742/1000 [=====================>........] - ETA: 9:38 - loss: 0.5280 - regression_loss: 0.4772 - classification_loss: 0.0508
 743/1000 [=====================>........] - ETA: 9:35 - loss: 0.5277 - regression_loss: 0.4769 - classification_loss: 0.0508
 744/1000 [=====================>........] - ETA: 9:33 - loss: 0.5278 - regression_loss: 0.4770 - classification_loss: 0.0508
 745/1000 [=====================>........] - ETA: 9:31 - loss: 0.5277 - regression_loss: 0.4769 - classification_loss: 0.0508
 746/1000 [=====================>........] - ETA: 9:29 - loss: 0.5273 - regression_loss: 0.4765 - classification_loss: 0.0508
 747/1000 [=====================>........] - ETA: 9:27 - loss: 0.5275 - regression_loss: 0.4768 - classification_loss: 0.0508
 748/1000 [=====================>........] - ETA: 9:24 - loss: 0.5275 - regression_loss: 0.4767 - classification_loss: 0.0507
 749/1000 [=====================>........] - ETA: 9:22 - loss: 0.5274 - regression_loss: 0.4766 - classification_loss: 0.0508
 750/1000 [=====================>........] - ETA: 9:20 - loss: 0.5275 - regression_loss: 0.4767 - classification_loss: 0.0508
 751/1000 [=====================>........] - ETA: 9:18 - loss: 0.5277 - regression_loss: 0.4769 - classification_loss: 0.0508
 752/1000 [=====================>........] - ETA: 9:15 - loss: 0.5273 - regression_loss: 0.4766 - classification_loss: 0.0507
 753/1000 [=====================>........] - ETA: 9:13 - loss: 0.5273 - regression_loss: 0.4766 - classification_loss: 0.0507
 754/1000 [=====================>........] - ETA: 9:11 - loss: 0.5275 - regression_loss: 0.4768 - classification_loss: 0.0507
 755/1000 [=====================>........] - ETA: 9:09 - loss: 0.5274 - regression_loss: 0.4767 - classification_loss: 0.0507
 756/1000 [=====================>........] - ETA: 9:06 - loss: 0.5268 - regression_loss: 0.4762 - classification_loss: 0.0507
 757/1000 [=====================>........] - ETA: 9:04 - loss: 0.5270 - regression_loss: 0.4763 - classification_loss: 0.0507
 758/1000 [=====================>........] - ETA: 9:02 - loss: 0.5272 - regression_loss: 0.4765 - classification_loss: 0.0507
 759/1000 [=====================>........] - ETA: 8:59 - loss: 0.5269 - regression_loss: 0.4763 - classification_loss: 0.0506
 760/1000 [=====================>........] - ETA: 8:57 - loss: 0.5272 - regression_loss: 0.4765 - classification_loss: 0.0506
 761/1000 [=====================>........] - ETA: 8:55 - loss: 0.5266 - regression_loss: 0.4760 - classification_loss: 0.0506
 762/1000 [=====================>........] - ETA: 8:52 - loss: 0.5263 - regression_loss: 0.4757 - classification_loss: 0.0506
 763/1000 [=====================>........] - ETA: 8:50 - loss: 0.5265 - regression_loss: 0.4759 - classification_loss: 0.0506
 764/1000 [=====================>........] - ETA: 8:48 - loss: 0.5266 - regression_loss: 0.4760 - classification_loss: 0.0506
 765/1000 [=====================>........] - ETA: 8:46 - loss: 0.5266 - regression_loss: 0.4761 - classification_loss: 0.0506
 766/1000 [=====================>........] - ETA: 8:44 - loss: 0.5267 - regression_loss: 0.4762 - classification_loss: 0.0506
 767/1000 [======================>.......] - ETA: 8:42 - loss: 0.5267 - regression_loss: 0.4761 - classification_loss: 0.0506
 768/1000 [======================>.......] - ETA: 8:39 - loss: 0.5262 - regression_loss: 0.4756 - classification_loss: 0.0505
 769/1000 [======================>.......] - ETA: 8:37 - loss: 0.5263 - regression_loss: 0.4757 - classification_loss: 0.0505
 770/1000 [======================>.......] - ETA: 8:35 - loss: 0.5265 - regression_loss: 0.4759 - classification_loss: 0.0505
 771/1000 [======================>.......] - ETA: 8:33 - loss: 0.5264 - regression_loss: 0.4759 - classification_loss: 0.0506
 772/1000 [======================>.......] - ETA: 8:30 - loss: 0.5263 - regression_loss: 0.4758 - classification_loss: 0.0505
 773/1000 [======================>.......] - ETA: 8:28 - loss: 0.5263 - regression_loss: 0.4758 - classification_loss: 0.0505
 774/1000 [======================>.......] - ETA: 8:26 - loss: 0.5265 - regression_loss: 0.4760 - classification_loss: 0.0505
 775/1000 [======================>.......] - ETA: 8:24 - loss: 0.5264 - regression_loss: 0.4759 - classification_loss: 0.0505
 776/1000 [======================>.......] - ETA: 8:21 - loss: 0.5262 - regression_loss: 0.4756 - classification_loss: 0.0505
 777/1000 [======================>.......] - ETA: 8:19 - loss: 0.5256 - regression_loss: 0.4752 - classification_loss: 0.0505
 778/1000 [======================>.......] - ETA: 8:17 - loss: 0.5256 - regression_loss: 0.4752 - classification_loss: 0.0505
 779/1000 [======================>.......] - ETA: 8:14 - loss: 0.5257 - regression_loss: 0.4752 - classification_loss: 0.0505
 780/1000 [======================>.......] - ETA: 8:12 - loss: 0.5258 - regression_loss: 0.4753 - classification_loss: 0.0505
 781/1000 [======================>.......] - ETA: 8:10 - loss: 0.5260 - regression_loss: 0.4755 - classification_loss: 0.0505
 782/1000 [======================>.......] - ETA: 8:08 - loss: 0.5258 - regression_loss: 0.4753 - classification_loss: 0.0505
 783/1000 [======================>.......] - ETA: 8:06 - loss: 0.5260 - regression_loss: 0.4755 - classification_loss: 0.0505
 784/1000 [======================>.......] - ETA: 8:04 - loss: 0.5260 - regression_loss: 0.4755 - classification_loss: 0.0505
 785/1000 [======================>.......] - ETA: 8:01 - loss: 0.5256 - regression_loss: 0.4751 - classification_loss: 0.0504
 786/1000 [======================>.......] - ETA: 7:59 - loss: 0.5255 - regression_loss: 0.4750 - classification_loss: 0.0504
 787/1000 [======================>.......] - ETA: 7:57 - loss: 0.5256 - regression_loss: 0.4752 - classification_loss: 0.0504
 788/1000 [======================>.......] - ETA: 7:55 - loss: 0.5256 - regression_loss: 0.4751 - classification_loss: 0.0504
 789/1000 [======================>.......] - ETA: 7:52 - loss: 0.5255 - regression_loss: 0.4750 - classification_loss: 0.0504
 790/1000 [======================>.......] - ETA: 7:50 - loss: 0.5255 - regression_loss: 0.4751 - classification_loss: 0.0504
 791/1000 [======================>.......] - ETA: 7:48 - loss: 0.5253 - regression_loss: 0.4749 - classification_loss: 0.0504
 792/1000 [======================>.......] - ETA: 7:45 - loss: 0.5248 - regression_loss: 0.4744 - classification_loss: 0.0504
 793/1000 [======================>.......] - ETA: 7:43 - loss: 0.5249 - regression_loss: 0.4746 - classification_loss: 0.0504
 794/1000 [======================>.......] - ETA: 7:41 - loss: 0.5249 - regression_loss: 0.4745 - classification_loss: 0.0504
 795/1000 [======================>.......] - ETA: 7:39 - loss: 0.5251 - regression_loss: 0.4747 - classification_loss: 0.0504
 796/1000 [======================>.......] - ETA: 7:37 - loss: 0.5250 - regression_loss: 0.4746 - classification_loss: 0.0504
 797/1000 [======================>.......] - ETA: 7:34 - loss: 0.5251 - regression_loss: 0.4747 - classification_loss: 0.0504
 798/1000 [======================>.......] - ETA: 7:32 - loss: 0.5248 - regression_loss: 0.4744 - classification_loss: 0.0504
 799/1000 [======================>.......] - ETA: 7:30 - loss: 0.5249 - regression_loss: 0.4746 - classification_loss: 0.0504
 800/1000 [=======================>......] - ETA: 7:28 - loss: 0.5249 - regression_loss: 0.4745 - classification_loss: 0.0504
 801/1000 [=======================>......] - ETA: 7:26 - loss: 0.5250 - regression_loss: 0.4746 - classification_loss: 0.0504
 802/1000 [=======================>......] - ETA: 7:23 - loss: 0.5245 - regression_loss: 0.4741 - classification_loss: 0.0503
 803/1000 [=======================>......] - ETA: 7:21 - loss: 0.5245 - regression_loss: 0.4742 - classification_loss: 0.0503
 804/1000 [=======================>......] - ETA: 7:19 - loss: 0.5239 - regression_loss: 0.4737 - classification_loss: 0.0503
 805/1000 [=======================>......] - ETA: 7:16 - loss: 0.5240 - regression_loss: 0.4737 - classification_loss: 0.0503
 806/1000 [=======================>......] - ETA: 7:14 - loss: 0.5237 - regression_loss: 0.4735 - classification_loss: 0.0502
 807/1000 [=======================>......] - ETA: 7:12 - loss: 0.5239 - regression_loss: 0.4736 - classification_loss: 0.0503
 808/1000 [=======================>......] - ETA: 7:10 - loss: 0.5238 - regression_loss: 0.4736 - classification_loss: 0.0503
 809/1000 [=======================>......] - ETA: 7:07 - loss: 0.5240 - regression_loss: 0.4737 - classification_loss: 0.0503
 810/1000 [=======================>......] - ETA: 7:05 - loss: 0.5241 - regression_loss: 0.4738 - classification_loss: 0.0503
 811/1000 [=======================>......] - ETA: 7:03 - loss: 0.5242 - regression_loss: 0.4740 - classification_loss: 0.0503
 812/1000 [=======================>......] - ETA: 7:01 - loss: 0.5242 - regression_loss: 0.4739 - classification_loss: 0.0503
 813/1000 [=======================>......] - ETA: 6:59 - loss: 0.5241 - regression_loss: 0.4739 - classification_loss: 0.0503
 814/1000 [=======================>......] - ETA: 6:56 - loss: 0.5239 - regression_loss: 0.4736 - classification_loss: 0.0502
 815/1000 [=======================>......] - ETA: 6:54 - loss: 0.5234 - regression_loss: 0.4732 - classification_loss: 0.0502
 816/1000 [=======================>......] - ETA: 6:52 - loss: 0.5234 - regression_loss: 0.4732 - classification_loss: 0.0502
 817/1000 [=======================>......] - ETA: 6:50 - loss: 0.5236 - regression_loss: 0.4734 - classification_loss: 0.0502
 818/1000 [=======================>......] - ETA: 6:47 - loss: 0.5233 - regression_loss: 0.4731 - classification_loss: 0.0502
 819/1000 [=======================>......] - ETA: 6:45 - loss: 0.5232 - regression_loss: 0.4730 - classification_loss: 0.0502
 820/1000 [=======================>......] - ETA: 6:43 - loss: 0.5232 - regression_loss: 0.4730 - classification_loss: 0.0502
 821/1000 [=======================>......] - ETA: 6:40 - loss: 0.5227 - regression_loss: 0.4725 - classification_loss: 0.0501
 822/1000 [=======================>......] - ETA: 6:38 - loss: 0.5228 - regression_loss: 0.4727 - classification_loss: 0.0501
 823/1000 [=======================>......] - ETA: 6:36 - loss: 0.5228 - regression_loss: 0.4727 - classification_loss: 0.0501
 824/1000 [=======================>......] - ETA: 6:34 - loss: 0.5230 - regression_loss: 0.4728 - classification_loss: 0.0501
 825/1000 [=======================>......] - ETA: 6:32 - loss: 0.5229 - regression_loss: 0.4728 - classification_loss: 0.0501
 826/1000 [=======================>......] - ETA: 6:29 - loss: 0.5225 - regression_loss: 0.4724 - classification_loss: 0.0501
 827/1000 [=======================>......] - ETA: 6:27 - loss: 0.5226 - regression_loss: 0.4725 - classification_loss: 0.0501
 828/1000 [=======================>......] - ETA: 6:25 - loss: 0.5228 - regression_loss: 0.4727 - classification_loss: 0.0501
 829/1000 [=======================>......] - ETA: 6:23 - loss: 0.5226 - regression_loss: 0.4725 - classification_loss: 0.0501
 830/1000 [=======================>......] - ETA: 6:20 - loss: 0.5225 - regression_loss: 0.4725 - classification_loss: 0.0501
 831/1000 [=======================>......] - ETA: 6:18 - loss: 0.5227 - regression_loss: 0.4726 - classification_loss: 0.0501
 832/1000 [=======================>......] - ETA: 6:16 - loss: 0.5226 - regression_loss: 0.4725 - classification_loss: 0.0501
 833/1000 [=======================>......] - ETA: 6:14 - loss: 0.5226 - regression_loss: 0.4725 - classification_loss: 0.0501
 834/1000 [========================>.....] - ETA: 6:11 - loss: 0.5221 - regression_loss: 0.4720 - classification_loss: 0.0500
 835/1000 [========================>.....] - ETA: 6:09 - loss: 0.5221 - regression_loss: 0.4721 - classification_loss: 0.0500
 836/1000 [========================>.....] - ETA: 6:07 - loss: 0.5218 - regression_loss: 0.4718 - classification_loss: 0.0500
 837/1000 [========================>.....] - ETA: 6:05 - loss: 0.5220 - regression_loss: 0.4719 - classification_loss: 0.0500
 838/1000 [========================>.....] - ETA: 6:02 - loss: 0.5217 - regression_loss: 0.4717 - classification_loss: 0.0500
 839/1000 [========================>.....] - ETA: 6:00 - loss: 0.5218 - regression_loss: 0.4718 - classification_loss: 0.0500
 840/1000 [========================>.....] - ETA: 5:58 - loss: 0.5219 - regression_loss: 0.4719 - classification_loss: 0.0500
 841/1000 [========================>.....] - ETA: 5:56 - loss: 0.5218 - regression_loss: 0.4718 - classification_loss: 0.0500
 842/1000 [========================>.....] - ETA: 5:54 - loss: 0.5213 - regression_loss: 0.4713 - classification_loss: 0.0500
 843/1000 [========================>.....] - ETA: 5:51 - loss: 0.5213 - regression_loss: 0.4713 - classification_loss: 0.0500
 844/1000 [========================>.....] - ETA: 5:49 - loss: 0.5213 - regression_loss: 0.4713 - classification_loss: 0.0500
 845/1000 [========================>.....] - ETA: 5:47 - loss: 0.5208 - regression_loss: 0.4709 - classification_loss: 0.0499
 846/1000 [========================>.....] - ETA: 5:44 - loss: 0.5208 - regression_loss: 0.4709 - classification_loss: 0.0499
 847/1000 [========================>.....] - ETA: 5:42 - loss: 0.5207 - regression_loss: 0.4708 - classification_loss: 0.0499
 848/1000 [========================>.....] - ETA: 5:40 - loss: 0.5209 - regression_loss: 0.4709 - classification_loss: 0.0499
 849/1000 [========================>.....] - ETA: 5:38 - loss: 0.5209 - regression_loss: 0.4710 - classification_loss: 0.0499
 850/1000 [========================>.....] - ETA: 5:36 - loss: 0.5208 - regression_loss: 0.4709 - classification_loss: 0.0499
 851/1000 [========================>.....] - ETA: 5:33 - loss: 0.5210 - regression_loss: 0.4711 - classification_loss: 0.0499
 852/1000 [========================>.....] - ETA: 5:31 - loss: 0.5211 - regression_loss: 0.4712 - classification_loss: 0.0499
 853/1000 [========================>.....] - ETA: 5:29 - loss: 0.5213 - regression_loss: 0.4713 - classification_loss: 0.0499
 854/1000 [========================>.....] - ETA: 5:27 - loss: 0.5213 - regression_loss: 0.4714 - classification_loss: 0.0499
 855/1000 [========================>.....] - ETA: 5:25 - loss: 0.5210 - regression_loss: 0.4711 - classification_loss: 0.0499
 856/1000 [========================>.....] - ETA: 5:22 - loss: 0.5208 - regression_loss: 0.4709 - classification_loss: 0.0499
 857/1000 [========================>.....] - ETA: 5:20 - loss: 0.5208 - regression_loss: 0.4709 - classification_loss: 0.0499
 858/1000 [========================>.....] - ETA: 5:18 - loss: 0.5207 - regression_loss: 0.4709 - classification_loss: 0.0499
 859/1000 [========================>.....] - ETA: 5:15 - loss: 0.5205 - regression_loss: 0.4706 - classification_loss: 0.0499
 860/1000 [========================>.....] - ETA: 5:13 - loss: 0.5205 - regression_loss: 0.4706 - classification_loss: 0.0498
 861/1000 [========================>.....] - ETA: 5:11 - loss: 0.5204 - regression_loss: 0.4706 - classification_loss: 0.0498
 862/1000 [========================>.....] - ETA: 5:09 - loss: 0.5200 - regression_loss: 0.4702 - classification_loss: 0.0498
 863/1000 [========================>.....] - ETA: 5:06 - loss: 0.5200 - regression_loss: 0.4702 - classification_loss: 0.0498
 864/1000 [========================>.....] - ETA: 5:04 - loss: 0.5202 - regression_loss: 0.4704 - classification_loss: 0.0498
 865/1000 [========================>.....] - ETA: 5:02 - loss: 0.5203 - regression_loss: 0.4705 - classification_loss: 0.0498
 866/1000 [========================>.....] - ETA: 5:00 - loss: 0.5203 - regression_loss: 0.4705 - classification_loss: 0.0498
 867/1000 [=========================>....] - ETA: 4:57 - loss: 0.5201 - regression_loss: 0.4703 - classification_loss: 0.0498
 868/1000 [=========================>....] - ETA: 4:55 - loss: 0.5201 - regression_loss: 0.4703 - classification_loss: 0.0498
 869/1000 [=========================>....] - ETA: 4:53 - loss: 0.5202 - regression_loss: 0.4704 - classification_loss: 0.0498
 870/1000 [=========================>....] - ETA: 4:51 - loss: 0.5204 - regression_loss: 0.4706 - classification_loss: 0.0498
 871/1000 [=========================>....] - ETA: 4:49 - loss: 0.5199 - regression_loss: 0.4701 - classification_loss: 0.0498
 872/1000 [=========================>....] - ETA: 4:46 - loss: 0.5199 - regression_loss: 0.4701 - classification_loss: 0.0498
 873/1000 [=========================>....] - ETA: 4:44 - loss: 0.5199 - regression_loss: 0.4702 - classification_loss: 0.0498
 874/1000 [=========================>....] - ETA: 4:42 - loss: 0.5195 - regression_loss: 0.4698 - classification_loss: 0.0497
 875/1000 [=========================>....] - ETA: 4:40 - loss: 0.5196 - regression_loss: 0.4699 - classification_loss: 0.0497
 876/1000 [=========================>....] - ETA: 4:37 - loss: 0.5194 - regression_loss: 0.4697 - classification_loss: 0.0497
 877/1000 [=========================>....] - ETA: 4:35 - loss: 0.5196 - regression_loss: 0.4698 - classification_loss: 0.0497
 878/1000 [=========================>....] - ETA: 4:33 - loss: 0.5195 - regression_loss: 0.4698 - classification_loss: 0.0497
 879/1000 [=========================>....] - ETA: 4:31 - loss: 0.5195 - regression_loss: 0.4697 - classification_loss: 0.0497
 880/1000 [=========================>....] - ETA: 4:28 - loss: 0.5195 - regression_loss: 0.4698 - classification_loss: 0.0497
 881/1000 [=========================>....] - ETA: 4:26 - loss: 0.5195 - regression_loss: 0.4698 - classification_loss: 0.0497
 882/1000 [=========================>....] - ETA: 4:24 - loss: 0.5196 - regression_loss: 0.4699 - classification_loss: 0.0497
 883/1000 [=========================>....] - ETA: 4:22 - loss: 0.5192 - regression_loss: 0.4695 - classification_loss: 0.0497
 884/1000 [=========================>....] - ETA: 4:19 - loss: 0.5189 - regression_loss: 0.4693 - classification_loss: 0.0496
 885/1000 [=========================>....] - ETA: 4:17 - loss: 0.5191 - regression_loss: 0.4694 - classification_loss: 0.0497
 886/1000 [=========================>....] - ETA: 4:15 - loss: 0.5190 - regression_loss: 0.4694 - classification_loss: 0.0497
 887/1000 [=========================>....] - ETA: 4:13 - loss: 0.5187 - regression_loss: 0.4691 - classification_loss: 0.0496
 888/1000 [=========================>....] - ETA: 4:10 - loss: 0.5182 - regression_loss: 0.4686 - classification_loss: 0.0496
 889/1000 [=========================>....] - ETA: 4:08 - loss: 0.5184 - regression_loss: 0.4688 - classification_loss: 0.0496
 890/1000 [=========================>....] - ETA: 4:06 - loss: 0.5183 - regression_loss: 0.4687 - classification_loss: 0.0496
 891/1000 [=========================>....] - ETA: 4:04 - loss: 0.5183 - regression_loss: 0.4687 - classification_loss: 0.0496
 892/1000 [=========================>....] - ETA: 4:01 - loss: 0.5184 - regression_loss: 0.4688 - classification_loss: 0.0496
 893/1000 [=========================>....] - ETA: 3:59 - loss: 0.5184 - regression_loss: 0.4688 - classification_loss: 0.0496
 894/1000 [=========================>....] - ETA: 3:57 - loss: 0.5180 - regression_loss: 0.4684 - classification_loss: 0.0496
 895/1000 [=========================>....] - ETA: 3:55 - loss: 0.5180 - regression_loss: 0.4684 - classification_loss: 0.0496
 896/1000 [=========================>....] - ETA: 3:52 - loss: 0.5180 - regression_loss: 0.4684 - classification_loss: 0.0495
 897/1000 [=========================>....] - ETA: 3:50 - loss: 0.5179 - regression_loss: 0.4683 - classification_loss: 0.0495
 898/1000 [=========================>....] - ETA: 3:48 - loss: 0.5177 - regression_loss: 0.4682 - classification_loss: 0.0495
 899/1000 [=========================>....] - ETA: 3:46 - loss: 0.5178 - regression_loss: 0.4683 - classification_loss: 0.0495
 900/1000 [==========================>...] - ETA: 3:44 - loss: 0.5179 - regression_loss: 0.4684 - classification_loss: 0.0495
 901/1000 [==========================>...] - ETA: 3:41 - loss: 0.5179 - regression_loss: 0.4684 - classification_loss: 0.0495
 902/1000 [==========================>...] - ETA: 3:39 - loss: 0.5180 - regression_loss: 0.4685 - classification_loss: 0.0495
 903/1000 [==========================>...] - ETA: 3:37 - loss: 0.5181 - regression_loss: 0.4685 - classification_loss: 0.0495
 904/1000 [==========================>...] - ETA: 3:35 - loss: 0.5182 - regression_loss: 0.4687 - classification_loss: 0.0495
 905/1000 [==========================>...] - ETA: 3:33 - loss: 0.5181 - regression_loss: 0.4686 - classification_loss: 0.0495
 906/1000 [==========================>...] - ETA: 3:30 - loss: 0.5179 - regression_loss: 0.4684 - classification_loss: 0.0495
 907/1000 [==========================>...] - ETA: 3:28 - loss: 0.5174 - regression_loss: 0.4680 - classification_loss: 0.0495
 908/1000 [==========================>...] - ETA: 3:26 - loss: 0.5176 - regression_loss: 0.4682 - classification_loss: 0.0495
 909/1000 [==========================>...] - ETA: 3:23 - loss: 0.5179 - regression_loss: 0.4685 - classification_loss: 0.0495
 910/1000 [==========================>...] - ETA: 3:21 - loss: 0.5175 - regression_loss: 0.4680 - classification_loss: 0.0494
 911/1000 [==========================>...] - ETA: 3:19 - loss: 0.5177 - regression_loss: 0.4682 - classification_loss: 0.0495
 912/1000 [==========================>...] - ETA: 3:17 - loss: 0.5177 - regression_loss: 0.4683 - classification_loss: 0.0495
 913/1000 [==========================>...] - ETA: 3:14 - loss: 0.5175 - regression_loss: 0.4680 - classification_loss: 0.0494
 914/1000 [==========================>...] - ETA: 3:12 - loss: 0.5175 - regression_loss: 0.4680 - classification_loss: 0.0494
 915/1000 [==========================>...] - ETA: 3:10 - loss: 0.5177 - regression_loss: 0.4683 - classification_loss: 0.0494
 916/1000 [==========================>...] - ETA: 3:08 - loss: 0.5174 - regression_loss: 0.4680 - classification_loss: 0.0494
 917/1000 [==========================>...] - ETA: 3:05 - loss: 0.5173 - regression_loss: 0.4679 - classification_loss: 0.0494
 918/1000 [==========================>...] - ETA: 3:03 - loss: 0.5171 - regression_loss: 0.4677 - classification_loss: 0.0494
 919/1000 [==========================>...] - ETA: 3:01 - loss: 0.5171 - regression_loss: 0.4677 - classification_loss: 0.0494
 920/1000 [==========================>...] - ETA: 2:59 - loss: 0.5171 - regression_loss: 0.4677 - classification_loss: 0.0494
 921/1000 [==========================>...] - ETA: 2:57 - loss: 0.5173 - regression_loss: 0.4679 - classification_loss: 0.0494
 922/1000 [==========================>...] - ETA: 2:54 - loss: 0.5170 - regression_loss: 0.4677 - classification_loss: 0.0494
 923/1000 [==========================>...] - ETA: 2:52 - loss: 0.5171 - regression_loss: 0.4678 - classification_loss: 0.0494
 924/1000 [==========================>...] - ETA: 2:50 - loss: 0.5167 - regression_loss: 0.4673 - classification_loss: 0.0493
 925/1000 [==========================>...] - ETA: 2:48 - loss: 0.5168 - regression_loss: 0.4675 - classification_loss: 0.0494
 926/1000 [==========================>...] - ETA: 2:45 - loss: 0.5168 - regression_loss: 0.4675 - classification_loss: 0.0494
 927/1000 [==========================>...] - ETA: 2:43 - loss: 0.5167 - regression_loss: 0.4674 - classification_loss: 0.0494
 928/1000 [==========================>...] - ETA: 2:41 - loss: 0.5167 - regression_loss: 0.4674 - classification_loss: 0.0493
 929/1000 [==========================>...] - ETA: 2:39 - loss: 0.5168 - regression_loss: 0.4675 - classification_loss: 0.0493
 930/1000 [==========================>...] - ETA: 2:36 - loss: 0.5167 - regression_loss: 0.4674 - classification_loss: 0.0493
 931/1000 [==========================>...] - ETA: 2:34 - loss: 0.5165 - regression_loss: 0.4672 - classification_loss: 0.0493
 932/1000 [==========================>...] - ETA: 2:32 - loss: 0.5161 - regression_loss: 0.4668 - classification_loss: 0.0493
 933/1000 [==========================>...] - ETA: 2:30 - loss: 0.5163 - regression_loss: 0.4670 - classification_loss: 0.0493
 934/1000 [===========================>..] - ETA: 2:27 - loss: 0.5163 - regression_loss: 0.4670 - classification_loss: 0.0493
 935/1000 [===========================>..] - ETA: 2:25 - loss: 0.5163 - regression_loss: 0.4670 - classification_loss: 0.0493
 936/1000 [===========================>..] - ETA: 2:23 - loss: 0.5163 - regression_loss: 0.4670 - classification_loss: 0.0493
 937/1000 [===========================>..] - ETA: 2:21 - loss: 0.5161 - regression_loss: 0.4668 - classification_loss: 0.0493
 938/1000 [===========================>..] - ETA: 2:18 - loss: 0.5162 - regression_loss: 0.4669 - classification_loss: 0.0493
 939/1000 [===========================>..] - ETA: 2:16 - loss: 0.5163 - regression_loss: 0.4671 - classification_loss: 0.0493
 940/1000 [===========================>..] - ETA: 2:14 - loss: 0.5163 - regression_loss: 0.4670 - classification_loss: 0.0493
 941/1000 [===========================>..] - ETA: 2:12 - loss: 0.5158 - regression_loss: 0.4666 - classification_loss: 0.0492
 942/1000 [===========================>..] - ETA: 2:09 - loss: 0.5158 - regression_loss: 0.4665 - classification_loss: 0.0492
 943/1000 [===========================>..] - ETA: 2:07 - loss: 0.5157 - regression_loss: 0.4665 - classification_loss: 0.0492
 944/1000 [===========================>..] - ETA: 2:05 - loss: 0.5153 - regression_loss: 0.4661 - classification_loss: 0.0492
 945/1000 [===========================>..] - ETA: 2:03 - loss: 0.5154 - regression_loss: 0.4662 - classification_loss: 0.0492
 946/1000 [===========================>..] - ETA: 2:01 - loss: 0.5154 - regression_loss: 0.4662 - classification_loss: 0.0492
 947/1000 [===========================>..] - ETA: 1:58 - loss: 0.5152 - regression_loss: 0.4660 - classification_loss: 0.0492
 948/1000 [===========================>..] - ETA: 1:56 - loss: 0.5153 - regression_loss: 0.4661 - classification_loss: 0.0492
 949/1000 [===========================>..] - ETA: 1:54 - loss: 0.5154 - regression_loss: 0.4662 - classification_loss: 0.0492
 950/1000 [===========================>..] - ETA: 1:52 - loss: 0.5154 - regression_loss: 0.4662 - classification_loss: 0.0492
 951/1000 [===========================>..] - ETA: 1:49 - loss: 0.5156 - regression_loss: 0.4664 - classification_loss: 0.0492
 952/1000 [===========================>..] - ETA: 1:47 - loss: 0.5157 - regression_loss: 0.4665 - classification_loss: 0.0492
 953/1000 [===========================>..] - ETA: 1:45 - loss: 0.5156 - regression_loss: 0.4664 - classification_loss: 0.0492
 954/1000 [===========================>..] - ETA: 1:43 - loss: 0.5154 - regression_loss: 0.4662 - classification_loss: 0.0492
 955/1000 [===========================>..] - ETA: 1:40 - loss: 0.5155 - regression_loss: 0.4663 - classification_loss: 0.0492
 956/1000 [===========================>..] - ETA: 1:38 - loss: 0.5151 - regression_loss: 0.4660 - classification_loss: 0.0491
 957/1000 [===========================>..] - ETA: 1:36 - loss: 0.5153 - regression_loss: 0.4662 - classification_loss: 0.0491
 958/1000 [===========================>..] - ETA: 1:34 - loss: 0.5156 - regression_loss: 0.4664 - classification_loss: 0.0491
 959/1000 [===========================>..] - ETA: 1:31 - loss: 0.5155 - regression_loss: 0.4663 - classification_loss: 0.0491
 960/1000 [===========================>..] - ETA: 1:29 - loss: 0.5155 - regression_loss: 0.4663 - classification_loss: 0.0492
 961/1000 [===========================>..] - ETA: 1:27 - loss: 0.5151 - regression_loss: 0.4660 - classification_loss: 0.0491
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.5150 - regression_loss: 0.4659 - classification_loss: 0.0491
 963/1000 [===========================>..] - ETA: 1:22 - loss: 0.5151 - regression_loss: 0.4660 - classification_loss: 0.0491
 964/1000 [===========================>..] - ETA: 1:20 - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 965/1000 [===========================>..] - ETA: 1:18 - loss: 0.5146 - regression_loss: 0.4655 - classification_loss: 0.0491
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.5145 - regression_loss: 0.4654 - classification_loss: 0.0491
 967/1000 [============================>.] - ETA: 1:13 - loss: 0.5145 - regression_loss: 0.4654 - classification_loss: 0.0491
 968/1000 [============================>.] - ETA: 1:11 - loss: 0.5146 - regression_loss: 0.4656 - classification_loss: 0.0491
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.5148 - regression_loss: 0.4657 - classification_loss: 0.0491
 971/1000 [============================>.] - ETA: 1:04 - loss: 0.5146 - regression_loss: 0.4655 - classification_loss: 0.0491
 972/1000 [============================>.] - ETA: 1:02 - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 974/1000 [============================>.] - ETA: 58s - loss: 0.5148 - regression_loss: 0.4657 - classification_loss: 0.0491 
 975/1000 [============================>.] - ETA: 56s - loss: 0.5150 - regression_loss: 0.4659 - classification_loss: 0.0491
 976/1000 [============================>.] - ETA: 53s - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 977/1000 [============================>.] - ETA: 51s - loss: 0.5146 - regression_loss: 0.4656 - classification_loss: 0.0491
 978/1000 [============================>.] - ETA: 49s - loss: 0.5147 - regression_loss: 0.4657 - classification_loss: 0.0491
 979/1000 [============================>.] - ETA: 47s - loss: 0.5145 - regression_loss: 0.4655 - classification_loss: 0.0491
 980/1000 [============================>.] - ETA: 44s - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 981/1000 [============================>.] - ETA: 42s - loss: 0.5147 - regression_loss: 0.4656 - classification_loss: 0.0491
 982/1000 [============================>.] - ETA: 40s - loss: 0.5143 - regression_loss: 0.4652 - classification_loss: 0.0490
 983/1000 [============================>.] - ETA: 38s - loss: 0.5142 - regression_loss: 0.4652 - classification_loss: 0.0490
 984/1000 [============================>.] - ETA: 35s - loss: 0.5144 - regression_loss: 0.4654 - classification_loss: 0.0490
 985/1000 [============================>.] - ETA: 33s - loss: 0.5145 - regression_loss: 0.4654 - classification_loss: 0.0490
 986/1000 [============================>.] - ETA: 31s - loss: 0.5142 - regression_loss: 0.4652 - classification_loss: 0.0490
 987/1000 [============================>.] - ETA: 29s - loss: 0.5138 - regression_loss: 0.4649 - classification_loss: 0.0490
 988/1000 [============================>.] - ETA: 26s - loss: 0.5138 - regression_loss: 0.4648 - classification_loss: 0.0490
 989/1000 [============================>.] - ETA: 24s - loss: 0.5139 - regression_loss: 0.4649 - classification_loss: 0.0490
 990/1000 [============================>.] - ETA: 22s - loss: 0.5139 - regression_loss: 0.4649 - classification_loss: 0.0490
 991/1000 [============================>.] - ETA: 20s - loss: 0.5140 - regression_loss: 0.4650 - classification_loss: 0.0490
 992/1000 [============================>.] - ETA: 17s - loss: 0.5141 - regression_loss: 0.4651 - classification_loss: 0.0490
 993/1000 [============================>.] - ETA: 15s - loss: 0.5141 - regression_loss: 0.4652 - classification_loss: 0.0490
 994/1000 [============================>.] - ETA: 13s - loss: 0.5141 - regression_loss: 0.4651 - classification_loss: 0.0490
 995/1000 [============================>.] - ETA: 11s - loss: 0.5142 - regression_loss: 0.4652 - classification_loss: 0.0490
 996/1000 [============================>.] - ETA: 8s - loss: 0.5140 - regression_loss: 0.4651 - classification_loss: 0.0490 
 997/1000 [============================>.] - ETA: 6s - loss: 0.5136 - regression_loss: 0.4647 - classification_loss: 0.0489
 998/1000 [============================>.] - ETA: 4s - loss: 0.5136 - regression_loss: 0.4646 - classification_loss: 0.0489
 999/1000 [============================>.] - ETA: 2s - loss: 0.5136 - regression_loss: 0.4647 - classification_loss: 0.0489
1000/1000 [==============================] - 2242s 2s/step - loss: 0.5137 - regression_loss: 0.4648 - classification_loss: 0.0489

Epoch 00005: saving model to ./snapshots/resnet50_csv_05.h5
Epoch 6/10

   1/1000 [..............................] - ETA: 12:35 - loss: 0.1099 - regression_loss: 0.0963 - classification_loss: 0.0136
   2/1000 [..............................] - ETA: 26:25 - loss: 0.3115 - regression_loss: 0.2797 - classification_loss: 0.0318
   3/1000 [..............................] - ETA: 31:26 - loss: 0.4061 - regression_loss: 0.3686 - classification_loss: 0.0374
   4/1000 [..............................] - ETA: 34:45 - loss: 0.4652 - regression_loss: 0.4234 - classification_loss: 0.0418
   5/1000 [..............................] - ETA: 33:29 - loss: 0.4353 - regression_loss: 0.3952 - classification_loss: 0.0401
   6/1000 [..............................] - ETA: 35:18 - loss: 0.4673 - regression_loss: 0.4250 - classification_loss: 0.0423
   7/1000 [..............................] - ETA: 34:18 - loss: 0.4380 - regression_loss: 0.3973 - classification_loss: 0.0407
   8/1000 [..............................] - ETA: 35:18 - loss: 0.4635 - regression_loss: 0.4214 - classification_loss: 0.0420
   9/1000 [..............................] - ETA: 35:56 - loss: 0.4743 - regression_loss: 0.4315 - classification_loss: 0.0428
  10/1000 [..............................] - ETA: 36:17 - loss: 0.4698 - regression_loss: 0.4264 - classification_loss: 0.0434
  11/1000 [..............................] - ETA: 36:10 - loss: 0.4709 - regression_loss: 0.4275 - classification_loss: 0.0433
  12/1000 [..............................] - ETA: 35:16 - loss: 0.4396 - regression_loss: 0.3988 - classification_loss: 0.0408
  13/1000 [..............................] - ETA: 35:42 - loss: 0.4485 - regression_loss: 0.4071 - classification_loss: 0.0414
  14/1000 [..............................] - ETA: 36:11 - loss: 0.4613 - regression_loss: 0.4193 - classification_loss: 0.0421
  15/1000 [..............................] - ETA: 36:05 - loss: 0.4623 - regression_loss: 0.4201 - classification_loss: 0.0422
  16/1000 [..............................] - ETA: 35:34 - loss: 0.4637 - regression_loss: 0.4215 - classification_loss: 0.0422
  17/1000 [..............................] - ETA: 34:58 - loss: 0.4407 - regression_loss: 0.4002 - classification_loss: 0.0405
  18/1000 [..............................] - ETA: 35:12 - loss: 0.4394 - regression_loss: 0.3985 - classification_loss: 0.0410
  19/1000 [..............................] - ETA: 35:39 - loss: 0.4485 - regression_loss: 0.4068 - classification_loss: 0.0417
  20/1000 [..............................] - ETA: 35:48 - loss: 0.4474 - regression_loss: 0.4054 - classification_loss: 0.0420
  21/1000 [..............................] - ETA: 35:24 - loss: 0.4409 - regression_loss: 0.3993 - classification_loss: 0.0416
  22/1000 [..............................] - ETA: 34:55 - loss: 0.4286 - regression_loss: 0.3881 - classification_loss: 0.0405
  23/1000 [..............................] - ETA: 35:17 - loss: 0.4374 - regression_loss: 0.3962 - classification_loss: 0.0411
  24/1000 [..............................] - ETA: 35:14 - loss: 0.4399 - regression_loss: 0.3986 - classification_loss: 0.0413
  25/1000 [..............................] - ETA: 35:29 - loss: 0.4470 - regression_loss: 0.4052 - classification_loss: 0.0418
  26/1000 [..............................] - ETA: 35:39 - loss: 0.4506 - regression_loss: 0.4085 - classification_loss: 0.0421
  27/1000 [..............................] - ETA: 35:14 - loss: 0.4372 - regression_loss: 0.3961 - classification_loss: 0.0411
  28/1000 [..............................] - ETA: 35:21 - loss: 0.4367 - regression_loss: 0.3953 - classification_loss: 0.0414
  29/1000 [..............................] - ETA: 35:03 - loss: 0.4325 - regression_loss: 0.3914 - classification_loss: 0.0412
  30/1000 [..............................] - ETA: 35:01 - loss: 0.4356 - regression_loss: 0.3942 - classification_loss: 0.0415
  31/1000 [..............................] - ETA: 35:15 - loss: 0.4418 - regression_loss: 0.4000 - classification_loss: 0.0419
  32/1000 [..............................] - ETA: 35:23 - loss: 0.4450 - regression_loss: 0.4029 - classification_loss: 0.0421
  33/1000 [..............................] - ETA: 35:33 - loss: 0.4502 - regression_loss: 0.4078 - classification_loss: 0.0424
  34/1000 [>.............................] - ETA: 35:39 - loss: 0.4529 - regression_loss: 0.4103 - classification_loss: 0.0426
  35/1000 [>.............................] - ETA: 35:43 - loss: 0.4523 - regression_loss: 0.4095 - classification_loss: 0.0428
  36/1000 [>.............................] - ETA: 35:23 - loss: 0.4448 - regression_loss: 0.4028 - classification_loss: 0.0420
  37/1000 [>.............................] - ETA: 35:36 - loss: 0.4496 - regression_loss: 0.4073 - classification_loss: 0.0423
  38/1000 [>.............................] - ETA: 35:43 - loss: 0.4540 - regression_loss: 0.4113 - classification_loss: 0.0426
  39/1000 [>.............................] - ETA: 35:39 - loss: 0.4546 - regression_loss: 0.4120 - classification_loss: 0.0427
  40/1000 [>.............................] - ETA: 35:24 - loss: 0.4515 - regression_loss: 0.4091 - classification_loss: 0.0425
  41/1000 [>.............................] - ETA: 35:20 - loss: 0.4524 - regression_loss: 0.4100 - classification_loss: 0.0425
  42/1000 [>.............................] - ETA: 35:29 - loss: 0.4566 - regression_loss: 0.4138 - classification_loss: 0.0427
  43/1000 [>.............................] - ETA: 35:35 - loss: 0.4601 - regression_loss: 0.4172 - classification_loss: 0.0429
  44/1000 [>.............................] - ETA: 35:18 - loss: 0.4514 - regression_loss: 0.4092 - classification_loss: 0.0422
  45/1000 [>.............................] - ETA: 35:05 - loss: 0.4495 - regression_loss: 0.4074 - classification_loss: 0.0421
  46/1000 [>.............................] - ETA: 35:10 - loss: 0.4525 - regression_loss: 0.4102 - classification_loss: 0.0422
  47/1000 [>.............................] - ETA: 35:12 - loss: 0.4524 - regression_loss: 0.4100 - classification_loss: 0.0423
  48/1000 [>.............................] - ETA: 35:17 - loss: 0.4562 - regression_loss: 0.4136 - classification_loss: 0.0425
  49/1000 [>.............................] - ETA: 35:20 - loss: 0.4577 - regression_loss: 0.4151 - classification_loss: 0.0426
  50/1000 [>.............................] - ETA: 35:06 - loss: 0.4510 - regression_loss: 0.4090 - classification_loss: 0.0420
  51/1000 [>.............................] - ETA: 35:03 - loss: 0.4520 - regression_loss: 0.4099 - classification_loss: 0.0421
  52/1000 [>.............................] - ETA: 34:51 - loss: 0.4494 - regression_loss: 0.4075 - classification_loss: 0.0419
  53/1000 [>.............................] - ETA: 34:53 - loss: 0.4505 - regression_loss: 0.4083 - classification_loss: 0.0421
  54/1000 [>.............................] - ETA: 35:01 - loss: 0.4539 - regression_loss: 0.4116 - classification_loss: 0.0423
  55/1000 [>.............................] - ETA: 35:08 - loss: 0.4569 - regression_loss: 0.4144 - classification_loss: 0.0425
  56/1000 [>.............................] - ETA: 34:57 - loss: 0.4538 - regression_loss: 0.4115 - classification_loss: 0.0424
  57/1000 [>.............................] - ETA: 34:58 - loss: 0.4537 - regression_loss: 0.4111 - classification_loss: 0.0426
  58/1000 [>.............................] - ETA: 35:02 - loss: 0.4568 - regression_loss: 0.4140 - classification_loss: 0.0428
  59/1000 [>.............................] - ETA: 34:59 - loss: 0.4577 - regression_loss: 0.4148 - classification_loss: 0.0428
  60/1000 [>.............................] - ETA: 34:46 - loss: 0.4519 - regression_loss: 0.4095 - classification_loss: 0.0423
  61/1000 [>.............................] - ETA: 34:48 - loss: 0.4533 - regression_loss: 0.4108 - classification_loss: 0.0424
  62/1000 [>.............................] - ETA: 34:38 - loss: 0.4513 - regression_loss: 0.4090 - classification_loss: 0.0423
  63/1000 [>.............................] - ETA: 34:35 - loss: 0.4519 - regression_loss: 0.4096 - classification_loss: 0.0423
  64/1000 [>.............................] - ETA: 34:24 - loss: 0.4464 - regression_loss: 0.4046 - classification_loss: 0.0418
  65/1000 [>.............................] - ETA: 34:29 - loss: 0.4488 - regression_loss: 0.4068 - classification_loss: 0.0420
  66/1000 [>.............................] - ETA: 34:32 - loss: 0.4510 - regression_loss: 0.4089 - classification_loss: 0.0421
  67/1000 [=>............................] - ETA: 34:33 - loss: 0.4521 - regression_loss: 0.4100 - classification_loss: 0.0422
  68/1000 [=>............................] - ETA: 34:34 - loss: 0.4515 - regression_loss: 0.4093 - classification_loss: 0.0422
  69/1000 [=>............................] - ETA: 34:30 - loss: 0.4520 - regression_loss: 0.4098 - classification_loss: 0.0423
  70/1000 [=>............................] - ETA: 34:33 - loss: 0.4540 - regression_loss: 0.4116 - classification_loss: 0.0424
  71/1000 [=>............................] - ETA: 34:22 - loss: 0.4493 - regression_loss: 0.4073 - classification_loss: 0.0419
  72/1000 [=>............................] - ETA: 34:26 - loss: 0.4514 - regression_loss: 0.4093 - classification_loss: 0.0421
  73/1000 [=>............................] - ETA: 34:27 - loss: 0.4528 - regression_loss: 0.4106 - classification_loss: 0.0422
  74/1000 [=>............................] - ETA: 34:18 - loss: 0.4503 - regression_loss: 0.4083 - classification_loss: 0.0420
  75/1000 [=>............................] - ETA: 34:19 - loss: 0.4500 - regression_loss: 0.4079 - classification_loss: 0.0421
  76/1000 [=>............................] - ETA: 34:20 - loss: 0.4510 - regression_loss: 0.4089 - classification_loss: 0.0421
  77/1000 [=>............................] - ETA: 34:20 - loss: 0.4506 - regression_loss: 0.4084 - classification_loss: 0.0422
  78/1000 [=>............................] - ETA: 34:22 - loss: 0.4525 - regression_loss: 0.4102 - classification_loss: 0.0423
  79/1000 [=>............................] - ETA: 34:18 - loss: 0.4526 - regression_loss: 0.4103 - classification_loss: 0.0423
  80/1000 [=>............................] - ETA: 34:08 - loss: 0.4483 - regression_loss: 0.4064 - classification_loss: 0.0419
  81/1000 [=>............................] - ETA: 34:11 - loss: 0.4505 - regression_loss: 0.4085 - classification_loss: 0.0420
  82/1000 [=>............................] - ETA: 34:03 - loss: 0.4486 - regression_loss: 0.4067 - classification_loss: 0.0419
  83/1000 [=>............................] - ETA: 34:03 - loss: 0.4491 - regression_loss: 0.4071 - classification_loss: 0.0420
  84/1000 [=>............................] - ETA: 33:55 - loss: 0.4468 - regression_loss: 0.4049 - classification_loss: 0.0419
  85/1000 [=>............................] - ETA: 33:52 - loss: 0.4475 - regression_loss: 0.4057 - classification_loss: 0.0419
  86/1000 [=>............................] - ETA: 33:55 - loss: 0.4495 - regression_loss: 0.4075 - classification_loss: 0.0420
  87/1000 [=>............................] - ETA: 33:56 - loss: 0.4503 - regression_loss: 0.4082 - classification_loss: 0.0421
  88/1000 [=>............................] - ETA: 33:47 - loss: 0.4461 - regression_loss: 0.4044 - classification_loss: 0.0417
  89/1000 [=>............................] - ETA: 33:48 - loss: 0.4479 - regression_loss: 0.4061 - classification_loss: 0.0418
  90/1000 [=>............................] - ETA: 33:39 - loss: 0.4439 - regression_loss: 0.4024 - classification_loss: 0.0415
  91/1000 [=>............................] - ETA: 33:40 - loss: 0.4454 - regression_loss: 0.4038 - classification_loss: 0.0416
  92/1000 [=>............................] - ETA: 33:41 - loss: 0.4472 - regression_loss: 0.4055 - classification_loss: 0.0417
  93/1000 [=>............................] - ETA: 33:41 - loss: 0.4468 - regression_loss: 0.4051 - classification_loss: 0.0417
  94/1000 [=>............................] - ETA: 33:34 - loss: 0.4454 - regression_loss: 0.4038 - classification_loss: 0.0416
  95/1000 [=>............................] - ETA: 33:36 - loss: 0.4473 - regression_loss: 0.4056 - classification_loss: 0.0417
  96/1000 [=>............................] - ETA: 33:34 - loss: 0.4480 - regression_loss: 0.4063 - classification_loss: 0.0417
  97/1000 [=>............................] - ETA: 33:35 - loss: 0.4499 - regression_loss: 0.4081 - classification_loss: 0.0418
  98/1000 [=>............................] - ETA: 33:26 - loss: 0.4468 - regression_loss: 0.4053 - classification_loss: 0.0415
  99/1000 [=>............................] - ETA: 33:19 - loss: 0.4450 - regression_loss: 0.4036 - classification_loss: 0.0414
 100/1000 [==>...........................] - ETA: 33:19 - loss: 0.4449 - regression_loss: 0.4034 - classification_loss: 0.0415
 101/1000 [==>...........................] - ETA: 33:19 - loss: 0.4458 - regression_loss: 0.4043 - classification_loss: 0.0416
 102/1000 [==>...........................] - ETA: 33:21 - loss: 0.4474 - regression_loss: 0.4058 - classification_loss: 0.0417
 103/1000 [==>...........................] - ETA: 33:19 - loss: 0.4477 - regression_loss: 0.4060 - classification_loss: 0.0417
 104/1000 [==>...........................] - ETA: 33:10 - loss: 0.4442 - regression_loss: 0.4028 - classification_loss: 0.0414
 105/1000 [==>...........................] - ETA: 33:11 - loss: 0.4458 - regression_loss: 0.4043 - classification_loss: 0.0415
 106/1000 [==>...........................] - ETA: 33:08 - loss: 0.4461 - regression_loss: 0.4046 - classification_loss: 0.0415
 107/1000 [==>...........................] - ETA: 33:08 - loss: 0.4458 - regression_loss: 0.4043 - classification_loss: 0.0416
 108/1000 [==>...........................] - ETA: 33:09 - loss: 0.4472 - regression_loss: 0.4055 - classification_loss: 0.0417
 109/1000 [==>...........................] - ETA: 33:03 - loss: 0.4466 - regression_loss: 0.4050 - classification_loss: 0.0416
 110/1000 [==>...........................] - ETA: 33:03 - loss: 0.4474 - regression_loss: 0.4057 - classification_loss: 0.0417
 111/1000 [==>...........................] - ETA: 33:04 - loss: 0.4489 - regression_loss: 0.4071 - classification_loss: 0.0418
 112/1000 [==>...........................] - ETA: 33:03 - loss: 0.4486 - regression_loss: 0.4068 - classification_loss: 0.0418
 113/1000 [==>...........................] - ETA: 32:57 - loss: 0.4469 - regression_loss: 0.4052 - classification_loss: 0.0417
 114/1000 [==>...........................] - ETA: 32:54 - loss: 0.4473 - regression_loss: 0.4055 - classification_loss: 0.0417
 115/1000 [==>...........................] - ETA: 32:55 - loss: 0.4488 - regression_loss: 0.4069 - classification_loss: 0.0418
 116/1000 [==>...........................] - ETA: 32:48 - loss: 0.4459 - regression_loss: 0.4043 - classification_loss: 0.0416
 117/1000 [==>...........................] - ETA: 32:48 - loss: 0.4470 - regression_loss: 0.4053 - classification_loss: 0.0416
 118/1000 [==>...........................] - ETA: 32:42 - loss: 0.4466 - regression_loss: 0.4050 - classification_loss: 0.0416
 119/1000 [==>...........................] - ETA: 32:41 - loss: 0.4464 - regression_loss: 0.4048 - classification_loss: 0.0416
 120/1000 [==>...........................] - ETA: 32:34 - loss: 0.4433 - regression_loss: 0.4019 - classification_loss: 0.0414
 121/1000 [==>...........................] - ETA: 32:31 - loss: 0.4436 - regression_loss: 0.4022 - classification_loss: 0.0414
 122/1000 [==>...........................] - ETA: 32:33 - loss: 0.4449 - regression_loss: 0.4034 - classification_loss: 0.0415
 123/1000 [==>...........................] - ETA: 32:33 - loss: 0.4462 - regression_loss: 0.4046 - classification_loss: 0.0416
 124/1000 [==>...........................] - ETA: 32:33 - loss: 0.4468 - regression_loss: 0.4052 - classification_loss: 0.0416
 125/1000 [==>...........................] - ETA: 32:30 - loss: 0.4468 - regression_loss: 0.4052 - classification_loss: 0.0416
 126/1000 [==>...........................] - ETA: 32:31 - loss: 0.4480 - regression_loss: 0.4063 - classification_loss: 0.0417
 127/1000 [==>...........................] - ETA: 32:30 - loss: 0.4485 - regression_loss: 0.4068 - classification_loss: 0.0417
 128/1000 [==>...........................] - ETA: 32:30 - loss: 0.4497 - regression_loss: 0.4079 - classification_loss: 0.0418
 129/1000 [==>...........................] - ETA: 32:25 - loss: 0.4484 - regression_loss: 0.4067 - classification_loss: 0.0417
 130/1000 [==>...........................] - ETA: 32:24 - loss: 0.4481 - regression_loss: 0.4064 - classification_loss: 0.0417
 131/1000 [==>...........................] - ETA: 32:17 - loss: 0.4457 - regression_loss: 0.4042 - classification_loss: 0.0415
 132/1000 [==>...........................] - ETA: 32:14 - loss: 0.4459 - regression_loss: 0.4044 - classification_loss: 0.0415
 133/1000 [==>...........................] - ETA: 32:14 - loss: 0.4465 - regression_loss: 0.4050 - classification_loss: 0.0415
 134/1000 [===>..........................] - ETA: 32:13 - loss: 0.4462 - regression_loss: 0.4046 - classification_loss: 0.0415
 135/1000 [===>..........................] - ETA: 32:13 - loss: 0.4472 - regression_loss: 0.4056 - classification_loss: 0.0416
 136/1000 [===>..........................] - ETA: 32:06 - loss: 0.4445 - regression_loss: 0.4031 - classification_loss: 0.0414
 137/1000 [===>..........................] - ETA: 32:07 - loss: 0.4458 - regression_loss: 0.4043 - classification_loss: 0.0414
 138/1000 [===>..........................] - ETA: 32:01 - loss: 0.4445 - regression_loss: 0.4031 - classification_loss: 0.0414
 139/1000 [===>..........................] - ETA: 31:56 - loss: 0.4431 - regression_loss: 0.4018 - classification_loss: 0.0413
 140/1000 [===>..........................] - ETA: 31:53 - loss: 0.4434 - regression_loss: 0.4021 - classification_loss: 0.0413
 141/1000 [===>..........................] - ETA: 31:53 - loss: 0.4446 - regression_loss: 0.4033 - classification_loss: 0.0414
 142/1000 [===>..........................] - ETA: 31:53 - loss: 0.4452 - regression_loss: 0.4038 - classification_loss: 0.0414
 143/1000 [===>..........................] - ETA: 31:52 - loss: 0.4449 - regression_loss: 0.4035 - classification_loss: 0.0414
 144/1000 [===>..........................] - ETA: 31:52 - loss: 0.4460 - regression_loss: 0.4045 - classification_loss: 0.0415
 145/1000 [===>..........................] - ETA: 31:46 - loss: 0.4443 - regression_loss: 0.4030 - classification_loss: 0.0413
 146/1000 [===>..........................] - ETA: 31:45 - loss: 0.4452 - regression_loss: 0.4038 - classification_loss: 0.0414
 147/1000 [===>..........................] - ETA: 31:46 - loss: 0.4465 - regression_loss: 0.4050 - classification_loss: 0.0414
 148/1000 [===>..........................] - ETA: 31:43 - loss: 0.4468 - regression_loss: 0.4054 - classification_loss: 0.0415
 149/1000 [===>..........................] - ETA: 31:38 - loss: 0.4473 - regression_loss: 0.4059 - classification_loss: 0.0414
 150/1000 [===>..........................] - ETA: 31:32 - loss: 0.4449 - regression_loss: 0.4037 - classification_loss: 0.0412
 151/1000 [===>..........................] - ETA: 31:31 - loss: 0.4450 - regression_loss: 0.4037 - classification_loss: 0.0413
 152/1000 [===>..........................] - ETA: 31:30 - loss: 0.4464 - regression_loss: 0.4050 - classification_loss: 0.0414
 153/1000 [===>..........................] - ETA: 31:25 - loss: 0.4444 - regression_loss: 0.4032 - classification_loss: 0.0412
 154/1000 [===>..........................] - ETA: 31:24 - loss: 0.4450 - regression_loss: 0.4038 - classification_loss: 0.0412
 155/1000 [===>..........................] - ETA: 31:24 - loss: 0.4460 - regression_loss: 0.4048 - classification_loss: 0.0413
 156/1000 [===>..........................] - ETA: 31:22 - loss: 0.4459 - regression_loss: 0.4045 - classification_loss: 0.0413
 157/1000 [===>..........................] - ETA: 31:17 - loss: 0.4454 - regression_loss: 0.4041 - classification_loss: 0.0413
 158/1000 [===>..........................] - ETA: 31:15 - loss: 0.4459 - regression_loss: 0.4046 - classification_loss: 0.0413
 159/1000 [===>..........................] - ETA: 31:15 - loss: 0.4472 - regression_loss: 0.4059 - classification_loss: 0.0414
 160/1000 [===>..........................] - ETA: 31:10 - loss: 0.4462 - regression_loss: 0.4048 - classification_loss: 0.0413
 161/1000 [===>..........................] - ETA: 31:05 - loss: 0.4440 - regression_loss: 0.4029 - classification_loss: 0.0411
 162/1000 [===>..........................] - ETA: 31:03 - loss: 0.4442 - regression_loss: 0.4030 - classification_loss: 0.0412
 163/1000 [===>..........................] - ETA: 31:04 - loss: 0.4453 - regression_loss: 0.4041 - classification_loss: 0.0412
 164/1000 [===>..........................] - ETA: 31:03 - loss: 0.4459 - regression_loss: 0.4046 - classification_loss: 0.0413
 165/1000 [===>..........................] - ETA: 31:03 - loss: 0.4468 - regression_loss: 0.4055 - classification_loss: 0.0413
 166/1000 [===>..........................] - ETA: 31:00 - loss: 0.4470 - regression_loss: 0.4057 - classification_loss: 0.0413
 167/1000 [====>.........................] - ETA: 30:58 - loss: 0.4472 - regression_loss: 0.4058 - classification_loss: 0.0413
 168/1000 [====>.........................] - ETA: 30:57 - loss: 0.4471 - regression_loss: 0.4057 - classification_loss: 0.0414
 169/1000 [====>.........................] - ETA: 30:56 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0414
 170/1000 [====>.........................] - ETA: 30:50 - loss: 0.4459 - regression_loss: 0.4047 - classification_loss: 0.0413
 171/1000 [====>.........................] - ETA: 30:46 - loss: 0.4458 - regression_loss: 0.4045 - classification_loss: 0.0412
 172/1000 [====>.........................] - ETA: 30:45 - loss: 0.4463 - regression_loss: 0.4051 - classification_loss: 0.0413
 173/1000 [====>.........................] - ETA: 30:45 - loss: 0.4472 - regression_loss: 0.4059 - classification_loss: 0.0413
 174/1000 [====>.........................] - ETA: 30:42 - loss: 0.4474 - regression_loss: 0.4061 - classification_loss: 0.0413
 175/1000 [====>.........................] - ETA: 30:38 - loss: 0.4467 - regression_loss: 0.4054 - classification_loss: 0.0413
 176/1000 [====>.........................] - ETA: 30:32 - loss: 0.4448 - regression_loss: 0.4037 - classification_loss: 0.0411
 177/1000 [====>.........................] - ETA: 30:32 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0412
 178/1000 [====>.........................] - ETA: 30:30 - loss: 0.4458 - regression_loss: 0.4045 - classification_loss: 0.0412
 179/1000 [====>.........................] - ETA: 30:29 - loss: 0.4464 - regression_loss: 0.4051 - classification_loss: 0.0413
 180/1000 [====>.........................] - ETA: 30:29 - loss: 0.4473 - regression_loss: 0.4060 - classification_loss: 0.0413
 181/1000 [====>.........................] - ETA: 30:28 - loss: 0.4471 - regression_loss: 0.4058 - classification_loss: 0.0414
 182/1000 [====>.........................] - ETA: 30:28 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0414
 183/1000 [====>.........................] - ETA: 30:27 - loss: 0.4485 - regression_loss: 0.4071 - classification_loss: 0.0414
 184/1000 [====>.........................] - ETA: 30:21 - loss: 0.4467 - regression_loss: 0.4054 - classification_loss: 0.0413
 185/1000 [====>.........................] - ETA: 30:17 - loss: 0.4462 - regression_loss: 0.4050 - classification_loss: 0.0412
 186/1000 [====>.........................] - ETA: 30:14 - loss: 0.4468 - regression_loss: 0.4055 - classification_loss: 0.0413
 187/1000 [====>.........................] - ETA: 30:14 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0413
 188/1000 [====>.........................] - ETA: 30:13 - loss: 0.4488 - regression_loss: 0.4075 - classification_loss: 0.0414
 189/1000 [====>.........................] - ETA: 30:12 - loss: 0.4486 - regression_loss: 0.4072 - classification_loss: 0.0414
 190/1000 [====>.........................] - ETA: 30:11 - loss: 0.4491 - regression_loss: 0.4076 - classification_loss: 0.0414
 191/1000 [====>.........................] - ETA: 30:08 - loss: 0.4493 - regression_loss: 0.4079 - classification_loss: 0.0414
 192/1000 [====>.........................] - ETA: 30:03 - loss: 0.4480 - regression_loss: 0.4067 - classification_loss: 0.0413
 193/1000 [====>.........................] - ETA: 30:03 - loss: 0.4488 - regression_loss: 0.4075 - classification_loss: 0.0413
 194/1000 [====>.........................] - ETA: 29:58 - loss: 0.4480 - regression_loss: 0.4067 - classification_loss: 0.0413
 195/1000 [====>.........................] - ETA: 29:54 - loss: 0.4471 - regression_loss: 0.4059 - classification_loss: 0.0413
 196/1000 [====>.........................] - ETA: 29:54 - loss: 0.4479 - regression_loss: 0.4066 - classification_loss: 0.0413
 197/1000 [====>.........................] - ETA: 29:53 - loss: 0.4488 - regression_loss: 0.4074 - classification_loss: 0.0414
 198/1000 [====>.........................] - ETA: 29:51 - loss: 0.4487 - regression_loss: 0.4072 - classification_loss: 0.0415
 199/1000 [====>.........................] - ETA: 29:50 - loss: 0.4492 - regression_loss: 0.4077 - classification_loss: 0.0415
 200/1000 [=====>........................] - ETA: 29:46 - loss: 0.4474 - regression_loss: 0.4060 - classification_loss: 0.0413
 201/1000 [=====>........................] - ETA: 29:43 - loss: 0.4476 - regression_loss: 0.4062 - classification_loss: 0.0414
 202/1000 [=====>........................] - ETA: 29:38 - loss: 0.4457 - regression_loss: 0.4045 - classification_loss: 0.0412
 203/1000 [=====>........................] - ETA: 29:37 - loss: 0.4461 - regression_loss: 0.4049 - classification_loss: 0.0412
 204/1000 [=====>........................] - ETA: 29:37 - loss: 0.4470 - regression_loss: 0.4057 - classification_loss: 0.0413
 205/1000 [=====>........................] - ETA: 29:34 - loss: 0.4472 - regression_loss: 0.4059 - classification_loss: 0.0413
 206/1000 [=====>........................] - ETA: 29:33 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0414
 207/1000 [=====>........................] - ETA: 29:29 - loss: 0.4481 - regression_loss: 0.4068 - classification_loss: 0.0414
 208/1000 [=====>........................] - ETA: 29:28 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0414
 209/1000 [=====>........................] - ETA: 29:27 - loss: 0.4484 - regression_loss: 0.4070 - classification_loss: 0.0414
 210/1000 [=====>........................] - ETA: 29:26 - loss: 0.4491 - regression_loss: 0.4076 - classification_loss: 0.0415
 211/1000 [=====>........................] - ETA: 29:22 - loss: 0.4475 - regression_loss: 0.4062 - classification_loss: 0.0413
 212/1000 [=====>........................] - ETA: 29:20 - loss: 0.4474 - regression_loss: 0.4060 - classification_loss: 0.0414
 213/1000 [=====>........................] - ETA: 29:18 - loss: 0.4475 - regression_loss: 0.4061 - classification_loss: 0.0414
 214/1000 [=====>........................] - ETA: 29:14 - loss: 0.4469 - regression_loss: 0.4056 - classification_loss: 0.0414
 215/1000 [=====>........................] - ETA: 29:13 - loss: 0.4476 - regression_loss: 0.4062 - classification_loss: 0.0414
 216/1000 [=====>........................] - ETA: 29:12 - loss: 0.4482 - regression_loss: 0.4068 - classification_loss: 0.0414
 217/1000 [=====>........................] - ETA: 29:11 - loss: 0.4485 - regression_loss: 0.4071 - classification_loss: 0.0415
 218/1000 [=====>........................] - ETA: 29:06 - loss: 0.4470 - regression_loss: 0.4057 - classification_loss: 0.0413
 219/1000 [=====>........................] - ETA: 29:02 - loss: 0.4461 - regression_loss: 0.4048 - classification_loss: 0.0413
 220/1000 [=====>........................] - ETA: 29:00 - loss: 0.4463 - regression_loss: 0.4050 - classification_loss: 0.0413
 221/1000 [=====>........................] - ETA: 28:58 - loss: 0.4463 - regression_loss: 0.4049 - classification_loss: 0.0413
 222/1000 [=====>........................] - ETA: 28:57 - loss: 0.4470 - regression_loss: 0.4056 - classification_loss: 0.0414
 223/1000 [=====>........................] - ETA: 28:53 - loss: 0.4461 - regression_loss: 0.4048 - classification_loss: 0.0413
 224/1000 [=====>........................] - ETA: 28:49 - loss: 0.4445 - regression_loss: 0.4033 - classification_loss: 0.0412
 225/1000 [=====>........................] - ETA: 28:48 - loss: 0.4451 - regression_loss: 0.4039 - classification_loss: 0.0412
 226/1000 [=====>........................] - ETA: 28:46 - loss: 0.4454 - regression_loss: 0.4042 - classification_loss: 0.0413
 227/1000 [=====>........................] - ETA: 28:45 - loss: 0.4452 - regression_loss: 0.4039 - classification_loss: 0.0413
 228/1000 [=====>........................] - ETA: 28:42 - loss: 0.4452 - regression_loss: 0.4040 - classification_loss: 0.0413
 229/1000 [=====>........................] - ETA: 28:42 - loss: 0.4459 - regression_loss: 0.4045 - classification_loss: 0.0413
 230/1000 [=====>........................] - ETA: 28:37 - loss: 0.4443 - regression_loss: 0.4031 - classification_loss: 0.0412
 231/1000 [=====>........................] - ETA: 28:37 - loss: 0.4451 - regression_loss: 0.4038 - classification_loss: 0.0413
 232/1000 [=====>........................] - ETA: 28:36 - loss: 0.4457 - regression_loss: 0.4044 - classification_loss: 0.0413
 233/1000 [=====>........................] - ETA: 28:33 - loss: 0.4459 - regression_loss: 0.4046 - classification_loss: 0.0413
 234/1000 [======>.......................] - ETA: 28:31 - loss: 0.4458 - regression_loss: 0.4045 - classification_loss: 0.0413
 235/1000 [======>.......................] - ETA: 28:28 - loss: 0.4454 - regression_loss: 0.4041 - classification_loss: 0.0413
 236/1000 [======>.......................] - ETA: 28:26 - loss: 0.4458 - regression_loss: 0.4045 - classification_loss: 0.0413
 237/1000 [======>.......................] - ETA: 28:25 - loss: 0.4462 - regression_loss: 0.4049 - classification_loss: 0.0413
 238/1000 [======>.......................] - ETA: 28:22 - loss: 0.4464 - regression_loss: 0.4051 - classification_loss: 0.0413
 239/1000 [======>.......................] - ETA: 28:18 - loss: 0.4455 - regression_loss: 0.4043 - classification_loss: 0.0412
 240/1000 [======>.......................] - ETA: 28:16 - loss: 0.4455 - regression_loss: 0.4042 - classification_loss: 0.0412
 241/1000 [======>.......................] - ETA: 28:16 - loss: 0.4462 - regression_loss: 0.4049 - classification_loss: 0.0413
 242/1000 [======>.......................] - ETA: 28:14 - loss: 0.4469 - regression_loss: 0.4056 - classification_loss: 0.0413
 243/1000 [======>.......................] - ETA: 28:11 - loss: 0.4464 - regression_loss: 0.4051 - classification_loss: 0.0413
 244/1000 [======>.......................] - ETA: 28:09 - loss: 0.4470 - regression_loss: 0.4057 - classification_loss: 0.0413
 245/1000 [======>.......................] - ETA: 28:08 - loss: 0.4469 - regression_loss: 0.4056 - classification_loss: 0.0413
 246/1000 [======>.......................] - ETA: 28:05 - loss: 0.4469 - regression_loss: 0.4056 - classification_loss: 0.0413
 247/1000 [======>.......................] - ETA: 28:04 - loss: 0.4475 - regression_loss: 0.4062 - classification_loss: 0.0413
 248/1000 [======>.......................] - ETA: 28:03 - loss: 0.4479 - regression_loss: 0.4065 - classification_loss: 0.0414
 249/1000 [======>.......................] - ETA: 27:59 - loss: 0.4468 - regression_loss: 0.4056 - classification_loss: 0.0412
 250/1000 [======>.......................] - ETA: 27:55 - loss: 0.4462 - regression_loss: 0.4050 - classification_loss: 0.0412
 251/1000 [======>.......................] - ETA: 27:51 - loss: 0.4454 - regression_loss: 0.4043 - classification_loss: 0.0412
 252/1000 [======>.......................] - ETA: 27:47 - loss: 0.4440 - regression_loss: 0.4030 - classification_loss: 0.0411
 253/1000 [======>.......................] - ETA: 27:45 - loss: 0.4441 - regression_loss: 0.4030 - classification_loss: 0.0411
 254/1000 [======>.......................] - ETA: 27:45 - loss: 0.4449 - regression_loss: 0.4037 - classification_loss: 0.0412
 255/1000 [======>.......................] - ETA: 27:42 - loss: 0.4450 - regression_loss: 0.4038 - classification_loss: 0.0412
 256/1000 [======>.......................] - ETA: 27:41 - loss: 0.4454 - regression_loss: 0.4042 - classification_loss: 0.0412
 257/1000 [======>.......................] - ETA: 27:39 - loss: 0.4461 - regression_loss: 0.4048 - classification_loss: 0.0413
 258/1000 [======>.......................] - ETA: 27:35 - loss: 0.4449 - regression_loss: 0.4038 - classification_loss: 0.0411
 259/1000 [======>.......................] - ETA: 27:34 - loss: 0.4455 - regression_loss: 0.4044 - classification_loss: 0.0412
 260/1000 [======>.......................] - ETA: 27:33 - loss: 0.4461 - regression_loss: 0.4049 - classification_loss: 0.0412
 261/1000 [======>.......................] - ETA: 27:31 - loss: 0.4463 - regression_loss: 0.4051 - classification_loss: 0.0412
 262/1000 [======>.......................] - ETA: 27:29 - loss: 0.4467 - regression_loss: 0.4055 - classification_loss: 0.0412
 263/1000 [======>.......................] - ETA: 27:27 - loss: 0.4467 - regression_loss: 0.4054 - classification_loss: 0.0413
 264/1000 [======>.......................] - ETA: 27:24 - loss: 0.4475 - regression_loss: 0.4062 - classification_loss: 0.0413
 265/1000 [======>.......................] - ETA: 27:21 - loss: 0.4480 - regression_loss: 0.4066 - classification_loss: 0.0413
 266/1000 [======>.......................] - ETA: 27:17 - loss: 0.4466 - regression_loss: 0.4053 - classification_loss: 0.0412
 267/1000 [=======>......................] - ETA: 27:16 - loss: 0.4472 - regression_loss: 0.4060 - classification_loss: 0.0412
 268/1000 [=======>......................] - ETA: 27:12 - loss: 0.4470 - regression_loss: 0.4058 - classification_loss: 0.0412
 269/1000 [=======>......................] - ETA: 27:11 - loss: 0.4477 - regression_loss: 0.4064 - classification_loss: 0.0413
 270/1000 [=======>......................] - ETA: 27:09 - loss: 0.4476 - regression_loss: 0.4064 - classification_loss: 0.0413
 271/1000 [=======>......................] - ETA: 27:08 - loss: 0.4482 - regression_loss: 0.4069 - classification_loss: 0.0413
 272/1000 [=======>......................] - ETA: 27:04 - loss: 0.4476 - regression_loss: 0.4064 - classification_loss: 0.0413
 273/1000 [=======>......................] - ETA: 27:00 - loss: 0.4464 - regression_loss: 0.4052 - classification_loss: 0.0412
 274/1000 [=======>......................] - ETA: 26:58 - loss: 0.4466 - regression_loss: 0.4054 - classification_loss: 0.0412
 275/1000 [=======>......................] - ETA: 26:57 - loss: 0.4472 - regression_loss: 0.4060 - classification_loss: 0.0412
 276/1000 [=======>......................] - ETA: 26:55 - loss: 0.4471 - regression_loss: 0.4059 - classification_loss: 0.0412
 277/1000 [=======>......................] - ETA: 26:54 - loss: 0.4477 - regression_loss: 0.4064 - classification_loss: 0.0413
 278/1000 [=======>......................] - ETA: 26:52 - loss: 0.4480 - regression_loss: 0.4067 - classification_loss: 0.0413
 279/1000 [=======>......................] - ETA: 26:48 - loss: 0.4470 - regression_loss: 0.4058 - classification_loss: 0.0412
 280/1000 [=======>......................] - ETA: 26:47 - loss: 0.4473 - regression_loss: 0.4061 - classification_loss: 0.0412
 281/1000 [=======>......................] - ETA: 26:46 - loss: 0.4479 - regression_loss: 0.4066 - classification_loss: 0.0412
 282/1000 [=======>......................] - ETA: 26:42 - loss: 0.4476 - regression_loss: 0.4064 - classification_loss: 0.0412
 283/1000 [=======>......................] - ETA: 26:40 - loss: 0.4477 - regression_loss: 0.4064 - classification_loss: 0.0412
 284/1000 [=======>......................] - ETA: 26:38 - loss: 0.4476 - regression_loss: 0.4064 - classification_loss: 0.0412
 285/1000 [=======>......................] - ETA: 26:37 - loss: 0.4481 - regression_loss: 0.4069 - classification_loss: 0.0413
 286/1000 [=======>......................] - ETA: 26:33 - loss: 0.4475 - regression_loss: 0.4063 - classification_loss: 0.0412
 287/1000 [=======>......................] - ETA: 26:32 - loss: 0.4480 - regression_loss: 0.4067 - classification_loss: 0.0412
 288/1000 [=======>......................] - ETA: 26:30 - loss: 0.4483 - regression_loss: 0.4070 - classification_loss: 0.0413
 289/1000 [=======>......................] - ETA: 26:29 - loss: 0.4481 - regression_loss: 0.4068 - classification_loss: 0.0413
 290/1000 [=======>......................] - ETA: 26:25 - loss: 0.4470 - regression_loss: 0.4058 - classification_loss: 0.0412
 291/1000 [=======>......................] - ETA: 26:24 - loss: 0.4476 - regression_loss: 0.4063 - classification_loss: 0.0412
 292/1000 [=======>......................] - ETA: 26:21 - loss: 0.4477 - regression_loss: 0.4064 - classification_loss: 0.0412
 293/1000 [=======>......................] - ETA: 26:19 - loss: 0.4475 - regression_loss: 0.4063 - classification_loss: 0.0412
 294/1000 [=======>......................] - ETA: 26:15 - loss: 0.4463 - regression_loss: 0.4052 - classification_loss: 0.0411
 295/1000 [=======>......................] - ETA: 26:13 - loss: 0.4466 - regression_loss: 0.4055 - classification_loss: 0.0411
 296/1000 [=======>......................] - ETA: 26:09 - loss: 0.4465 - regression_loss: 0.4054 - classification_loss: 0.0411
 297/1000 [=======>......................] - ETA: 26:08 - loss: 0.4475 - regression_loss: 0.4064 - classification_loss: 0.0411
 298/1000 [=======>......................] - ETA: 26:07 - loss: 0.4484 - regression_loss: 0.4072 - classification_loss: 0.0412
 299/1000 [=======>......................] - ETA: 26:05 - loss: 0.4488 - regression_loss: 0.4076 - classification_loss: 0.0412
 300/1000 [========>.....................] - ETA: 26:04 - loss: 0.4490 - regression_loss: 0.4078 - classification_loss: 0.0412
 301/1000 [========>.....................] - ETA: 26:03 - loss: 0.4496 - regression_loss: 0.4083 - classification_loss: 0.0412
 302/1000 [========>.....................] - ETA: 26:01 - loss: 0.4495 - regression_loss: 0.4083 - classification_loss: 0.0413
 303/1000 [========>.....................] - ETA: 25:59 - loss: 0.4500 - regression_loss: 0.4087 - classification_loss: 0.0413
 304/1000 [========>.....................] - ETA: 25:56 - loss: 0.4494 - regression_loss: 0.4082 - classification_loss: 0.0413
 305/1000 [========>.....................] - ETA: 25:53 - loss: 0.4497 - regression_loss: 0.4084 - classification_loss: 0.0413
 306/1000 [========>.....................] - ETA: 25:50 - loss: 0.4488 - regression_loss: 0.4076 - classification_loss: 0.0412
 307/1000 [========>.....................] - ETA: 25:48 - loss: 0.4495 - regression_loss: 0.4083 - classification_loss: 0.0412
 308/1000 [========>.....................] - ETA: 25:45 - loss: 0.4489 - regression_loss: 0.4078 - classification_loss: 0.0412
 309/1000 [========>.....................] - ETA: 25:43 - loss: 0.4491 - regression_loss: 0.4080 - classification_loss: 0.0412
 310/1000 [========>.....................] - ETA: 25:41 - loss: 0.4493 - regression_loss: 0.4081 - classification_loss: 0.0412
 311/1000 [========>.....................] - ETA: 25:39 - loss: 0.4492 - regression_loss: 0.4080 - classification_loss: 0.0412
 312/1000 [========>.....................] - ETA: 25:35 - loss: 0.4479 - regression_loss: 0.4069 - classification_loss: 0.0411
 313/1000 [========>.....................] - ETA: 25:34 - loss: 0.4484 - regression_loss: 0.4073 - classification_loss: 0.0411
 314/1000 [========>.....................] - ETA: 25:33 - loss: 0.4489 - regression_loss: 0.4077 - classification_loss: 0.0411
 315/1000 [========>.....................] - ETA: 25:31 - loss: 0.4489 - regression_loss: 0.4077 - classification_loss: 0.0411
 316/1000 [========>.....................] - ETA: 25:29 - loss: 0.4487 - regression_loss: 0.4076 - classification_loss: 0.0412
 317/1000 [========>.....................] - ETA: 25:25 - loss: 0.4478 - regression_loss: 0.4067 - classification_loss: 0.0411
 318/1000 [========>.....................] - ETA: 25:24 - loss: 0.4482 - regression_loss: 0.4071 - classification_loss: 0.0411
 319/1000 [========>.....................] - ETA: 25:20 - loss: 0.4477 - regression_loss: 0.4067 - classification_loss: 0.0410
 320/1000 [========>.....................] - ETA: 25:19 - loss: 0.4480 - regression_loss: 0.4069 - classification_loss: 0.0411
 321/1000 [========>.....................] - ETA: 25:15 - loss: 0.4469 - regression_loss: 0.4059 - classification_loss: 0.0410
 322/1000 [========>.....................] - ETA: 25:13 - loss: 0.4471 - regression_loss: 0.4061 - classification_loss: 0.0410
 323/1000 [========>.....................] - ETA: 25:11 - loss: 0.4472 - regression_loss: 0.4062 - classification_loss: 0.0410
 324/1000 [========>.....................] - ETA: 25:08 - loss: 0.4466 - regression_loss: 0.4056 - classification_loss: 0.0410
 325/1000 [========>.....................] - ETA: 25:06 - loss: 0.4472 - regression_loss: 0.4062 - classification_loss: 0.0410
 326/1000 [========>.....................] - ETA: 25:05 - loss: 0.4478 - regression_loss: 0.4068 - classification_loss: 0.0410
 327/1000 [========>.....................] - ETA: 25:03 - loss: 0.4476 - regression_loss: 0.4066 - classification_loss: 0.0410
 328/1000 [========>.....................] - ETA: 25:01 - loss: 0.4475 - regression_loss: 0.4065 - classification_loss: 0.0410
 329/1000 [========>.....................] - ETA: 25:00 - loss: 0.4480 - regression_loss: 0.4069 - classification_loss: 0.0411
 330/1000 [========>.....................] - ETA: 24:58 - loss: 0.4485 - regression_loss: 0.4074 - classification_loss: 0.0411
 331/1000 [========>.....................] - ETA: 24:57 - loss: 0.4487 - regression_loss: 0.4076 - classification_loss: 0.0411
 332/1000 [========>.....................] - ETA: 24:53 - loss: 0.4482 - regression_loss: 0.4071 - classification_loss: 0.0411
 333/1000 [========>.....................] - ETA: 24:50 - loss: 0.4473 - regression_loss: 0.4063 - classification_loss: 0.0410
 334/1000 [=========>....................] - ETA: 24:47 - loss: 0.4474 - regression_loss: 0.4064 - classification_loss: 0.0410
 335/1000 [=========>....................] - ETA: 24:46 - loss: 0.4477 - regression_loss: 0.4067 - classification_loss: 0.0410
 336/1000 [=========>....................] - ETA: 24:43 - loss: 0.4478 - regression_loss: 0.4068 - classification_loss: 0.0410
 337/1000 [=========>....................] - ETA: 24:42 - loss: 0.4480 - regression_loss: 0.4069 - classification_loss: 0.0410
 338/1000 [=========>....................] - ETA: 24:40 - loss: 0.4483 - regression_loss: 0.4073 - classification_loss: 0.0410
 339/1000 [=========>....................] - ETA: 24:38 - loss: 0.4482 - regression_loss: 0.4071 - classification_loss: 0.0410
 340/1000 [=========>....................] - ETA: 24:35 - loss: 0.4478 - regression_loss: 0.4068 - classification_loss: 0.0410
 341/1000 [=========>....................] - ETA: 24:32 - loss: 0.4467 - regression_loss: 0.4057 - classification_loss: 0.0409
 342/1000 [=========>....................] - ETA: 24:30 - loss: 0.4469 - regression_loss: 0.4060 - classification_loss: 0.0409
 343/1000 [=========>....................] - ETA: 24:28 - loss: 0.4468 - regression_loss: 0.4059 - classification_loss: 0.0410
 344/1000 [=========>....................] - ETA: 24:26 - loss: 0.4470 - regression_loss: 0.4060 - classification_loss: 0.0410
 345/1000 [=========>....................] - ETA: 24:24 - loss: 0.4474 - regression_loss: 0.4064 - classification_loss: 0.0410
 346/1000 [=========>....................] - ETA: 24:21 - loss: 0.4469 - regression_loss: 0.4059 - classification_loss: 0.0410
 347/1000 [=========>....................] - ETA: 24:17 - loss: 0.4460 - regression_loss: 0.4051 - classification_loss: 0.0409
 348/1000 [=========>....................] - ETA: 24:16 - loss: 0.4465 - regression_loss: 0.4056 - classification_loss: 0.0409
 349/1000 [=========>....................] - ETA: 24:12 - loss: 0.4454 - regression_loss: 0.4046 - classification_loss: 0.0409
 350/1000 [=========>....................] - ETA: 24:11 - loss: 0.4453 - regression_loss: 0.4044 - classification_loss: 0.0409
 351/1000 [=========>....................] - ETA: 24:08 - loss: 0.4454 - regression_loss: 0.4045 - classification_loss: 0.0409
 352/1000 [=========>....................] - ETA: 24:06 - loss: 0.4457 - regression_loss: 0.4047 - classification_loss: 0.0409
 353/1000 [=========>....................] - ETA: 24:05 - loss: 0.4461 - regression_loss: 0.4051 - classification_loss: 0.0409
 354/1000 [=========>....................] - ETA: 24:02 - loss: 0.4456 - regression_loss: 0.4047 - classification_loss: 0.0409
 355/1000 [=========>....................] - ETA: 24:00 - loss: 0.4461 - regression_loss: 0.4051 - classification_loss: 0.0410
 356/1000 [=========>....................] - ETA: 23:57 - loss: 0.4452 - regression_loss: 0.4043 - classification_loss: 0.0409
 357/1000 [=========>....................] - ETA: 23:55 - loss: 0.4455 - regression_loss: 0.4046 - classification_loss: 0.0409
 358/1000 [=========>....................] - ETA: 23:53 - loss: 0.4457 - regression_loss: 0.4048 - classification_loss: 0.0409
 359/1000 [=========>....................] - ETA: 23:51 - loss: 0.4455 - regression_loss: 0.4046 - classification_loss: 0.0409
 360/1000 [=========>....................] - ETA: 23:49 - loss: 0.4459 - regression_loss: 0.4050 - classification_loss: 0.0409
 361/1000 [=========>....................] - ETA: 23:46 - loss: 0.4454 - regression_loss: 0.4045 - classification_loss: 0.0409
 362/1000 [=========>....................] - ETA: 23:45 - loss: 0.4459 - regression_loss: 0.4049 - classification_loss: 0.0410
 363/1000 [=========>....................] - ETA: 23:43 - loss: 0.4458 - regression_loss: 0.4048 - classification_loss: 0.0410
 364/1000 [=========>....................] - ETA: 23:42 - loss: 0.4462 - regression_loss: 0.4052 - classification_loss: 0.0410
 365/1000 [=========>....................] - ETA: 23:40 - loss: 0.4463 - regression_loss: 0.4053 - classification_loss: 0.0410
 366/1000 [=========>....................] - ETA: 23:38 - loss: 0.4467 - regression_loss: 0.4057 - classification_loss: 0.0410
 367/1000 [==========>...................] - ETA: 23:35 - loss: 0.4462 - regression_loss: 0.4052 - classification_loss: 0.0410
 368/1000 [==========>...................] - ETA: 23:32 - loss: 0.4455 - regression_loss: 0.4046 - classification_loss: 0.0409
 369/1000 [==========>...................] - ETA: 23:29 - loss: 0.4457 - regression_loss: 0.4048 - classification_loss: 0.0409
 370/1000 [==========>...................] - ETA: 23:26 - loss: 0.4453 - regression_loss: 0.4044 - classification_loss: 0.0409
 371/1000 [==========>...................] - ETA: 23:23 - loss: 0.4443 - regression_loss: 0.4035 - classification_loss: 0.0408
 372/1000 [==========>...................] - ETA: 23:22 - loss: 0.4447 - regression_loss: 0.4039 - classification_loss: 0.0409
 373/1000 [==========>...................] - ETA: 23:20 - loss: 0.4449 - regression_loss: 0.4041 - classification_loss: 0.0409
 374/1000 [==========>...................] - ETA: 23:18 - loss: 0.4448 - regression_loss: 0.4040 - classification_loss: 0.0409
 375/1000 [==========>...................] - ETA: 23:15 - loss: 0.4449 - regression_loss: 0.4040 - classification_loss: 0.0409
 376/1000 [==========>...................] - ETA: 23:14 - loss: 0.4452 - regression_loss: 0.4043 - classification_loss: 0.0409
 377/1000 [==========>...................] - ETA: 23:12 - loss: 0.4456 - regression_loss: 0.4047 - classification_loss: 0.0409
 378/1000 [==========>...................] - ETA: 23:10 - loss: 0.4458 - regression_loss: 0.4049 - classification_loss: 0.0409
 379/1000 [==========>...................] - ETA: 23:09 - loss: 0.4462 - regression_loss: 0.4052 - classification_loss: 0.0409
 380/1000 [==========>...................] - ETA: 23:07 - loss: 0.4460 - regression_loss: 0.4050 - classification_loss: 0.0410
 381/1000 [==========>...................] - ETA: 23:04 - loss: 0.4457 - regression_loss: 0.4047 - classification_loss: 0.0409
 382/1000 [==========>...................] - ETA: 23:01 - loss: 0.4447 - regression_loss: 0.4038 - classification_loss: 0.0408
 383/1000 [==========>...................] - ETA: 22:58 - loss: 0.4449 - regression_loss: 0.4040 - classification_loss: 0.0408
 384/1000 [==========>...................] - ETA: 22:57 - loss: 0.4453 - regression_loss: 0.4044 - classification_loss: 0.0409
 385/1000 [==========>...................] - ETA: 22:55 - loss: 0.4458 - regression_loss: 0.4049 - classification_loss: 0.0409
 386/1000 [==========>...................] - ETA: 22:53 - loss: 0.4463 - regression_loss: 0.4054 - classification_loss: 0.0409
 387/1000 [==========>...................] - ETA: 22:52 - loss: 0.4462 - regression_loss: 0.4053 - classification_loss: 0.0410
 388/1000 [==========>...................] - ETA: 22:49 - loss: 0.4463 - regression_loss: 0.4053 - classification_loss: 0.0410
 389/1000 [==========>...................] - ETA: 22:46 - loss: 0.4453 - regression_loss: 0.4044 - classification_loss: 0.0409
 390/1000 [==========>...................] - ETA: 22:43 - loss: 0.4449 - regression_loss: 0.4040 - classification_loss: 0.0409
 391/1000 [==========>...................] - ETA: 22:41 - loss: 0.4454 - regression_loss: 0.4045 - classification_loss: 0.0409
 392/1000 [==========>...................] - ETA: 22:38 - loss: 0.4447 - regression_loss: 0.4038 - classification_loss: 0.0408
 393/1000 [==========>...................] - ETA: 22:36 - loss: 0.4449 - regression_loss: 0.4041 - classification_loss: 0.0409
 394/1000 [==========>...................] - ETA: 22:34 - loss: 0.4450 - regression_loss: 0.4042 - classification_loss: 0.0409
 395/1000 [==========>...................] - ETA: 22:32 - loss: 0.4454 - regression_loss: 0.4045 - classification_loss: 0.0409
 396/1000 [==========>...................] - ETA: 22:30 - loss: 0.4452 - regression_loss: 0.4043 - classification_loss: 0.0409
 397/1000 [==========>...................] - ETA: 22:27 - loss: 0.4450 - regression_loss: 0.4041 - classification_loss: 0.0409
 398/1000 [==========>...................] - ETA: 22:24 - loss: 0.4448 - regression_loss: 0.4039 - classification_loss: 0.0409
 399/1000 [==========>...................] - ETA: 22:22 - loss: 0.4453 - regression_loss: 0.4043 - classification_loss: 0.0410
 400/1000 [===========>..................] - ETA: 22:19 - loss: 0.4444 - regression_loss: 0.4035 - classification_loss: 0.0409
 401/1000 [===========>..................] - ETA: 22:17 - loss: 0.4445 - regression_loss: 0.4035 - classification_loss: 0.0410
 402/1000 [===========>..................] - ETA: 22:15 - loss: 0.4448 - regression_loss: 0.4038 - classification_loss: 0.0410
 403/1000 [===========>..................] - ETA: 22:13 - loss: 0.4453 - regression_loss: 0.4043 - classification_loss: 0.0411
 404/1000 [===========>..................] - ETA: 22:12 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0411
 405/1000 [===========>..................] - ETA: 22:10 - loss: 0.4461 - regression_loss: 0.4050 - classification_loss: 0.0411
 406/1000 [===========>..................] - ETA: 22:08 - loss: 0.4465 - regression_loss: 0.4054 - classification_loss: 0.0411
 407/1000 [===========>..................] - ETA: 22:06 - loss: 0.4466 - regression_loss: 0.4054 - classification_loss: 0.0412
 408/1000 [===========>..................] - ETA: 22:04 - loss: 0.4465 - regression_loss: 0.4054 - classification_loss: 0.0412
 409/1000 [===========>..................] - ETA: 22:02 - loss: 0.4468 - regression_loss: 0.4056 - classification_loss: 0.0412
 410/1000 [===========>..................] - ETA: 21:59 - loss: 0.4468 - regression_loss: 0.4056 - classification_loss: 0.0412
 411/1000 [===========>..................] - ETA: 21:56 - loss: 0.4459 - regression_loss: 0.4048 - classification_loss: 0.0411
 412/1000 [===========>..................] - ETA: 21:54 - loss: 0.4460 - regression_loss: 0.4049 - classification_loss: 0.0411
 413/1000 [===========>..................] - ETA: 21:52 - loss: 0.4465 - regression_loss: 0.4054 - classification_loss: 0.0411
 414/1000 [===========>..................] - ETA: 21:50 - loss: 0.4467 - regression_loss: 0.4056 - classification_loss: 0.0411
 415/1000 [===========>..................] - ETA: 21:47 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0411
 416/1000 [===========>..................] - ETA: 21:45 - loss: 0.4457 - regression_loss: 0.4046 - classification_loss: 0.0411
 417/1000 [===========>..................] - ETA: 21:43 - loss: 0.4461 - regression_loss: 0.4050 - classification_loss: 0.0411
 418/1000 [===========>..................] - ETA: 21:40 - loss: 0.4457 - regression_loss: 0.4046 - classification_loss: 0.0411
 419/1000 [===========>..................] - ETA: 21:37 - loss: 0.4448 - regression_loss: 0.4038 - classification_loss: 0.0410
 420/1000 [===========>..................] - ETA: 21:34 - loss: 0.4444 - regression_loss: 0.4034 - classification_loss: 0.0410
 421/1000 [===========>..................] - ETA: 21:32 - loss: 0.4445 - regression_loss: 0.4035 - classification_loss: 0.0410
 422/1000 [===========>..................] - ETA: 21:30 - loss: 0.4448 - regression_loss: 0.4037 - classification_loss: 0.0410
 423/1000 [===========>..................] - ETA: 21:28 - loss: 0.4447 - regression_loss: 0.4037 - classification_loss: 0.0410
 424/1000 [===========>..................] - ETA: 21:26 - loss: 0.4451 - regression_loss: 0.4040 - classification_loss: 0.0411
 425/1000 [===========>..................] - ETA: 21:25 - loss: 0.4454 - regression_loss: 0.4043 - classification_loss: 0.0411
 426/1000 [===========>..................] - ETA: 21:23 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0411
 427/1000 [===========>..................] - ETA: 21:21 - loss: 0.4459 - regression_loss: 0.4048 - classification_loss: 0.0411
 428/1000 [===========>..................] - ETA: 21:19 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0411
 429/1000 [===========>..................] - ETA: 21:17 - loss: 0.4457 - regression_loss: 0.4046 - classification_loss: 0.0411
 430/1000 [===========>..................] - ETA: 21:13 - loss: 0.4450 - regression_loss: 0.4040 - classification_loss: 0.0410
 431/1000 [===========>..................] - ETA: 21:11 - loss: 0.4451 - regression_loss: 0.4040 - classification_loss: 0.0410
 432/1000 [===========>..................] - ETA: 21:09 - loss: 0.4455 - regression_loss: 0.4044 - classification_loss: 0.0411
 433/1000 [===========>..................] - ETA: 21:06 - loss: 0.4451 - regression_loss: 0.4041 - classification_loss: 0.0410
 434/1000 [============>.................] - ETA: 21:04 - loss: 0.4450 - regression_loss: 0.4039 - classification_loss: 0.0410
 435/1000 [============>.................] - ETA: 21:03 - loss: 0.4452 - regression_loss: 0.4041 - classification_loss: 0.0411
 436/1000 [============>.................] - ETA: 21:01 - loss: 0.4455 - regression_loss: 0.4044 - classification_loss: 0.0411
 437/1000 [============>.................] - ETA: 20:59 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0411
 438/1000 [============>.................] - ETA: 20:56 - loss: 0.4449 - regression_loss: 0.4039 - classification_loss: 0.0410
 439/1000 [============>.................] - ETA: 20:54 - loss: 0.4450 - regression_loss: 0.4039 - classification_loss: 0.0410
 440/1000 [============>.................] - ETA: 20:51 - loss: 0.4447 - regression_loss: 0.4037 - classification_loss: 0.0410
 441/1000 [============>.................] - ETA: 20:48 - loss: 0.4442 - regression_loss: 0.4032 - classification_loss: 0.0409
 442/1000 [============>.................] - ETA: 20:46 - loss: 0.4442 - regression_loss: 0.4033 - classification_loss: 0.0410
 443/1000 [============>.................] - ETA: 20:44 - loss: 0.4445 - regression_loss: 0.4036 - classification_loss: 0.0410
 444/1000 [============>.................] - ETA: 20:42 - loss: 0.4449 - regression_loss: 0.4039 - classification_loss: 0.0410
 445/1000 [============>.................] - ETA: 20:40 - loss: 0.4452 - regression_loss: 0.4042 - classification_loss: 0.0410
 446/1000 [============>.................] - ETA: 20:38 - loss: 0.4454 - regression_loss: 0.4043 - classification_loss: 0.0410
 447/1000 [============>.................] - ETA: 20:36 - loss: 0.4456 - regression_loss: 0.4045 - classification_loss: 0.0410
 448/1000 [============>.................] - ETA: 20:33 - loss: 0.4448 - regression_loss: 0.4038 - classification_loss: 0.0410
 449/1000 [============>.................] - ETA: 20:31 - loss: 0.4451 - regression_loss: 0.4040 - classification_loss: 0.0410
 450/1000 [============>.................] - ETA: 20:29 - loss: 0.4451 - regression_loss: 0.4041 - classification_loss: 0.0411
 451/1000 [============>.................] - ETA: 20:26 - loss: 0.4448 - regression_loss: 0.4038 - classification_loss: 0.0410
 452/1000 [============>.................] - ETA: 20:24 - loss: 0.4451 - regression_loss: 0.4041 - classification_loss: 0.0411
 453/1000 [============>.................] - ETA: 20:22 - loss: 0.4454 - regression_loss: 0.4044 - classification_loss: 0.0411
 454/1000 [============>.................] - ETA: 20:19 - loss: 0.4447 - regression_loss: 0.4036 - classification_loss: 0.0410
 455/1000 [============>.................] - ETA: 20:16 - loss: 0.4445 - regression_loss: 0.4034 - classification_loss: 0.0410
 456/1000 [============>.................] - ETA: 20:14 - loss: 0.4446 - regression_loss: 0.4036 - classification_loss: 0.0410
 457/1000 [============>.................] - ETA: 20:12 - loss: 0.4449 - regression_loss: 0.4039 - classification_loss: 0.0411
 458/1000 [============>.................] - ETA: 20:10 - loss: 0.4449 - regression_loss: 0.4038 - classification_loss: 0.0411
 459/1000 [============>.................] - ETA: 20:09 - loss: 0.4451 - regression_loss: 0.4040 - classification_loss: 0.0411
 460/1000 [============>.................] - ETA: 20:07 - loss: 0.4456 - regression_loss: 0.4045 - classification_loss: 0.0411
 461/1000 [============>.................] - ETA: 20:04 - loss: 0.4451 - regression_loss: 0.4041 - classification_loss: 0.0410
 462/1000 [============>.................] - ETA: 20:01 - loss: 0.4444 - regression_loss: 0.4035 - classification_loss: 0.0410
 463/1000 [============>.................] - ETA: 19:59 - loss: 0.4445 - regression_loss: 0.4035 - classification_loss: 0.0410
 464/1000 [============>.................] - ETA: 19:57 - loss: 0.4449 - regression_loss: 0.4039 - classification_loss: 0.0410
 465/1000 [============>.................] - ETA: 19:55 - loss: 0.4453 - regression_loss: 0.4043 - classification_loss: 0.0410
 466/1000 [============>.................] - ETA: 19:53 - loss: 0.4452 - regression_loss: 0.4042 - classification_loss: 0.0410
 467/1000 [=============>................] - ETA: 19:51 - loss: 0.4454 - regression_loss: 0.4044 - classification_loss: 0.0410
 468/1000 [=============>................] - ETA: 19:50 - loss: 0.4458 - regression_loss: 0.4047 - classification_loss: 0.0410
 469/1000 [=============>................] - ETA: 19:48 - loss: 0.4459 - regression_loss: 0.4049 - classification_loss: 0.0411
 470/1000 [=============>................] - ETA: 19:46 - loss: 0.4458 - regression_loss: 0.4048 - classification_loss: 0.0411
 471/1000 [=============>................] - ETA: 19:43 - loss: 0.4458 - regression_loss: 0.4048 - classification_loss: 0.0411
 472/1000 [=============>................] - ETA: 19:40 - loss: 0.4457 - regression_loss: 0.4047 - classification_loss: 0.0410
 473/1000 [=============>................] - ETA: 19:37 - loss: 0.4450 - regression_loss: 0.4041 - classification_loss: 0.0410
 474/1000 [=============>................] - ETA: 19:36 - loss: 0.4453 - regression_loss: 0.4043 - classification_loss: 0.0410
 475/1000 [=============>................] - ETA: 19:33 - loss: 0.4445 - regression_loss: 0.4035 - classification_loss: 0.0409
 476/1000 [=============>................] - ETA: 19:31 - loss: 0.4443 - regression_loss: 0.4034 - classification_loss: 0.0409
 477/1000 [=============>................] - ETA: 19:29 - loss: 0.4445 - regression_loss: 0.4036 - classification_loss: 0.0409
 478/1000 [=============>................] - ETA: 19:26 - loss: 0.4441 - regression_loss: 0.4032 - classification_loss: 0.0409
 479/1000 [=============>................] - ETA: 19:24 - loss: 0.4443 - regression_loss: 0.4034 - classification_loss: 0.0409
 480/1000 [=============>................] - ETA: 19:22 - loss: 0.4446 - regression_loss: 0.4037 - classification_loss: 0.0409
 481/1000 [=============>................] - ETA: 19:20 - loss: 0.4447 - regression_loss: 0.4037 - classification_loss: 0.0409
 482/1000 [=============>................] - ETA: 19:18 - loss: 0.4449 - regression_loss: 0.4040 - classification_loss: 0.0410
 483/1000 [=============>................] - ETA: 19:15 - loss: 0.4442 - regression_loss: 0.4033 - classification_loss: 0.0409
 484/1000 [=============>................] - ETA: 19:13 - loss: 0.4438 - regression_loss: 0.4029 - classification_loss: 0.0409
 485/1000 [=============>................] - ETA: 19:10 - loss: 0.4441 - regression_loss: 0.4032 - classification_loss: 0.0409
 486/1000 [=============>................] - ETA: 19:08 - loss: 0.4443 - regression_loss: 0.4034 - classification_loss: 0.0409
 487/1000 [=============>................] - ETA: 19:07 - loss: 0.4446 - regression_loss: 0.4037 - classification_loss: 0.0409
 488/1000 [=============>................] - ETA: 19:04 - loss: 0.4446 - regression_loss: 0.4036 - classification_loss: 0.0409
 489/1000 [=============>................] - ETA: 19:03 - loss: 0.4447 - regression_loss: 0.4038 - classification_loss: 0.0409
 490/1000 [=============>................] - ETA: 19:00 - loss: 0.4441 - regression_loss: 0.4033 - classification_loss: 0.0409
 491/1000 [=============>................] - ETA: 18:58 - loss: 0.4444 - regression_loss: 0.4035 - classification_loss: 0.0409
 492/1000 [=============>................] - ETA: 18:56 - loss: 0.4446 - regression_loss: 0.4037 - classification_loss: 0.0409
 493/1000 [=============>................] - ETA: 18:54 - loss: 0.4445 - regression_loss: 0.4036 - classification_loss: 0.0409
 494/1000 [=============>................] - ETA: 18:52 - loss: 0.4445 - regression_loss: 0.4036 - classification_loss: 0.0409
 495/1000 [=============>................] - ETA: 18:49 - loss: 0.4441 - regression_loss: 0.4033 - classification_loss: 0.0409
 496/1000 [=============>................] - ETA: 18:46 - loss: 0.4434 - regression_loss: 0.4026 - classification_loss: 0.0408
 497/1000 [=============>................] - ETA: 18:43 - loss: 0.4431 - regression_loss: 0.4023 - classification_loss: 0.0408
 498/1000 [=============>................] - ETA: 18:42 - loss: 0.4433 - regression_loss: 0.4026 - classification_loss: 0.0408
 499/1000 [=============>................] - ETA: 18:39 - loss: 0.4434 - regression_loss: 0.4026 - classification_loss: 0.0408
 500/1000 [==============>...............] - ETA: 18:37 - loss: 0.4433 - regression_loss: 0.4025 - classification_loss: 0.0408
 501/1000 [==============>...............] - ETA: 18:35 - loss: 0.4436 - regression_loss: 0.4028 - classification_loss: 0.0408
 502/1000 [==============>...............] - ETA: 18:33 - loss: 0.4437 - regression_loss: 0.4029 - classification_loss: 0.0408
 503/1000 [==============>...............] - ETA: 18:32 - loss: 0.4440 - regression_loss: 0.4032 - classification_loss: 0.0408
 504/1000 [==============>...............] - ETA: 18:29 - loss: 0.4434 - regression_loss: 0.4026 - classification_loss: 0.0408
 505/1000 [==============>...............] - ETA: 18:26 - loss: 0.4434 - regression_loss: 0.4027 - classification_loss: 0.0408
 506/1000 [==============>...............] - ETA: 18:24 - loss: 0.4433 - regression_loss: 0.4025 - classification_loss: 0.0407
 507/1000 [==============>...............] - ETA: 18:22 - loss: 0.4436 - regression_loss: 0.4028 - classification_loss: 0.0407
 508/1000 [==============>...............] - ETA: 18:20 - loss: 0.4435 - regression_loss: 0.4027 - classification_loss: 0.0408
 509/1000 [==============>...............] - ETA: 18:18 - loss: 0.4436 - regression_loss: 0.4029 - classification_loss: 0.0408
 510/1000 [==============>...............] - ETA: 18:16 - loss: 0.4438 - regression_loss: 0.4031 - classification_loss: 0.0408
 511/1000 [==============>...............] - ETA: 18:13 - loss: 0.4435 - regression_loss: 0.4028 - classification_loss: 0.0407
 512/1000 [==============>...............] - ETA: 18:11 - loss: 0.4434 - regression_loss: 0.4027 - classification_loss: 0.0408
 513/1000 [==============>...............] - ETA: 18:08 - loss: 0.4427 - regression_loss: 0.4020 - classification_loss: 0.0407
 514/1000 [==============>...............] - ETA: 18:06 - loss: 0.4428 - regression_loss: 0.4022 - classification_loss: 0.0407
 515/1000 [==============>...............] - ETA: 18:04 - loss: 0.4429 - regression_loss: 0.4022 - classification_loss: 0.0407
 516/1000 [==============>...............] - ETA: 18:02 - loss: 0.4431 - regression_loss: 0.4024 - classification_loss: 0.0407
 517/1000 [==============>...............] - ETA: 17:59 - loss: 0.4427 - regression_loss: 0.4020 - classification_loss: 0.0407
 518/1000 [==============>...............] - ETA: 17:57 - loss: 0.4426 - regression_loss: 0.4019 - classification_loss: 0.0407
 519/1000 [==============>...............] - ETA: 17:56 - loss: 0.4428 - regression_loss: 0.4021 - classification_loss: 0.0407
 520/1000 [==============>...............] - ETA: 17:53 - loss: 0.4429 - regression_loss: 0.4022 - classification_loss: 0.0407
 521/1000 [==============>...............] - ETA: 17:51 - loss: 0.4431 - regression_loss: 0.4024 - classification_loss: 0.0407
 522/1000 [==============>...............] - ETA: 17:49 - loss: 0.4432 - regression_loss: 0.4025 - classification_loss: 0.0407
 523/1000 [==============>...............] - ETA: 17:46 - loss: 0.4425 - regression_loss: 0.4019 - classification_loss: 0.0407
 524/1000 [==============>...............] - ETA: 17:44 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 525/1000 [==============>...............] - ETA: 17:41 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 526/1000 [==============>...............] - ETA: 17:39 - loss: 0.4418 - regression_loss: 0.4012 - classification_loss: 0.0406
 527/1000 [==============>...............] - ETA: 17:37 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 528/1000 [==============>...............] - ETA: 17:35 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 529/1000 [==============>...............] - ETA: 17:33 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 530/1000 [==============>...............] - ETA: 17:31 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 531/1000 [==============>...............] - ETA: 17:28 - loss: 0.4418 - regression_loss: 0.4012 - classification_loss: 0.0406
 532/1000 [==============>...............] - ETA: 17:25 - loss: 0.4411 - regression_loss: 0.4006 - classification_loss: 0.0405
 533/1000 [==============>...............] - ETA: 17:23 - loss: 0.4412 - regression_loss: 0.4007 - classification_loss: 0.0406
 534/1000 [===============>..............] - ETA: 17:21 - loss: 0.4412 - regression_loss: 0.4006 - classification_loss: 0.0406
 535/1000 [===============>..............] - ETA: 17:19 - loss: 0.4415 - regression_loss: 0.4009 - classification_loss: 0.0406
 536/1000 [===============>..............] - ETA: 17:17 - loss: 0.4417 - regression_loss: 0.4011 - classification_loss: 0.0406
 537/1000 [===============>..............] - ETA: 17:15 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 538/1000 [===============>..............] - ETA: 17:13 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 539/1000 [===============>..............] - ETA: 17:11 - loss: 0.4422 - regression_loss: 0.4016 - classification_loss: 0.0406
 540/1000 [===============>..............] - ETA: 17:09 - loss: 0.4422 - regression_loss: 0.4016 - classification_loss: 0.0406
 541/1000 [===============>..............] - ETA: 17:07 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 542/1000 [===============>..............] - ETA: 17:05 - loss: 0.4424 - regression_loss: 0.4017 - classification_loss: 0.0406
 543/1000 [===============>..............] - ETA: 17:02 - loss: 0.4417 - regression_loss: 0.4012 - classification_loss: 0.0406
 544/1000 [===============>..............] - ETA: 17:00 - loss: 0.4418 - regression_loss: 0.4012 - classification_loss: 0.0406
 545/1000 [===============>..............] - ETA: 16:58 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 546/1000 [===============>..............] - ETA: 16:56 - loss: 0.4424 - regression_loss: 0.4018 - classification_loss: 0.0406
 547/1000 [===============>..............] - ETA: 16:53 - loss: 0.4421 - regression_loss: 0.4015 - classification_loss: 0.0406
 548/1000 [===============>..............] - ETA: 16:50 - loss: 0.4414 - regression_loss: 0.4009 - classification_loss: 0.0405
 549/1000 [===============>..............] - ETA: 16:48 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0405
 550/1000 [===============>..............] - ETA: 16:46 - loss: 0.4417 - regression_loss: 0.4012 - classification_loss: 0.0405
 551/1000 [===============>..............] - ETA: 16:44 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0406
 552/1000 [===============>..............] - ETA: 16:41 - loss: 0.4414 - regression_loss: 0.4009 - classification_loss: 0.0405
 553/1000 [===============>..............] - ETA: 16:39 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0405
 554/1000 [===============>..............] - ETA: 16:37 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 555/1000 [===============>..............] - ETA: 16:35 - loss: 0.4420 - regression_loss: 0.4014 - classification_loss: 0.0406
 556/1000 [===============>..............] - ETA: 16:33 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0406
 557/1000 [===============>..............] - ETA: 16:31 - loss: 0.4422 - regression_loss: 0.4017 - classification_loss: 0.0406
 558/1000 [===============>..............] - ETA: 16:28 - loss: 0.4417 - regression_loss: 0.4012 - classification_loss: 0.0405
 559/1000 [===============>..............] - ETA: 16:26 - loss: 0.4421 - regression_loss: 0.4016 - classification_loss: 0.0405
 560/1000 [===============>..............] - ETA: 16:24 - loss: 0.4424 - regression_loss: 0.4018 - classification_loss: 0.0406
 561/1000 [===============>..............] - ETA: 16:22 - loss: 0.4427 - regression_loss: 0.4021 - classification_loss: 0.0406
 562/1000 [===============>..............] - ETA: 16:20 - loss: 0.4426 - regression_loss: 0.4021 - classification_loss: 0.0406
 563/1000 [===============>..............] - ETA: 16:18 - loss: 0.4420 - regression_loss: 0.4015 - classification_loss: 0.0405
 564/1000 [===============>..............] - ETA: 16:15 - loss: 0.4419 - regression_loss: 0.4013 - classification_loss: 0.0405
 565/1000 [===============>..............] - ETA: 16:13 - loss: 0.4420 - regression_loss: 0.4014 - classification_loss: 0.0405
 566/1000 [===============>..............] - ETA: 16:11 - loss: 0.4421 - regression_loss: 0.4016 - classification_loss: 0.0405
 567/1000 [================>.............] - ETA: 16:08 - loss: 0.4415 - regression_loss: 0.4011 - classification_loss: 0.0405
 568/1000 [================>.............] - ETA: 16:06 - loss: 0.4418 - regression_loss: 0.4013 - classification_loss: 0.0405
 569/1000 [================>.............] - ETA: 16:03 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0405
 570/1000 [================>.............] - ETA: 16:01 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0405
 571/1000 [================>.............] - ETA: 15:59 - loss: 0.4418 - regression_loss: 0.4013 - classification_loss: 0.0405
 572/1000 [================>.............] - ETA: 15:57 - loss: 0.4418 - regression_loss: 0.4013 - classification_loss: 0.0405
 573/1000 [================>.............] - ETA: 15:55 - loss: 0.4417 - regression_loss: 0.4012 - classification_loss: 0.0405
 574/1000 [================>.............] - ETA: 15:53 - loss: 0.4420 - regression_loss: 0.4015 - classification_loss: 0.0405
 575/1000 [================>.............] - ETA: 15:50 - loss: 0.4414 - regression_loss: 0.4009 - classification_loss: 0.0405
 576/1000 [================>.............] - ETA: 15:48 - loss: 0.4417 - regression_loss: 0.4012 - classification_loss: 0.0405
 577/1000 [================>.............] - ETA: 15:46 - loss: 0.4418 - regression_loss: 0.4013 - classification_loss: 0.0405
 578/1000 [================>.............] - ETA: 15:44 - loss: 0.4415 - regression_loss: 0.4010 - classification_loss: 0.0405
 579/1000 [================>.............] - ETA: 15:41 - loss: 0.4415 - regression_loss: 0.4010 - classification_loss: 0.0405
 580/1000 [================>.............] - ETA: 15:39 - loss: 0.4415 - regression_loss: 0.4010 - classification_loss: 0.0404
 581/1000 [================>.............] - ETA: 15:36 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 582/1000 [================>.............] - ETA: 15:34 - loss: 0.4408 - regression_loss: 0.4004 - classification_loss: 0.0404
 583/1000 [================>.............] - ETA: 15:31 - loss: 0.4405 - regression_loss: 0.4001 - classification_loss: 0.0404
 584/1000 [================>.............] - ETA: 15:29 - loss: 0.4408 - regression_loss: 0.4004 - classification_loss: 0.0404
 585/1000 [================>.............] - ETA: 15:27 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 586/1000 [================>.............] - ETA: 15:25 - loss: 0.4411 - regression_loss: 0.4007 - classification_loss: 0.0404
 587/1000 [================>.............] - ETA: 15:23 - loss: 0.4411 - regression_loss: 0.4007 - classification_loss: 0.0404
 588/1000 [================>.............] - ETA: 15:21 - loss: 0.4413 - regression_loss: 0.4009 - classification_loss: 0.0404
 589/1000 [================>.............] - ETA: 15:19 - loss: 0.4416 - regression_loss: 0.4011 - classification_loss: 0.0404
 590/1000 [================>.............] - ETA: 15:17 - loss: 0.4418 - regression_loss: 0.4014 - classification_loss: 0.0405
 591/1000 [================>.............] - ETA: 15:15 - loss: 0.4415 - regression_loss: 0.4011 - classification_loss: 0.0404
 592/1000 [================>.............] - ETA: 15:13 - loss: 0.4415 - regression_loss: 0.4010 - classification_loss: 0.0404
 593/1000 [================>.............] - ETA: 15:10 - loss: 0.4410 - regression_loss: 0.4006 - classification_loss: 0.0404
 594/1000 [================>.............] - ETA: 15:07 - loss: 0.4405 - regression_loss: 0.4002 - classification_loss: 0.0404
 595/1000 [================>.............] - ETA: 15:05 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 596/1000 [================>.............] - ETA: 15:03 - loss: 0.4408 - regression_loss: 0.4004 - classification_loss: 0.0404
 597/1000 [================>.............] - ETA: 15:01 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 598/1000 [================>.............] - ETA: 14:58 - loss: 0.4406 - regression_loss: 0.4002 - classification_loss: 0.0404
 599/1000 [================>.............] - ETA: 14:56 - loss: 0.4408 - regression_loss: 0.4004 - classification_loss: 0.0404
 600/1000 [=================>............] - ETA: 14:54 - loss: 0.4412 - regression_loss: 0.4008 - classification_loss: 0.0404
 601/1000 [=================>............] - ETA: 14:52 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 602/1000 [=================>............] - ETA: 14:49 - loss: 0.4409 - regression_loss: 0.4005 - classification_loss: 0.0404
 603/1000 [=================>............] - ETA: 14:47 - loss: 0.4405 - regression_loss: 0.4002 - classification_loss: 0.0403
 604/1000 [=================>............] - ETA: 14:45 - loss: 0.4404 - regression_loss: 0.4001 - classification_loss: 0.0403
 605/1000 [=================>............] - ETA: 14:42 - loss: 0.4406 - regression_loss: 0.4003 - classification_loss: 0.0403
 606/1000 [=================>............] - ETA: 14:41 - loss: 0.4409 - regression_loss: 0.4006 - classification_loss: 0.0403
 607/1000 [=================>............] - ETA: 14:39 - loss: 0.4412 - regression_loss: 0.4008 - classification_loss: 0.0404
 608/1000 [=================>............] - ETA: 14:36 - loss: 0.4413 - regression_loss: 0.4009 - classification_loss: 0.0404
 609/1000 [=================>............] - ETA: 14:34 - loss: 0.4411 - regression_loss: 0.4008 - classification_loss: 0.0403
 610/1000 [=================>............] - ETA: 14:31 - loss: 0.4408 - regression_loss: 0.4005 - classification_loss: 0.0403
 611/1000 [=================>............] - ETA: 14:29 - loss: 0.4414 - regression_loss: 0.4010 - classification_loss: 0.0403
 612/1000 [=================>............] - ETA: 14:27 - loss: 0.4417 - regression_loss: 0.4013 - classification_loss: 0.0403
 613/1000 [=================>............] - ETA: 14:25 - loss: 0.4422 - regression_loss: 0.4018 - classification_loss: 0.0404
 614/1000 [=================>............] - ETA: 14:23 - loss: 0.4424 - regression_loss: 0.4020 - classification_loss: 0.0404
 615/1000 [=================>............] - ETA: 14:20 - loss: 0.4418 - regression_loss: 0.4015 - classification_loss: 0.0403
 616/1000 [=================>............] - ETA: 14:18 - loss: 0.4418 - regression_loss: 0.4014 - classification_loss: 0.0403
 617/1000 [=================>............] - ETA: 14:16 - loss: 0.4421 - regression_loss: 0.4018 - classification_loss: 0.0403
 618/1000 [=================>............] - ETA: 14:14 - loss: 0.4424 - regression_loss: 0.4021 - classification_loss: 0.0403
 619/1000 [=================>............] - ETA: 14:12 - loss: 0.4423 - regression_loss: 0.4020 - classification_loss: 0.0403
 620/1000 [=================>............] - ETA: 14:09 - loss: 0.4425 - regression_loss: 0.4021 - classification_loss: 0.0403
 621/1000 [=================>............] - ETA: 14:07 - loss: 0.4426 - regression_loss: 0.4023 - classification_loss: 0.0403
 622/1000 [=================>............] - ETA: 14:05 - loss: 0.4421 - regression_loss: 0.4018 - classification_loss: 0.0403
 623/1000 [=================>............] - ETA: 14:03 - loss: 0.4424 - regression_loss: 0.4021 - classification_loss: 0.0403
 624/1000 [=================>............] - ETA: 14:01 - loss: 0.4426 - regression_loss: 0.4023 - classification_loss: 0.0403
 625/1000 [=================>............] - ETA: 13:59 - loss: 0.4428 - regression_loss: 0.4025 - classification_loss: 0.0403
 626/1000 [=================>............] - ETA: 13:56 - loss: 0.4428 - regression_loss: 0.4025 - classification_loss: 0.0403
 627/1000 [=================>............] - ETA: 13:54 - loss: 0.4425 - regression_loss: 0.4022 - classification_loss: 0.0403
 628/1000 [=================>............] - ETA: 13:52 - loss: 0.4425 - regression_loss: 0.4022 - classification_loss: 0.0403
 629/1000 [=================>............] - ETA: 13:50 - loss: 0.4424 - regression_loss: 0.4021 - classification_loss: 0.0403
 630/1000 [=================>............] - ETA: 13:47 - loss: 0.4424 - regression_loss: 0.4022 - classification_loss: 0.0403
 631/1000 [=================>............] - ETA: 13:45 - loss: 0.4419 - regression_loss: 0.4016 - classification_loss: 0.0403
 632/1000 [=================>............] - ETA: 13:43 - loss: 0.4421 - regression_loss: 0.4018 - classification_loss: 0.0403
 633/1000 [=================>............] - ETA: 13:41 - loss: 0.4422 - regression_loss: 0.4019 - classification_loss: 0.0403
 634/1000 [==================>...........] - ETA: 13:39 - loss: 0.4424 - regression_loss: 0.4021 - classification_loss: 0.0403
 635/1000 [==================>...........] - ETA: 13:36 - loss: 0.4421 - regression_loss: 0.4018 - classification_loss: 0.0403
 636/1000 [==================>...........] - ETA: 13:34 - loss: 0.4418 - regression_loss: 0.4015 - classification_loss: 0.0402
 637/1000 [==================>...........] - ETA: 13:32 - loss: 0.4420 - regression_loss: 0.4018 - classification_loss: 0.0402
 638/1000 [==================>...........] - ETA: 13:29 - loss: 0.4420 - regression_loss: 0.4018 - classification_loss: 0.0402
 639/1000 [==================>...........] - ETA: 13:27 - loss: 0.4422 - regression_loss: 0.4020 - classification_loss: 0.0402
 640/1000 [==================>...........] - ETA: 13:25 - loss: 0.4422 - regression_loss: 0.4019 - classification_loss: 0.0403
 641/1000 [==================>...........] - ETA: 13:23 - loss: 0.4423 - regression_loss: 0.4020 - classification_loss: 0.0403
 642/1000 [==================>...........] - ETA: 13:20 - loss: 0.4417 - regression_loss: 0.4015 - classification_loss: 0.0402
 643/1000 [==================>...........] - ETA: 13:18 - loss: 0.4418 - regression_loss: 0.4016 - classification_loss: 0.0402
 644/1000 [==================>...........] - ETA: 13:16 - loss: 0.4420 - regression_loss: 0.4018 - classification_loss: 0.0402
 645/1000 [==================>...........] - ETA: 13:14 - loss: 0.4418 - regression_loss: 0.4016 - classification_loss: 0.0402
 646/1000 [==================>...........] - ETA: 13:12 - loss: 0.4420 - regression_loss: 0.4018 - classification_loss: 0.0402
 647/1000 [==================>...........] - ETA: 13:10 - loss: 0.4421 - regression_loss: 0.4019 - classification_loss: 0.0402
 648/1000 [==================>...........] - ETA: 13:07 - loss: 0.4416 - regression_loss: 0.4014 - classification_loss: 0.0402
 649/1000 [==================>...........] - ETA: 13:05 - loss: 0.4415 - regression_loss: 0.4013 - classification_loss: 0.0402
 650/1000 [==================>...........] - ETA: 13:03 - loss: 0.4416 - regression_loss: 0.4014 - classification_loss: 0.0402
 651/1000 [==================>...........] - ETA: 13:01 - loss: 0.4418 - regression_loss: 0.4016 - classification_loss: 0.0402
 652/1000 [==================>...........] - ETA: 12:59 - loss: 0.4417 - regression_loss: 0.4015 - classification_loss: 0.0402
 653/1000 [==================>...........] - ETA: 12:56 - loss: 0.4414 - regression_loss: 0.4012 - classification_loss: 0.0402
 654/1000 [==================>...........] - ETA: 12:54 - loss: 0.4414 - regression_loss: 0.4012 - classification_loss: 0.0402
 655/1000 [==================>...........] - ETA: 12:52 - loss: 0.4416 - regression_loss: 0.4014 - classification_loss: 0.0402
 656/1000 [==================>...........] - ETA: 12:49 - loss: 0.4411 - regression_loss: 0.4010 - classification_loss: 0.0401
 657/1000 [==================>...........] - ETA: 12:47 - loss: 0.4414 - regression_loss: 0.4012 - classification_loss: 0.0402
 658/1000 [==================>...........] - ETA: 12:45 - loss: 0.4414 - regression_loss: 0.4013 - classification_loss: 0.0402
 659/1000 [==================>...........] - ETA: 12:43 - loss: 0.4411 - regression_loss: 0.4010 - classification_loss: 0.0401
 660/1000 [==================>...........] - ETA: 12:41 - loss: 0.4414 - regression_loss: 0.4012 - classification_loss: 0.0401
 661/1000 [==================>...........] - ETA: 12:38 - loss: 0.4414 - regression_loss: 0.4013 - classification_loss: 0.0401
 662/1000 [==================>...........] - ETA: 12:36 - loss: 0.4413 - regression_loss: 0.4012 - classification_loss: 0.0401
 663/1000 [==================>...........] - ETA: 12:34 - loss: 0.4408 - regression_loss: 0.4007 - classification_loss: 0.0401
 664/1000 [==================>...........] - ETA: 12:31 - loss: 0.4403 - regression_loss: 0.4003 - classification_loss: 0.0401
 665/1000 [==================>...........] - ETA: 12:29 - loss: 0.4405 - regression_loss: 0.4005 - classification_loss: 0.0401
 666/1000 [==================>...........] - ETA: 12:27 - loss: 0.4403 - regression_loss: 0.4002 - classification_loss: 0.0400
 667/1000 [===================>..........] - ETA: 12:24 - loss: 0.4404 - regression_loss: 0.4003 - classification_loss: 0.0400
 668/1000 [===================>..........] - ETA: 12:22 - loss: 0.4403 - regression_loss: 0.4003 - classification_loss: 0.0400
 669/1000 [===================>..........] - ETA: 12:20 - loss: 0.4405 - regression_loss: 0.4005 - classification_loss: 0.0401
 670/1000 [===================>..........] - ETA: 12:18 - loss: 0.4408 - regression_loss: 0.4007 - classification_loss: 0.0401
 671/1000 [===================>..........] - ETA: 12:16 - loss: 0.4410 - regression_loss: 0.4009 - classification_loss: 0.0401
 672/1000 [===================>..........] - ETA: 12:14 - loss: 0.4409 - regression_loss: 0.4008 - classification_loss: 0.0401
 673/1000 [===================>..........] - ETA: 12:12 - loss: 0.4410 - regression_loss: 0.4009 - classification_loss: 0.0401
 674/1000 [===================>..........] - ETA: 12:09 - loss: 0.4410 - regression_loss: 0.4009 - classification_loss: 0.0401
 675/1000 [===================>..........] - ETA: 12:07 - loss: 0.4408 - regression_loss: 0.4007 - classification_loss: 0.0401
 676/1000 [===================>..........] - ETA: 12:05 - loss: 0.4410 - regression_loss: 0.4009 - classification_loss: 0.0401
 677/1000 [===================>..........] - ETA: 12:02 - loss: 0.4408 - regression_loss: 0.4008 - classification_loss: 0.0400
 678/1000 [===================>..........] - ETA: 12:00 - loss: 0.4405 - regression_loss: 0.4005 - classification_loss: 0.0400
 679/1000 [===================>..........] - ETA: 11:58 - loss: 0.4405 - regression_loss: 0.4005 - classification_loss: 0.0400
 680/1000 [===================>..........] - ETA: 11:55 - loss: 0.4401 - regression_loss: 0.4001 - classification_loss: 0.0400
 681/1000 [===================>..........] - ETA: 11:53 - loss: 0.4402 - regression_loss: 0.4002 - classification_loss: 0.0400
 682/1000 [===================>..........] - ETA: 11:51 - loss: 0.4404 - regression_loss: 0.4004 - classification_loss: 0.0400
 683/1000 [===================>..........] - ETA: 11:49 - loss: 0.4406 - regression_loss: 0.4006 - classification_loss: 0.0400
 684/1000 [===================>..........] - ETA: 11:47 - loss: 0.4408 - regression_loss: 0.4008 - classification_loss: 0.0400
 685/1000 [===================>..........] - ETA: 11:45 - loss: 0.4410 - regression_loss: 0.4010 - classification_loss: 0.0400
 686/1000 [===================>..........] - ETA: 11:42 - loss: 0.4410 - regression_loss: 0.4010 - classification_loss: 0.0400
 687/1000 [===================>..........] - ETA: 11:40 - loss: 0.4412 - regression_loss: 0.4012 - classification_loss: 0.0400
 688/1000 [===================>..........] - ETA: 11:38 - loss: 0.4413 - regression_loss: 0.4013 - classification_loss: 0.0400
 689/1000 [===================>..........] - ETA: 11:36 - loss: 0.4415 - regression_loss: 0.4015 - classification_loss: 0.0400
 690/1000 [===================>..........] - ETA: 11:34 - loss: 0.4418 - regression_loss: 0.4018 - classification_loss: 0.0400
 691/1000 [===================>..........] - ETA: 11:31 - loss: 0.4413 - regression_loss: 0.4013 - classification_loss: 0.0400
 692/1000 [===================>..........] - ETA: 11:29 - loss: 0.4409 - regression_loss: 0.4009 - classification_loss: 0.0400
 693/1000 [===================>..........] - ETA: 11:26 - loss: 0.4411 - regression_loss: 0.4012 - classification_loss: 0.0400
 694/1000 [===================>..........] - ETA: 11:24 - loss: 0.4413 - regression_loss: 0.4013 - classification_loss: 0.0400
 695/1000 [===================>..........] - ETA: 11:22 - loss: 0.4416 - regression_loss: 0.4016 - classification_loss: 0.0400
 696/1000 [===================>..........] - ETA: 11:20 - loss: 0.4418 - regression_loss: 0.4018 - classification_loss: 0.0400
 697/1000 [===================>..........] - ETA: 11:18 - loss: 0.4418 - regression_loss: 0.4017 - classification_loss: 0.0400
 698/1000 [===================>..........] - ETA: 11:15 - loss: 0.4416 - regression_loss: 0.4015 - classification_loss: 0.0400
 699/1000 [===================>..........] - ETA: 11:13 - loss: 0.4419 - regression_loss: 0.4018 - classification_loss: 0.0401
 700/1000 [====================>.........] - ETA: 11:11 - loss: 0.4420 - regression_loss: 0.4019 - classification_loss: 0.0401
 701/1000 [====================>.........] - ETA: 11:09 - loss: 0.4422 - regression_loss: 0.4021 - classification_loss: 0.0401
 702/1000 [====================>.........] - ETA: 11:07 - loss: 0.4423 - regression_loss: 0.4022 - classification_loss: 0.0401
 703/1000 [====================>.........] - ETA: 11:04 - loss: 0.4419 - regression_loss: 0.4019 - classification_loss: 0.0400
 704/1000 [====================>.........] - ETA: 11:02 - loss: 0.4424 - regression_loss: 0.4023 - classification_loss: 0.0400
 705/1000 [====================>.........] - ETA: 11:00 - loss: 0.4425 - regression_loss: 0.4024 - classification_loss: 0.0400
 706/1000 [====================>.........] - ETA: 10:58 - loss: 0.4427 - regression_loss: 0.4027 - classification_loss: 0.0400
 707/1000 [====================>.........] - ETA: 10:55 - loss: 0.4429 - regression_loss: 0.4029 - classification_loss: 0.0400
 708/1000 [====================>.........] - ETA: 10:53 - loss: 0.4433 - regression_loss: 0.4032 - classification_loss: 0.0400
 709/1000 [====================>.........] - ETA: 10:51 - loss: 0.4429 - regression_loss: 0.4029 - classification_loss: 0.0400
 710/1000 [====================>.........] - ETA: 10:48 - loss: 0.4427 - regression_loss: 0.4027 - classification_loss: 0.0400
 711/1000 [====================>.........] - ETA: 10:46 - loss: 0.4430 - regression_loss: 0.4029 - classification_loss: 0.0400
 712/1000 [====================>.........] - ETA: 10:44 - loss: 0.4431 - regression_loss: 0.4031 - classification_loss: 0.0400
 713/1000 [====================>.........] - ETA: 10:42 - loss: 0.4431 - regression_loss: 0.4031 - classification_loss: 0.0400
 714/1000 [====================>.........] - ETA: 10:40 - loss: 0.4433 - regression_loss: 0.4033 - classification_loss: 0.0400
 715/1000 [====================>.........] - ETA: 10:38 - loss: 0.4435 - regression_loss: 0.4034 - classification_loss: 0.0400
 716/1000 [====================>.........] - ETA: 10:35 - loss: 0.4433 - regression_loss: 0.4033 - classification_loss: 0.0400
 717/1000 [====================>.........] - ETA: 10:33 - loss: 0.4428 - regression_loss: 0.4029 - classification_loss: 0.0400
 718/1000 [====================>.........] - ETA: 10:31 - loss: 0.4428 - regression_loss: 0.4029 - classification_loss: 0.0400
 719/1000 [====================>.........] - ETA: 10:28 - loss: 0.4430 - regression_loss: 0.4030 - classification_loss: 0.0400
 720/1000 [====================>.........] - ETA: 10:26 - loss: 0.4425 - regression_loss: 0.4025 - classification_loss: 0.0399
 721/1000 [====================>.........] - ETA: 10:24 - loss: 0.4424 - regression_loss: 0.4024 - classification_loss: 0.0399
 722/1000 [====================>.........] - ETA: 10:22 - loss: 0.4425 - regression_loss: 0.4025 - classification_loss: 0.0399
 723/1000 [====================>.........] - ETA: 10:19 - loss: 0.4425 - regression_loss: 0.4026 - classification_loss: 0.0399
 724/1000 [====================>.........] - ETA: 10:17 - loss: 0.4423 - regression_loss: 0.4023 - classification_loss: 0.0399
 725/1000 [====================>.........] - ETA: 10:15 - loss: 0.4425 - regression_loss: 0.4026 - classification_loss: 0.0399
 726/1000 [====================>.........] - ETA: 10:13 - loss: 0.4428 - regression_loss: 0.4029 - classification_loss: 0.0400
 727/1000 [====================>.........] - ETA: 10:11 - loss: 0.4429 - regression_loss: 0.4029 - classification_loss: 0.0400
 728/1000 [====================>.........] - ETA: 10:08 - loss: 0.4429 - regression_loss: 0.4029 - classification_loss: 0.0400
 729/1000 [====================>.........] - ETA: 10:06 - loss: 0.4431 - regression_loss: 0.4031 - classification_loss: 0.0400
 730/1000 [====================>.........] - ETA: 10:04 - loss: 0.4433 - regression_loss: 0.4033 - classification_loss: 0.0400
 731/1000 [====================>.........] - ETA: 10:02 - loss: 0.4431 - regression_loss: 0.4031 - classification_loss: 0.0400
 732/1000 [====================>.........] - ETA: 9:59 - loss: 0.4426 - regression_loss: 0.4027 - classification_loss: 0.0399 
 733/1000 [====================>.........] - ETA: 9:57 - loss: 0.4428 - regression_loss: 0.4028 - classification_loss: 0.0399
 734/1000 [=====================>........] - ETA: 9:55 - loss: 0.4425 - regression_loss: 0.4026 - classification_loss: 0.0399
 735/1000 [=====================>........] - ETA: 9:53 - loss: 0.4426 - regression_loss: 0.4027 - classification_loss: 0.0399
 736/1000 [=====================>........] - ETA: 9:50 - loss: 0.4427 - regression_loss: 0.4027 - classification_loss: 0.0399
 737/1000 [=====================>........] - ETA: 9:48 - loss: 0.4428 - regression_loss: 0.4029 - classification_loss: 0.0400
 738/1000 [=====================>........] - ETA: 9:46 - loss: 0.4423 - regression_loss: 0.4024 - classification_loss: 0.0399
 739/1000 [=====================>........] - ETA: 9:44 - loss: 0.4426 - regression_loss: 0.4026 - classification_loss: 0.0399
 740/1000 [=====================>........] - ETA: 9:42 - loss: 0.4425 - regression_loss: 0.4026 - classification_loss: 0.0399
 741/1000 [=====================>........] - ETA: 9:39 - loss: 0.4420 - regression_loss: 0.4021 - classification_loss: 0.0399
 742/1000 [=====================>........] - ETA: 9:37 - loss: 0.4422 - regression_loss: 0.4023 - classification_loss: 0.0399
 743/1000 [=====================>........] - ETA: 9:35 - loss: 0.4422 - regression_loss: 0.4023 - classification_loss: 0.0399
 744/1000 [=====================>........] - ETA: 9:33 - loss: 0.4423 - regression_loss: 0.4024 - classification_loss: 0.0399
 745/1000 [=====================>........] - ETA: 9:30 - loss: 0.4423 - regression_loss: 0.4023 - classification_loss: 0.0399
 746/1000 [=====================>........] - ETA: 9:28 - loss: 0.4424 - regression_loss: 0.4025 - classification_loss: 0.0399
 747/1000 [=====================>........] - ETA: 9:26 - loss: 0.4423 - regression_loss: 0.4024 - classification_loss: 0.0399
 748/1000 [=====================>........] - ETA: 9:23 - loss: 0.4418 - regression_loss: 0.4020 - classification_loss: 0.0399
 749/1000 [=====================>........] - ETA: 9:21 - loss: 0.4420 - regression_loss: 0.4022 - classification_loss: 0.0399
 750/1000 [=====================>........] - ETA: 9:19 - loss: 0.4418 - regression_loss: 0.4019 - classification_loss: 0.0399
 751/1000 [=====================>........] - ETA: 9:17 - loss: 0.4419 - regression_loss: 0.4020 - classification_loss: 0.0399
 752/1000 [=====================>........] - ETA: 9:14 - loss: 0.4419 - regression_loss: 0.4020 - classification_loss: 0.0399
 753/1000 [=====================>........] - ETA: 9:12 - loss: 0.4421 - regression_loss: 0.4022 - classification_loss: 0.0399
 754/1000 [=====================>........] - ETA: 9:10 - loss: 0.4420 - regression_loss: 0.4021 - classification_loss: 0.0399
 755/1000 [=====================>........] - ETA: 9:08 - loss: 0.4419 - regression_loss: 0.4020 - classification_loss: 0.0399
 756/1000 [=====================>........] - ETA: 9:06 - loss: 0.4420 - regression_loss: 0.4021 - classification_loss: 0.0399
 757/1000 [=====================>........] - ETA: 9:04 - loss: 0.4422 - regression_loss: 0.4023 - classification_loss: 0.0399
 758/1000 [=====================>........] - ETA: 9:01 - loss: 0.4418 - regression_loss: 0.4019 - classification_loss: 0.0399
 759/1000 [=====================>........] - ETA: 8:59 - loss: 0.4418 - regression_loss: 0.4020 - classification_loss: 0.0399
 760/1000 [=====================>........] - ETA: 8:57 - loss: 0.4418 - regression_loss: 0.4020 - classification_loss: 0.0399
 761/1000 [=====================>........] - ETA: 8:55 - loss: 0.4417 - regression_loss: 0.4018 - classification_loss: 0.0398
 762/1000 [=====================>........] - ETA: 8:52 - loss: 0.4418 - regression_loss: 0.4019 - classification_loss: 0.0399
 763/1000 [=====================>........] - ETA: 8:50 - loss: 0.4417 - regression_loss: 0.4018 - classification_loss: 0.0399
 764/1000 [=====================>........] - ETA: 8:48 - loss: 0.4418 - regression_loss: 0.4019 - classification_loss: 0.0399
 765/1000 [=====================>........] - ETA: 8:46 - loss: 0.4419 - regression_loss: 0.4020 - classification_loss: 0.0399
 766/1000 [=====================>........] - ETA: 8:43 - loss: 0.4416 - regression_loss: 0.4018 - classification_loss: 0.0399
 767/1000 [======================>.......] - ETA: 8:41 - loss: 0.4419 - regression_loss: 0.4020 - classification_loss: 0.0399
 768/1000 [======================>.......] - ETA: 8:39 - loss: 0.4414 - regression_loss: 0.4016 - classification_loss: 0.0398
 769/1000 [======================>.......] - ETA: 8:37 - loss: 0.4417 - regression_loss: 0.4019 - classification_loss: 0.0398
 770/1000 [======================>.......] - ETA: 8:35 - loss: 0.4420 - regression_loss: 0.4021 - classification_loss: 0.0398
 771/1000 [======================>.......] - ETA: 8:32 - loss: 0.4415 - regression_loss: 0.4017 - classification_loss: 0.0398
 772/1000 [======================>.......] - ETA: 8:30 - loss: 0.4415 - regression_loss: 0.4017 - classification_loss: 0.0398
 773/1000 [======================>.......] - ETA: 8:28 - loss: 0.4413 - regression_loss: 0.4015 - classification_loss: 0.0398
 774/1000 [======================>.......] - ETA: 8:25 - loss: 0.4414 - regression_loss: 0.4016 - classification_loss: 0.0398
 775/1000 [======================>.......] - ETA: 8:23 - loss: 0.4413 - regression_loss: 0.4015 - classification_loss: 0.0398
 776/1000 [======================>.......] - ETA: 8:21 - loss: 0.4415 - regression_loss: 0.4017 - classification_loss: 0.0398
 777/1000 [======================>.......] - ETA: 8:19 - loss: 0.4414 - regression_loss: 0.4016 - classification_loss: 0.0398
 778/1000 [======================>.......] - ETA: 8:17 - loss: 0.4414 - regression_loss: 0.4015 - classification_loss: 0.0398
 779/1000 [======================>.......] - ETA: 8:14 - loss: 0.4411 - regression_loss: 0.4013 - classification_loss: 0.0398
 780/1000 [======================>.......] - ETA: 8:12 - loss: 0.4406 - regression_loss: 0.4008 - classification_loss: 0.0398
 781/1000 [======================>.......] - ETA: 8:10 - loss: 0.4407 - regression_loss: 0.4010 - classification_loss: 0.0398
 782/1000 [======================>.......] - ETA: 8:08 - loss: 0.4410 - regression_loss: 0.4012 - classification_loss: 0.0398
 783/1000 [======================>.......] - ETA: 8:05 - loss: 0.4407 - regression_loss: 0.4009 - classification_loss: 0.0398
 784/1000 [======================>.......] - ETA: 8:03 - loss: 0.4408 - regression_loss: 0.4010 - classification_loss: 0.0398
 785/1000 [======================>.......] - ETA: 8:01 - loss: 0.4404 - regression_loss: 0.4007 - classification_loss: 0.0397
 786/1000 [======================>.......] - ETA: 7:58 - loss: 0.4404 - regression_loss: 0.4007 - classification_loss: 0.0397
 787/1000 [======================>.......] - ETA: 7:56 - loss: 0.4405 - regression_loss: 0.4008 - classification_loss: 0.0397
 788/1000 [======================>.......] - ETA: 7:54 - loss: 0.4407 - regression_loss: 0.4010 - classification_loss: 0.0397
 789/1000 [======================>.......] - ETA: 7:52 - loss: 0.4406 - regression_loss: 0.4009 - classification_loss: 0.0397
 790/1000 [======================>.......] - ETA: 7:50 - loss: 0.4406 - regression_loss: 0.4009 - classification_loss: 0.0397
 791/1000 [======================>.......] - ETA: 7:47 - loss: 0.4407 - regression_loss: 0.4010 - classification_loss: 0.0397
 792/1000 [======================>.......] - ETA: 7:45 - loss: 0.4408 - regression_loss: 0.4011 - classification_loss: 0.0397
 793/1000 [======================>.......] - ETA: 7:43 - loss: 0.4404 - regression_loss: 0.4007 - classification_loss: 0.0397
 794/1000 [======================>.......] - ETA: 7:41 - loss: 0.4405 - regression_loss: 0.4008 - classification_loss: 0.0397
 795/1000 [======================>.......] - ETA: 7:39 - loss: 0.4404 - regression_loss: 0.4007 - classification_loss: 0.0397
 796/1000 [======================>.......] - ETA: 7:36 - loss: 0.4402 - regression_loss: 0.4005 - classification_loss: 0.0397
 797/1000 [======================>.......] - ETA: 7:34 - loss: 0.4403 - regression_loss: 0.4006 - classification_loss: 0.0397
 798/1000 [======================>.......] - ETA: 7:32 - loss: 0.4398 - regression_loss: 0.4001 - classification_loss: 0.0397
 799/1000 [======================>.......] - ETA: 7:29 - loss: 0.4395 - regression_loss: 0.3999 - classification_loss: 0.0396
 800/1000 [=======================>......] - ETA: 7:27 - loss: 0.4396 - regression_loss: 0.3999 - classification_loss: 0.0396
 801/1000 [=======================>......] - ETA: 7:25 - loss: 0.4397 - regression_loss: 0.4001 - classification_loss: 0.0397
 802/1000 [=======================>......] - ETA: 7:23 - loss: 0.4399 - regression_loss: 0.4002 - classification_loss: 0.0397
 803/1000 [=======================>......] - ETA: 7:21 - loss: 0.4400 - regression_loss: 0.4004 - classification_loss: 0.0397
 804/1000 [=======================>......] - ETA: 7:18 - loss: 0.4400 - regression_loss: 0.4003 - classification_loss: 0.0397
 805/1000 [=======================>......] - ETA: 7:16 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 806/1000 [=======================>......] - ETA: 7:14 - loss: 0.4397 - regression_loss: 0.4001 - classification_loss: 0.0396
 807/1000 [=======================>......] - ETA: 7:12 - loss: 0.4397 - regression_loss: 0.4001 - classification_loss: 0.0396
 808/1000 [=======================>......] - ETA: 7:09 - loss: 0.4395 - regression_loss: 0.3999 - classification_loss: 0.0396
 809/1000 [=======================>......] - ETA: 7:07 - loss: 0.4398 - regression_loss: 0.4002 - classification_loss: 0.0396
 810/1000 [=======================>......] - ETA: 7:05 - loss: 0.4401 - regression_loss: 0.4005 - classification_loss: 0.0396
 811/1000 [=======================>......] - ETA: 7:03 - loss: 0.4403 - regression_loss: 0.4006 - classification_loss: 0.0396
 812/1000 [=======================>......] - ETA: 7:00 - loss: 0.4403 - regression_loss: 0.4007 - classification_loss: 0.0396
 813/1000 [=======================>......] - ETA: 6:58 - loss: 0.4405 - regression_loss: 0.4008 - classification_loss: 0.0397
 814/1000 [=======================>......] - ETA: 6:56 - loss: 0.4407 - regression_loss: 0.4010 - classification_loss: 0.0397
 815/1000 [=======================>......] - ETA: 6:54 - loss: 0.4402 - regression_loss: 0.4006 - classification_loss: 0.0396
 816/1000 [=======================>......] - ETA: 6:51 - loss: 0.4401 - regression_loss: 0.4005 - classification_loss: 0.0396
 817/1000 [=======================>......] - ETA: 6:49 - loss: 0.4400 - regression_loss: 0.4004 - classification_loss: 0.0396
 818/1000 [=======================>......] - ETA: 6:47 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 819/1000 [=======================>......] - ETA: 6:44 - loss: 0.4394 - regression_loss: 0.3998 - classification_loss: 0.0396
 820/1000 [=======================>......] - ETA: 6:42 - loss: 0.4393 - regression_loss: 0.3997 - classification_loss: 0.0396
 821/1000 [=======================>......] - ETA: 6:40 - loss: 0.4393 - regression_loss: 0.3997 - classification_loss: 0.0396
 822/1000 [=======================>......] - ETA: 6:38 - loss: 0.4394 - regression_loss: 0.3998 - classification_loss: 0.0396
 823/1000 [=======================>......] - ETA: 6:36 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 824/1000 [=======================>......] - ETA: 6:34 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 825/1000 [=======================>......] - ETA: 6:31 - loss: 0.4395 - regression_loss: 0.3999 - classification_loss: 0.0396
 826/1000 [=======================>......] - ETA: 6:29 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 827/1000 [=======================>......] - ETA: 6:27 - loss: 0.4397 - regression_loss: 0.4001 - classification_loss: 0.0396
 828/1000 [=======================>......] - ETA: 6:25 - loss: 0.4398 - regression_loss: 0.4002 - classification_loss: 0.0396
 829/1000 [=======================>......] - ETA: 6:23 - loss: 0.4398 - regression_loss: 0.4002 - classification_loss: 0.0396
 830/1000 [=======================>......] - ETA: 6:20 - loss: 0.4396 - regression_loss: 0.4000 - classification_loss: 0.0396
 831/1000 [=======================>......] - ETA: 6:18 - loss: 0.4392 - regression_loss: 0.3997 - classification_loss: 0.0396
 832/1000 [=======================>......] - ETA: 6:16 - loss: 0.4392 - regression_loss: 0.3996 - classification_loss: 0.0396
 833/1000 [=======================>......] - ETA: 6:13 - loss: 0.4393 - regression_loss: 0.3997 - classification_loss: 0.0396
 834/1000 [========================>.....] - ETA: 6:11 - loss: 0.4391 - regression_loss: 0.3996 - classification_loss: 0.0396
 835/1000 [========================>.....] - ETA: 6:09 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 836/1000 [========================>.....] - ETA: 6:07 - loss: 0.4388 - regression_loss: 0.3993 - classification_loss: 0.0395
 837/1000 [========================>.....] - ETA: 6:04 - loss: 0.4389 - regression_loss: 0.3994 - classification_loss: 0.0395
 838/1000 [========================>.....] - ETA: 6:02 - loss: 0.4389 - regression_loss: 0.3994 - classification_loss: 0.0395
 839/1000 [========================>.....] - ETA: 6:00 - loss: 0.4391 - regression_loss: 0.3995 - classification_loss: 0.0395
 840/1000 [========================>.....] - ETA: 5:58 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 841/1000 [========================>.....] - ETA: 5:55 - loss: 0.4386 - regression_loss: 0.3991 - classification_loss: 0.0395
 842/1000 [========================>.....] - ETA: 5:53 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 843/1000 [========================>.....] - ETA: 5:51 - loss: 0.4388 - regression_loss: 0.3993 - classification_loss: 0.0395
 844/1000 [========================>.....] - ETA: 5:49 - loss: 0.4388 - regression_loss: 0.3993 - classification_loss: 0.0395
 845/1000 [========================>.....] - ETA: 5:46 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 846/1000 [========================>.....] - ETA: 5:44 - loss: 0.4388 - regression_loss: 0.3993 - classification_loss: 0.0395
 847/1000 [========================>.....] - ETA: 5:42 - loss: 0.4389 - regression_loss: 0.3994 - classification_loss: 0.0395
 848/1000 [========================>.....] - ETA: 5:40 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 849/1000 [========================>.....] - ETA: 5:38 - loss: 0.4387 - regression_loss: 0.3992 - classification_loss: 0.0395
 850/1000 [========================>.....] - ETA: 5:35 - loss: 0.4388 - regression_loss: 0.3993 - classification_loss: 0.0395
 851/1000 [========================>.....] - ETA: 5:33 - loss: 0.4384 - regression_loss: 0.3989 - classification_loss: 0.0395
 852/1000 [========================>.....] - ETA: 5:31 - loss: 0.4383 - regression_loss: 0.3988 - classification_loss: 0.0395
 853/1000 [========================>.....] - ETA: 5:28 - loss: 0.4380 - regression_loss: 0.3986 - classification_loss: 0.0394
 854/1000 [========================>.....] - ETA: 5:26 - loss: 0.4382 - regression_loss: 0.3987 - classification_loss: 0.0394
 855/1000 [========================>.....] - ETA: 5:24 - loss: 0.4378 - regression_loss: 0.3984 - classification_loss: 0.0394
 856/1000 [========================>.....] - ETA: 5:22 - loss: 0.4379 - regression_loss: 0.3985 - classification_loss: 0.0394
 857/1000 [========================>.....] - ETA: 5:20 - loss: 0.4380 - regression_loss: 0.3986 - classification_loss: 0.0394
 858/1000 [========================>.....] - ETA: 5:17 - loss: 0.4379 - regression_loss: 0.3985 - classification_loss: 0.0394
 859/1000 [========================>.....] - ETA: 5:15 - loss: 0.4379 - regression_loss: 0.3985 - classification_loss: 0.0394
 860/1000 [========================>.....] - ETA: 5:13 - loss: 0.4379 - regression_loss: 0.3985 - classification_loss: 0.0394
 861/1000 [========================>.....] - ETA: 5:11 - loss: 0.4378 - regression_loss: 0.3984 - classification_loss: 0.0394
 862/1000 [========================>.....] - ETA: 5:08 - loss: 0.4376 - regression_loss: 0.3982 - classification_loss: 0.0394
 863/1000 [========================>.....] - ETA: 5:06 - loss: 0.4377 - regression_loss: 0.3983 - classification_loss: 0.0394
 864/1000 [========================>.....] - ETA: 5:04 - loss: 0.4373 - regression_loss: 0.3979 - classification_loss: 0.0394
 865/1000 [========================>.....] - ETA: 5:02 - loss: 0.4374 - regression_loss: 0.3980 - classification_loss: 0.0394
 866/1000 [========================>.....] - ETA: 4:59 - loss: 0.4374 - regression_loss: 0.3980 - classification_loss: 0.0394
 867/1000 [=========================>....] - ETA: 4:57 - loss: 0.4372 - regression_loss: 0.3978 - classification_loss: 0.0394
 868/1000 [=========================>....] - ETA: 4:55 - loss: 0.4373 - regression_loss: 0.3980 - classification_loss: 0.0394
 869/1000 [=========================>....] - ETA: 4:53 - loss: 0.4374 - regression_loss: 0.3980 - classification_loss: 0.0394
 870/1000 [=========================>....] - ETA: 4:51 - loss: 0.4374 - regression_loss: 0.3981 - classification_loss: 0.0394
 871/1000 [=========================>....] - ETA: 4:48 - loss: 0.4371 - regression_loss: 0.3977 - classification_loss: 0.0393
 872/1000 [=========================>....] - ETA: 4:46 - loss: 0.4370 - regression_loss: 0.3977 - classification_loss: 0.0393
 873/1000 [=========================>....] - ETA: 4:44 - loss: 0.4372 - regression_loss: 0.3978 - classification_loss: 0.0394
 874/1000 [=========================>....] - ETA: 4:42 - loss: 0.4373 - regression_loss: 0.3979 - classification_loss: 0.0394
 875/1000 [=========================>....] - ETA: 4:39 - loss: 0.4369 - regression_loss: 0.3975 - classification_loss: 0.0393
 876/1000 [=========================>....] - ETA: 4:37 - loss: 0.4368 - regression_loss: 0.3975 - classification_loss: 0.0393
 877/1000 [=========================>....] - ETA: 4:35 - loss: 0.4368 - regression_loss: 0.3975 - classification_loss: 0.0393
 878/1000 [=========================>....] - ETA: 4:33 - loss: 0.4369 - regression_loss: 0.3976 - classification_loss: 0.0393
 879/1000 [=========================>....] - ETA: 4:30 - loss: 0.4367 - regression_loss: 0.3974 - classification_loss: 0.0393
 880/1000 [=========================>....] - ETA: 4:28 - loss: 0.4368 - regression_loss: 0.3975 - classification_loss: 0.0393
 881/1000 [=========================>....] - ETA: 4:26 - loss: 0.4365 - regression_loss: 0.3972 - classification_loss: 0.0393
 882/1000 [=========================>....] - ETA: 4:24 - loss: 0.4366 - regression_loss: 0.3973 - classification_loss: 0.0393
 883/1000 [=========================>....] - ETA: 4:21 - loss: 0.4365 - regression_loss: 0.3972 - classification_loss: 0.0393
 884/1000 [=========================>....] - ETA: 4:19 - loss: 0.4366 - regression_loss: 0.3973 - classification_loss: 0.0393
 885/1000 [=========================>....] - ETA: 4:17 - loss: 0.4367 - regression_loss: 0.3974 - classification_loss: 0.0393
 886/1000 [=========================>....] - ETA: 4:15 - loss: 0.4367 - regression_loss: 0.3974 - classification_loss: 0.0393
 887/1000 [=========================>....] - ETA: 4:12 - loss: 0.4365 - regression_loss: 0.3972 - classification_loss: 0.0393
 888/1000 [=========================>....] - ETA: 4:10 - loss: 0.4361 - regression_loss: 0.3968 - classification_loss: 0.0393
 889/1000 [=========================>....] - ETA: 4:08 - loss: 0.4359 - regression_loss: 0.3966 - classification_loss: 0.0392
 890/1000 [=========================>....] - ETA: 4:06 - loss: 0.4359 - regression_loss: 0.3967 - classification_loss: 0.0392
 891/1000 [=========================>....] - ETA: 4:03 - loss: 0.4361 - regression_loss: 0.3968 - classification_loss: 0.0393
 892/1000 [=========================>....] - ETA: 4:01 - loss: 0.4360 - regression_loss: 0.3967 - classification_loss: 0.0393
 893/1000 [=========================>....] - ETA: 3:59 - loss: 0.4360 - regression_loss: 0.3967 - classification_loss: 0.0392
 894/1000 [=========================>....] - ETA: 3:57 - loss: 0.4361 - regression_loss: 0.3968 - classification_loss: 0.0393
 895/1000 [=========================>....] - ETA: 3:55 - loss: 0.4362 - regression_loss: 0.3970 - classification_loss: 0.0393
 896/1000 [=========================>....] - ETA: 3:52 - loss: 0.4361 - regression_loss: 0.3969 - classification_loss: 0.0393
 897/1000 [=========================>....] - ETA: 3:50 - loss: 0.4361 - regression_loss: 0.3968 - classification_loss: 0.0392
 898/1000 [=========================>....] - ETA: 3:48 - loss: 0.4361 - regression_loss: 0.3969 - classification_loss: 0.0393
 899/1000 [=========================>....] - ETA: 3:46 - loss: 0.4361 - regression_loss: 0.3969 - classification_loss: 0.0393
 900/1000 [==========================>...] - ETA: 3:43 - loss: 0.4363 - regression_loss: 0.3970 - classification_loss: 0.0393
 901/1000 [==========================>...] - ETA: 3:41 - loss: 0.4359 - regression_loss: 0.3967 - classification_loss: 0.0392
 902/1000 [==========================>...] - ETA: 3:39 - loss: 0.4357 - regression_loss: 0.3965 - classification_loss: 0.0392
 903/1000 [==========================>...] - ETA: 3:37 - loss: 0.4358 - regression_loss: 0.3966 - classification_loss: 0.0392
 904/1000 [==========================>...] - ETA: 3:34 - loss: 0.4359 - regression_loss: 0.3967 - classification_loss: 0.0392
 905/1000 [==========================>...] - ETA: 3:32 - loss: 0.4359 - regression_loss: 0.3967 - classification_loss: 0.0392
 906/1000 [==========================>...] - ETA: 3:30 - loss: 0.4358 - regression_loss: 0.3966 - classification_loss: 0.0392
 907/1000 [==========================>...] - ETA: 3:28 - loss: 0.4359 - regression_loss: 0.3967 - classification_loss: 0.0392
 908/1000 [==========================>...] - ETA: 3:25 - loss: 0.4355 - regression_loss: 0.3963 - classification_loss: 0.0392
 909/1000 [==========================>...] - ETA: 3:23 - loss: 0.4354 - regression_loss: 0.3962 - classification_loss: 0.0392
 910/1000 [==========================>...] - ETA: 3:21 - loss: 0.4354 - regression_loss: 0.3962 - classification_loss: 0.0392
 911/1000 [==========================>...] - ETA: 3:19 - loss: 0.4352 - regression_loss: 0.3960 - classification_loss: 0.0392
 912/1000 [==========================>...] - ETA: 3:16 - loss: 0.4353 - regression_loss: 0.3961 - classification_loss: 0.0392
 913/1000 [==========================>...] - ETA: 3:14 - loss: 0.4354 - regression_loss: 0.3962 - classification_loss: 0.0392
 914/1000 [==========================>...] - ETA: 3:12 - loss: 0.4355 - regression_loss: 0.3964 - classification_loss: 0.0392
 915/1000 [==========================>...] - ETA: 3:10 - loss: 0.4353 - regression_loss: 0.3961 - classification_loss: 0.0392
 916/1000 [==========================>...] - ETA: 3:08 - loss: 0.4354 - regression_loss: 0.3962 - classification_loss: 0.0392
 917/1000 [==========================>...] - ETA: 3:05 - loss: 0.4355 - regression_loss: 0.3964 - classification_loss: 0.0392
 918/1000 [==========================>...] - ETA: 3:03 - loss: 0.4356 - regression_loss: 0.3965 - classification_loss: 0.0392
 919/1000 [==========================>...] - ETA: 3:01 - loss: 0.4352 - regression_loss: 0.3961 - classification_loss: 0.0391
 920/1000 [==========================>...] - ETA: 2:59 - loss: 0.4351 - regression_loss: 0.3960 - classification_loss: 0.0391
 921/1000 [==========================>...] - ETA: 2:56 - loss: 0.4351 - regression_loss: 0.3960 - classification_loss: 0.0391
 922/1000 [==========================>...] - ETA: 2:54 - loss: 0.4351 - regression_loss: 0.3959 - classification_loss: 0.0391
 923/1000 [==========================>...] - ETA: 2:52 - loss: 0.4347 - regression_loss: 0.3956 - classification_loss: 0.0391
 924/1000 [==========================>...] - ETA: 2:50 - loss: 0.4345 - regression_loss: 0.3954 - classification_loss: 0.0391
 925/1000 [==========================>...] - ETA: 2:47 - loss: 0.4347 - regression_loss: 0.3956 - classification_loss: 0.0391
 926/1000 [==========================>...] - ETA: 2:45 - loss: 0.4347 - regression_loss: 0.3956 - classification_loss: 0.0391
 927/1000 [==========================>...] - ETA: 2:43 - loss: 0.4347 - regression_loss: 0.3956 - classification_loss: 0.0391
 928/1000 [==========================>...] - ETA: 2:41 - loss: 0.4348 - regression_loss: 0.3957 - classification_loss: 0.0391
 929/1000 [==========================>...] - ETA: 2:38 - loss: 0.4348 - regression_loss: 0.3957 - classification_loss: 0.0391
 930/1000 [==========================>...] - ETA: 2:36 - loss: 0.4346 - regression_loss: 0.3955 - classification_loss: 0.0391
 931/1000 [==========================>...] - ETA: 2:34 - loss: 0.4342 - regression_loss: 0.3952 - classification_loss: 0.0390
 932/1000 [==========================>...] - ETA: 2:32 - loss: 0.4342 - regression_loss: 0.3952 - classification_loss: 0.0390
 933/1000 [==========================>...] - ETA: 2:29 - loss: 0.4344 - regression_loss: 0.3953 - classification_loss: 0.0390
 934/1000 [===========================>..] - ETA: 2:27 - loss: 0.4343 - regression_loss: 0.3953 - classification_loss: 0.0390
 935/1000 [===========================>..] - ETA: 2:25 - loss: 0.4344 - regression_loss: 0.3954 - classification_loss: 0.0390
 936/1000 [===========================>..] - ETA: 2:23 - loss: 0.4345 - regression_loss: 0.3954 - classification_loss: 0.0390
 937/1000 [===========================>..] - ETA: 2:21 - loss: 0.4346 - regression_loss: 0.3955 - classification_loss: 0.0391
 938/1000 [===========================>..] - ETA: 2:18 - loss: 0.4346 - regression_loss: 0.3956 - classification_loss: 0.0391
 939/1000 [===========================>..] - ETA: 2:16 - loss: 0.4347 - regression_loss: 0.3957 - classification_loss: 0.0391
 940/1000 [===========================>..] - ETA: 2:14 - loss: 0.4347 - regression_loss: 0.3956 - classification_loss: 0.0391
 941/1000 [===========================>..] - ETA: 2:12 - loss: 0.4345 - regression_loss: 0.3955 - classification_loss: 0.0390
 942/1000 [===========================>..] - ETA: 2:09 - loss: 0.4343 - regression_loss: 0.3953 - classification_loss: 0.0390
 943/1000 [===========================>..] - ETA: 2:07 - loss: 0.4343 - regression_loss: 0.3953 - classification_loss: 0.0390
 944/1000 [===========================>..] - ETA: 2:05 - loss: 0.4344 - regression_loss: 0.3954 - classification_loss: 0.0390
 945/1000 [===========================>..] - ETA: 2:03 - loss: 0.4344 - regression_loss: 0.3954 - classification_loss: 0.0390
 946/1000 [===========================>..] - ETA: 2:00 - loss: 0.4342 - regression_loss: 0.3952 - classification_loss: 0.0390
 947/1000 [===========================>..] - ETA: 1:58 - loss: 0.4338 - regression_loss: 0.3949 - classification_loss: 0.0390
 948/1000 [===========================>..] - ETA: 1:56 - loss: 0.4340 - regression_loss: 0.3950 - classification_loss: 0.0390
 949/1000 [===========================>..] - ETA: 1:54 - loss: 0.4341 - regression_loss: 0.3951 - classification_loss: 0.0390
 950/1000 [===========================>..] - ETA: 1:51 - loss: 0.4340 - regression_loss: 0.3951 - classification_loss: 0.0390
 951/1000 [===========================>..] - ETA: 1:49 - loss: 0.4342 - regression_loss: 0.3952 - classification_loss: 0.0390
 952/1000 [===========================>..] - ETA: 1:47 - loss: 0.4339 - regression_loss: 0.3950 - classification_loss: 0.0390
 953/1000 [===========================>..] - ETA: 1:45 - loss: 0.4340 - regression_loss: 0.3950 - classification_loss: 0.0390
 954/1000 [===========================>..] - ETA: 1:42 - loss: 0.4340 - regression_loss: 0.3950 - classification_loss: 0.0390
 955/1000 [===========================>..] - ETA: 1:40 - loss: 0.4337 - regression_loss: 0.3948 - classification_loss: 0.0389
 956/1000 [===========================>..] - ETA: 1:38 - loss: 0.4338 - regression_loss: 0.3949 - classification_loss: 0.0389
 957/1000 [===========================>..] - ETA: 1:36 - loss: 0.4338 - regression_loss: 0.3949 - classification_loss: 0.0389
 958/1000 [===========================>..] - ETA: 1:33 - loss: 0.4334 - regression_loss: 0.3945 - classification_loss: 0.0389
 959/1000 [===========================>..] - ETA: 1:31 - loss: 0.4335 - regression_loss: 0.3946 - classification_loss: 0.0389
 960/1000 [===========================>..] - ETA: 1:29 - loss: 0.4334 - regression_loss: 0.3945 - classification_loss: 0.0389
 961/1000 [===========================>..] - ETA: 1:27 - loss: 0.4334 - regression_loss: 0.3945 - classification_loss: 0.0389
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.4333 - regression_loss: 0.3945 - classification_loss: 0.0389
 963/1000 [===========================>..] - ETA: 1:22 - loss: 0.4335 - regression_loss: 0.3945 - classification_loss: 0.0389
 964/1000 [===========================>..] - ETA: 1:20 - loss: 0.4335 - regression_loss: 0.3946 - classification_loss: 0.0389
 965/1000 [===========================>..] - ETA: 1:18 - loss: 0.4333 - regression_loss: 0.3944 - classification_loss: 0.0389
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.4333 - regression_loss: 0.3944 - classification_loss: 0.0389
 967/1000 [============================>.] - ETA: 1:13 - loss: 0.4334 - regression_loss: 0.3945 - classification_loss: 0.0389
 968/1000 [============================>.] - ETA: 1:11 - loss: 0.4332 - regression_loss: 0.3943 - classification_loss: 0.0389
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.4331 - regression_loss: 0.3943 - classification_loss: 0.0389
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.4333 - regression_loss: 0.3944 - classification_loss: 0.0389
 971/1000 [============================>.] - ETA: 1:04 - loss: 0.4332 - regression_loss: 0.3943 - classification_loss: 0.0389
 972/1000 [============================>.] - ETA: 1:02 - loss: 0.4333 - regression_loss: 0.3944 - classification_loss: 0.0389
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.4332 - regression_loss: 0.3943 - classification_loss: 0.0389
 974/1000 [============================>.] - ETA: 58s - loss: 0.4330 - regression_loss: 0.3941 - classification_loss: 0.0388 
 975/1000 [============================>.] - ETA: 55s - loss: 0.4331 - regression_loss: 0.3942 - classification_loss: 0.0389
 976/1000 [============================>.] - ETA: 53s - loss: 0.4331 - regression_loss: 0.3942 - classification_loss: 0.0389
 977/1000 [============================>.] - ETA: 51s - loss: 0.4329 - regression_loss: 0.3941 - classification_loss: 0.0388
 978/1000 [============================>.] - ETA: 49s - loss: 0.4329 - regression_loss: 0.3941 - classification_loss: 0.0388
 979/1000 [============================>.] - ETA: 47s - loss: 0.4327 - regression_loss: 0.3939 - classification_loss: 0.0388
 980/1000 [============================>.] - ETA: 44s - loss: 0.4328 - regression_loss: 0.3940 - classification_loss: 0.0388
 981/1000 [============================>.] - ETA: 42s - loss: 0.4324 - regression_loss: 0.3936 - classification_loss: 0.0388
 982/1000 [============================>.] - ETA: 40s - loss: 0.4325 - regression_loss: 0.3937 - classification_loss: 0.0388
 983/1000 [============================>.] - ETA: 38s - loss: 0.4324 - regression_loss: 0.3936 - classification_loss: 0.0388
 984/1000 [============================>.] - ETA: 35s - loss: 0.4325 - regression_loss: 0.3937 - classification_loss: 0.0388
 985/1000 [============================>.] - ETA: 33s - loss: 0.4325 - regression_loss: 0.3937 - classification_loss: 0.0388
 986/1000 [============================>.] - ETA: 31s - loss: 0.4321 - regression_loss: 0.3934 - classification_loss: 0.0388
 987/1000 [============================>.] - ETA: 29s - loss: 0.4322 - regression_loss: 0.3934 - classification_loss: 0.0388
 988/1000 [============================>.] - ETA: 26s - loss: 0.4321 - regression_loss: 0.3933 - classification_loss: 0.0388
 989/1000 [============================>.] - ETA: 24s - loss: 0.4319 - regression_loss: 0.3932 - classification_loss: 0.0387
 990/1000 [============================>.] - ETA: 22s - loss: 0.4320 - regression_loss: 0.3933 - classification_loss: 0.0387
 991/1000 [============================>.] - ETA: 20s - loss: 0.4322 - regression_loss: 0.3934 - classification_loss: 0.0388
 992/1000 [============================>.] - ETA: 17s - loss: 0.4322 - regression_loss: 0.3934 - classification_loss: 0.0387
 993/1000 [============================>.] - ETA: 15s - loss: 0.4323 - regression_loss: 0.3935 - classification_loss: 0.0388
 994/1000 [============================>.] - ETA: 13s - loss: 0.4324 - regression_loss: 0.3937 - classification_loss: 0.0388
 995/1000 [============================>.] - ETA: 11s - loss: 0.4321 - regression_loss: 0.3934 - classification_loss: 0.0387
 996/1000 [============================>.] - ETA: 8s - loss: 0.4321 - regression_loss: 0.3933 - classification_loss: 0.0387 
 997/1000 [============================>.] - ETA: 6s - loss: 0.4319 - regression_loss: 0.3931 - classification_loss: 0.0387
 998/1000 [============================>.] - ETA: 4s - loss: 0.4319 - regression_loss: 0.3932 - classification_loss: 0.0387
 999/1000 [============================>.] - ETA: 2s - loss: 0.4319 - regression_loss: 0.3931 - classification_loss: 0.0387
1000/1000 [==============================] - 2239s 2s/step - loss: 0.4320 - regression_loss: 0.3932 - classification_loss: 0.0387

Epoch 00006: saving model to ./snapshots/resnet50_csv_06.h5
Epoch 7/10

   1/1000 [..............................] - ETA: 28:07 - loss: 0.3412 - regression_loss: 0.3032 - classification_loss: 0.0381
   2/1000 [..............................] - ETA: 31:52 - loss: 0.3751 - regression_loss: 0.3381 - classification_loss: 0.0369
   3/1000 [..............................] - ETA: 36:17 - loss: 0.4265 - regression_loss: 0.3878 - classification_loss: 0.0387
   4/1000 [..............................] - ETA: 34:21 - loss: 0.3892 - regression_loss: 0.3539 - classification_loss: 0.0353
   5/1000 [..............................] - ETA: 32:38 - loss: 0.3321 - regression_loss: 0.3019 - classification_loss: 0.0303
   6/1000 [..............................] - ETA: 34:07 - loss: 0.3636 - regression_loss: 0.3301 - classification_loss: 0.0334
   7/1000 [..............................] - ETA: 35:13 - loss: 0.3865 - regression_loss: 0.3504 - classification_loss: 0.0362
   8/1000 [..............................] - ETA: 36:12 - loss: 0.4096 - regression_loss: 0.3720 - classification_loss: 0.0376
   9/1000 [..............................] - ETA: 35:01 - loss: 0.3756 - regression_loss: 0.3411 - classification_loss: 0.0344
  10/1000 [..............................] - ETA: 34:19 - loss: 0.3727 - regression_loss: 0.3387 - classification_loss: 0.0340
  11/1000 [..............................] - ETA: 35:15 - loss: 0.3912 - regression_loss: 0.3558 - classification_loss: 0.0354
  12/1000 [..............................] - ETA: 35:37 - loss: 0.3900 - regression_loss: 0.3538 - classification_loss: 0.0362
  13/1000 [..............................] - ETA: 35:34 - loss: 0.3940 - regression_loss: 0.3575 - classification_loss: 0.0365
  14/1000 [..............................] - ETA: 34:50 - loss: 0.3703 - regression_loss: 0.3358 - classification_loss: 0.0346
  15/1000 [..............................] - ETA: 35:08 - loss: 0.3702 - regression_loss: 0.3353 - classification_loss: 0.0349
  16/1000 [..............................] - ETA: 35:08 - loss: 0.3729 - regression_loss: 0.3379 - classification_loss: 0.0350
  17/1000 [..............................] - ETA: 34:41 - loss: 0.3631 - regression_loss: 0.3287 - classification_loss: 0.0344
  18/1000 [..............................] - ETA: 35:05 - loss: 0.3727 - regression_loss: 0.3379 - classification_loss: 0.0348
  19/1000 [..............................] - ETA: 35:22 - loss: 0.3787 - regression_loss: 0.3437 - classification_loss: 0.0351
  20/1000 [..............................] - ETA: 35:47 - loss: 0.3863 - regression_loss: 0.3508 - classification_loss: 0.0355
  21/1000 [..............................] - ETA: 36:03 - loss: 0.3932 - regression_loss: 0.3573 - classification_loss: 0.0359
  22/1000 [..............................] - ETA: 36:17 - loss: 0.3966 - regression_loss: 0.3606 - classification_loss: 0.0361
  23/1000 [..............................] - ETA: 36:35 - loss: 0.4023 - regression_loss: 0.3660 - classification_loss: 0.0364
  24/1000 [..............................] - ETA: 36:28 - loss: 0.4027 - regression_loss: 0.3665 - classification_loss: 0.0363
  25/1000 [..............................] - ETA: 36:33 - loss: 0.4005 - regression_loss: 0.3641 - classification_loss: 0.0363
  26/1000 [..............................] - ETA: 36:05 - loss: 0.3876 - regression_loss: 0.3523 - classification_loss: 0.0353
  27/1000 [..............................] - ETA: 35:45 - loss: 0.3850 - regression_loss: 0.3500 - classification_loss: 0.0351
  28/1000 [..............................] - ETA: 36:02 - loss: 0.3910 - regression_loss: 0.3556 - classification_loss: 0.0354
  29/1000 [..............................] - ETA: 36:07 - loss: 0.3898 - regression_loss: 0.3543 - classification_loss: 0.0355
  30/1000 [..............................] - ETA: 35:44 - loss: 0.3783 - regression_loss: 0.3438 - classification_loss: 0.0346
  31/1000 [..............................] - ETA: 35:51 - loss: 0.3813 - regression_loss: 0.3466 - classification_loss: 0.0347
  32/1000 [..............................] - ETA: 36:00 - loss: 0.3860 - regression_loss: 0.3510 - classification_loss: 0.0350
  33/1000 [..............................] - ETA: 35:42 - loss: 0.3812 - regression_loss: 0.3465 - classification_loss: 0.0347
  34/1000 [>.............................] - ETA: 35:38 - loss: 0.3819 - regression_loss: 0.3473 - classification_loss: 0.0347
  35/1000 [>.............................] - ETA: 35:50 - loss: 0.3858 - regression_loss: 0.3509 - classification_loss: 0.0349
  36/1000 [>.............................] - ETA: 35:56 - loss: 0.3875 - regression_loss: 0.3526 - classification_loss: 0.0350
  37/1000 [>.............................] - ETA: 35:36 - loss: 0.3787 - regression_loss: 0.3444 - classification_loss: 0.0343
  38/1000 [>.............................] - ETA: 35:32 - loss: 0.3797 - regression_loss: 0.3454 - classification_loss: 0.0343
  39/1000 [>.............................] - ETA: 35:35 - loss: 0.3790 - regression_loss: 0.3447 - classification_loss: 0.0344
  40/1000 [>.............................] - ETA: 35:41 - loss: 0.3831 - regression_loss: 0.3485 - classification_loss: 0.0346
  41/1000 [>.............................] - ETA: 35:28 - loss: 0.3798 - regression_loss: 0.3454 - classification_loss: 0.0344
  42/1000 [>.............................] - ETA: 35:24 - loss: 0.3804 - regression_loss: 0.3460 - classification_loss: 0.0344
  43/1000 [>.............................] - ETA: 35:11 - loss: 0.3764 - regression_loss: 0.3423 - classification_loss: 0.0341
  44/1000 [>.............................] - ETA: 35:15 - loss: 0.3790 - regression_loss: 0.3448 - classification_loss: 0.0342
  45/1000 [>.............................] - ETA: 35:18 - loss: 0.3791 - regression_loss: 0.3448 - classification_loss: 0.0343
  46/1000 [>.............................] - ETA: 35:23 - loss: 0.3825 - regression_loss: 0.3480 - classification_loss: 0.0345
  47/1000 [>.............................] - ETA: 35:08 - loss: 0.3757 - regression_loss: 0.3417 - classification_loss: 0.0339
  48/1000 [>.............................] - ETA: 35:16 - loss: 0.3792 - regression_loss: 0.3450 - classification_loss: 0.0342
  49/1000 [>.............................] - ETA: 35:18 - loss: 0.3789 - regression_loss: 0.3445 - classification_loss: 0.0344
  50/1000 [>.............................] - ETA: 35:03 - loss: 0.3724 - regression_loss: 0.3385 - classification_loss: 0.0338
  51/1000 [>.............................] - ETA: 35:07 - loss: 0.3756 - regression_loss: 0.3416 - classification_loss: 0.0340
  52/1000 [>.............................] - ETA: 35:04 - loss: 0.3763 - regression_loss: 0.3423 - classification_loss: 0.0340
  53/1000 [>.............................] - ETA: 35:07 - loss: 0.3777 - regression_loss: 0.3436 - classification_loss: 0.0341
  54/1000 [>.............................] - ETA: 35:15 - loss: 0.3803 - regression_loss: 0.3461 - classification_loss: 0.0343
  55/1000 [>.............................] - ETA: 35:04 - loss: 0.3800 - regression_loss: 0.3459 - classification_loss: 0.0341
  56/1000 [>.............................] - ETA: 35:07 - loss: 0.3817 - regression_loss: 0.3475 - classification_loss: 0.0343
  57/1000 [>.............................] - ETA: 35:10 - loss: 0.3845 - regression_loss: 0.3500 - classification_loss: 0.0344
  58/1000 [>.............................] - ETA: 35:07 - loss: 0.3848 - regression_loss: 0.3504 - classification_loss: 0.0344
  59/1000 [>.............................] - ETA: 34:56 - loss: 0.3823 - regression_loss: 0.3481 - classification_loss: 0.0343
  60/1000 [>.............................] - ETA: 34:57 - loss: 0.3822 - regression_loss: 0.3477 - classification_loss: 0.0345
  61/1000 [>.............................] - ETA: 35:02 - loss: 0.3851 - regression_loss: 0.3504 - classification_loss: 0.0347
  62/1000 [>.............................] - ETA: 34:50 - loss: 0.3804 - regression_loss: 0.3461 - classification_loss: 0.0344
  63/1000 [>.............................] - ETA: 34:52 - loss: 0.3820 - regression_loss: 0.3476 - classification_loss: 0.0344
  64/1000 [>.............................] - ETA: 34:57 - loss: 0.3846 - regression_loss: 0.3501 - classification_loss: 0.0345
  65/1000 [>.............................] - ETA: 34:54 - loss: 0.3850 - regression_loss: 0.3505 - classification_loss: 0.0345
  66/1000 [>.............................] - ETA: 34:57 - loss: 0.3873 - regression_loss: 0.3526 - classification_loss: 0.0347
  67/1000 [=>............................] - ETA: 34:45 - loss: 0.3840 - regression_loss: 0.3498 - classification_loss: 0.0343
  68/1000 [=>............................] - ETA: 34:45 - loss: 0.3838 - regression_loss: 0.3495 - classification_loss: 0.0344
  69/1000 [=>............................] - ETA: 34:36 - loss: 0.3833 - regression_loss: 0.3490 - classification_loss: 0.0343
  70/1000 [=>............................] - ETA: 34:37 - loss: 0.3828 - regression_loss: 0.3484 - classification_loss: 0.0343
  71/1000 [=>............................] - ETA: 34:26 - loss: 0.3784 - regression_loss: 0.3444 - classification_loss: 0.0340
  72/1000 [=>............................] - ETA: 34:28 - loss: 0.3806 - regression_loss: 0.3465 - classification_loss: 0.0341
  73/1000 [=>............................] - ETA: 34:25 - loss: 0.3810 - regression_loss: 0.3469 - classification_loss: 0.0341
  74/1000 [=>............................] - ETA: 34:26 - loss: 0.3822 - regression_loss: 0.3480 - classification_loss: 0.0342
  75/1000 [=>............................] - ETA: 34:17 - loss: 0.3801 - regression_loss: 0.3460 - classification_loss: 0.0341
  76/1000 [=>............................] - ETA: 34:21 - loss: 0.3824 - regression_loss: 0.3482 - classification_loss: 0.0342
  77/1000 [=>............................] - ETA: 34:17 - loss: 0.3830 - regression_loss: 0.3489 - classification_loss: 0.0342
  78/1000 [=>............................] - ETA: 34:17 - loss: 0.3826 - regression_loss: 0.3484 - classification_loss: 0.0342
  79/1000 [=>............................] - ETA: 34:07 - loss: 0.3793 - regression_loss: 0.3454 - classification_loss: 0.0339
  80/1000 [=>............................] - ETA: 34:10 - loss: 0.3810 - regression_loss: 0.3470 - classification_loss: 0.0341
  81/1000 [=>............................] - ETA: 34:02 - loss: 0.3793 - regression_loss: 0.3454 - classification_loss: 0.0339
  82/1000 [=>............................] - ETA: 34:04 - loss: 0.3805 - regression_loss: 0.3465 - classification_loss: 0.0340
  83/1000 [=>............................] - ETA: 34:06 - loss: 0.3823 - regression_loss: 0.3482 - classification_loss: 0.0341
  84/1000 [=>............................] - ETA: 34:03 - loss: 0.3825 - regression_loss: 0.3484 - classification_loss: 0.0341
  85/1000 [=>............................] - ETA: 34:04 - loss: 0.3842 - regression_loss: 0.3500 - classification_loss: 0.0342
  86/1000 [=>............................] - ETA: 33:57 - loss: 0.3824 - regression_loss: 0.3484 - classification_loss: 0.0341
  87/1000 [=>............................] - ETA: 33:48 - loss: 0.3787 - regression_loss: 0.3449 - classification_loss: 0.0338
  88/1000 [=>............................] - ETA: 33:48 - loss: 0.3784 - regression_loss: 0.3445 - classification_loss: 0.0339
  89/1000 [=>............................] - ETA: 33:48 - loss: 0.3793 - regression_loss: 0.3453 - classification_loss: 0.0339
  90/1000 [=>............................] - ETA: 33:51 - loss: 0.3808 - regression_loss: 0.3467 - classification_loss: 0.0340
  91/1000 [=>............................] - ETA: 33:42 - loss: 0.3774 - regression_loss: 0.3436 - classification_loss: 0.0338
  92/1000 [=>............................] - ETA: 33:43 - loss: 0.3790 - regression_loss: 0.3451 - classification_loss: 0.0339
  93/1000 [=>............................] - ETA: 33:41 - loss: 0.3793 - regression_loss: 0.3454 - classification_loss: 0.0339
  94/1000 [=>............................] - ETA: 33:40 - loss: 0.3788 - regression_loss: 0.3449 - classification_loss: 0.0339
  95/1000 [=>............................] - ETA: 33:42 - loss: 0.3803 - regression_loss: 0.3463 - classification_loss: 0.0340
  96/1000 [=>............................] - ETA: 33:36 - loss: 0.3788 - regression_loss: 0.3449 - classification_loss: 0.0339
  97/1000 [=>............................] - ETA: 33:37 - loss: 0.3796 - regression_loss: 0.3457 - classification_loss: 0.0340
  98/1000 [=>............................] - ETA: 33:37 - loss: 0.3805 - regression_loss: 0.3465 - classification_loss: 0.0340
  99/1000 [=>............................] - ETA: 33:34 - loss: 0.3811 - regression_loss: 0.3470 - classification_loss: 0.0341
 100/1000 [==>...........................] - ETA: 33:26 - loss: 0.3784 - regression_loss: 0.3446 - classification_loss: 0.0338
 101/1000 [==>...........................] - ETA: 33:27 - loss: 0.3800 - regression_loss: 0.3461 - classification_loss: 0.0339
 102/1000 [==>...........................] - ETA: 33:29 - loss: 0.3813 - regression_loss: 0.3473 - classification_loss: 0.0340
 103/1000 [==>...........................] - ETA: 33:28 - loss: 0.3810 - regression_loss: 0.3469 - classification_loss: 0.0341
 104/1000 [==>...........................] - ETA: 33:21 - loss: 0.3796 - regression_loss: 0.3456 - classification_loss: 0.0340
 105/1000 [==>...........................] - ETA: 33:19 - loss: 0.3808 - regression_loss: 0.3467 - classification_loss: 0.0340
 106/1000 [==>...........................] - ETA: 33:20 - loss: 0.3826 - regression_loss: 0.3485 - classification_loss: 0.0341
 107/1000 [==>...........................] - ETA: 33:20 - loss: 0.3838 - regression_loss: 0.3496 - classification_loss: 0.0342
 108/1000 [==>...........................] - ETA: 33:20 - loss: 0.3838 - regression_loss: 0.3496 - classification_loss: 0.0342
 109/1000 [==>...........................] - ETA: 33:13 - loss: 0.3823 - regression_loss: 0.3482 - classification_loss: 0.0342
 110/1000 [==>...........................] - ETA: 33:14 - loss: 0.3837 - regression_loss: 0.3495 - classification_loss: 0.0342
 111/1000 [==>...........................] - ETA: 33:07 - loss: 0.3807 - regression_loss: 0.3467 - classification_loss: 0.0340
 112/1000 [==>...........................] - ETA: 33:08 - loss: 0.3820 - regression_loss: 0.3479 - classification_loss: 0.0341
 113/1000 [==>...........................] - ETA: 33:01 - loss: 0.3790 - regression_loss: 0.3452 - classification_loss: 0.0339
 114/1000 [==>...........................] - ETA: 33:00 - loss: 0.3789 - regression_loss: 0.3449 - classification_loss: 0.0339
 115/1000 [==>...........................] - ETA: 32:57 - loss: 0.3790 - regression_loss: 0.3451 - classification_loss: 0.0339
 116/1000 [==>...........................] - ETA: 32:57 - loss: 0.3797 - regression_loss: 0.3457 - classification_loss: 0.0339
 117/1000 [==>...........................] - ETA: 32:51 - loss: 0.3785 - regression_loss: 0.3447 - classification_loss: 0.0338
 118/1000 [==>...........................] - ETA: 32:51 - loss: 0.3800 - regression_loss: 0.3461 - classification_loss: 0.0339
 119/1000 [==>...........................] - ETA: 32:51 - loss: 0.3808 - regression_loss: 0.3468 - classification_loss: 0.0340
 120/1000 [==>...........................] - ETA: 32:44 - loss: 0.3788 - regression_loss: 0.3450 - classification_loss: 0.0338
 121/1000 [==>...........................] - ETA: 32:41 - loss: 0.3790 - regression_loss: 0.3452 - classification_loss: 0.0338
 122/1000 [==>...........................] - ETA: 32:40 - loss: 0.3787 - regression_loss: 0.3449 - classification_loss: 0.0338
 123/1000 [==>...........................] - ETA: 32:35 - loss: 0.3774 - regression_loss: 0.3437 - classification_loss: 0.0337
 124/1000 [==>...........................] - ETA: 32:36 - loss: 0.3786 - regression_loss: 0.3448 - classification_loss: 0.0338
 125/1000 [==>...........................] - ETA: 32:37 - loss: 0.3797 - regression_loss: 0.3458 - classification_loss: 0.0338
 126/1000 [==>...........................] - ETA: 32:30 - loss: 0.3772 - regression_loss: 0.3436 - classification_loss: 0.0336
 127/1000 [==>...........................] - ETA: 32:24 - loss: 0.3766 - regression_loss: 0.3430 - classification_loss: 0.0336
 128/1000 [==>...........................] - ETA: 32:25 - loss: 0.3783 - regression_loss: 0.3447 - classification_loss: 0.0336
 129/1000 [==>...........................] - ETA: 32:24 - loss: 0.3785 - regression_loss: 0.3448 - classification_loss: 0.0337
 130/1000 [==>...........................] - ETA: 32:24 - loss: 0.3792 - regression_loss: 0.3455 - classification_loss: 0.0337
 131/1000 [==>...........................] - ETA: 32:21 - loss: 0.3795 - regression_loss: 0.3458 - classification_loss: 0.0337
 132/1000 [==>...........................] - ETA: 32:21 - loss: 0.3809 - regression_loss: 0.3471 - classification_loss: 0.0338
 133/1000 [==>...........................] - ETA: 32:22 - loss: 0.3819 - regression_loss: 0.3480 - classification_loss: 0.0338
 134/1000 [===>..........................] - ETA: 32:22 - loss: 0.3824 - regression_loss: 0.3486 - classification_loss: 0.0339
 135/1000 [===>..........................] - ETA: 32:16 - loss: 0.3814 - regression_loss: 0.3476 - classification_loss: 0.0338
 136/1000 [===>..........................] - ETA: 32:15 - loss: 0.3815 - regression_loss: 0.3476 - classification_loss: 0.0338
 137/1000 [===>..........................] - ETA: 32:09 - loss: 0.3791 - regression_loss: 0.3454 - classification_loss: 0.0337
 138/1000 [===>..........................] - ETA: 32:06 - loss: 0.3803 - regression_loss: 0.3466 - classification_loss: 0.0337
 139/1000 [===>..........................] - ETA: 32:06 - loss: 0.3815 - regression_loss: 0.3478 - classification_loss: 0.0337
 140/1000 [===>..........................] - ETA: 32:07 - loss: 0.3826 - regression_loss: 0.3488 - classification_loss: 0.0338
 141/1000 [===>..........................] - ETA: 32:06 - loss: 0.3826 - regression_loss: 0.3488 - classification_loss: 0.0338
 142/1000 [===>..........................] - ETA: 31:59 - loss: 0.3806 - regression_loss: 0.3469 - classification_loss: 0.0337
 143/1000 [===>..........................] - ETA: 31:59 - loss: 0.3821 - regression_loss: 0.3484 - classification_loss: 0.0338
 144/1000 [===>..........................] - ETA: 31:54 - loss: 0.3814 - regression_loss: 0.3477 - classification_loss: 0.0337
 145/1000 [===>..........................] - ETA: 31:53 - loss: 0.3821 - regression_loss: 0.3483 - classification_loss: 0.0338
 146/1000 [===>..........................] - ETA: 31:51 - loss: 0.3824 - regression_loss: 0.3486 - classification_loss: 0.0338
 147/1000 [===>..........................] - ETA: 31:50 - loss: 0.3824 - regression_loss: 0.3486 - classification_loss: 0.0338
 148/1000 [===>..........................] - ETA: 31:49 - loss: 0.3829 - regression_loss: 0.3491 - classification_loss: 0.0338
 149/1000 [===>..........................] - ETA: 31:43 - loss: 0.3809 - regression_loss: 0.3473 - classification_loss: 0.0336
 150/1000 [===>..........................] - ETA: 31:43 - loss: 0.3823 - regression_loss: 0.3486 - classification_loss: 0.0337
 151/1000 [===>..........................] - ETA: 31:43 - loss: 0.3836 - regression_loss: 0.3498 - classification_loss: 0.0338
 152/1000 [===>..........................] - ETA: 31:38 - loss: 0.3828 - regression_loss: 0.3491 - classification_loss: 0.0337
 153/1000 [===>..........................] - ETA: 31:35 - loss: 0.3832 - regression_loss: 0.3495 - classification_loss: 0.0337
 154/1000 [===>..........................] - ETA: 31:35 - loss: 0.3838 - regression_loss: 0.3500 - classification_loss: 0.0337
 155/1000 [===>..........................] - ETA: 31:33 - loss: 0.3837 - regression_loss: 0.3499 - classification_loss: 0.0338
 156/1000 [===>..........................] - ETA: 31:28 - loss: 0.3817 - regression_loss: 0.3480 - classification_loss: 0.0336
 157/1000 [===>..........................] - ETA: 31:25 - loss: 0.3819 - regression_loss: 0.3482 - classification_loss: 0.0336
 158/1000 [===>..........................] - ETA: 31:24 - loss: 0.3828 - regression_loss: 0.3491 - classification_loss: 0.0337
 159/1000 [===>..........................] - ETA: 31:20 - loss: 0.3818 - regression_loss: 0.3481 - classification_loss: 0.0337
 160/1000 [===>..........................] - ETA: 31:20 - loss: 0.3829 - regression_loss: 0.3492 - classification_loss: 0.0337
 161/1000 [===>..........................] - ETA: 31:20 - loss: 0.3838 - regression_loss: 0.3500 - classification_loss: 0.0338
 162/1000 [===>..........................] - ETA: 31:17 - loss: 0.3840 - regression_loss: 0.3502 - classification_loss: 0.0338
 163/1000 [===>..........................] - ETA: 31:16 - loss: 0.3837 - regression_loss: 0.3499 - classification_loss: 0.0338
 164/1000 [===>..........................] - ETA: 31:16 - loss: 0.3847 - regression_loss: 0.3509 - classification_loss: 0.0339
 165/1000 [===>..........................] - ETA: 31:10 - loss: 0.3829 - regression_loss: 0.3492 - classification_loss: 0.0337
 166/1000 [===>..........................] - ETA: 31:09 - loss: 0.3835 - regression_loss: 0.3497 - classification_loss: 0.0337
 167/1000 [====>.........................] - ETA: 31:04 - loss: 0.3831 - regression_loss: 0.3494 - classification_loss: 0.0337
 168/1000 [====>.........................] - ETA: 30:59 - loss: 0.3822 - regression_loss: 0.3486 - classification_loss: 0.0336
 169/1000 [====>.........................] - ETA: 31:00 - loss: 0.3831 - regression_loss: 0.3494 - classification_loss: 0.0337
 170/1000 [====>.........................] - ETA: 30:54 - loss: 0.3814 - regression_loss: 0.3478 - classification_loss: 0.0336
 171/1000 [====>.........................] - ETA: 30:51 - loss: 0.3817 - regression_loss: 0.3482 - classification_loss: 0.0336
 172/1000 [====>.........................] - ETA: 30:51 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 173/1000 [====>.........................] - ETA: 30:50 - loss: 0.3825 - regression_loss: 0.3488 - classification_loss: 0.0337
 174/1000 [====>.........................] - ETA: 30:49 - loss: 0.3830 - regression_loss: 0.3493 - classification_loss: 0.0337
 175/1000 [====>.........................] - ETA: 30:49 - loss: 0.3842 - regression_loss: 0.3504 - classification_loss: 0.0338
 176/1000 [====>.........................] - ETA: 30:43 - loss: 0.3823 - regression_loss: 0.3486 - classification_loss: 0.0336
 177/1000 [====>.........................] - ETA: 30:42 - loss: 0.3821 - regression_loss: 0.3484 - classification_loss: 0.0337
 178/1000 [====>.........................] - ETA: 30:41 - loss: 0.3829 - regression_loss: 0.3492 - classification_loss: 0.0337
 179/1000 [====>.........................] - ETA: 30:40 - loss: 0.3833 - regression_loss: 0.3496 - classification_loss: 0.0337
 180/1000 [====>.........................] - ETA: 30:38 - loss: 0.3835 - regression_loss: 0.3497 - classification_loss: 0.0337
 181/1000 [====>.........................] - ETA: 30:33 - loss: 0.3834 - regression_loss: 0.3497 - classification_loss: 0.0337
 182/1000 [====>.........................] - ETA: 30:33 - loss: 0.3842 - regression_loss: 0.3504 - classification_loss: 0.0338
 183/1000 [====>.........................] - ETA: 30:32 - loss: 0.3851 - regression_loss: 0.3513 - classification_loss: 0.0338
 184/1000 [====>.........................] - ETA: 30:31 - loss: 0.3848 - regression_loss: 0.3510 - classification_loss: 0.0338
 185/1000 [====>.........................] - ETA: 30:30 - loss: 0.3852 - regression_loss: 0.3514 - classification_loss: 0.0338
 186/1000 [====>.........................] - ETA: 30:24 - loss: 0.3839 - regression_loss: 0.3502 - classification_loss: 0.0337
 187/1000 [====>.........................] - ETA: 30:20 - loss: 0.3831 - regression_loss: 0.3495 - classification_loss: 0.0337
 188/1000 [====>.........................] - ETA: 30:17 - loss: 0.3834 - regression_loss: 0.3498 - classification_loss: 0.0337
 189/1000 [====>.........................] - ETA: 30:15 - loss: 0.3837 - regression_loss: 0.3500 - classification_loss: 0.0337
 190/1000 [====>.........................] - ETA: 30:13 - loss: 0.3835 - regression_loss: 0.3498 - classification_loss: 0.0337
 191/1000 [====>.........................] - ETA: 30:09 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 192/1000 [====>.........................] - ETA: 30:08 - loss: 0.3832 - regression_loss: 0.3495 - classification_loss: 0.0336
 193/1000 [====>.........................] - ETA: 30:03 - loss: 0.3814 - regression_loss: 0.3479 - classification_loss: 0.0335
 194/1000 [====>.........................] - ETA: 30:02 - loss: 0.3823 - regression_loss: 0.3487 - classification_loss: 0.0336
 195/1000 [====>.........................] - ETA: 30:02 - loss: 0.3830 - regression_loss: 0.3494 - classification_loss: 0.0336
 196/1000 [====>.........................] - ETA: 30:00 - loss: 0.3834 - regression_loss: 0.3497 - classification_loss: 0.0336
 197/1000 [====>.........................] - ETA: 30:00 - loss: 0.3840 - regression_loss: 0.3504 - classification_loss: 0.0337
 198/1000 [====>.........................] - ETA: 29:58 - loss: 0.3837 - regression_loss: 0.3501 - classification_loss: 0.0337
 199/1000 [====>.........................] - ETA: 29:53 - loss: 0.3824 - regression_loss: 0.3488 - classification_loss: 0.0335
 200/1000 [=====>........................] - ETA: 29:50 - loss: 0.3825 - regression_loss: 0.3489 - classification_loss: 0.0335
 201/1000 [=====>........................] - ETA: 29:46 - loss: 0.3822 - regression_loss: 0.3487 - classification_loss: 0.0335
 202/1000 [=====>........................] - ETA: 29:46 - loss: 0.3829 - regression_loss: 0.3494 - classification_loss: 0.0335
 203/1000 [=====>........................] - ETA: 29:41 - loss: 0.3822 - regression_loss: 0.3487 - classification_loss: 0.0335
 204/1000 [=====>........................] - ETA: 29:41 - loss: 0.3829 - regression_loss: 0.3494 - classification_loss: 0.0336
 205/1000 [=====>........................] - ETA: 29:39 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 206/1000 [=====>........................] - ETA: 29:34 - loss: 0.3813 - regression_loss: 0.3478 - classification_loss: 0.0335
 207/1000 [=====>........................] - ETA: 29:33 - loss: 0.3817 - regression_loss: 0.3482 - classification_loss: 0.0335
 208/1000 [=====>........................] - ETA: 29:30 - loss: 0.3819 - regression_loss: 0.3484 - classification_loss: 0.0335
 209/1000 [=====>........................] - ETA: 29:30 - loss: 0.3826 - regression_loss: 0.3491 - classification_loss: 0.0335
 210/1000 [=====>........................] - ETA: 29:29 - loss: 0.3830 - regression_loss: 0.3494 - classification_loss: 0.0336
 211/1000 [=====>........................] - ETA: 29:26 - loss: 0.3831 - regression_loss: 0.3495 - classification_loss: 0.0336
 212/1000 [=====>........................] - ETA: 29:26 - loss: 0.3837 - regression_loss: 0.3501 - classification_loss: 0.0336
 213/1000 [=====>........................] - ETA: 29:22 - loss: 0.3833 - regression_loss: 0.3498 - classification_loss: 0.0336
 214/1000 [=====>........................] - ETA: 29:17 - loss: 0.3818 - regression_loss: 0.3483 - classification_loss: 0.0335
 215/1000 [=====>........................] - ETA: 29:16 - loss: 0.3826 - regression_loss: 0.3491 - classification_loss: 0.0335
 216/1000 [=====>........................] - ETA: 29:14 - loss: 0.3825 - regression_loss: 0.3490 - classification_loss: 0.0335
 217/1000 [=====>........................] - ETA: 29:13 - loss: 0.3828 - regression_loss: 0.3493 - classification_loss: 0.0336
 218/1000 [=====>........................] - ETA: 29:12 - loss: 0.3834 - regression_loss: 0.3498 - classification_loss: 0.0336
 219/1000 [=====>........................] - ETA: 29:09 - loss: 0.3835 - regression_loss: 0.3499 - classification_loss: 0.0336
 220/1000 [=====>........................] - ETA: 29:08 - loss: 0.3833 - regression_loss: 0.3497 - classification_loss: 0.0336
 221/1000 [=====>........................] - ETA: 29:04 - loss: 0.3826 - regression_loss: 0.3491 - classification_loss: 0.0336
 222/1000 [=====>........................] - ETA: 28:59 - loss: 0.3812 - regression_loss: 0.3477 - classification_loss: 0.0335
 223/1000 [=====>........................] - ETA: 28:59 - loss: 0.3821 - regression_loss: 0.3486 - classification_loss: 0.0335
 224/1000 [=====>........................] - ETA: 28:57 - loss: 0.3821 - regression_loss: 0.3486 - classification_loss: 0.0335
 225/1000 [=====>........................] - ETA: 28:56 - loss: 0.3830 - regression_loss: 0.3494 - classification_loss: 0.0336
 226/1000 [=====>........................] - ETA: 28:51 - loss: 0.3816 - regression_loss: 0.3481 - classification_loss: 0.0335
 227/1000 [=====>........................] - ETA: 28:51 - loss: 0.3826 - regression_loss: 0.3491 - classification_loss: 0.0335
 228/1000 [=====>........................] - ETA: 28:48 - loss: 0.3829 - regression_loss: 0.3493 - classification_loss: 0.0335
 229/1000 [=====>........................] - ETA: 28:47 - loss: 0.3835 - regression_loss: 0.3499 - classification_loss: 0.0336
 230/1000 [=====>........................] - ETA: 28:43 - loss: 0.3829 - regression_loss: 0.3494 - classification_loss: 0.0335
 231/1000 [=====>........................] - ETA: 28:41 - loss: 0.3828 - regression_loss: 0.3492 - classification_loss: 0.0335
 232/1000 [=====>........................] - ETA: 28:40 - loss: 0.3832 - regression_loss: 0.3496 - classification_loss: 0.0336
 233/1000 [=====>........................] - ETA: 28:36 - loss: 0.3825 - regression_loss: 0.3490 - classification_loss: 0.0335
 234/1000 [======>.......................] - ETA: 28:35 - loss: 0.3832 - regression_loss: 0.3496 - classification_loss: 0.0336
 235/1000 [======>.......................] - ETA: 28:31 - loss: 0.3820 - regression_loss: 0.3485 - classification_loss: 0.0335
 236/1000 [======>.......................] - ETA: 28:28 - loss: 0.3822 - regression_loss: 0.3487 - classification_loss: 0.0335
 237/1000 [======>.......................] - ETA: 28:27 - loss: 0.3829 - regression_loss: 0.3493 - classification_loss: 0.0336
 238/1000 [======>.......................] - ETA: 28:23 - loss: 0.3815 - regression_loss: 0.3480 - classification_loss: 0.0335
 239/1000 [======>.......................] - ETA: 28:19 - loss: 0.3810 - regression_loss: 0.3476 - classification_loss: 0.0334
 240/1000 [======>.......................] - ETA: 28:18 - loss: 0.3814 - regression_loss: 0.3479 - classification_loss: 0.0335
 241/1000 [======>.......................] - ETA: 28:17 - loss: 0.3821 - regression_loss: 0.3485 - classification_loss: 0.0335
 242/1000 [======>.......................] - ETA: 28:16 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 243/1000 [======>.......................] - ETA: 28:14 - loss: 0.3825 - regression_loss: 0.3489 - classification_loss: 0.0336
 244/1000 [======>.......................] - ETA: 28:12 - loss: 0.3826 - regression_loss: 0.3490 - classification_loss: 0.0336
 245/1000 [======>.......................] - ETA: 28:11 - loss: 0.3829 - regression_loss: 0.3493 - classification_loss: 0.0336
 246/1000 [======>.......................] - ETA: 28:08 - loss: 0.3831 - regression_loss: 0.3495 - classification_loss: 0.0336
 247/1000 [======>.......................] - ETA: 28:07 - loss: 0.3838 - regression_loss: 0.3501 - classification_loss: 0.0337
 248/1000 [======>.......................] - ETA: 28:03 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 249/1000 [======>.......................] - ETA: 28:02 - loss: 0.3832 - regression_loss: 0.3496 - classification_loss: 0.0336
 250/1000 [======>.......................] - ETA: 28:00 - loss: 0.3830 - regression_loss: 0.3494 - classification_loss: 0.0336
 251/1000 [======>.......................] - ETA: 27:56 - loss: 0.3824 - regression_loss: 0.3488 - classification_loss: 0.0336
 252/1000 [======>.......................] - ETA: 27:55 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 253/1000 [======>.......................] - ETA: 27:51 - loss: 0.3814 - regression_loss: 0.3478 - classification_loss: 0.0335
 254/1000 [======>.......................] - ETA: 27:49 - loss: 0.3812 - regression_loss: 0.3477 - classification_loss: 0.0335
 255/1000 [======>.......................] - ETA: 27:48 - loss: 0.3819 - regression_loss: 0.3483 - classification_loss: 0.0336
 256/1000 [======>.......................] - ETA: 27:47 - loss: 0.3824 - regression_loss: 0.3488 - classification_loss: 0.0336
 257/1000 [======>.......................] - ETA: 27:44 - loss: 0.3826 - regression_loss: 0.3490 - classification_loss: 0.0336
 258/1000 [======>.......................] - ETA: 27:41 - loss: 0.3823 - regression_loss: 0.3487 - classification_loss: 0.0336
 259/1000 [======>.......................] - ETA: 27:40 - loss: 0.3830 - regression_loss: 0.3494 - classification_loss: 0.0336
 260/1000 [======>.......................] - ETA: 27:36 - loss: 0.3819 - regression_loss: 0.3484 - classification_loss: 0.0335
 261/1000 [======>.......................] - ETA: 27:34 - loss: 0.3827 - regression_loss: 0.3491 - classification_loss: 0.0336
 262/1000 [======>.......................] - ETA: 27:33 - loss: 0.3830 - regression_loss: 0.3495 - classification_loss: 0.0336
 263/1000 [======>.......................] - ETA: 27:31 - loss: 0.3829 - regression_loss: 0.3493 - classification_loss: 0.0336
 264/1000 [======>.......................] - ETA: 27:27 - loss: 0.3829 - regression_loss: 0.3494 - classification_loss: 0.0336
 265/1000 [======>.......................] - ETA: 27:25 - loss: 0.3834 - regression_loss: 0.3499 - classification_loss: 0.0336
 266/1000 [======>.......................] - ETA: 27:23 - loss: 0.3837 - regression_loss: 0.3501 - classification_loss: 0.0336
 267/1000 [=======>......................] - ETA: 27:20 - loss: 0.3832 - regression_loss: 0.3496 - classification_loss: 0.0336
 268/1000 [=======>......................] - ETA: 27:17 - loss: 0.3834 - regression_loss: 0.3498 - classification_loss: 0.0336
 269/1000 [=======>......................] - ETA: 27:16 - loss: 0.3839 - regression_loss: 0.3503 - classification_loss: 0.0336
 270/1000 [=======>......................] - ETA: 27:15 - loss: 0.3845 - regression_loss: 0.3508 - classification_loss: 0.0337
 271/1000 [=======>......................] - ETA: 27:11 - loss: 0.3832 - regression_loss: 0.3497 - classification_loss: 0.0336
 272/1000 [=======>......................] - ETA: 27:09 - loss: 0.3838 - regression_loss: 0.3502 - classification_loss: 0.0336
 273/1000 [=======>......................] - ETA: 27:06 - loss: 0.3837 - regression_loss: 0.3502 - classification_loss: 0.0336
 274/1000 [=======>......................] - ETA: 27:04 - loss: 0.3841 - regression_loss: 0.3505 - classification_loss: 0.0336
 275/1000 [=======>......................] - ETA: 27:03 - loss: 0.3846 - regression_loss: 0.3510 - classification_loss: 0.0336
 276/1000 [=======>......................] - ETA: 27:01 - loss: 0.3844 - regression_loss: 0.3508 - classification_loss: 0.0336
 277/1000 [=======>......................] - ETA: 26:59 - loss: 0.3845 - regression_loss: 0.3509 - classification_loss: 0.0336
 278/1000 [=======>......................] - ETA: 26:55 - loss: 0.3834 - regression_loss: 0.3499 - classification_loss: 0.0336
 279/1000 [=======>......................] - ETA: 26:53 - loss: 0.3839 - regression_loss: 0.3503 - classification_loss: 0.0336
 280/1000 [=======>......................] - ETA: 26:51 - loss: 0.3842 - regression_loss: 0.3506 - classification_loss: 0.0336
 281/1000 [=======>......................] - ETA: 26:50 - loss: 0.3847 - regression_loss: 0.3511 - classification_loss: 0.0336
 282/1000 [=======>......................] - ETA: 26:48 - loss: 0.3850 - regression_loss: 0.3514 - classification_loss: 0.0336
 283/1000 [=======>......................] - ETA: 26:47 - loss: 0.3854 - regression_loss: 0.3518 - classification_loss: 0.0336
 284/1000 [=======>......................] - ETA: 26:45 - loss: 0.3852 - regression_loss: 0.3516 - classification_loss: 0.0337
 285/1000 [=======>......................] - ETA: 26:41 - loss: 0.3843 - regression_loss: 0.3507 - classification_loss: 0.0336
 286/1000 [=======>......................] - ETA: 26:38 - loss: 0.3842 - regression_loss: 0.3506 - classification_loss: 0.0335
 287/1000 [=======>......................] - ETA: 26:36 - loss: 0.3845 - regression_loss: 0.3509 - classification_loss: 0.0336
 288/1000 [=======>......................] - ETA: 26:34 - loss: 0.3844 - regression_loss: 0.3508 - classification_loss: 0.0336
 289/1000 [=======>......................] - ETA: 26:33 - loss: 0.3848 - regression_loss: 0.3512 - classification_loss: 0.0336
 290/1000 [=======>......................] - ETA: 26:31 - loss: 0.3850 - regression_loss: 0.3514 - classification_loss: 0.0336
 291/1000 [=======>......................] - ETA: 26:27 - loss: 0.3845 - regression_loss: 0.3509 - classification_loss: 0.0336
 292/1000 [=======>......................] - ETA: 26:26 - loss: 0.3850 - regression_loss: 0.3515 - classification_loss: 0.0336
 293/1000 [=======>......................] - ETA: 26:22 - loss: 0.3840 - regression_loss: 0.3505 - classification_loss: 0.0335
 294/1000 [=======>......................] - ETA: 26:20 - loss: 0.3843 - regression_loss: 0.3507 - classification_loss: 0.0335
 295/1000 [=======>......................] - ETA: 26:17 - loss: 0.3837 - regression_loss: 0.3502 - classification_loss: 0.0335
 296/1000 [=======>......................] - ETA: 26:13 - loss: 0.3826 - regression_loss: 0.3491 - classification_loss: 0.0334
 297/1000 [=======>......................] - ETA: 26:12 - loss: 0.3830 - regression_loss: 0.3496 - classification_loss: 0.0335
 298/1000 [=======>......................] - ETA: 26:10 - loss: 0.3833 - regression_loss: 0.3498 - classification_loss: 0.0335
 299/1000 [=======>......................] - ETA: 26:09 - loss: 0.3837 - regression_loss: 0.3502 - classification_loss: 0.0335
 300/1000 [========>.....................] - ETA: 26:06 - loss: 0.3838 - regression_loss: 0.3503 - classification_loss: 0.0335
 301/1000 [========>.....................] - ETA: 26:04 - loss: 0.3838 - regression_loss: 0.3503 - classification_loss: 0.0335
 302/1000 [========>.....................] - ETA: 26:02 - loss: 0.3841 - regression_loss: 0.3506 - classification_loss: 0.0335
 303/1000 [========>.....................] - ETA: 26:01 - loss: 0.3839 - regression_loss: 0.3504 - classification_loss: 0.0335
 304/1000 [========>.....................] - ETA: 25:59 - loss: 0.3843 - regression_loss: 0.3508 - classification_loss: 0.0335
 305/1000 [========>.....................] - ETA: 25:55 - loss: 0.3834 - regression_loss: 0.3499 - classification_loss: 0.0335
 306/1000 [========>.....................] - ETA: 25:54 - loss: 0.3838 - regression_loss: 0.3503 - classification_loss: 0.0335
 307/1000 [========>.....................] - ETA: 25:51 - loss: 0.3835 - regression_loss: 0.3500 - classification_loss: 0.0334
 308/1000 [========>.....................] - ETA: 25:49 - loss: 0.3839 - regression_loss: 0.3504 - classification_loss: 0.0335
 309/1000 [========>.....................] - ETA: 25:46 - loss: 0.3827 - regression_loss: 0.3494 - classification_loss: 0.0334
 310/1000 [========>.....................] - ETA: 25:44 - loss: 0.3826 - regression_loss: 0.3492 - classification_loss: 0.0334
 311/1000 [========>.....................] - ETA: 25:43 - loss: 0.3830 - regression_loss: 0.3496 - classification_loss: 0.0334
 312/1000 [========>.....................] - ETA: 25:39 - loss: 0.3825 - regression_loss: 0.3491 - classification_loss: 0.0334
 313/1000 [========>.....................] - ETA: 25:37 - loss: 0.3826 - regression_loss: 0.3493 - classification_loss: 0.0334
 314/1000 [========>.....................] - ETA: 25:35 - loss: 0.3829 - regression_loss: 0.3495 - classification_loss: 0.0334
 315/1000 [========>.....................] - ETA: 25:31 - loss: 0.3819 - regression_loss: 0.3486 - classification_loss: 0.0333
 316/1000 [========>.....................] - ETA: 25:29 - loss: 0.3820 - regression_loss: 0.3486 - classification_loss: 0.0333
 317/1000 [========>.....................] - ETA: 25:28 - loss: 0.3824 - regression_loss: 0.3490 - classification_loss: 0.0334
 318/1000 [========>.....................] - ETA: 25:26 - loss: 0.3826 - regression_loss: 0.3492 - classification_loss: 0.0334
 319/1000 [========>.....................] - ETA: 25:25 - loss: 0.3829 - regression_loss: 0.3496 - classification_loss: 0.0334
 320/1000 [========>.....................] - ETA: 25:21 - loss: 0.3825 - regression_loss: 0.3491 - classification_loss: 0.0333
 321/1000 [========>.....................] - ETA: 25:20 - loss: 0.3824 - regression_loss: 0.3490 - classification_loss: 0.0334
 322/1000 [========>.....................] - ETA: 25:16 - loss: 0.3818 - regression_loss: 0.3485 - classification_loss: 0.0333
 323/1000 [========>.....................] - ETA: 25:14 - loss: 0.3819 - regression_loss: 0.3486 - classification_loss: 0.0334
 324/1000 [========>.....................] - ETA: 25:10 - loss: 0.3809 - regression_loss: 0.3477 - classification_loss: 0.0333
 325/1000 [========>.....................] - ETA: 25:09 - loss: 0.3812 - regression_loss: 0.3479 - classification_loss: 0.0333
 326/1000 [========>.....................] - ETA: 25:07 - loss: 0.3816 - regression_loss: 0.3483 - classification_loss: 0.0333
 327/1000 [========>.....................] - ETA: 25:06 - loss: 0.3821 - regression_loss: 0.3487 - classification_loss: 0.0334
 328/1000 [========>.....................] - ETA: 25:04 - loss: 0.3819 - regression_loss: 0.3485 - classification_loss: 0.0334
 329/1000 [========>.....................] - ETA: 25:00 - loss: 0.3809 - regression_loss: 0.3476 - classification_loss: 0.0333
 330/1000 [========>.....................] - ETA: 24:58 - loss: 0.3811 - regression_loss: 0.3478 - classification_loss: 0.0333
 331/1000 [========>.....................] - ETA: 24:56 - loss: 0.3810 - regression_loss: 0.3477 - classification_loss: 0.0333
 332/1000 [========>.....................] - ETA: 24:53 - loss: 0.3810 - regression_loss: 0.3477 - classification_loss: 0.0333
 333/1000 [========>.....................] - ETA: 24:52 - loss: 0.3815 - regression_loss: 0.3482 - classification_loss: 0.0333
 334/1000 [=========>....................] - ETA: 24:50 - loss: 0.3818 - regression_loss: 0.3484 - classification_loss: 0.0333
 335/1000 [=========>....................] - ETA: 24:48 - loss: 0.3823 - regression_loss: 0.3489 - classification_loss: 0.0334
 336/1000 [=========>....................] - ETA: 24:45 - loss: 0.3813 - regression_loss: 0.3480 - classification_loss: 0.0333
 337/1000 [=========>....................] - ETA: 24:42 - loss: 0.3811 - regression_loss: 0.3479 - classification_loss: 0.0333
 338/1000 [=========>....................] - ETA: 24:40 - loss: 0.3810 - regression_loss: 0.3477 - classification_loss: 0.0333
 339/1000 [=========>....................] - ETA: 24:38 - loss: 0.3814 - regression_loss: 0.3481 - classification_loss: 0.0333
 340/1000 [=========>....................] - ETA: 24:36 - loss: 0.3814 - regression_loss: 0.3482 - classification_loss: 0.0333
 341/1000 [=========>....................] - ETA: 24:35 - loss: 0.3818 - regression_loss: 0.3485 - classification_loss: 0.0333
 342/1000 [=========>....................] - ETA: 24:33 - loss: 0.3820 - regression_loss: 0.3487 - classification_loss: 0.0333
 343/1000 [=========>....................] - ETA: 24:31 - loss: 0.3819 - regression_loss: 0.3486 - classification_loss: 0.0333
 344/1000 [=========>....................] - ETA: 24:29 - loss: 0.3820 - regression_loss: 0.3486 - classification_loss: 0.0333
 345/1000 [=========>....................] - ETA: 24:27 - loss: 0.3823 - regression_loss: 0.3490 - classification_loss: 0.0333
 346/1000 [=========>....................] - ETA: 24:23 - loss: 0.3815 - regression_loss: 0.3482 - classification_loss: 0.0333
 347/1000 [=========>....................] - ETA: 24:22 - loss: 0.3819 - regression_loss: 0.3486 - classification_loss: 0.0333
 348/1000 [=========>....................] - ETA: 24:19 - loss: 0.3815 - regression_loss: 0.3482 - classification_loss: 0.0333
 349/1000 [=========>....................] - ETA: 24:17 - loss: 0.3817 - regression_loss: 0.3484 - classification_loss: 0.0333
 350/1000 [=========>....................] - ETA: 24:16 - loss: 0.3819 - regression_loss: 0.3486 - classification_loss: 0.0333
 351/1000 [=========>....................] - ETA: 24:14 - loss: 0.3818 - regression_loss: 0.3485 - classification_loss: 0.0333
 352/1000 [=========>....................] - ETA: 24:10 - loss: 0.3810 - regression_loss: 0.3478 - classification_loss: 0.0332
 353/1000 [=========>....................] - ETA: 24:07 - loss: 0.3806 - regression_loss: 0.3474 - classification_loss: 0.0332
 354/1000 [=========>....................] - ETA: 24:05 - loss: 0.3810 - regression_loss: 0.3478 - classification_loss: 0.0332
 355/1000 [=========>....................] - ETA: 24:03 - loss: 0.3811 - regression_loss: 0.3479 - classification_loss: 0.0332
 356/1000 [=========>....................] - ETA: 24:02 - loss: 0.3815 - regression_loss: 0.3482 - classification_loss: 0.0332
 357/1000 [=========>....................] - ETA: 24:00 - loss: 0.3813 - regression_loss: 0.3481 - classification_loss: 0.0332
 358/1000 [=========>....................] - ETA: 23:57 - loss: 0.3815 - regression_loss: 0.3482 - classification_loss: 0.0332
 359/1000 [=========>....................] - ETA: 23:56 - loss: 0.3817 - regression_loss: 0.3485 - classification_loss: 0.0332
 360/1000 [=========>....................] - ETA: 23:52 - loss: 0.3808 - regression_loss: 0.3476 - classification_loss: 0.0332
 361/1000 [=========>....................] - ETA: 23:51 - loss: 0.3813 - regression_loss: 0.3481 - classification_loss: 0.0332
 362/1000 [=========>....................] - ETA: 23:48 - loss: 0.3808 - regression_loss: 0.3477 - classification_loss: 0.0332
 363/1000 [=========>....................] - ETA: 23:46 - loss: 0.3813 - regression_loss: 0.3481 - classification_loss: 0.0332
 364/1000 [=========>....................] - ETA: 23:43 - loss: 0.3805 - regression_loss: 0.3474 - classification_loss: 0.0331
 365/1000 [=========>....................] - ETA: 23:40 - loss: 0.3800 - regression_loss: 0.3469 - classification_loss: 0.0331
 366/1000 [=========>....................] - ETA: 23:38 - loss: 0.3804 - regression_loss: 0.3473 - classification_loss: 0.0331
 367/1000 [==========>...................] - ETA: 23:37 - loss: 0.3802 - regression_loss: 0.3471 - classification_loss: 0.0331
 368/1000 [==========>...................] - ETA: 23:35 - loss: 0.3805 - regression_loss: 0.3474 - classification_loss: 0.0331
 369/1000 [==========>...................] - ETA: 23:33 - loss: 0.3807 - regression_loss: 0.3476 - classification_loss: 0.0332
 370/1000 [==========>...................] - ETA: 23:31 - loss: 0.3807 - regression_loss: 0.3476 - classification_loss: 0.0332
 371/1000 [==========>...................] - ETA: 23:29 - loss: 0.3809 - regression_loss: 0.3478 - classification_loss: 0.0332
 372/1000 [==========>...................] - ETA: 23:26 - loss: 0.3800 - regression_loss: 0.3469 - classification_loss: 0.0331
 373/1000 [==========>...................] - ETA: 23:24 - loss: 0.3804 - regression_loss: 0.3473 - classification_loss: 0.0331
 374/1000 [==========>...................] - ETA: 23:22 - loss: 0.3804 - regression_loss: 0.3473 - classification_loss: 0.0331
 375/1000 [==========>...................] - ETA: 23:20 - loss: 0.3802 - regression_loss: 0.3471 - classification_loss: 0.0331
 376/1000 [==========>...................] - ETA: 23:18 - loss: 0.3806 - regression_loss: 0.3474 - classification_loss: 0.0331
 377/1000 [==========>...................] - ETA: 23:15 - loss: 0.3804 - regression_loss: 0.3473 - classification_loss: 0.0331
 378/1000 [==========>...................] - ETA: 23:13 - loss: 0.3803 - regression_loss: 0.3472 - classification_loss: 0.0331
 379/1000 [==========>...................] - ETA: 23:10 - loss: 0.3795 - regression_loss: 0.3465 - classification_loss: 0.0331
 380/1000 [==========>...................] - ETA: 23:07 - loss: 0.3791 - regression_loss: 0.3461 - classification_loss: 0.0330
 381/1000 [==========>...................] - ETA: 23:05 - loss: 0.3795 - regression_loss: 0.3464 - classification_loss: 0.0331
 382/1000 [==========>...................] - ETA: 23:04 - loss: 0.3798 - regression_loss: 0.3467 - classification_loss: 0.0331
 383/1000 [==========>...................] - ETA: 23:02 - loss: 0.3799 - regression_loss: 0.3468 - classification_loss: 0.0331
 384/1000 [==========>...................] - ETA: 22:59 - loss: 0.3799 - regression_loss: 0.3469 - classification_loss: 0.0331
 385/1000 [==========>...................] - ETA: 22:58 - loss: 0.3803 - regression_loss: 0.3472 - classification_loss: 0.0331
 386/1000 [==========>...................] - ETA: 22:56 - loss: 0.3801 - regression_loss: 0.3470 - classification_loss: 0.0331
 387/1000 [==========>...................] - ETA: 22:54 - loss: 0.3803 - regression_loss: 0.3472 - classification_loss: 0.0331
 388/1000 [==========>...................] - ETA: 22:53 - loss: 0.3806 - regression_loss: 0.3475 - classification_loss: 0.0331
 389/1000 [==========>...................] - ETA: 22:50 - loss: 0.3804 - regression_loss: 0.3473 - classification_loss: 0.0331
 390/1000 [==========>...................] - ETA: 22:46 - loss: 0.3795 - regression_loss: 0.3465 - classification_loss: 0.0330
 391/1000 [==========>...................] - ETA: 22:44 - loss: 0.3797 - regression_loss: 0.3466 - classification_loss: 0.0331
 392/1000 [==========>...................] - ETA: 22:42 - loss: 0.3796 - regression_loss: 0.3465 - classification_loss: 0.0331
 393/1000 [==========>...................] - ETA: 22:39 - loss: 0.3792 - regression_loss: 0.3462 - classification_loss: 0.0330
 394/1000 [==========>...................] - ETA: 22:36 - loss: 0.3792 - regression_loss: 0.3462 - classification_loss: 0.0330
 395/1000 [==========>...................] - ETA: 22:35 - loss: 0.3795 - regression_loss: 0.3465 - classification_loss: 0.0331
 396/1000 [==========>...................] - ETA: 22:33 - loss: 0.3797 - regression_loss: 0.3466 - classification_loss: 0.0331
 397/1000 [==========>...................] - ETA: 22:30 - loss: 0.3788 - regression_loss: 0.3458 - classification_loss: 0.0330
 398/1000 [==========>...................] - ETA: 22:28 - loss: 0.3792 - regression_loss: 0.3461 - classification_loss: 0.0330
 399/1000 [==========>...................] - ETA: 22:25 - loss: 0.3783 - regression_loss: 0.3453 - classification_loss: 0.0330
 400/1000 [===========>..................] - ETA: 22:23 - loss: 0.3784 - regression_loss: 0.3454 - classification_loss: 0.0330
 401/1000 [===========>..................] - ETA: 22:21 - loss: 0.3787 - regression_loss: 0.3457 - classification_loss: 0.0330
 402/1000 [===========>..................] - ETA: 22:19 - loss: 0.3788 - regression_loss: 0.3458 - classification_loss: 0.0330
 403/1000 [===========>..................] - ETA: 22:16 - loss: 0.3786 - regression_loss: 0.3456 - classification_loss: 0.0330
 404/1000 [===========>..................] - ETA: 22:14 - loss: 0.3784 - regression_loss: 0.3455 - classification_loss: 0.0330
 405/1000 [===========>..................] - ETA: 22:13 - loss: 0.3787 - regression_loss: 0.3458 - classification_loss: 0.0330
 406/1000 [===========>..................] - ETA: 22:09 - loss: 0.3780 - regression_loss: 0.3451 - classification_loss: 0.0329
 407/1000 [===========>..................] - ETA: 22:08 - loss: 0.3783 - regression_loss: 0.3454 - classification_loss: 0.0329
 408/1000 [===========>..................] - ETA: 22:05 - loss: 0.3780 - regression_loss: 0.3451 - classification_loss: 0.0329
 409/1000 [===========>..................] - ETA: 22:03 - loss: 0.3782 - regression_loss: 0.3453 - classification_loss: 0.0329
 410/1000 [===========>..................] - ETA: 22:01 - loss: 0.3785 - regression_loss: 0.3455 - classification_loss: 0.0329
 411/1000 [===========>..................] - ETA: 21:59 - loss: 0.3783 - regression_loss: 0.3454 - classification_loss: 0.0330
 412/1000 [===========>..................] - ETA: 21:57 - loss: 0.3784 - regression_loss: 0.3455 - classification_loss: 0.0330
 413/1000 [===========>..................] - ETA: 21:55 - loss: 0.3786 - regression_loss: 0.3456 - classification_loss: 0.0330
 414/1000 [===========>..................] - ETA: 21:53 - loss: 0.3788 - regression_loss: 0.3458 - classification_loss: 0.0330
 415/1000 [===========>..................] - ETA: 21:51 - loss: 0.3791 - regression_loss: 0.3461 - classification_loss: 0.0330
 416/1000 [===========>..................] - ETA: 21:48 - loss: 0.3788 - regression_loss: 0.3459 - classification_loss: 0.0330
 417/1000 [===========>..................] - ETA: 21:46 - loss: 0.3792 - regression_loss: 0.3462 - classification_loss: 0.0330
 418/1000 [===========>..................] - ETA: 21:43 - loss: 0.3786 - regression_loss: 0.3457 - classification_loss: 0.0329
 419/1000 [===========>..................] - ETA: 21:41 - loss: 0.3787 - regression_loss: 0.3457 - classification_loss: 0.0330
 420/1000 [===========>..................] - ETA: 21:39 - loss: 0.3787 - regression_loss: 0.3457 - classification_loss: 0.0330
 421/1000 [===========>..................] - ETA: 21:36 - loss: 0.3780 - regression_loss: 0.3451 - classification_loss: 0.0329
 422/1000 [===========>..................] - ETA: 21:34 - loss: 0.3781 - regression_loss: 0.3452 - classification_loss: 0.0329
 423/1000 [===========>..................] - ETA: 21:32 - loss: 0.3786 - regression_loss: 0.3457 - classification_loss: 0.0330
 424/1000 [===========>..................] - ETA: 21:30 - loss: 0.3789 - regression_loss: 0.3459 - classification_loss: 0.0330
 425/1000 [===========>..................] - ETA: 21:29 - loss: 0.3793 - regression_loss: 0.3463 - classification_loss: 0.0330
 426/1000 [===========>..................] - ETA: 21:26 - loss: 0.3790 - regression_loss: 0.3460 - classification_loss: 0.0330
 427/1000 [===========>..................] - ETA: 21:23 - loss: 0.3786 - regression_loss: 0.3457 - classification_loss: 0.0330
 428/1000 [===========>..................] - ETA: 21:21 - loss: 0.3792 - regression_loss: 0.3462 - classification_loss: 0.0330
 429/1000 [===========>..................] - ETA: 21:19 - loss: 0.3794 - regression_loss: 0.3465 - classification_loss: 0.0330
 430/1000 [===========>..................] - ETA: 21:17 - loss: 0.3798 - regression_loss: 0.3468 - classification_loss: 0.0330
 431/1000 [===========>..................] - ETA: 21:15 - loss: 0.3799 - regression_loss: 0.3469 - classification_loss: 0.0330
 432/1000 [===========>..................] - ETA: 21:12 - loss: 0.3793 - regression_loss: 0.3463 - classification_loss: 0.0329
 433/1000 [===========>..................] - ETA: 21:10 - loss: 0.3802 - regression_loss: 0.3472 - classification_loss: 0.0330
 434/1000 [============>.................] - ETA: 21:07 - loss: 0.3795 - regression_loss: 0.3466 - classification_loss: 0.0329
 435/1000 [============>.................] - ETA: 21:05 - loss: 0.3805 - regression_loss: 0.3475 - classification_loss: 0.0330
 436/1000 [============>.................] - ETA: 21:03 - loss: 0.3813 - regression_loss: 0.3483 - classification_loss: 0.0330
 437/1000 [============>.................] - ETA: 21:01 - loss: 0.3821 - regression_loss: 0.3491 - classification_loss: 0.0330
 438/1000 [============>.................] - ETA: 20:58 - loss: 0.3832 - regression_loss: 0.3501 - classification_loss: 0.0331
 439/1000 [============>.................] - ETA: 20:56 - loss: 0.3833 - regression_loss: 0.3502 - classification_loss: 0.0331
 440/1000 [============>.................] - ETA: 20:54 - loss: 0.3838 - regression_loss: 0.3507 - classification_loss: 0.0331
 441/1000 [============>.................] - ETA: 20:52 - loss: 0.3843 - regression_loss: 0.3512 - classification_loss: 0.0331
 442/1000 [============>.................] - ETA: 20:49 - loss: 0.3844 - regression_loss: 0.3513 - classification_loss: 0.0331
 443/1000 [============>.................] - ETA: 20:46 - loss: 0.3843 - regression_loss: 0.3513 - classification_loss: 0.0331
 444/1000 [============>.................] - ETA: 20:44 - loss: 0.3850 - regression_loss: 0.3519 - classification_loss: 0.0331
 445/1000 [============>.................] - ETA: 20:42 - loss: 0.3852 - regression_loss: 0.3520 - classification_loss: 0.0331
 446/1000 [============>.................] - ETA: 20:40 - loss: 0.3855 - regression_loss: 0.3523 - classification_loss: 0.0332
 447/1000 [============>.................] - ETA: 20:39 - loss: 0.3859 - regression_loss: 0.3527 - classification_loss: 0.0332
 448/1000 [============>.................] - ETA: 20:37 - loss: 0.3863 - regression_loss: 0.3531 - classification_loss: 0.0332
 449/1000 [============>.................] - ETA: 20:34 - loss: 0.3863 - regression_loss: 0.3531 - classification_loss: 0.0332
 450/1000 [============>.................] - ETA: 20:32 - loss: 0.3865 - regression_loss: 0.3533 - classification_loss: 0.0332
 451/1000 [============>.................] - ETA: 20:30 - loss: 0.3864 - regression_loss: 0.3531 - classification_loss: 0.0332
 452/1000 [============>.................] - ETA: 20:27 - loss: 0.3858 - regression_loss: 0.3526 - classification_loss: 0.0332
 453/1000 [============>.................] - ETA: 20:26 - loss: 0.3862 - regression_loss: 0.3530 - classification_loss: 0.0332
 454/1000 [============>.................] - ETA: 20:23 - loss: 0.3859 - regression_loss: 0.3527 - classification_loss: 0.0332
 455/1000 [============>.................] - ETA: 20:20 - loss: 0.3861 - regression_loss: 0.3528 - classification_loss: 0.0332
 456/1000 [============>.................] - ETA: 20:17 - loss: 0.3853 - regression_loss: 0.3522 - classification_loss: 0.0332
 457/1000 [============>.................] - ETA: 20:15 - loss: 0.3855 - regression_loss: 0.3523 - classification_loss: 0.0332
 458/1000 [============>.................] - ETA: 20:13 - loss: 0.3854 - regression_loss: 0.3522 - classification_loss: 0.0332
 459/1000 [============>.................] - ETA: 20:11 - loss: 0.3851 - regression_loss: 0.3519 - classification_loss: 0.0332
 460/1000 [============>.................] - ETA: 20:09 - loss: 0.3854 - regression_loss: 0.3522 - classification_loss: 0.0332
 461/1000 [============>.................] - ETA: 20:07 - loss: 0.3856 - regression_loss: 0.3524 - classification_loss: 0.0332
 462/1000 [============>.................] - ETA: 20:04 - loss: 0.3853 - regression_loss: 0.3521 - classification_loss: 0.0332
 463/1000 [============>.................] - ETA: 20:01 - loss: 0.3847 - regression_loss: 0.3516 - classification_loss: 0.0331
 464/1000 [============>.................] - ETA: 19:59 - loss: 0.3847 - regression_loss: 0.3516 - classification_loss: 0.0331
 465/1000 [============>.................] - ETA: 19:57 - loss: 0.3846 - regression_loss: 0.3515 - classification_loss: 0.0332
 466/1000 [============>.................] - ETA: 19:55 - loss: 0.3849 - regression_loss: 0.3518 - classification_loss: 0.0332
 467/1000 [=============>................] - ETA: 19:53 - loss: 0.3852 - regression_loss: 0.3520 - classification_loss: 0.0332
 468/1000 [=============>................] - ETA: 19:51 - loss: 0.3853 - regression_loss: 0.3521 - classification_loss: 0.0332
 469/1000 [=============>................] - ETA: 19:49 - loss: 0.3856 - regression_loss: 0.3524 - classification_loss: 0.0332
 470/1000 [=============>................] - ETA: 19:47 - loss: 0.3857 - regression_loss: 0.3524 - classification_loss: 0.0332
 471/1000 [=============>................] - ETA: 19:45 - loss: 0.3859 - regression_loss: 0.3526 - classification_loss: 0.0332
 472/1000 [=============>................] - ETA: 19:43 - loss: 0.3860 - regression_loss: 0.3527 - classification_loss: 0.0333
 473/1000 [=============>................] - ETA: 19:41 - loss: 0.3858 - regression_loss: 0.3526 - classification_loss: 0.0333
 474/1000 [=============>................] - ETA: 19:38 - loss: 0.3851 - regression_loss: 0.3519 - classification_loss: 0.0332
 475/1000 [=============>................] - ETA: 19:36 - loss: 0.3848 - regression_loss: 0.3516 - classification_loss: 0.0332
 476/1000 [=============>................] - ETA: 19:34 - loss: 0.3851 - regression_loss: 0.3519 - classification_loss: 0.0332
 477/1000 [=============>................] - ETA: 19:31 - loss: 0.3847 - regression_loss: 0.3515 - classification_loss: 0.0332
 478/1000 [=============>................] - ETA: 19:29 - loss: 0.3847 - regression_loss: 0.3515 - classification_loss: 0.0332
 479/1000 [=============>................] - ETA: 19:26 - loss: 0.3841 - regression_loss: 0.3509 - classification_loss: 0.0332
 480/1000 [=============>................] - ETA: 19:24 - loss: 0.3842 - regression_loss: 0.3511 - classification_loss: 0.0332
 481/1000 [=============>................] - ETA: 19:22 - loss: 0.3845 - regression_loss: 0.3513 - classification_loss: 0.0332
 482/1000 [=============>................] - ETA: 19:20 - loss: 0.3843 - regression_loss: 0.3511 - classification_loss: 0.0332
 483/1000 [=============>................] - ETA: 19:18 - loss: 0.3844 - regression_loss: 0.3512 - classification_loss: 0.0332
 484/1000 [=============>................] - ETA: 19:15 - loss: 0.3841 - regression_loss: 0.3510 - classification_loss: 0.0332
 485/1000 [=============>................] - ETA: 19:12 - loss: 0.3835 - regression_loss: 0.3504 - classification_loss: 0.0331
 486/1000 [=============>................] - ETA: 19:10 - loss: 0.3838 - regression_loss: 0.3507 - classification_loss: 0.0331
 487/1000 [=============>................] - ETA: 19:08 - loss: 0.3841 - regression_loss: 0.3510 - classification_loss: 0.0332
 488/1000 [=============>................] - ETA: 19:06 - loss: 0.3840 - regression_loss: 0.3509 - classification_loss: 0.0332
 489/1000 [=============>................] - ETA: 19:04 - loss: 0.3841 - regression_loss: 0.3509 - classification_loss: 0.0332
 490/1000 [=============>................] - ETA: 19:02 - loss: 0.3842 - regression_loss: 0.3510 - classification_loss: 0.0332
 491/1000 [=============>................] - ETA: 18:59 - loss: 0.3838 - regression_loss: 0.3506 - classification_loss: 0.0332
 492/1000 [=============>................] - ETA: 18:58 - loss: 0.3841 - regression_loss: 0.3508 - classification_loss: 0.0332
 493/1000 [=============>................] - ETA: 18:56 - loss: 0.3843 - regression_loss: 0.3511 - classification_loss: 0.0332
 494/1000 [=============>................] - ETA: 18:54 - loss: 0.3841 - regression_loss: 0.3509 - classification_loss: 0.0332
 495/1000 [=============>................] - ETA: 18:51 - loss: 0.3834 - regression_loss: 0.3502 - classification_loss: 0.0332
 496/1000 [=============>................] - ETA: 18:48 - loss: 0.3834 - regression_loss: 0.3502 - classification_loss: 0.0332
 497/1000 [=============>................] - ETA: 18:46 - loss: 0.3830 - regression_loss: 0.3499 - classification_loss: 0.0332
 498/1000 [=============>................] - ETA: 18:44 - loss: 0.3833 - regression_loss: 0.3501 - classification_loss: 0.0332
 499/1000 [=============>................] - ETA: 18:42 - loss: 0.3834 - regression_loss: 0.3502 - classification_loss: 0.0332
 500/1000 [==============>...............] - ETA: 18:39 - loss: 0.3834 - regression_loss: 0.3502 - classification_loss: 0.0332
 501/1000 [==============>...............] - ETA: 18:36 - loss: 0.3828 - regression_loss: 0.3496 - classification_loss: 0.0331
 502/1000 [==============>...............] - ETA: 18:34 - loss: 0.3827 - regression_loss: 0.3496 - classification_loss: 0.0331
 503/1000 [==============>...............] - ETA: 18:33 - loss: 0.3829 - regression_loss: 0.3498 - classification_loss: 0.0331
 504/1000 [==============>...............] - ETA: 18:30 - loss: 0.3823 - regression_loss: 0.3492 - classification_loss: 0.0331
 505/1000 [==============>...............] - ETA: 18:27 - loss: 0.3822 - regression_loss: 0.3492 - classification_loss: 0.0331
 506/1000 [==============>...............] - ETA: 18:25 - loss: 0.3820 - regression_loss: 0.3489 - classification_loss: 0.0331
 507/1000 [==============>...............] - ETA: 18:23 - loss: 0.3822 - regression_loss: 0.3491 - classification_loss: 0.0331
 508/1000 [==============>...............] - ETA: 18:21 - loss: 0.3823 - regression_loss: 0.3492 - classification_loss: 0.0331
 509/1000 [==============>...............] - ETA: 18:19 - loss: 0.3821 - regression_loss: 0.3491 - classification_loss: 0.0331
 510/1000 [==============>...............] - ETA: 18:17 - loss: 0.3823 - regression_loss: 0.3492 - classification_loss: 0.0331
 511/1000 [==============>...............] - ETA: 18:14 - loss: 0.3820 - regression_loss: 0.3489 - classification_loss: 0.0331
 512/1000 [==============>...............] - ETA: 18:12 - loss: 0.3822 - regression_loss: 0.3491 - classification_loss: 0.0331
 513/1000 [==============>...............] - ETA: 18:10 - loss: 0.3823 - regression_loss: 0.3492 - classification_loss: 0.0331
 514/1000 [==============>...............] - ETA: 18:09 - loss: 0.3825 - regression_loss: 0.3494 - classification_loss: 0.0331
 515/1000 [==============>...............] - ETA: 18:06 - loss: 0.3820 - regression_loss: 0.3489 - classification_loss: 0.0330
 516/1000 [==============>...............] - ETA: 18:03 - loss: 0.3820 - regression_loss: 0.3489 - classification_loss: 0.0330
 517/1000 [==============>...............] - ETA: 18:01 - loss: 0.3819 - regression_loss: 0.3488 - classification_loss: 0.0330
 518/1000 [==============>...............] - ETA: 17:58 - loss: 0.3813 - regression_loss: 0.3483 - classification_loss: 0.0330
 519/1000 [==============>...............] - ETA: 17:57 - loss: 0.3815 - regression_loss: 0.3485 - classification_loss: 0.0330
 520/1000 [==============>...............] - ETA: 17:54 - loss: 0.3812 - regression_loss: 0.3482 - classification_loss: 0.0330
 521/1000 [==============>...............] - ETA: 17:52 - loss: 0.3812 - regression_loss: 0.3483 - classification_loss: 0.0330
 522/1000 [==============>...............] - ETA: 17:50 - loss: 0.3815 - regression_loss: 0.3485 - classification_loss: 0.0330
 523/1000 [==============>...............] - ETA: 17:48 - loss: 0.3813 - regression_loss: 0.3483 - classification_loss: 0.0330
 524/1000 [==============>...............] - ETA: 17:46 - loss: 0.3814 - regression_loss: 0.3484 - classification_loss: 0.0330
 525/1000 [==============>...............] - ETA: 17:44 - loss: 0.3816 - regression_loss: 0.3486 - classification_loss: 0.0330
 526/1000 [==============>...............] - ETA: 17:41 - loss: 0.3810 - regression_loss: 0.3480 - classification_loss: 0.0330
 527/1000 [==============>...............] - ETA: 17:39 - loss: 0.3810 - regression_loss: 0.3480 - classification_loss: 0.0330
 528/1000 [==============>...............] - ETA: 17:37 - loss: 0.3812 - regression_loss: 0.3482 - classification_loss: 0.0330
 529/1000 [==============>...............] - ETA: 17:35 - loss: 0.3813 - regression_loss: 0.3483 - classification_loss: 0.0330
 530/1000 [==============>...............] - ETA: 17:33 - loss: 0.3811 - regression_loss: 0.3481 - classification_loss: 0.0330
 531/1000 [==============>...............] - ETA: 17:30 - loss: 0.3808 - regression_loss: 0.3478 - classification_loss: 0.0329
 532/1000 [==============>...............] - ETA: 17:28 - loss: 0.3809 - regression_loss: 0.3479 - classification_loss: 0.0330
 533/1000 [==============>...............] - ETA: 17:26 - loss: 0.3808 - regression_loss: 0.3478 - classification_loss: 0.0330
 534/1000 [===============>..............] - ETA: 17:24 - loss: 0.3810 - regression_loss: 0.3480 - classification_loss: 0.0330
 535/1000 [===============>..............] - ETA: 17:21 - loss: 0.3804 - regression_loss: 0.3474 - classification_loss: 0.0329
 536/1000 [===============>..............] - ETA: 17:19 - loss: 0.3806 - regression_loss: 0.3477 - classification_loss: 0.0329
 537/1000 [===============>..............] - ETA: 17:17 - loss: 0.3803 - regression_loss: 0.3474 - classification_loss: 0.0329
 538/1000 [===============>..............] - ETA: 17:14 - loss: 0.3804 - regression_loss: 0.3475 - classification_loss: 0.0329
 539/1000 [===============>..............] - ETA: 17:12 - loss: 0.3803 - regression_loss: 0.3474 - classification_loss: 0.0329
 540/1000 [===============>..............] - ETA: 17:09 - loss: 0.3800 - regression_loss: 0.3471 - classification_loss: 0.0329
 541/1000 [===============>..............] - ETA: 17:07 - loss: 0.3801 - regression_loss: 0.3472 - classification_loss: 0.0329
 542/1000 [===============>..............] - ETA: 17:06 - loss: 0.3803 - regression_loss: 0.3474 - classification_loss: 0.0329
 543/1000 [===============>..............] - ETA: 17:03 - loss: 0.3803 - regression_loss: 0.3474 - classification_loss: 0.0329
 544/1000 [===============>..............] - ETA: 17:00 - loss: 0.3798 - regression_loss: 0.3469 - classification_loss: 0.0329
 545/1000 [===============>..............] - ETA: 16:59 - loss: 0.3800 - regression_loss: 0.3471 - classification_loss: 0.0329
 546/1000 [===============>..............] - ETA: 16:56 - loss: 0.3799 - regression_loss: 0.3471 - classification_loss: 0.0329
 547/1000 [===============>..............] - ETA: 16:55 - loss: 0.3801 - regression_loss: 0.3472 - classification_loss: 0.0329
 548/1000 [===============>..............] - ETA: 16:52 - loss: 0.3798 - regression_loss: 0.3469 - classification_loss: 0.0329
 549/1000 [===============>..............] - ETA: 16:50 - loss: 0.3801 - regression_loss: 0.3473 - classification_loss: 0.0329
 550/1000 [===============>..............] - ETA: 16:48 - loss: 0.3804 - regression_loss: 0.3475 - classification_loss: 0.0329
 551/1000 [===============>..............] - ETA: 16:46 - loss: 0.3804 - regression_loss: 0.3476 - classification_loss: 0.0329
 552/1000 [===============>..............] - ETA: 16:43 - loss: 0.3800 - regression_loss: 0.3471 - classification_loss: 0.0328
 553/1000 [===============>..............] - ETA: 16:41 - loss: 0.3800 - regression_loss: 0.3472 - classification_loss: 0.0328
 554/1000 [===============>..............] - ETA: 16:38 - loss: 0.3797 - regression_loss: 0.3469 - classification_loss: 0.0328
 555/1000 [===============>..............] - ETA: 16:36 - loss: 0.3800 - regression_loss: 0.3472 - classification_loss: 0.0328
 556/1000 [===============>..............] - ETA: 16:33 - loss: 0.3794 - regression_loss: 0.3466 - classification_loss: 0.0328
 557/1000 [===============>..............] - ETA: 16:31 - loss: 0.3795 - regression_loss: 0.3467 - classification_loss: 0.0328
 558/1000 [===============>..............] - ETA: 16:29 - loss: 0.3794 - regression_loss: 0.3466 - classification_loss: 0.0328
 559/1000 [===============>..............] - ETA: 16:27 - loss: 0.3796 - regression_loss: 0.3468 - classification_loss: 0.0328
 560/1000 [===============>..............] - ETA: 16:25 - loss: 0.3796 - regression_loss: 0.3469 - classification_loss: 0.0328
 561/1000 [===============>..............] - ETA: 16:22 - loss: 0.3794 - regression_loss: 0.3466 - classification_loss: 0.0328
 562/1000 [===============>..............] - ETA: 16:20 - loss: 0.3796 - regression_loss: 0.3468 - classification_loss: 0.0328
 563/1000 [===============>..............] - ETA: 16:18 - loss: 0.3790 - regression_loss: 0.3463 - classification_loss: 0.0327
 564/1000 [===============>..............] - ETA: 16:16 - loss: 0.3790 - regression_loss: 0.3462 - classification_loss: 0.0327
 565/1000 [===============>..............] - ETA: 16:14 - loss: 0.3792 - regression_loss: 0.3464 - classification_loss: 0.0328
 566/1000 [===============>..............] - ETA: 16:12 - loss: 0.3793 - regression_loss: 0.3465 - classification_loss: 0.0328
 567/1000 [================>.............] - ETA: 16:10 - loss: 0.3794 - regression_loss: 0.3466 - classification_loss: 0.0328
 568/1000 [================>.............] - ETA: 16:08 - loss: 0.3796 - regression_loss: 0.3468 - classification_loss: 0.0328
 569/1000 [================>.............] - ETA: 16:05 - loss: 0.3791 - regression_loss: 0.3463 - classification_loss: 0.0327
 570/1000 [================>.............] - ETA: 16:02 - loss: 0.3788 - regression_loss: 0.3461 - classification_loss: 0.0327
 571/1000 [================>.............] - ETA: 16:00 - loss: 0.3791 - regression_loss: 0.3463 - classification_loss: 0.0327
 572/1000 [================>.............] - ETA: 15:58 - loss: 0.3790 - regression_loss: 0.3462 - classification_loss: 0.0328
 573/1000 [================>.............] - ETA: 15:56 - loss: 0.3790 - regression_loss: 0.3462 - classification_loss: 0.0328
 574/1000 [================>.............] - ETA: 15:54 - loss: 0.3790 - regression_loss: 0.3462 - classification_loss: 0.0328
 575/1000 [================>.............] - ETA: 15:52 - loss: 0.3791 - regression_loss: 0.3463 - classification_loss: 0.0328
 576/1000 [================>.............] - ETA: 15:50 - loss: 0.3793 - regression_loss: 0.3465 - classification_loss: 0.0328
 577/1000 [================>.............] - ETA: 15:47 - loss: 0.3787 - regression_loss: 0.3460 - classification_loss: 0.0327
 578/1000 [================>.............] - ETA: 15:45 - loss: 0.3786 - regression_loss: 0.3458 - classification_loss: 0.0327
 579/1000 [================>.............] - ETA: 15:42 - loss: 0.3783 - regression_loss: 0.3455 - classification_loss: 0.0327
 580/1000 [================>.............] - ETA: 15:40 - loss: 0.3785 - regression_loss: 0.3458 - classification_loss: 0.0327
 581/1000 [================>.............] - ETA: 15:38 - loss: 0.3788 - regression_loss: 0.3460 - classification_loss: 0.0328
 582/1000 [================>.............] - ETA: 15:36 - loss: 0.3785 - regression_loss: 0.3458 - classification_loss: 0.0327
 583/1000 [================>.............] - ETA: 15:34 - loss: 0.3787 - regression_loss: 0.3460 - classification_loss: 0.0328
 584/1000 [================>.............] - ETA: 15:31 - loss: 0.3783 - regression_loss: 0.3456 - classification_loss: 0.0327
 585/1000 [================>.............] - ETA: 15:29 - loss: 0.3784 - regression_loss: 0.3456 - classification_loss: 0.0327
 586/1000 [================>.............] - ETA: 15:27 - loss: 0.3784 - regression_loss: 0.3457 - classification_loss: 0.0327
 587/1000 [================>.............] - ETA: 15:25 - loss: 0.3783 - regression_loss: 0.3456 - classification_loss: 0.0327
 588/1000 [================>.............] - ETA: 15:22 - loss: 0.3781 - regression_loss: 0.3454 - classification_loss: 0.0327
 589/1000 [================>.............] - ETA: 15:20 - loss: 0.3782 - regression_loss: 0.3455 - classification_loss: 0.0327
 590/1000 [================>.............] - ETA: 15:18 - loss: 0.3782 - regression_loss: 0.3455 - classification_loss: 0.0327
 591/1000 [================>.............] - ETA: 15:15 - loss: 0.3777 - regression_loss: 0.3450 - classification_loss: 0.0327
 592/1000 [================>.............] - ETA: 15:13 - loss: 0.3776 - regression_loss: 0.3449 - classification_loss: 0.0327
 593/1000 [================>.............] - ETA: 15:11 - loss: 0.3777 - regression_loss: 0.3450 - classification_loss: 0.0327
 594/1000 [================>.............] - ETA: 15:09 - loss: 0.3779 - regression_loss: 0.3452 - classification_loss: 0.0327
 595/1000 [================>.............] - ETA: 15:06 - loss: 0.3776 - regression_loss: 0.3449 - classification_loss: 0.0327
 596/1000 [================>.............] - ETA: 15:04 - loss: 0.3775 - regression_loss: 0.3448 - classification_loss: 0.0327
 597/1000 [================>.............] - ETA: 15:02 - loss: 0.3776 - regression_loss: 0.3450 - classification_loss: 0.0327
 598/1000 [================>.............] - ETA: 15:00 - loss: 0.3773 - regression_loss: 0.3447 - classification_loss: 0.0326
 599/1000 [================>.............] - ETA: 14:57 - loss: 0.3773 - regression_loss: 0.3447 - classification_loss: 0.0326
 600/1000 [=================>............] - ETA: 14:55 - loss: 0.3775 - regression_loss: 0.3448 - classification_loss: 0.0326
 601/1000 [=================>............] - ETA: 14:53 - loss: 0.3775 - regression_loss: 0.3449 - classification_loss: 0.0326
 602/1000 [=================>............] - ETA: 14:51 - loss: 0.3775 - regression_loss: 0.3449 - classification_loss: 0.0326
 603/1000 [=================>............] - ETA: 14:48 - loss: 0.3772 - regression_loss: 0.3446 - classification_loss: 0.0326
 604/1000 [=================>............] - ETA: 14:46 - loss: 0.3774 - regression_loss: 0.3448 - classification_loss: 0.0326
 605/1000 [=================>............] - ETA: 14:44 - loss: 0.3776 - regression_loss: 0.3450 - classification_loss: 0.0326
 606/1000 [=================>............] - ETA: 14:42 - loss: 0.3771 - regression_loss: 0.3445 - classification_loss: 0.0326
 607/1000 [=================>............] - ETA: 14:40 - loss: 0.3770 - regression_loss: 0.3444 - classification_loss: 0.0326
 608/1000 [=================>............] - ETA: 14:38 - loss: 0.3770 - regression_loss: 0.3445 - classification_loss: 0.0326
 609/1000 [=================>............] - ETA: 14:36 - loss: 0.3772 - regression_loss: 0.3446 - classification_loss: 0.0326
 610/1000 [=================>............] - ETA: 14:34 - loss: 0.3774 - regression_loss: 0.3448 - classification_loss: 0.0326
 611/1000 [=================>............] - ETA: 14:32 - loss: 0.3773 - regression_loss: 0.3447 - classification_loss: 0.0326
 612/1000 [=================>............] - ETA: 14:29 - loss: 0.3773 - regression_loss: 0.3447 - classification_loss: 0.0326
 613/1000 [=================>............] - ETA: 14:27 - loss: 0.3771 - regression_loss: 0.3445 - classification_loss: 0.0326
 614/1000 [=================>............] - ETA: 14:25 - loss: 0.3772 - regression_loss: 0.3446 - classification_loss: 0.0326
 615/1000 [=================>............] - ETA: 14:22 - loss: 0.3767 - regression_loss: 0.3442 - classification_loss: 0.0325
 616/1000 [=================>............] - ETA: 14:20 - loss: 0.3770 - regression_loss: 0.3444 - classification_loss: 0.0326
 617/1000 [=================>............] - ETA: 14:18 - loss: 0.3770 - regression_loss: 0.3444 - classification_loss: 0.0325
 618/1000 [=================>............] - ETA: 14:15 - loss: 0.3767 - regression_loss: 0.3441 - classification_loss: 0.0325
 619/1000 [=================>............] - ETA: 14:13 - loss: 0.3768 - regression_loss: 0.3443 - classification_loss: 0.0325
 620/1000 [=================>............] - ETA: 14:11 - loss: 0.3768 - regression_loss: 0.3442 - classification_loss: 0.0325
 621/1000 [=================>............] - ETA: 14:08 - loss: 0.3763 - regression_loss: 0.3438 - classification_loss: 0.0325
 622/1000 [=================>............] - ETA: 14:06 - loss: 0.3766 - regression_loss: 0.3440 - classification_loss: 0.0325
 623/1000 [=================>............] - ETA: 14:04 - loss: 0.3764 - regression_loss: 0.3439 - classification_loss: 0.0325
 624/1000 [=================>............] - ETA: 14:01 - loss: 0.3759 - regression_loss: 0.3434 - classification_loss: 0.0324
 625/1000 [=================>............] - ETA: 13:59 - loss: 0.3762 - regression_loss: 0.3437 - classification_loss: 0.0325
 626/1000 [=================>............] - ETA: 13:57 - loss: 0.3764 - regression_loss: 0.3440 - classification_loss: 0.0325
 627/1000 [=================>............] - ETA: 13:55 - loss: 0.3765 - regression_loss: 0.3440 - classification_loss: 0.0325
 628/1000 [=================>............] - ETA: 13:53 - loss: 0.3765 - regression_loss: 0.3441 - classification_loss: 0.0325
 629/1000 [=================>............] - ETA: 13:51 - loss: 0.3764 - regression_loss: 0.3440 - classification_loss: 0.0325
 630/1000 [=================>............] - ETA: 13:48 - loss: 0.3766 - regression_loss: 0.3442 - classification_loss: 0.0325
 631/1000 [=================>............] - ETA: 13:46 - loss: 0.3766 - regression_loss: 0.3442 - classification_loss: 0.0325
 632/1000 [=================>............] - ETA: 13:44 - loss: 0.3762 - regression_loss: 0.3438 - classification_loss: 0.0324
 633/1000 [=================>............] - ETA: 13:41 - loss: 0.3759 - regression_loss: 0.3435 - classification_loss: 0.0324
 634/1000 [==================>...........] - ETA: 13:39 - loss: 0.3760 - regression_loss: 0.3436 - classification_loss: 0.0324
 635/1000 [==================>...........] - ETA: 13:37 - loss: 0.3759 - regression_loss: 0.3435 - classification_loss: 0.0324
 636/1000 [==================>...........] - ETA: 13:35 - loss: 0.3761 - regression_loss: 0.3437 - classification_loss: 0.0324
 637/1000 [==================>...........] - ETA: 13:33 - loss: 0.3761 - regression_loss: 0.3436 - classification_loss: 0.0324
 638/1000 [==================>...........] - ETA: 13:30 - loss: 0.3759 - regression_loss: 0.3435 - classification_loss: 0.0324
 639/1000 [==================>...........] - ETA: 13:28 - loss: 0.3754 - regression_loss: 0.3431 - classification_loss: 0.0324
 640/1000 [==================>...........] - ETA: 13:25 - loss: 0.3755 - regression_loss: 0.3432 - classification_loss: 0.0324
 641/1000 [==================>...........] - ETA: 13:23 - loss: 0.3757 - regression_loss: 0.3433 - classification_loss: 0.0324
 642/1000 [==================>...........] - ETA: 13:21 - loss: 0.3757 - regression_loss: 0.3433 - classification_loss: 0.0324
 643/1000 [==================>...........] - ETA: 13:19 - loss: 0.3759 - regression_loss: 0.3435 - classification_loss: 0.0324
 644/1000 [==================>...........] - ETA: 13:17 - loss: 0.3759 - regression_loss: 0.3435 - classification_loss: 0.0324
 645/1000 [==================>...........] - ETA: 13:15 - loss: 0.3761 - regression_loss: 0.3437 - classification_loss: 0.0324
 646/1000 [==================>...........] - ETA: 13:13 - loss: 0.3763 - regression_loss: 0.3438 - classification_loss: 0.0324
 647/1000 [==================>...........] - ETA: 13:11 - loss: 0.3762 - regression_loss: 0.3438 - classification_loss: 0.0324
 648/1000 [==================>...........] - ETA: 13:08 - loss: 0.3757 - regression_loss: 0.3433 - classification_loss: 0.0324
 649/1000 [==================>...........] - ETA: 13:06 - loss: 0.3758 - regression_loss: 0.3434 - classification_loss: 0.0324
 650/1000 [==================>...........] - ETA: 13:03 - loss: 0.3756 - regression_loss: 0.3432 - classification_loss: 0.0324
 651/1000 [==================>...........] - ETA: 13:01 - loss: 0.3755 - regression_loss: 0.3431 - classification_loss: 0.0324
 652/1000 [==================>...........] - ETA: 12:59 - loss: 0.3752 - regression_loss: 0.3428 - classification_loss: 0.0323
 653/1000 [==================>...........] - ETA: 12:57 - loss: 0.3753 - regression_loss: 0.3430 - classification_loss: 0.0324
 654/1000 [==================>...........] - ETA: 12:54 - loss: 0.3748 - regression_loss: 0.3425 - classification_loss: 0.0323
 655/1000 [==================>...........] - ETA: 12:52 - loss: 0.3751 - regression_loss: 0.3428 - classification_loss: 0.0323
 656/1000 [==================>...........] - ETA: 12:50 - loss: 0.3752 - regression_loss: 0.3429 - classification_loss: 0.0323
 657/1000 [==================>...........] - ETA: 12:48 - loss: 0.3753 - regression_loss: 0.3430 - classification_loss: 0.0323
 658/1000 [==================>...........] - ETA: 12:46 - loss: 0.3755 - regression_loss: 0.3431 - classification_loss: 0.0323
 659/1000 [==================>...........] - ETA: 12:44 - loss: 0.3754 - regression_loss: 0.3430 - classification_loss: 0.0323
 660/1000 [==================>...........] - ETA: 12:42 - loss: 0.3755 - regression_loss: 0.3431 - classification_loss: 0.0323
 661/1000 [==================>...........] - ETA: 12:40 - loss: 0.3756 - regression_loss: 0.3433 - classification_loss: 0.0324
 662/1000 [==================>...........] - ETA: 12:37 - loss: 0.3756 - regression_loss: 0.3433 - classification_loss: 0.0324
 663/1000 [==================>...........] - ETA: 12:35 - loss: 0.3751 - regression_loss: 0.3428 - classification_loss: 0.0323
 664/1000 [==================>...........] - ETA: 12:32 - loss: 0.3752 - regression_loss: 0.3428 - classification_loss: 0.0323
 665/1000 [==================>...........] - ETA: 12:30 - loss: 0.3747 - regression_loss: 0.3424 - classification_loss: 0.0323
 666/1000 [==================>...........] - ETA: 12:27 - loss: 0.3746 - regression_loss: 0.3423 - classification_loss: 0.0323
 667/1000 [===================>..........] - ETA: 12:25 - loss: 0.3748 - regression_loss: 0.3426 - classification_loss: 0.0323
 668/1000 [===================>..........] - ETA: 12:23 - loss: 0.3753 - regression_loss: 0.3430 - classification_loss: 0.0323
 669/1000 [===================>..........] - ETA: 12:21 - loss: 0.3755 - regression_loss: 0.3432 - classification_loss: 0.0323
 670/1000 [===================>..........] - ETA: 12:19 - loss: 0.3757 - regression_loss: 0.3434 - classification_loss: 0.0323
 671/1000 [===================>..........] - ETA: 12:16 - loss: 0.3759 - regression_loss: 0.3436 - classification_loss: 0.0323
 672/1000 [===================>..........] - ETA: 12:14 - loss: 0.3762 - regression_loss: 0.3439 - classification_loss: 0.0323
 673/1000 [===================>..........] - ETA: 12:12 - loss: 0.3763 - regression_loss: 0.3440 - classification_loss: 0.0323
 674/1000 [===================>..........] - ETA: 12:10 - loss: 0.3761 - regression_loss: 0.3438 - classification_loss: 0.0323
 675/1000 [===================>..........] - ETA: 12:07 - loss: 0.3761 - regression_loss: 0.3438 - classification_loss: 0.0323
 676/1000 [===================>..........] - ETA: 12:05 - loss: 0.3764 - regression_loss: 0.3441 - classification_loss: 0.0323
 677/1000 [===================>..........] - ETA: 12:03 - loss: 0.3765 - regression_loss: 0.3442 - classification_loss: 0.0323
 678/1000 [===================>..........] - ETA: 12:01 - loss: 0.3763 - regression_loss: 0.3440 - classification_loss: 0.0323
 679/1000 [===================>..........] - ETA: 11:59 - loss: 0.3764 - regression_loss: 0.3441 - classification_loss: 0.0323
 680/1000 [===================>..........] - ETA: 11:56 - loss: 0.3766 - regression_loss: 0.3443 - classification_loss: 0.0323
 681/1000 [===================>..........] - ETA: 11:54 - loss: 0.3765 - regression_loss: 0.3442 - classification_loss: 0.0323
 682/1000 [===================>..........] - ETA: 11:52 - loss: 0.3763 - regression_loss: 0.3440 - classification_loss: 0.0323
 683/1000 [===================>..........] - ETA: 11:50 - loss: 0.3764 - regression_loss: 0.3441 - classification_loss: 0.0323
 684/1000 [===================>..........] - ETA: 11:48 - loss: 0.3765 - regression_loss: 0.3442 - classification_loss: 0.0323
 685/1000 [===================>..........] - ETA: 11:45 - loss: 0.3761 - regression_loss: 0.3438 - classification_loss: 0.0323
 686/1000 [===================>..........] - ETA: 11:43 - loss: 0.3761 - regression_loss: 0.3438 - classification_loss: 0.0323
 687/1000 [===================>..........] - ETA: 11:41 - loss: 0.3763 - regression_loss: 0.3440 - classification_loss: 0.0323
 688/1000 [===================>..........] - ETA: 11:38 - loss: 0.3758 - regression_loss: 0.3435 - classification_loss: 0.0322
 689/1000 [===================>..........] - ETA: 11:36 - loss: 0.3756 - regression_loss: 0.3434 - classification_loss: 0.0322
 690/1000 [===================>..........] - ETA: 11:34 - loss: 0.3756 - regression_loss: 0.3434 - classification_loss: 0.0322
 691/1000 [===================>..........] - ETA: 11:32 - loss: 0.3758 - regression_loss: 0.3436 - classification_loss: 0.0322
 692/1000 [===================>..........] - ETA: 11:29 - loss: 0.3760 - regression_loss: 0.3438 - classification_loss: 0.0323
 693/1000 [===================>..........] - ETA: 11:27 - loss: 0.3762 - regression_loss: 0.3439 - classification_loss: 0.0323
 694/1000 [===================>..........] - ETA: 11:25 - loss: 0.3761 - regression_loss: 0.3439 - classification_loss: 0.0323
 695/1000 [===================>..........] - ETA: 11:23 - loss: 0.3763 - regression_loss: 0.3440 - classification_loss: 0.0323
 696/1000 [===================>..........] - ETA: 11:21 - loss: 0.3763 - regression_loss: 0.3441 - classification_loss: 0.0323
 697/1000 [===================>..........] - ETA: 11:19 - loss: 0.3761 - regression_loss: 0.3439 - classification_loss: 0.0323
 698/1000 [===================>..........] - ETA: 11:16 - loss: 0.3758 - regression_loss: 0.3435 - classification_loss: 0.0322
 699/1000 [===================>..........] - ETA: 11:14 - loss: 0.3758 - regression_loss: 0.3436 - classification_loss: 0.0322
 700/1000 [====================>.........] - ETA: 11:12 - loss: 0.3759 - regression_loss: 0.3437 - classification_loss: 0.0322
 701/1000 [====================>.........] - ETA: 11:10 - loss: 0.3761 - regression_loss: 0.3438 - classification_loss: 0.0322
 702/1000 [====================>.........] - ETA: 11:07 - loss: 0.3756 - regression_loss: 0.3434 - classification_loss: 0.0322
 703/1000 [====================>.........] - ETA: 11:05 - loss: 0.3758 - regression_loss: 0.3436 - classification_loss: 0.0322
 704/1000 [====================>.........] - ETA: 11:03 - loss: 0.3758 - regression_loss: 0.3436 - classification_loss: 0.0322
 705/1000 [====================>.........] - ETA: 11:00 - loss: 0.3756 - regression_loss: 0.3434 - classification_loss: 0.0322
 706/1000 [====================>.........] - ETA: 10:58 - loss: 0.3755 - regression_loss: 0.3433 - classification_loss: 0.0322
 707/1000 [====================>.........] - ETA: 10:56 - loss: 0.3753 - regression_loss: 0.3431 - classification_loss: 0.0322
 708/1000 [====================>.........] - ETA: 10:53 - loss: 0.3753 - regression_loss: 0.3431 - classification_loss: 0.0322
 709/1000 [====================>.........] - ETA: 10:51 - loss: 0.3754 - regression_loss: 0.3433 - classification_loss: 0.0322
 710/1000 [====================>.........] - ETA: 10:49 - loss: 0.3754 - regression_loss: 0.3432 - classification_loss: 0.0322
 711/1000 [====================>.........] - ETA: 10:47 - loss: 0.3749 - regression_loss: 0.3428 - classification_loss: 0.0322
 712/1000 [====================>.........] - ETA: 10:45 - loss: 0.3751 - regression_loss: 0.3429 - classification_loss: 0.0322
 713/1000 [====================>.........] - ETA: 10:42 - loss: 0.3752 - regression_loss: 0.3431 - classification_loss: 0.0322
 714/1000 [====================>.........] - ETA: 10:40 - loss: 0.3748 - regression_loss: 0.3426 - classification_loss: 0.0321
 715/1000 [====================>.........] - ETA: 10:38 - loss: 0.3747 - regression_loss: 0.3426 - classification_loss: 0.0321
 716/1000 [====================>.........] - ETA: 10:36 - loss: 0.3749 - regression_loss: 0.3427 - classification_loss: 0.0321
 717/1000 [====================>.........] - ETA: 10:34 - loss: 0.3749 - regression_loss: 0.3428 - classification_loss: 0.0321
 718/1000 [====================>.........] - ETA: 10:32 - loss: 0.3751 - regression_loss: 0.3429 - classification_loss: 0.0321
 719/1000 [====================>.........] - ETA: 10:29 - loss: 0.3751 - regression_loss: 0.3430 - classification_loss: 0.0321
 720/1000 [====================>.........] - ETA: 10:27 - loss: 0.3750 - regression_loss: 0.3428 - classification_loss: 0.0321
 721/1000 [====================>.........] - ETA: 10:25 - loss: 0.3749 - regression_loss: 0.3428 - classification_loss: 0.0321
 722/1000 [====================>.........] - ETA: 10:22 - loss: 0.3748 - regression_loss: 0.3427 - classification_loss: 0.0321
 723/1000 [====================>.........] - ETA: 10:20 - loss: 0.3750 - regression_loss: 0.3429 - classification_loss: 0.0321
 724/1000 [====================>.........] - ETA: 10:18 - loss: 0.3751 - regression_loss: 0.3430 - classification_loss: 0.0321
 725/1000 [====================>.........] - ETA: 10:16 - loss: 0.3747 - regression_loss: 0.3426 - classification_loss: 0.0321
 726/1000 [====================>.........] - ETA: 10:13 - loss: 0.3744 - regression_loss: 0.3423 - classification_loss: 0.0321
 727/1000 [====================>.........] - ETA: 10:11 - loss: 0.3745 - regression_loss: 0.3424 - classification_loss: 0.0321
 728/1000 [====================>.........] - ETA: 10:09 - loss: 0.3742 - regression_loss: 0.3422 - classification_loss: 0.0321
 729/1000 [====================>.........] - ETA: 10:06 - loss: 0.3739 - regression_loss: 0.3418 - classification_loss: 0.0320
 730/1000 [====================>.........] - ETA: 10:04 - loss: 0.3741 - regression_loss: 0.3420 - classification_loss: 0.0320
 731/1000 [====================>.........] - ETA: 10:02 - loss: 0.3740 - regression_loss: 0.3419 - classification_loss: 0.0321
 732/1000 [====================>.........] - ETA: 10:00 - loss: 0.3741 - regression_loss: 0.3420 - classification_loss: 0.0321
 733/1000 [====================>.........] - ETA: 9:58 - loss: 0.3741 - regression_loss: 0.3421 - classification_loss: 0.0321 
 734/1000 [=====================>........] - ETA: 9:56 - loss: 0.3743 - regression_loss: 0.3422 - classification_loss: 0.0321
 735/1000 [=====================>........] - ETA: 9:53 - loss: 0.3738 - regression_loss: 0.3418 - classification_loss: 0.0320
 736/1000 [=====================>........] - ETA: 9:51 - loss: 0.3740 - regression_loss: 0.3419 - classification_loss: 0.0320
 737/1000 [=====================>........] - ETA: 9:49 - loss: 0.3740 - regression_loss: 0.3419 - classification_loss: 0.0320
 738/1000 [=====================>........] - ETA: 9:47 - loss: 0.3739 - regression_loss: 0.3418 - classification_loss: 0.0320
 739/1000 [=====================>........] - ETA: 9:44 - loss: 0.3739 - regression_loss: 0.3419 - classification_loss: 0.0320
 740/1000 [=====================>........] - ETA: 9:42 - loss: 0.3741 - regression_loss: 0.3420 - classification_loss: 0.0320
 741/1000 [=====================>........] - ETA: 9:40 - loss: 0.3739 - regression_loss: 0.3419 - classification_loss: 0.0320
 742/1000 [=====================>........] - ETA: 9:38 - loss: 0.3738 - regression_loss: 0.3418 - classification_loss: 0.0320
 743/1000 [=====================>........] - ETA: 9:36 - loss: 0.3738 - regression_loss: 0.3418 - classification_loss: 0.0320
 744/1000 [=====================>........] - ETA: 9:33 - loss: 0.3740 - regression_loss: 0.3419 - classification_loss: 0.0320
 745/1000 [=====================>........] - ETA: 9:31 - loss: 0.3740 - regression_loss: 0.3419 - classification_loss: 0.0320
 746/1000 [=====================>........] - ETA: 9:29 - loss: 0.3737 - regression_loss: 0.3417 - classification_loss: 0.0320
 747/1000 [=====================>........] - ETA: 9:26 - loss: 0.3734 - regression_loss: 0.3414 - classification_loss: 0.0320
 748/1000 [=====================>........] - ETA: 9:24 - loss: 0.3736 - regression_loss: 0.3416 - classification_loss: 0.0320
 749/1000 [=====================>........] - ETA: 9:22 - loss: 0.3736 - regression_loss: 0.3416 - classification_loss: 0.0320
 750/1000 [=====================>........] - ETA: 9:20 - loss: 0.3738 - regression_loss: 0.3418 - classification_loss: 0.0320
 751/1000 [=====================>........] - ETA: 9:18 - loss: 0.3739 - regression_loss: 0.3419 - classification_loss: 0.0320
 752/1000 [=====================>........] - ETA: 9:15 - loss: 0.3737 - regression_loss: 0.3417 - classification_loss: 0.0320
 753/1000 [=====================>........] - ETA: 9:13 - loss: 0.3734 - regression_loss: 0.3414 - classification_loss: 0.0320
 754/1000 [=====================>........] - ETA: 9:11 - loss: 0.3734 - regression_loss: 0.3414 - classification_loss: 0.0320
 755/1000 [=====================>........] - ETA: 9:08 - loss: 0.3735 - regression_loss: 0.3415 - classification_loss: 0.0320
 756/1000 [=====================>........] - ETA: 9:06 - loss: 0.3733 - regression_loss: 0.3413 - classification_loss: 0.0320
 757/1000 [=====================>........] - ETA: 9:04 - loss: 0.3732 - regression_loss: 0.3413 - classification_loss: 0.0320
 758/1000 [=====================>........] - ETA: 9:02 - loss: 0.3733 - regression_loss: 0.3413 - classification_loss: 0.0320
 759/1000 [=====================>........] - ETA: 9:00 - loss: 0.3734 - regression_loss: 0.3415 - classification_loss: 0.0320
 760/1000 [=====================>........] - ETA: 8:57 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 761/1000 [=====================>........] - ETA: 8:55 - loss: 0.3732 - regression_loss: 0.3412 - classification_loss: 0.0320
 762/1000 [=====================>........] - ETA: 8:53 - loss: 0.3732 - regression_loss: 0.3413 - classification_loss: 0.0319
 763/1000 [=====================>........] - ETA: 8:51 - loss: 0.3732 - regression_loss: 0.3412 - classification_loss: 0.0319
 764/1000 [=====================>........] - ETA: 8:48 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 765/1000 [=====================>........] - ETA: 8:46 - loss: 0.3725 - regression_loss: 0.3406 - classification_loss: 0.0319
 766/1000 [=====================>........] - ETA: 8:44 - loss: 0.3725 - regression_loss: 0.3406 - classification_loss: 0.0319
 767/1000 [======================>.......] - ETA: 8:41 - loss: 0.3727 - regression_loss: 0.3407 - classification_loss: 0.0319
 768/1000 [======================>.......] - ETA: 8:39 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 769/1000 [======================>.......] - ETA: 8:37 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 770/1000 [======================>.......] - ETA: 8:35 - loss: 0.3731 - regression_loss: 0.3412 - classification_loss: 0.0319
 771/1000 [======================>.......] - ETA: 8:33 - loss: 0.3733 - regression_loss: 0.3414 - classification_loss: 0.0319
 772/1000 [======================>.......] - ETA: 8:30 - loss: 0.3731 - regression_loss: 0.3412 - classification_loss: 0.0319
 773/1000 [======================>.......] - ETA: 8:28 - loss: 0.3731 - regression_loss: 0.3411 - classification_loss: 0.0319
 774/1000 [======================>.......] - ETA: 8:26 - loss: 0.3727 - regression_loss: 0.3408 - classification_loss: 0.0319
 775/1000 [======================>.......] - ETA: 8:24 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 776/1000 [======================>.......] - ETA: 8:21 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 777/1000 [======================>.......] - ETA: 8:19 - loss: 0.3732 - regression_loss: 0.3413 - classification_loss: 0.0319
 778/1000 [======================>.......] - ETA: 8:17 - loss: 0.3733 - regression_loss: 0.3414 - classification_loss: 0.0319
 779/1000 [======================>.......] - ETA: 8:15 - loss: 0.3731 - regression_loss: 0.3412 - classification_loss: 0.0319
 780/1000 [======================>.......] - ETA: 8:13 - loss: 0.3733 - regression_loss: 0.3414 - classification_loss: 0.0319
 781/1000 [======================>.......] - ETA: 8:10 - loss: 0.3734 - regression_loss: 0.3415 - classification_loss: 0.0319
 782/1000 [======================>.......] - ETA: 8:08 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 783/1000 [======================>.......] - ETA: 8:06 - loss: 0.3729 - regression_loss: 0.3411 - classification_loss: 0.0319
 784/1000 [======================>.......] - ETA: 8:03 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 785/1000 [======================>.......] - ETA: 8:01 - loss: 0.3729 - regression_loss: 0.3411 - classification_loss: 0.0319
 786/1000 [======================>.......] - ETA: 7:59 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 787/1000 [======================>.......] - ETA: 7:57 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 788/1000 [======================>.......] - ETA: 7:55 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 789/1000 [======================>.......] - ETA: 7:52 - loss: 0.3726 - regression_loss: 0.3407 - classification_loss: 0.0319
 790/1000 [======================>.......] - ETA: 7:50 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 791/1000 [======================>.......] - ETA: 7:48 - loss: 0.3728 - regression_loss: 0.3409 - classification_loss: 0.0319
 792/1000 [======================>.......] - ETA: 7:46 - loss: 0.3728 - regression_loss: 0.3410 - classification_loss: 0.0319
 793/1000 [======================>.......] - ETA: 7:44 - loss: 0.3730 - regression_loss: 0.3411 - classification_loss: 0.0319
 794/1000 [======================>.......] - ETA: 7:41 - loss: 0.3726 - regression_loss: 0.3408 - classification_loss: 0.0318
 795/1000 [======================>.......] - ETA: 7:39 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 796/1000 [======================>.......] - ETA: 7:37 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 797/1000 [======================>.......] - ETA: 7:34 - loss: 0.3725 - regression_loss: 0.3407 - classification_loss: 0.0318
 798/1000 [======================>.......] - ETA: 7:32 - loss: 0.3723 - regression_loss: 0.3405 - classification_loss: 0.0318
 799/1000 [======================>.......] - ETA: 7:30 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 800/1000 [=======================>......] - ETA: 7:28 - loss: 0.3725 - regression_loss: 0.3407 - classification_loss: 0.0318
 801/1000 [=======================>......] - ETA: 7:26 - loss: 0.3727 - regression_loss: 0.3408 - classification_loss: 0.0318
 802/1000 [=======================>......] - ETA: 7:23 - loss: 0.3722 - regression_loss: 0.3405 - classification_loss: 0.0318
 803/1000 [=======================>......] - ETA: 7:21 - loss: 0.3723 - regression_loss: 0.3405 - classification_loss: 0.0318
 804/1000 [=======================>......] - ETA: 7:19 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 805/1000 [=======================>......] - ETA: 7:17 - loss: 0.3727 - regression_loss: 0.3409 - classification_loss: 0.0318
 806/1000 [=======================>......] - ETA: 7:14 - loss: 0.3726 - regression_loss: 0.3408 - classification_loss: 0.0318
 807/1000 [=======================>......] - ETA: 7:12 - loss: 0.3725 - regression_loss: 0.3407 - classification_loss: 0.0318
 808/1000 [=======================>......] - ETA: 7:10 - loss: 0.3727 - regression_loss: 0.3409 - classification_loss: 0.0318
 809/1000 [=======================>......] - ETA: 7:08 - loss: 0.3728 - regression_loss: 0.3410 - classification_loss: 0.0318
 810/1000 [=======================>......] - ETA: 7:05 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 811/1000 [=======================>......] - ETA: 7:03 - loss: 0.3725 - regression_loss: 0.3407 - classification_loss: 0.0318
 812/1000 [=======================>......] - ETA: 7:01 - loss: 0.3725 - regression_loss: 0.3407 - classification_loss: 0.0318
 813/1000 [=======================>......] - ETA: 6:58 - loss: 0.3723 - regression_loss: 0.3405 - classification_loss: 0.0318
 814/1000 [=======================>......] - ETA: 6:56 - loss: 0.3720 - regression_loss: 0.3402 - classification_loss: 0.0317
 815/1000 [=======================>......] - ETA: 6:54 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 816/1000 [=======================>......] - ETA: 6:52 - loss: 0.3721 - regression_loss: 0.3403 - classification_loss: 0.0318
 817/1000 [=======================>......] - ETA: 6:50 - loss: 0.3722 - regression_loss: 0.3404 - classification_loss: 0.0318
 818/1000 [=======================>......] - ETA: 6:47 - loss: 0.3723 - regression_loss: 0.3405 - classification_loss: 0.0318
 819/1000 [=======================>......] - ETA: 6:45 - loss: 0.3723 - regression_loss: 0.3406 - classification_loss: 0.0318
 820/1000 [=======================>......] - ETA: 6:43 - loss: 0.3724 - regression_loss: 0.3407 - classification_loss: 0.0318
 821/1000 [=======================>......] - ETA: 6:41 - loss: 0.3724 - regression_loss: 0.3407 - classification_loss: 0.0318
 822/1000 [=======================>......] - ETA: 6:39 - loss: 0.3726 - regression_loss: 0.3408 - classification_loss: 0.0318
 823/1000 [=======================>......] - ETA: 6:36 - loss: 0.3728 - regression_loss: 0.3410 - classification_loss: 0.0318
 824/1000 [=======================>......] - ETA: 6:34 - loss: 0.3724 - regression_loss: 0.3406 - classification_loss: 0.0318
 825/1000 [=======================>......] - ETA: 6:32 - loss: 0.3722 - regression_loss: 0.3404 - classification_loss: 0.0318
 826/1000 [=======================>......] - ETA: 6:29 - loss: 0.3718 - regression_loss: 0.3400 - classification_loss: 0.0317
 827/1000 [=======================>......] - ETA: 6:27 - loss: 0.3719 - regression_loss: 0.3401 - classification_loss: 0.0317
 828/1000 [=======================>......] - ETA: 6:25 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0317
 829/1000 [=======================>......] - ETA: 6:23 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 830/1000 [=======================>......] - ETA: 6:20 - loss: 0.3721 - regression_loss: 0.3403 - classification_loss: 0.0317
 831/1000 [=======================>......] - ETA: 6:18 - loss: 0.3722 - regression_loss: 0.3404 - classification_loss: 0.0317
 832/1000 [=======================>......] - ETA: 6:16 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 833/1000 [=======================>......] - ETA: 6:14 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0317
 834/1000 [========================>.....] - ETA: 6:12 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 835/1000 [========================>.....] - ETA: 6:09 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 836/1000 [========================>.....] - ETA: 6:07 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 837/1000 [========================>.....] - ETA: 6:05 - loss: 0.3717 - regression_loss: 0.3400 - classification_loss: 0.0317
 838/1000 [========================>.....] - ETA: 6:03 - loss: 0.3717 - regression_loss: 0.3400 - classification_loss: 0.0317
 839/1000 [========================>.....] - ETA: 6:00 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0317
 840/1000 [========================>.....] - ETA: 5:58 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0317
 841/1000 [========================>.....] - ETA: 5:56 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 842/1000 [========================>.....] - ETA: 5:54 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 843/1000 [========================>.....] - ETA: 5:52 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 844/1000 [========================>.....] - ETA: 5:49 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 845/1000 [========================>.....] - ETA: 5:47 - loss: 0.3716 - regression_loss: 0.3399 - classification_loss: 0.0317
 846/1000 [========================>.....] - ETA: 5:45 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0317
 847/1000 [========================>.....] - ETA: 5:42 - loss: 0.3722 - regression_loss: 0.3404 - classification_loss: 0.0317
 848/1000 [========================>.....] - ETA: 5:40 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 849/1000 [========================>.....] - ETA: 5:38 - loss: 0.3721 - regression_loss: 0.3404 - classification_loss: 0.0317
 850/1000 [========================>.....] - ETA: 5:36 - loss: 0.3722 - regression_loss: 0.3405 - classification_loss: 0.0317
 851/1000 [========================>.....] - ETA: 5:33 - loss: 0.3721 - regression_loss: 0.3404 - classification_loss: 0.0317
 852/1000 [========================>.....] - ETA: 5:31 - loss: 0.3723 - regression_loss: 0.3406 - classification_loss: 0.0317
 853/1000 [========================>.....] - ETA: 5:29 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 854/1000 [========================>.....] - ETA: 5:27 - loss: 0.3718 - regression_loss: 0.3402 - classification_loss: 0.0317
 855/1000 [========================>.....] - ETA: 5:24 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 856/1000 [========================>.....] - ETA: 5:22 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 857/1000 [========================>.....] - ETA: 5:20 - loss: 0.3722 - regression_loss: 0.3405 - classification_loss: 0.0317
 858/1000 [========================>.....] - ETA: 5:18 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 859/1000 [========================>.....] - ETA: 5:15 - loss: 0.3720 - regression_loss: 0.3404 - classification_loss: 0.0317
 860/1000 [========================>.....] - ETA: 5:13 - loss: 0.3720 - regression_loss: 0.3403 - classification_loss: 0.0317
 861/1000 [========================>.....] - ETA: 5:11 - loss: 0.3721 - regression_loss: 0.3404 - classification_loss: 0.0317
 862/1000 [========================>.....] - ETA: 5:09 - loss: 0.3722 - regression_loss: 0.3406 - classification_loss: 0.0317
 863/1000 [========================>.....] - ETA: 5:07 - loss: 0.3723 - regression_loss: 0.3406 - classification_loss: 0.0317
 864/1000 [========================>.....] - ETA: 5:05 - loss: 0.3723 - regression_loss: 0.3406 - classification_loss: 0.0317
 865/1000 [========================>.....] - ETA: 5:02 - loss: 0.3723 - regression_loss: 0.3406 - classification_loss: 0.0317
 866/1000 [========================>.....] - ETA: 5:00 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0317
 867/1000 [=========================>....] - ETA: 4:58 - loss: 0.3717 - regression_loss: 0.3401 - classification_loss: 0.0316
 868/1000 [=========================>....] - ETA: 4:55 - loss: 0.3719 - regression_loss: 0.3402 - classification_loss: 0.0316
 869/1000 [=========================>....] - ETA: 4:53 - loss: 0.3717 - regression_loss: 0.3401 - classification_loss: 0.0316
 870/1000 [=========================>....] - ETA: 4:51 - loss: 0.3718 - regression_loss: 0.3402 - classification_loss: 0.0316
 871/1000 [=========================>....] - ETA: 4:49 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0316
 872/1000 [=========================>....] - ETA: 4:46 - loss: 0.3718 - regression_loss: 0.3401 - classification_loss: 0.0316
 873/1000 [=========================>....] - ETA: 4:44 - loss: 0.3714 - regression_loss: 0.3398 - classification_loss: 0.0316
 874/1000 [=========================>....] - ETA: 4:42 - loss: 0.3715 - regression_loss: 0.3398 - classification_loss: 0.0316
 875/1000 [=========================>....] - ETA: 4:40 - loss: 0.3716 - regression_loss: 0.3400 - classification_loss: 0.0316
 876/1000 [=========================>....] - ETA: 4:37 - loss: 0.3715 - regression_loss: 0.3399 - classification_loss: 0.0316
 877/1000 [=========================>....] - ETA: 4:35 - loss: 0.3716 - regression_loss: 0.3400 - classification_loss: 0.0316
 878/1000 [=========================>....] - ETA: 4:33 - loss: 0.3715 - regression_loss: 0.3399 - classification_loss: 0.0316
 879/1000 [=========================>....] - ETA: 4:31 - loss: 0.3716 - regression_loss: 0.3399 - classification_loss: 0.0316
 880/1000 [=========================>....] - ETA: 4:29 - loss: 0.3716 - regression_loss: 0.3400 - classification_loss: 0.0316
 881/1000 [=========================>....] - ETA: 4:26 - loss: 0.3712 - regression_loss: 0.3397 - classification_loss: 0.0316
 882/1000 [=========================>....] - ETA: 4:24 - loss: 0.3714 - regression_loss: 0.3398 - classification_loss: 0.0316
 883/1000 [=========================>....] - ETA: 4:22 - loss: 0.3710 - regression_loss: 0.3395 - classification_loss: 0.0316
 884/1000 [=========================>....] - ETA: 4:19 - loss: 0.3710 - regression_loss: 0.3395 - classification_loss: 0.0316
 885/1000 [=========================>....] - ETA: 4:17 - loss: 0.3711 - regression_loss: 0.3395 - classification_loss: 0.0316
 886/1000 [=========================>....] - ETA: 4:15 - loss: 0.3710 - regression_loss: 0.3395 - classification_loss: 0.0316
 887/1000 [=========================>....] - ETA: 4:13 - loss: 0.3711 - regression_loss: 0.3396 - classification_loss: 0.0316
 888/1000 [=========================>....] - ETA: 4:11 - loss: 0.3710 - regression_loss: 0.3394 - classification_loss: 0.0316
 889/1000 [=========================>....] - ETA: 4:08 - loss: 0.3710 - regression_loss: 0.3394 - classification_loss: 0.0316
 890/1000 [=========================>....] - ETA: 4:06 - loss: 0.3706 - regression_loss: 0.3391 - classification_loss: 0.0315
 891/1000 [=========================>....] - ETA: 4:04 - loss: 0.3707 - regression_loss: 0.3392 - classification_loss: 0.0315
 892/1000 [=========================>....] - ETA: 4:02 - loss: 0.3708 - regression_loss: 0.3393 - classification_loss: 0.0315
 893/1000 [=========================>....] - ETA: 3:59 - loss: 0.3709 - regression_loss: 0.3394 - classification_loss: 0.0315
 894/1000 [=========================>....] - ETA: 3:57 - loss: 0.3707 - regression_loss: 0.3392 - classification_loss: 0.0315
 895/1000 [=========================>....] - ETA: 3:55 - loss: 0.3707 - regression_loss: 0.3392 - classification_loss: 0.0315
 896/1000 [=========================>....] - ETA: 3:53 - loss: 0.3709 - regression_loss: 0.3393 - classification_loss: 0.0315
 897/1000 [=========================>....] - ETA: 3:50 - loss: 0.3709 - regression_loss: 0.3393 - classification_loss: 0.0315
 898/1000 [=========================>....] - ETA: 3:48 - loss: 0.3709 - regression_loss: 0.3394 - classification_loss: 0.0315
 899/1000 [=========================>....] - ETA: 3:46 - loss: 0.3708 - regression_loss: 0.3393 - classification_loss: 0.0315
 900/1000 [==========================>...] - ETA: 3:44 - loss: 0.3705 - regression_loss: 0.3390 - classification_loss: 0.0315
 901/1000 [==========================>...] - ETA: 3:41 - loss: 0.3703 - regression_loss: 0.3388 - classification_loss: 0.0315
 902/1000 [==========================>...] - ETA: 3:39 - loss: 0.3705 - regression_loss: 0.3389 - classification_loss: 0.0315
 903/1000 [==========================>...] - ETA: 3:37 - loss: 0.3706 - regression_loss: 0.3391 - classification_loss: 0.0315
 904/1000 [==========================>...] - ETA: 3:35 - loss: 0.3706 - regression_loss: 0.3391 - classification_loss: 0.0315
 905/1000 [==========================>...] - ETA: 3:32 - loss: 0.3706 - regression_loss: 0.3391 - classification_loss: 0.0315
 906/1000 [==========================>...] - ETA: 3:30 - loss: 0.3704 - regression_loss: 0.3389 - classification_loss: 0.0315
 907/1000 [==========================>...] - ETA: 3:28 - loss: 0.3704 - regression_loss: 0.3389 - classification_loss: 0.0315
 908/1000 [==========================>...] - ETA: 3:26 - loss: 0.3700 - regression_loss: 0.3385 - classification_loss: 0.0315
 909/1000 [==========================>...] - ETA: 3:23 - loss: 0.3701 - regression_loss: 0.3387 - classification_loss: 0.0315
 910/1000 [==========================>...] - ETA: 3:21 - loss: 0.3698 - regression_loss: 0.3383 - classification_loss: 0.0314
 911/1000 [==========================>...] - ETA: 3:19 - loss: 0.3696 - regression_loss: 0.3381 - classification_loss: 0.0314
 912/1000 [==========================>...] - ETA: 3:17 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 913/1000 [==========================>...] - ETA: 3:14 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 914/1000 [==========================>...] - ETA: 3:12 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 915/1000 [==========================>...] - ETA: 3:10 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 916/1000 [==========================>...] - ETA: 3:08 - loss: 0.3698 - regression_loss: 0.3384 - classification_loss: 0.0314
 917/1000 [==========================>...] - ETA: 3:06 - loss: 0.3699 - regression_loss: 0.3384 - classification_loss: 0.0314
 918/1000 [==========================>...] - ETA: 3:03 - loss: 0.3700 - regression_loss: 0.3385 - classification_loss: 0.0314
 919/1000 [==========================>...] - ETA: 3:01 - loss: 0.3699 - regression_loss: 0.3385 - classification_loss: 0.0314
 920/1000 [==========================>...] - ETA: 2:59 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 921/1000 [==========================>...] - ETA: 2:57 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 922/1000 [==========================>...] - ETA: 2:54 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 923/1000 [==========================>...] - ETA: 2:52 - loss: 0.3699 - regression_loss: 0.3385 - classification_loss: 0.0314
 924/1000 [==========================>...] - ETA: 2:50 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 925/1000 [==========================>...] - ETA: 2:48 - loss: 0.3695 - regression_loss: 0.3381 - classification_loss: 0.0314
 926/1000 [==========================>...] - ETA: 2:45 - loss: 0.3693 - regression_loss: 0.3379 - classification_loss: 0.0314
 927/1000 [==========================>...] - ETA: 2:43 - loss: 0.3694 - regression_loss: 0.3380 - classification_loss: 0.0314
 928/1000 [==========================>...] - ETA: 2:41 - loss: 0.3695 - regression_loss: 0.3381 - classification_loss: 0.0314
 929/1000 [==========================>...] - ETA: 2:39 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 930/1000 [==========================>...] - ETA: 2:36 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 931/1000 [==========================>...] - ETA: 2:34 - loss: 0.3698 - regression_loss: 0.3383 - classification_loss: 0.0314
 932/1000 [==========================>...] - ETA: 2:32 - loss: 0.3698 - regression_loss: 0.3384 - classification_loss: 0.0314
 933/1000 [==========================>...] - ETA: 2:30 - loss: 0.3695 - regression_loss: 0.3381 - classification_loss: 0.0314
 934/1000 [===========================>..] - ETA: 2:27 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 935/1000 [===========================>..] - ETA: 2:25 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 936/1000 [===========================>..] - ETA: 2:23 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 937/1000 [===========================>..] - ETA: 2:21 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 938/1000 [===========================>..] - ETA: 2:19 - loss: 0.3697 - regression_loss: 0.3383 - classification_loss: 0.0314
 939/1000 [===========================>..] - ETA: 2:16 - loss: 0.3693 - regression_loss: 0.3380 - classification_loss: 0.0314
 940/1000 [===========================>..] - ETA: 2:14 - loss: 0.3693 - regression_loss: 0.3380 - classification_loss: 0.0314
 941/1000 [===========================>..] - ETA: 2:12 - loss: 0.3693 - regression_loss: 0.3380 - classification_loss: 0.0314
 942/1000 [===========================>..] - ETA: 2:09 - loss: 0.3691 - regression_loss: 0.3378 - classification_loss: 0.0314
 943/1000 [===========================>..] - ETA: 2:07 - loss: 0.3693 - regression_loss: 0.3379 - classification_loss: 0.0314
 944/1000 [===========================>..] - ETA: 2:05 - loss: 0.3693 - regression_loss: 0.3379 - classification_loss: 0.0314
 945/1000 [===========================>..] - ETA: 2:03 - loss: 0.3694 - regression_loss: 0.3381 - classification_loss: 0.0314
 946/1000 [===========================>..] - ETA: 2:01 - loss: 0.3695 - regression_loss: 0.3382 - classification_loss: 0.0314
 947/1000 [===========================>..] - ETA: 1:58 - loss: 0.3696 - regression_loss: 0.3382 - classification_loss: 0.0314
 948/1000 [===========================>..] - ETA: 1:56 - loss: 0.3695 - regression_loss: 0.3381 - classification_loss: 0.0314
 949/1000 [===========================>..] - ETA: 1:54 - loss: 0.3693 - regression_loss: 0.3380 - classification_loss: 0.0314
 950/1000 [===========================>..] - ETA: 1:52 - loss: 0.3690 - regression_loss: 0.3377 - classification_loss: 0.0313
 951/1000 [===========================>..] - ETA: 1:49 - loss: 0.3690 - regression_loss: 0.3377 - classification_loss: 0.0313
 952/1000 [===========================>..] - ETA: 1:47 - loss: 0.3688 - regression_loss: 0.3375 - classification_loss: 0.0313
 953/1000 [===========================>..] - ETA: 1:45 - loss: 0.3689 - regression_loss: 0.3376 - classification_loss: 0.0313
 954/1000 [===========================>..] - ETA: 1:43 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 955/1000 [===========================>..] - ETA: 1:40 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 956/1000 [===========================>..] - ETA: 1:38 - loss: 0.3687 - regression_loss: 0.3373 - classification_loss: 0.0313
 957/1000 [===========================>..] - ETA: 1:36 - loss: 0.3687 - regression_loss: 0.3374 - classification_loss: 0.0313
 958/1000 [===========================>..] - ETA: 1:34 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 959/1000 [===========================>..] - ETA: 1:31 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 960/1000 [===========================>..] - ETA: 1:29 - loss: 0.3685 - regression_loss: 0.3372 - classification_loss: 0.0313
 961/1000 [===========================>..] - ETA: 1:27 - loss: 0.3685 - regression_loss: 0.3372 - classification_loss: 0.0313
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.3682 - regression_loss: 0.3369 - classification_loss: 0.0313
 963/1000 [===========================>..] - ETA: 1:22 - loss: 0.3683 - regression_loss: 0.3370 - classification_loss: 0.0313
 964/1000 [===========================>..] - ETA: 1:20 - loss: 0.3684 - regression_loss: 0.3371 - classification_loss: 0.0313
 965/1000 [===========================>..] - ETA: 1:18 - loss: 0.3685 - regression_loss: 0.3372 - classification_loss: 0.0313
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.3685 - regression_loss: 0.3372 - classification_loss: 0.0313
 967/1000 [============================>.] - ETA: 1:13 - loss: 0.3683 - regression_loss: 0.3370 - classification_loss: 0.0313
 968/1000 [============================>.] - ETA: 1:11 - loss: 0.3684 - regression_loss: 0.3372 - classification_loss: 0.0313
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.3685 - regression_loss: 0.3372 - classification_loss: 0.0313
 971/1000 [============================>.] - ETA: 1:05 - loss: 0.3686 - regression_loss: 0.3373 - classification_loss: 0.0313
 972/1000 [============================>.] - ETA: 1:02 - loss: 0.3682 - regression_loss: 0.3369 - classification_loss: 0.0313
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.3679 - regression_loss: 0.3366 - classification_loss: 0.0312
 974/1000 [============================>.] - ETA: 58s - loss: 0.3680 - regression_loss: 0.3367 - classification_loss: 0.0313 
 975/1000 [============================>.] - ETA: 56s - loss: 0.3680 - regression_loss: 0.3368 - classification_loss: 0.0313
 976/1000 [============================>.] - ETA: 53s - loss: 0.3680 - regression_loss: 0.3367 - classification_loss: 0.0313
 977/1000 [============================>.] - ETA: 51s - loss: 0.3680 - regression_loss: 0.3368 - classification_loss: 0.0312
 978/1000 [============================>.] - ETA: 49s - loss: 0.3681 - regression_loss: 0.3369 - classification_loss: 0.0313
 979/1000 [============================>.] - ETA: 47s - loss: 0.3680 - regression_loss: 0.3367 - classification_loss: 0.0312
 980/1000 [============================>.] - ETA: 44s - loss: 0.3681 - regression_loss: 0.3369 - classification_loss: 0.0312
 981/1000 [============================>.] - ETA: 42s - loss: 0.3683 - regression_loss: 0.3370 - classification_loss: 0.0313
 982/1000 [============================>.] - ETA: 40s - loss: 0.3681 - regression_loss: 0.3368 - classification_loss: 0.0312
 983/1000 [============================>.] - ETA: 38s - loss: 0.3681 - regression_loss: 0.3368 - classification_loss: 0.0312
 984/1000 [============================>.] - ETA: 35s - loss: 0.3681 - regression_loss: 0.3369 - classification_loss: 0.0312
 985/1000 [============================>.] - ETA: 33s - loss: 0.3681 - regression_loss: 0.3368 - classification_loss: 0.0312
 986/1000 [============================>.] - ETA: 31s - loss: 0.3679 - regression_loss: 0.3366 - classification_loss: 0.0312
 987/1000 [============================>.] - ETA: 29s - loss: 0.3677 - regression_loss: 0.3365 - classification_loss: 0.0312
 988/1000 [============================>.] - ETA: 26s - loss: 0.3677 - regression_loss: 0.3365 - classification_loss: 0.0312
 989/1000 [============================>.] - ETA: 24s - loss: 0.3678 - regression_loss: 0.3366 - classification_loss: 0.0312
 990/1000 [============================>.] - ETA: 22s - loss: 0.3678 - regression_loss: 0.3366 - classification_loss: 0.0312
 991/1000 [============================>.] - ETA: 20s - loss: 0.3679 - regression_loss: 0.3367 - classification_loss: 0.0312
 992/1000 [============================>.] - ETA: 17s - loss: 0.3678 - regression_loss: 0.3367 - classification_loss: 0.0312
 993/1000 [============================>.] - ETA: 15s - loss: 0.3678 - regression_loss: 0.3366 - classification_loss: 0.0312
 994/1000 [============================>.] - ETA: 13s - loss: 0.3679 - regression_loss: 0.3367 - classification_loss: 0.0312
 995/1000 [============================>.] - ETA: 11s - loss: 0.3680 - regression_loss: 0.3368 - classification_loss: 0.0312
 996/1000 [============================>.] - ETA: 8s - loss: 0.3678 - regression_loss: 0.3367 - classification_loss: 0.0312 
 997/1000 [============================>.] - ETA: 6s - loss: 0.3676 - regression_loss: 0.3364 - classification_loss: 0.0312
 998/1000 [============================>.] - ETA: 4s - loss: 0.3676 - regression_loss: 0.3365 - classification_loss: 0.0312
 999/1000 [============================>.] - ETA: 2s - loss: 0.3676 - regression_loss: 0.3365 - classification_loss: 0.0312
1000/1000 [==============================] - 2242s 2s/step - loss: 0.3677 - regression_loss: 0.3366 - classification_loss: 0.0312

Epoch 00007: saving model to ./snapshots/resnet50_csv_07.h5
Epoch 8/10

   1/1000 [..............................] - ETA: 35:45 - loss: 0.4790 - regression_loss: 0.4418 - classification_loss: 0.0371
   2/1000 [..............................] - ETA: 38:12 - loss: 0.3866 - regression_loss: 0.3524 - classification_loss: 0.0342
   3/1000 [..............................] - ETA: 39:28 - loss: 0.3914 - regression_loss: 0.3581 - classification_loss: 0.0333
   4/1000 [..............................] - ETA: 40:16 - loss: 0.4083 - regression_loss: 0.3746 - classification_loss: 0.0336
   5/1000 [..............................] - ETA: 37:54 - loss: 0.3799 - regression_loss: 0.3488 - classification_loss: 0.0310
   6/1000 [..............................] - ETA: 35:51 - loss: 0.3220 - regression_loss: 0.2949 - classification_loss: 0.0271
   7/1000 [..............................] - ETA: 35:47 - loss: 0.3318 - regression_loss: 0.3044 - classification_loss: 0.0274
   8/1000 [..............................] - ETA: 36:23 - loss: 0.3351 - regression_loss: 0.3070 - classification_loss: 0.0282
   9/1000 [..............................] - ETA: 35:09 - loss: 0.3016 - regression_loss: 0.2757 - classification_loss: 0.0259
  10/1000 [..............................] - ETA: 35:50 - loss: 0.3188 - regression_loss: 0.2919 - classification_loss: 0.0269
  11/1000 [..............................] - ETA: 35:44 - loss: 0.3267 - regression_loss: 0.2995 - classification_loss: 0.0272
  12/1000 [..............................] - ETA: 36:24 - loss: 0.3386 - regression_loss: 0.3107 - classification_loss: 0.0279
  13/1000 [..............................] - ETA: 36:44 - loss: 0.3439 - regression_loss: 0.3157 - classification_loss: 0.0283
  14/1000 [..............................] - ETA: 36:05 - loss: 0.3335 - regression_loss: 0.3059 - classification_loss: 0.0275
  15/1000 [..............................] - ETA: 36:27 - loss: 0.3453 - regression_loss: 0.3172 - classification_loss: 0.0281
  16/1000 [..............................] - ETA: 36:42 - loss: 0.3506 - regression_loss: 0.3224 - classification_loss: 0.0283
  17/1000 [..............................] - ETA: 36:49 - loss: 0.3486 - regression_loss: 0.3202 - classification_loss: 0.0285
  18/1000 [..............................] - ETA: 36:18 - loss: 0.3390 - regression_loss: 0.3112 - classification_loss: 0.0279
  19/1000 [..............................] - ETA: 36:41 - loss: 0.3471 - regression_loss: 0.3187 - classification_loss: 0.0283
  20/1000 [..............................] - ETA: 36:05 - loss: 0.3348 - regression_loss: 0.3075 - classification_loss: 0.0273
  21/1000 [..............................] - ETA: 35:59 - loss: 0.3367 - regression_loss: 0.3093 - classification_loss: 0.0274
  22/1000 [..............................] - ETA: 36:19 - loss: 0.3432 - regression_loss: 0.3153 - classification_loss: 0.0278
  23/1000 [..............................] - ETA: 35:48 - loss: 0.3300 - regression_loss: 0.3030 - classification_loss: 0.0270
  24/1000 [..............................] - ETA: 35:55 - loss: 0.3288 - regression_loss: 0.3017 - classification_loss: 0.0271
  25/1000 [..............................] - ETA: 35:50 - loss: 0.3311 - regression_loss: 0.3039 - classification_loss: 0.0272
  26/1000 [..............................] - ETA: 36:02 - loss: 0.3366 - regression_loss: 0.3091 - classification_loss: 0.0275
  27/1000 [..............................] - ETA: 36:15 - loss: 0.3397 - regression_loss: 0.3121 - classification_loss: 0.0277
  28/1000 [..............................] - ETA: 35:55 - loss: 0.3359 - regression_loss: 0.3085 - classification_loss: 0.0274
  29/1000 [..............................] - ETA: 36:05 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
  30/1000 [..............................] - ETA: 35:46 - loss: 0.3358 - regression_loss: 0.3084 - classification_loss: 0.0274
  31/1000 [..............................] - ETA: 35:42 - loss: 0.3368 - regression_loss: 0.3094 - classification_loss: 0.0274
  32/1000 [..............................] - ETA: 35:48 - loss: 0.3391 - regression_loss: 0.3115 - classification_loss: 0.0276
  33/1000 [..............................] - ETA: 35:52 - loss: 0.3375 - regression_loss: 0.3098 - classification_loss: 0.0277
  34/1000 [>.............................] - ETA: 35:31 - loss: 0.3300 - regression_loss: 0.3029 - classification_loss: 0.0271
  35/1000 [>.............................] - ETA: 35:42 - loss: 0.3346 - regression_loss: 0.3072 - classification_loss: 0.0274
  36/1000 [>.............................] - ETA: 35:23 - loss: 0.3296 - regression_loss: 0.3028 - classification_loss: 0.0268
  37/1000 [>.............................] - ETA: 35:08 - loss: 0.3311 - regression_loss: 0.3043 - classification_loss: 0.0268
  38/1000 [>.............................] - ETA: 35:17 - loss: 0.3362 - regression_loss: 0.3091 - classification_loss: 0.0271
  39/1000 [>.............................] - ETA: 35:27 - loss: 0.3403 - regression_loss: 0.3128 - classification_loss: 0.0274
  40/1000 [>.............................] - ETA: 35:32 - loss: 0.3425 - regression_loss: 0.3149 - classification_loss: 0.0276
  41/1000 [>.............................] - ETA: 35:28 - loss: 0.3433 - regression_loss: 0.3156 - classification_loss: 0.0276
  42/1000 [>.............................] - ETA: 35:30 - loss: 0.3422 - regression_loss: 0.3145 - classification_loss: 0.0277
  43/1000 [>.............................] - ETA: 35:36 - loss: 0.3455 - regression_loss: 0.3175 - classification_loss: 0.0280
  44/1000 [>.............................] - ETA: 35:38 - loss: 0.3445 - regression_loss: 0.3164 - classification_loss: 0.0281
  45/1000 [>.............................] - ETA: 35:41 - loss: 0.3460 - regression_loss: 0.3178 - classification_loss: 0.0282
  46/1000 [>.............................] - ETA: 35:37 - loss: 0.3462 - regression_loss: 0.3180 - classification_loss: 0.0282
  47/1000 [>.............................] - ETA: 35:24 - loss: 0.3449 - regression_loss: 0.3169 - classification_loss: 0.0280
  48/1000 [>.............................] - ETA: 35:32 - loss: 0.3482 - regression_loss: 0.3200 - classification_loss: 0.0282
  49/1000 [>.............................] - ETA: 35:16 - loss: 0.3425 - regression_loss: 0.3147 - classification_loss: 0.0278
  50/1000 [>.............................] - ETA: 35:17 - loss: 0.3428 - regression_loss: 0.3148 - classification_loss: 0.0280
  51/1000 [>.............................] - ETA: 35:22 - loss: 0.3453 - regression_loss: 0.3172 - classification_loss: 0.0281
  52/1000 [>.............................] - ETA: 35:18 - loss: 0.3460 - regression_loss: 0.3178 - classification_loss: 0.0282
  53/1000 [>.............................] - ETA: 35:24 - loss: 0.3488 - regression_loss: 0.3205 - classification_loss: 0.0283
  54/1000 [>.............................] - ETA: 35:10 - loss: 0.3437 - regression_loss: 0.3157 - classification_loss: 0.0280
  55/1000 [>.............................] - ETA: 34:59 - loss: 0.3413 - regression_loss: 0.3134 - classification_loss: 0.0278
  56/1000 [>.............................] - ETA: 35:01 - loss: 0.3429 - regression_loss: 0.3149 - classification_loss: 0.0280
  57/1000 [>.............................] - ETA: 34:48 - loss: 0.3373 - regression_loss: 0.3097 - classification_loss: 0.0277
  58/1000 [>.............................] - ETA: 34:52 - loss: 0.3401 - regression_loss: 0.3123 - classification_loss: 0.0278
  59/1000 [>.............................] - ETA: 34:57 - loss: 0.3422 - regression_loss: 0.3143 - classification_loss: 0.0279
  60/1000 [>.............................] - ETA: 34:58 - loss: 0.3413 - regression_loss: 0.3133 - classification_loss: 0.0280
  61/1000 [>.............................] - ETA: 34:55 - loss: 0.3416 - regression_loss: 0.3135 - classification_loss: 0.0280
  62/1000 [>.............................] - ETA: 34:56 - loss: 0.3427 - regression_loss: 0.3146 - classification_loss: 0.0281
  63/1000 [>.............................] - ETA: 34:47 - loss: 0.3405 - regression_loss: 0.3125 - classification_loss: 0.0279
  64/1000 [>.............................] - ETA: 34:51 - loss: 0.3425 - regression_loss: 0.3144 - classification_loss: 0.0281
  65/1000 [>.............................] - ETA: 34:42 - loss: 0.3398 - regression_loss: 0.3119 - classification_loss: 0.0279
  66/1000 [>.............................] - ETA: 34:42 - loss: 0.3392 - regression_loss: 0.3112 - classification_loss: 0.0280
  67/1000 [=>............................] - ETA: 34:39 - loss: 0.3394 - regression_loss: 0.3115 - classification_loss: 0.0280
  68/1000 [=>............................] - ETA: 34:40 - loss: 0.3405 - regression_loss: 0.3125 - classification_loss: 0.0280
  69/1000 [=>............................] - ETA: 34:29 - loss: 0.3366 - regression_loss: 0.3089 - classification_loss: 0.0277
  70/1000 [=>............................] - ETA: 34:32 - loss: 0.3389 - regression_loss: 0.3110 - classification_loss: 0.0279
  71/1000 [=>............................] - ETA: 34:21 - loss: 0.3345 - regression_loss: 0.3069 - classification_loss: 0.0276
  72/1000 [=>............................] - ETA: 34:25 - loss: 0.3365 - regression_loss: 0.3087 - classification_loss: 0.0278
  73/1000 [=>............................] - ETA: 34:26 - loss: 0.3377 - regression_loss: 0.3099 - classification_loss: 0.0278
  74/1000 [=>............................] - ETA: 34:26 - loss: 0.3371 - regression_loss: 0.3093 - classification_loss: 0.0279
  75/1000 [=>............................] - ETA: 34:18 - loss: 0.3373 - regression_loss: 0.3096 - classification_loss: 0.0278
  76/1000 [=>............................] - ETA: 34:15 - loss: 0.3378 - regression_loss: 0.3099 - classification_loss: 0.0278
  77/1000 [=>............................] - ETA: 34:17 - loss: 0.3398 - regression_loss: 0.3118 - classification_loss: 0.0280
  78/1000 [=>............................] - ETA: 34:19 - loss: 0.3415 - regression_loss: 0.3134 - classification_loss: 0.0281
  79/1000 [=>............................] - ETA: 34:15 - loss: 0.3417 - regression_loss: 0.3136 - classification_loss: 0.0281
  80/1000 [=>............................] - ETA: 34:15 - loss: 0.3410 - regression_loss: 0.3128 - classification_loss: 0.0282
  81/1000 [=>............................] - ETA: 34:07 - loss: 0.3395 - regression_loss: 0.3114 - classification_loss: 0.0281
  82/1000 [=>............................] - ETA: 33:57 - loss: 0.3367 - regression_loss: 0.3089 - classification_loss: 0.0278
  83/1000 [=>............................] - ETA: 34:00 - loss: 0.3383 - regression_loss: 0.3104 - classification_loss: 0.0280
  84/1000 [=>............................] - ETA: 34:01 - loss: 0.3392 - regression_loss: 0.3112 - classification_loss: 0.0280
  85/1000 [=>............................] - ETA: 33:53 - loss: 0.3374 - regression_loss: 0.3095 - classification_loss: 0.0279
  86/1000 [=>............................] - ETA: 33:56 - loss: 0.3391 - regression_loss: 0.3111 - classification_loss: 0.0280
  87/1000 [=>............................] - ETA: 33:57 - loss: 0.3410 - regression_loss: 0.3129 - classification_loss: 0.0281
  88/1000 [=>............................] - ETA: 33:54 - loss: 0.3411 - regression_loss: 0.3130 - classification_loss: 0.0281
  89/1000 [=>............................] - ETA: 33:53 - loss: 0.3406 - regression_loss: 0.3125 - classification_loss: 0.0281
  90/1000 [=>............................] - ETA: 33:44 - loss: 0.3385 - regression_loss: 0.3106 - classification_loss: 0.0279
  91/1000 [=>............................] - ETA: 33:45 - loss: 0.3396 - regression_loss: 0.3117 - classification_loss: 0.0279
  92/1000 [=>............................] - ETA: 33:38 - loss: 0.3385 - regression_loss: 0.3107 - classification_loss: 0.0278
  93/1000 [=>............................] - ETA: 33:37 - loss: 0.3380 - regression_loss: 0.3101 - classification_loss: 0.0279
  94/1000 [=>............................] - ETA: 33:29 - loss: 0.3347 - regression_loss: 0.3071 - classification_loss: 0.0276
  95/1000 [=>............................] - ETA: 33:26 - loss: 0.3351 - regression_loss: 0.3074 - classification_loss: 0.0277
  96/1000 [=>............................] - ETA: 33:26 - loss: 0.3360 - regression_loss: 0.3083 - classification_loss: 0.0277
  97/1000 [=>............................] - ETA: 33:28 - loss: 0.3373 - regression_loss: 0.3095 - classification_loss: 0.0278
  98/1000 [=>............................] - ETA: 33:29 - loss: 0.3385 - regression_loss: 0.3107 - classification_loss: 0.0278
  99/1000 [=>............................] - ETA: 33:22 - loss: 0.3368 - regression_loss: 0.3091 - classification_loss: 0.0277
 100/1000 [==>...........................] - ETA: 33:23 - loss: 0.3383 - regression_loss: 0.3105 - classification_loss: 0.0278
 101/1000 [==>...........................] - ETA: 33:15 - loss: 0.3353 - regression_loss: 0.3077 - classification_loss: 0.0276
 102/1000 [==>...........................] - ETA: 33:14 - loss: 0.3348 - regression_loss: 0.3072 - classification_loss: 0.0276
 103/1000 [==>...........................] - ETA: 33:14 - loss: 0.3354 - regression_loss: 0.3078 - classification_loss: 0.0277
 104/1000 [==>...........................] - ETA: 33:12 - loss: 0.3356 - regression_loss: 0.3079 - classification_loss: 0.0277
 105/1000 [==>...........................] - ETA: 33:13 - loss: 0.3367 - regression_loss: 0.3090 - classification_loss: 0.0277
 106/1000 [==>...........................] - ETA: 33:15 - loss: 0.3379 - regression_loss: 0.3101 - classification_loss: 0.0278
 107/1000 [==>...........................] - ETA: 33:14 - loss: 0.3374 - regression_loss: 0.3095 - classification_loss: 0.0278
 108/1000 [==>...........................] - ETA: 33:06 - loss: 0.3348 - regression_loss: 0.3071 - classification_loss: 0.0276
 109/1000 [==>...........................] - ETA: 33:06 - loss: 0.3354 - regression_loss: 0.3077 - classification_loss: 0.0277
 110/1000 [==>...........................] - ETA: 33:04 - loss: 0.3356 - regression_loss: 0.3079 - classification_loss: 0.0277
 111/1000 [==>...........................] - ETA: 32:57 - loss: 0.3350 - regression_loss: 0.3073 - classification_loss: 0.0276
 112/1000 [==>...........................] - ETA: 32:58 - loss: 0.3369 - regression_loss: 0.3092 - classification_loss: 0.0277
 113/1000 [==>...........................] - ETA: 32:58 - loss: 0.3378 - regression_loss: 0.3101 - classification_loss: 0.0277
 114/1000 [==>...........................] - ETA: 32:59 - loss: 0.3389 - regression_loss: 0.3111 - classification_loss: 0.0278
 115/1000 [==>...........................] - ETA: 32:56 - loss: 0.3395 - regression_loss: 0.3117 - classification_loss: 0.0278
 116/1000 [==>...........................] - ETA: 32:57 - loss: 0.3410 - regression_loss: 0.3131 - classification_loss: 0.0279
 117/1000 [==>...........................] - ETA: 32:49 - loss: 0.3383 - regression_loss: 0.3106 - classification_loss: 0.0277
 118/1000 [==>...........................] - ETA: 32:43 - loss: 0.3374 - regression_loss: 0.3098 - classification_loss: 0.0276
 119/1000 [==>...........................] - ETA: 32:42 - loss: 0.3371 - regression_loss: 0.3094 - classification_loss: 0.0277
 120/1000 [==>...........................] - ETA: 32:40 - loss: 0.3372 - regression_loss: 0.3096 - classification_loss: 0.0277
 121/1000 [==>...........................] - ETA: 32:39 - loss: 0.3370 - regression_loss: 0.3094 - classification_loss: 0.0277
 122/1000 [==>...........................] - ETA: 32:33 - loss: 0.3362 - regression_loss: 0.3086 - classification_loss: 0.0276
 123/1000 [==>...........................] - ETA: 32:33 - loss: 0.3375 - regression_loss: 0.3098 - classification_loss: 0.0277
 124/1000 [==>...........................] - ETA: 32:26 - loss: 0.3357 - regression_loss: 0.3082 - classification_loss: 0.0275
 125/1000 [==>...........................] - ETA: 32:26 - loss: 0.3365 - regression_loss: 0.3090 - classification_loss: 0.0276
 126/1000 [==>...........................] - ETA: 32:27 - loss: 0.3378 - regression_loss: 0.3102 - classification_loss: 0.0276
 127/1000 [==>...........................] - ETA: 32:27 - loss: 0.3375 - regression_loss: 0.3098 - classification_loss: 0.0276
 128/1000 [==>...........................] - ETA: 32:27 - loss: 0.3385 - regression_loss: 0.3108 - classification_loss: 0.0277
 129/1000 [==>...........................] - ETA: 32:21 - loss: 0.3373 - regression_loss: 0.3096 - classification_loss: 0.0276
 130/1000 [==>...........................] - ETA: 32:15 - loss: 0.3353 - regression_loss: 0.3078 - classification_loss: 0.0275
 131/1000 [==>...........................] - ETA: 32:12 - loss: 0.3358 - regression_loss: 0.3083 - classification_loss: 0.0275
 132/1000 [==>...........................] - ETA: 32:13 - loss: 0.3371 - regression_loss: 0.3095 - classification_loss: 0.0275
 133/1000 [==>...........................] - ETA: 32:13 - loss: 0.3376 - regression_loss: 0.3101 - classification_loss: 0.0276
 134/1000 [===>..........................] - ETA: 32:06 - loss: 0.3357 - regression_loss: 0.3082 - classification_loss: 0.0274
 135/1000 [===>..........................] - ETA: 32:05 - loss: 0.3354 - regression_loss: 0.3080 - classification_loss: 0.0274
 136/1000 [===>..........................] - ETA: 32:05 - loss: 0.3366 - regression_loss: 0.3091 - classification_loss: 0.0275
 137/1000 [===>..........................] - ETA: 32:00 - loss: 0.3364 - regression_loss: 0.3089 - classification_loss: 0.0275
 138/1000 [===>..........................] - ETA: 31:59 - loss: 0.3369 - regression_loss: 0.3094 - classification_loss: 0.0275
 139/1000 [===>..........................] - ETA: 31:57 - loss: 0.3370 - regression_loss: 0.3095 - classification_loss: 0.0275
 140/1000 [===>..........................] - ETA: 31:57 - loss: 0.3381 - regression_loss: 0.3106 - classification_loss: 0.0275
 141/1000 [===>..........................] - ETA: 31:54 - loss: 0.3383 - regression_loss: 0.3108 - classification_loss: 0.0275
 142/1000 [===>..........................] - ETA: 31:48 - loss: 0.3364 - regression_loss: 0.3090 - classification_loss: 0.0274
 143/1000 [===>..........................] - ETA: 31:49 - loss: 0.3374 - regression_loss: 0.3100 - classification_loss: 0.0274
 144/1000 [===>..........................] - ETA: 31:44 - loss: 0.3363 - regression_loss: 0.3089 - classification_loss: 0.0274
 145/1000 [===>..........................] - ETA: 31:44 - loss: 0.3372 - regression_loss: 0.3098 - classification_loss: 0.0274
 146/1000 [===>..........................] - ETA: 31:43 - loss: 0.3369 - regression_loss: 0.3095 - classification_loss: 0.0274
 147/1000 [===>..........................] - ETA: 31:42 - loss: 0.3374 - regression_loss: 0.3099 - classification_loss: 0.0275
 148/1000 [===>..........................] - ETA: 31:41 - loss: 0.3372 - regression_loss: 0.3097 - classification_loss: 0.0275
 149/1000 [===>..........................] - ETA: 31:35 - loss: 0.3354 - regression_loss: 0.3081 - classification_loss: 0.0273
 150/1000 [===>..........................] - ETA: 31:34 - loss: 0.3361 - regression_loss: 0.3087 - classification_loss: 0.0274
 151/1000 [===>..........................] - ETA: 31:35 - loss: 0.3370 - regression_loss: 0.3096 - classification_loss: 0.0274
 152/1000 [===>..........................] - ETA: 31:32 - loss: 0.3371 - regression_loss: 0.3097 - classification_loss: 0.0274
 153/1000 [===>..........................] - ETA: 31:27 - loss: 0.3365 - regression_loss: 0.3091 - classification_loss: 0.0274
 154/1000 [===>..........................] - ETA: 31:28 - loss: 0.3373 - regression_loss: 0.3099 - classification_loss: 0.0274
 155/1000 [===>..........................] - ETA: 31:28 - loss: 0.3379 - regression_loss: 0.3104 - classification_loss: 0.0275
 156/1000 [===>..........................] - ETA: 31:26 - loss: 0.3381 - regression_loss: 0.3106 - classification_loss: 0.0275
 157/1000 [===>..........................] - ETA: 31:28 - loss: 0.3378 - regression_loss: 0.3103 - classification_loss: 0.0275
 158/1000 [===>..........................] - ETA: 31:24 - loss: 0.3359 - regression_loss: 0.3085 - classification_loss: 0.0274
 159/1000 [===>..........................] - ETA: 31:28 - loss: 0.3367 - regression_loss: 0.3093 - classification_loss: 0.0274
 160/1000 [===>..........................] - ETA: 31:31 - loss: 0.3377 - regression_loss: 0.3102 - classification_loss: 0.0275
 161/1000 [===>..........................] - ETA: 31:28 - loss: 0.3368 - regression_loss: 0.3093 - classification_loss: 0.0275
 162/1000 [===>..........................] - ETA: 31:24 - loss: 0.3357 - regression_loss: 0.3083 - classification_loss: 0.0274
 163/1000 [===>..........................] - ETA: 31:20 - loss: 0.3340 - regression_loss: 0.3067 - classification_loss: 0.0273
 164/1000 [===>..........................] - ETA: 31:21 - loss: 0.3338 - regression_loss: 0.3065 - classification_loss: 0.0273
 165/1000 [===>..........................] - ETA: 31:22 - loss: 0.3343 - regression_loss: 0.3069 - classification_loss: 0.0274
 166/1000 [===>..........................] - ETA: 31:24 - loss: 0.3350 - regression_loss: 0.3076 - classification_loss: 0.0274
 167/1000 [====>.........................] - ETA: 31:27 - loss: 0.3357 - regression_loss: 0.3083 - classification_loss: 0.0274
 168/1000 [====>.........................] - ETA: 31:25 - loss: 0.3358 - regression_loss: 0.3083 - classification_loss: 0.0275
 169/1000 [====>.........................] - ETA: 31:26 - loss: 0.3355 - regression_loss: 0.3080 - classification_loss: 0.0275
 170/1000 [====>.........................] - ETA: 31:27 - loss: 0.3358 - regression_loss: 0.3083 - classification_loss: 0.0275
 171/1000 [====>.........................] - ETA: 31:22 - loss: 0.3340 - regression_loss: 0.3066 - classification_loss: 0.0274
 172/1000 [====>.........................] - ETA: 31:24 - loss: 0.3347 - regression_loss: 0.3073 - classification_loss: 0.0274
 173/1000 [====>.........................] - ETA: 31:26 - loss: 0.3354 - regression_loss: 0.3079 - classification_loss: 0.0275
 174/1000 [====>.........................] - ETA: 31:24 - loss: 0.3353 - regression_loss: 0.3078 - classification_loss: 0.0275
 175/1000 [====>.........................] - ETA: 31:23 - loss: 0.3353 - regression_loss: 0.3079 - classification_loss: 0.0275
 176/1000 [====>.........................] - ETA: 31:25 - loss: 0.3360 - regression_loss: 0.3085 - classification_loss: 0.0275
 177/1000 [====>.........................] - ETA: 31:26 - loss: 0.3364 - regression_loss: 0.3089 - classification_loss: 0.0275
 178/1000 [====>.........................] - ETA: 31:27 - loss: 0.3361 - regression_loss: 0.3085 - classification_loss: 0.0275
 179/1000 [====>.........................] - ETA: 31:22 - loss: 0.3347 - regression_loss: 0.3073 - classification_loss: 0.0274
 180/1000 [====>.........................] - ETA: 31:19 - loss: 0.3339 - regression_loss: 0.3065 - classification_loss: 0.0274
 181/1000 [====>.........................] - ETA: 31:18 - loss: 0.3341 - regression_loss: 0.3067 - classification_loss: 0.0274
 182/1000 [====>.........................] - ETA: 31:19 - loss: 0.3350 - regression_loss: 0.3075 - classification_loss: 0.0274
 183/1000 [====>.........................] - ETA: 31:19 - loss: 0.3351 - regression_loss: 0.3077 - classification_loss: 0.0274
 184/1000 [====>.........................] - ETA: 31:22 - loss: 0.3359 - regression_loss: 0.3085 - classification_loss: 0.0275
 185/1000 [====>.........................] - ETA: 31:20 - loss: 0.3354 - regression_loss: 0.3080 - classification_loss: 0.0274
 186/1000 [====>.........................] - ETA: 31:23 - loss: 0.3361 - regression_loss: 0.3086 - classification_loss: 0.0274
 187/1000 [====>.........................] - ETA: 31:24 - loss: 0.3359 - regression_loss: 0.3084 - classification_loss: 0.0275
 188/1000 [====>.........................] - ETA: 31:21 - loss: 0.3344 - regression_loss: 0.3071 - classification_loss: 0.0273
 189/1000 [====>.........................] - ETA: 31:20 - loss: 0.3347 - regression_loss: 0.3074 - classification_loss: 0.0274
 190/1000 [====>.........................] - ETA: 31:17 - loss: 0.3348 - regression_loss: 0.3075 - classification_loss: 0.0274
 191/1000 [====>.........................] - ETA: 31:18 - loss: 0.3352 - regression_loss: 0.3079 - classification_loss: 0.0274
 192/1000 [====>.........................] - ETA: 31:14 - loss: 0.3345 - regression_loss: 0.3071 - classification_loss: 0.0273
 193/1000 [====>.........................] - ETA: 31:16 - loss: 0.3345 - regression_loss: 0.3071 - classification_loss: 0.0273
 194/1000 [====>.........................] - ETA: 31:17 - loss: 0.3352 - regression_loss: 0.3078 - classification_loss: 0.0274
 195/1000 [====>.........................] - ETA: 31:18 - loss: 0.3358 - regression_loss: 0.3084 - classification_loss: 0.0274
 196/1000 [====>.........................] - ETA: 31:14 - loss: 0.3344 - regression_loss: 0.3071 - classification_loss: 0.0273
 197/1000 [====>.........................] - ETA: 31:15 - loss: 0.3351 - regression_loss: 0.3077 - classification_loss: 0.0274
 198/1000 [====>.........................] - ETA: 31:15 - loss: 0.3357 - regression_loss: 0.3083 - classification_loss: 0.0274
 199/1000 [====>.........................] - ETA: 31:15 - loss: 0.3360 - regression_loss: 0.3086 - classification_loss: 0.0274
 200/1000 [=====>........................] - ETA: 31:13 - loss: 0.3354 - regression_loss: 0.3081 - classification_loss: 0.0274
 201/1000 [=====>........................] - ETA: 31:14 - loss: 0.3354 - regression_loss: 0.3080 - classification_loss: 0.0274
 202/1000 [=====>........................] - ETA: 31:09 - loss: 0.3340 - regression_loss: 0.3067 - classification_loss: 0.0273
 203/1000 [=====>........................] - ETA: 31:08 - loss: 0.3341 - regression_loss: 0.3068 - classification_loss: 0.0273
 204/1000 [=====>........................] - ETA: 31:06 - loss: 0.3334 - regression_loss: 0.3062 - classification_loss: 0.0272
 205/1000 [=====>........................] - ETA: 31:04 - loss: 0.3336 - regression_loss: 0.3063 - classification_loss: 0.0272
 206/1000 [=====>........................] - ETA: 31:00 - loss: 0.3321 - regression_loss: 0.3049 - classification_loss: 0.0272
 207/1000 [=====>........................] - ETA: 31:00 - loss: 0.3319 - regression_loss: 0.3047 - classification_loss: 0.0272
 208/1000 [=====>........................] - ETA: 31:00 - loss: 0.3325 - regression_loss: 0.3053 - classification_loss: 0.0272
 209/1000 [=====>........................] - ETA: 31:00 - loss: 0.3331 - regression_loss: 0.3059 - classification_loss: 0.0272
 210/1000 [=====>........................] - ETA: 31:00 - loss: 0.3334 - regression_loss: 0.3062 - classification_loss: 0.0273
 211/1000 [=====>........................] - ETA: 30:55 - loss: 0.3320 - regression_loss: 0.3049 - classification_loss: 0.0272
 212/1000 [=====>........................] - ETA: 30:55 - loss: 0.3320 - regression_loss: 0.3048 - classification_loss: 0.0272
 213/1000 [=====>........................] - ETA: 30:55 - loss: 0.3324 - regression_loss: 0.3053 - classification_loss: 0.0272
 214/1000 [=====>........................] - ETA: 30:55 - loss: 0.3330 - regression_loss: 0.3058 - classification_loss: 0.0272
 215/1000 [=====>........................] - ETA: 30:53 - loss: 0.3325 - regression_loss: 0.3053 - classification_loss: 0.0272
 216/1000 [=====>........................] - ETA: 30:53 - loss: 0.3334 - regression_loss: 0.3062 - classification_loss: 0.0272
 217/1000 [=====>........................] - ETA: 30:52 - loss: 0.3338 - regression_loss: 0.3066 - classification_loss: 0.0272
 218/1000 [=====>........................] - ETA: 30:50 - loss: 0.3340 - regression_loss: 0.3068 - classification_loss: 0.0272
 219/1000 [=====>........................] - ETA: 30:47 - loss: 0.3327 - regression_loss: 0.3055 - classification_loss: 0.0271
 220/1000 [=====>........................] - ETA: 30:48 - loss: 0.3333 - regression_loss: 0.3061 - classification_loss: 0.0272
 221/1000 [=====>........................] - ETA: 30:49 - loss: 0.3332 - regression_loss: 0.3060 - classification_loss: 0.0272
 222/1000 [=====>........................] - ETA: 30:49 - loss: 0.3337 - regression_loss: 0.3065 - classification_loss: 0.0272
 223/1000 [=====>........................] - ETA: 30:46 - loss: 0.3331 - regression_loss: 0.3060 - classification_loss: 0.0272
 224/1000 [=====>........................] - ETA: 30:48 - loss: 0.3337 - regression_loss: 0.3065 - classification_loss: 0.0272
 225/1000 [=====>........................] - ETA: 30:44 - loss: 0.3325 - regression_loss: 0.3053 - classification_loss: 0.0271
 226/1000 [=====>........................] - ETA: 30:43 - loss: 0.3335 - regression_loss: 0.3064 - classification_loss: 0.0272
 227/1000 [=====>........................] - ETA: 30:41 - loss: 0.3340 - regression_loss: 0.3068 - classification_loss: 0.0272
 228/1000 [=====>........................] - ETA: 30:42 - loss: 0.3351 - regression_loss: 0.3079 - classification_loss: 0.0272
 229/1000 [=====>........................] - ETA: 30:42 - loss: 0.3352 - regression_loss: 0.3080 - classification_loss: 0.0272
 230/1000 [=====>........................] - ETA: 30:39 - loss: 0.3349 - regression_loss: 0.3077 - classification_loss: 0.0272
 231/1000 [=====>........................] - ETA: 30:39 - loss: 0.3354 - regression_loss: 0.3082 - classification_loss: 0.0272
 232/1000 [=====>........................] - ETA: 30:37 - loss: 0.3364 - regression_loss: 0.3091 - classification_loss: 0.0273
 233/1000 [=====>........................] - ETA: 30:36 - loss: 0.3367 - regression_loss: 0.3093 - classification_loss: 0.0273
 234/1000 [======>.......................] - ETA: 30:37 - loss: 0.3374 - regression_loss: 0.3100 - classification_loss: 0.0274
 235/1000 [======>.......................] - ETA: 30:37 - loss: 0.3380 - regression_loss: 0.3106 - classification_loss: 0.0274
 236/1000 [======>.......................] - ETA: 30:36 - loss: 0.3384 - regression_loss: 0.3110 - classification_loss: 0.0274
 237/1000 [======>.......................] - ETA: 30:32 - loss: 0.3378 - regression_loss: 0.3104 - classification_loss: 0.0274
 238/1000 [======>.......................] - ETA: 30:28 - loss: 0.3366 - regression_loss: 0.3093 - classification_loss: 0.0273
 239/1000 [======>.......................] - ETA: 30:28 - loss: 0.3367 - regression_loss: 0.3094 - classification_loss: 0.0273
 240/1000 [======>.......................] - ETA: 30:24 - loss: 0.3354 - regression_loss: 0.3082 - classification_loss: 0.0272
 241/1000 [======>.......................] - ETA: 30:22 - loss: 0.3356 - regression_loss: 0.3084 - classification_loss: 0.0272
 242/1000 [======>.......................] - ETA: 30:22 - loss: 0.3362 - regression_loss: 0.3089 - classification_loss: 0.0273
 243/1000 [======>.......................] - ETA: 30:22 - loss: 0.3367 - regression_loss: 0.3094 - classification_loss: 0.0273
 244/1000 [======>.......................] - ETA: 30:19 - loss: 0.3362 - regression_loss: 0.3089 - classification_loss: 0.0273
 245/1000 [======>.......................] - ETA: 30:18 - loss: 0.3364 - regression_loss: 0.3091 - classification_loss: 0.0273
 246/1000 [======>.......................] - ETA: 30:16 - loss: 0.3365 - regression_loss: 0.3092 - classification_loss: 0.0273
 247/1000 [======>.......................] - ETA: 30:15 - loss: 0.3363 - regression_loss: 0.3090 - classification_loss: 0.0273
 248/1000 [======>.......................] - ETA: 30:12 - loss: 0.3356 - regression_loss: 0.3084 - classification_loss: 0.0272
 249/1000 [======>.......................] - ETA: 30:07 - loss: 0.3346 - regression_loss: 0.3075 - classification_loss: 0.0272
 250/1000 [======>.......................] - ETA: 30:07 - loss: 0.3353 - regression_loss: 0.3081 - classification_loss: 0.0272
 251/1000 [======>.......................] - ETA: 30:07 - loss: 0.3359 - regression_loss: 0.3087 - classification_loss: 0.0272
 252/1000 [======>.......................] - ETA: 30:06 - loss: 0.3361 - regression_loss: 0.3089 - classification_loss: 0.0272
 253/1000 [======>.......................] - ETA: 30:06 - loss: 0.3368 - regression_loss: 0.3095 - classification_loss: 0.0273
 254/1000 [======>.......................] - ETA: 30:06 - loss: 0.3376 - regression_loss: 0.3103 - classification_loss: 0.0273
 255/1000 [======>.......................] - ETA: 30:05 - loss: 0.3379 - regression_loss: 0.3106 - classification_loss: 0.0273
 256/1000 [======>.......................] - ETA: 30:02 - loss: 0.3377 - regression_loss: 0.3104 - classification_loss: 0.0273
 257/1000 [======>.......................] - ETA: 29:59 - loss: 0.3367 - regression_loss: 0.3095 - classification_loss: 0.0272
 258/1000 [======>.......................] - ETA: 29:59 - loss: 0.3369 - regression_loss: 0.3097 - classification_loss: 0.0272
 259/1000 [======>.......................] - ETA: 29:59 - loss: 0.3367 - regression_loss: 0.3095 - classification_loss: 0.0272
 260/1000 [======>.......................] - ETA: 29:58 - loss: 0.3372 - regression_loss: 0.3100 - classification_loss: 0.0273
 261/1000 [======>.......................] - ETA: 29:53 - loss: 0.3361 - regression_loss: 0.3090 - classification_loss: 0.0272
 262/1000 [======>.......................] - ETA: 29:52 - loss: 0.3364 - regression_loss: 0.3093 - classification_loss: 0.0272
 263/1000 [======>.......................] - ETA: 29:51 - loss: 0.3370 - regression_loss: 0.3097 - classification_loss: 0.0272
 264/1000 [======>.......................] - ETA: 29:50 - loss: 0.3368 - regression_loss: 0.3096 - classification_loss: 0.0272
 265/1000 [======>.......................] - ETA: 29:49 - loss: 0.3371 - regression_loss: 0.3098 - classification_loss: 0.0272
 266/1000 [======>.......................] - ETA: 29:46 - loss: 0.3364 - regression_loss: 0.3092 - classification_loss: 0.0272
 267/1000 [=======>......................] - ETA: 29:46 - loss: 0.3369 - regression_loss: 0.3097 - classification_loss: 0.0272
 268/1000 [=======>......................] - ETA: 29:46 - loss: 0.3374 - regression_loss: 0.3101 - classification_loss: 0.0273
 269/1000 [=======>......................] - ETA: 29:45 - loss: 0.3376 - regression_loss: 0.3103 - classification_loss: 0.0273
 270/1000 [=======>......................] - ETA: 29:43 - loss: 0.3377 - regression_loss: 0.3104 - classification_loss: 0.0273
 271/1000 [=======>......................] - ETA: 29:40 - loss: 0.3366 - regression_loss: 0.3094 - classification_loss: 0.0272
 272/1000 [=======>......................] - ETA: 29:36 - loss: 0.3361 - regression_loss: 0.3089 - classification_loss: 0.0272
 273/1000 [=======>......................] - ETA: 29:35 - loss: 0.3359 - regression_loss: 0.3087 - classification_loss: 0.0272
 274/1000 [=======>......................] - ETA: 29:31 - loss: 0.3348 - regression_loss: 0.3076 - classification_loss: 0.0271
 275/1000 [=======>......................] - ETA: 29:31 - loss: 0.3350 - regression_loss: 0.3079 - classification_loss: 0.0271
 276/1000 [=======>......................] - ETA: 29:31 - loss: 0.3355 - regression_loss: 0.3083 - classification_loss: 0.0272
 277/1000 [=======>......................] - ETA: 29:29 - loss: 0.3353 - regression_loss: 0.3081 - classification_loss: 0.0272
 278/1000 [=======>......................] - ETA: 29:28 - loss: 0.3357 - regression_loss: 0.3085 - classification_loss: 0.0272
 279/1000 [=======>......................] - ETA: 29:26 - loss: 0.3357 - regression_loss: 0.3085 - classification_loss: 0.0272
 280/1000 [=======>......................] - ETA: 29:22 - loss: 0.3353 - regression_loss: 0.3081 - classification_loss: 0.0272
 281/1000 [=======>......................] - ETA: 29:19 - loss: 0.3342 - regression_loss: 0.3071 - classification_loss: 0.0271
 282/1000 [=======>......................] - ETA: 29:18 - loss: 0.3342 - regression_loss: 0.3071 - classification_loss: 0.0271
 283/1000 [=======>......................] - ETA: 29:17 - loss: 0.3348 - regression_loss: 0.3076 - classification_loss: 0.0272
 284/1000 [=======>......................] - ETA: 29:13 - loss: 0.3345 - regression_loss: 0.3074 - classification_loss: 0.0271
 285/1000 [=======>......................] - ETA: 29:13 - loss: 0.3351 - regression_loss: 0.3079 - classification_loss: 0.0272
 286/1000 [=======>......................] - ETA: 29:12 - loss: 0.3356 - regression_loss: 0.3085 - classification_loss: 0.0272
 287/1000 [=======>......................] - ETA: 29:10 - loss: 0.3359 - regression_loss: 0.3087 - classification_loss: 0.0272
 288/1000 [=======>......................] - ETA: 29:10 - loss: 0.3364 - regression_loss: 0.3091 - classification_loss: 0.0272
 289/1000 [=======>......................] - ETA: 29:08 - loss: 0.3365 - regression_loss: 0.3092 - classification_loss: 0.0272
 290/1000 [=======>......................] - ETA: 29:08 - loss: 0.3371 - regression_loss: 0.3098 - classification_loss: 0.0273
 291/1000 [=======>......................] - ETA: 29:05 - loss: 0.3365 - regression_loss: 0.3093 - classification_loss: 0.0272
 292/1000 [=======>......................] - ETA: 29:04 - loss: 0.3368 - regression_loss: 0.3095 - classification_loss: 0.0272
 293/1000 [=======>......................] - ETA: 29:01 - loss: 0.3360 - regression_loss: 0.3087 - classification_loss: 0.0272
 294/1000 [=======>......................] - ETA: 29:00 - loss: 0.3358 - regression_loss: 0.3086 - classification_loss: 0.0272
 295/1000 [=======>......................] - ETA: 28:58 - loss: 0.3357 - regression_loss: 0.3085 - classification_loss: 0.0272
 296/1000 [=======>......................] - ETA: 28:58 - loss: 0.3362 - regression_loss: 0.3089 - classification_loss: 0.0273
 297/1000 [=======>......................] - ETA: 28:54 - loss: 0.3351 - regression_loss: 0.3079 - classification_loss: 0.0272
 298/1000 [=======>......................] - ETA: 28:50 - loss: 0.3348 - regression_loss: 0.3076 - classification_loss: 0.0272
 299/1000 [=======>......................] - ETA: 28:49 - loss: 0.3357 - regression_loss: 0.3085 - classification_loss: 0.0272
 300/1000 [========>.....................] - ETA: 28:48 - loss: 0.3370 - regression_loss: 0.3097 - classification_loss: 0.0273
 301/1000 [========>.....................] - ETA: 28:46 - loss: 0.3379 - regression_loss: 0.3106 - classification_loss: 0.0274
 302/1000 [========>.....................] - ETA: 28:45 - loss: 0.3388 - regression_loss: 0.3114 - classification_loss: 0.0274
 303/1000 [========>.....................] - ETA: 28:43 - loss: 0.3391 - regression_loss: 0.3117 - classification_loss: 0.0274
 304/1000 [========>.....................] - ETA: 28:41 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0275
 305/1000 [========>.....................] - ETA: 28:40 - loss: 0.3397 - regression_loss: 0.3122 - classification_loss: 0.0275
 306/1000 [========>.....................] - ETA: 28:36 - loss: 0.3387 - regression_loss: 0.3113 - classification_loss: 0.0274
 307/1000 [========>.....................] - ETA: 28:33 - loss: 0.3388 - regression_loss: 0.3113 - classification_loss: 0.0274
 308/1000 [========>.....................] - ETA: 28:32 - loss: 0.3394 - regression_loss: 0.3119 - classification_loss: 0.0275
 309/1000 [========>.....................] - ETA: 28:29 - loss: 0.3391 - regression_loss: 0.3116 - classification_loss: 0.0275
 310/1000 [========>.....................] - ETA: 28:27 - loss: 0.3394 - regression_loss: 0.3119 - classification_loss: 0.0275
 311/1000 [========>.....................] - ETA: 28:26 - loss: 0.3400 - regression_loss: 0.3124 - classification_loss: 0.0275
 312/1000 [========>.....................] - ETA: 28:24 - loss: 0.3402 - regression_loss: 0.3127 - classification_loss: 0.0276
 313/1000 [========>.....................] - ETA: 28:23 - loss: 0.3407 - regression_loss: 0.3131 - classification_loss: 0.0276
 314/1000 [========>.....................] - ETA: 28:20 - loss: 0.3403 - regression_loss: 0.3128 - classification_loss: 0.0275
 315/1000 [========>.....................] - ETA: 28:17 - loss: 0.3405 - regression_loss: 0.3129 - classification_loss: 0.0276
 316/1000 [========>.....................] - ETA: 28:16 - loss: 0.3412 - regression_loss: 0.3136 - classification_loss: 0.0276
 317/1000 [========>.....................] - ETA: 28:15 - loss: 0.3417 - regression_loss: 0.3140 - classification_loss: 0.0277
 318/1000 [========>.....................] - ETA: 28:13 - loss: 0.3423 - regression_loss: 0.3145 - classification_loss: 0.0278
 319/1000 [========>.....................] - ETA: 28:10 - loss: 0.3427 - regression_loss: 0.3150 - classification_loss: 0.0278
 320/1000 [========>.....................] - ETA: 28:08 - loss: 0.3429 - regression_loss: 0.3151 - classification_loss: 0.0278
 321/1000 [========>.....................] - ETA: 28:04 - loss: 0.3421 - regression_loss: 0.3144 - classification_loss: 0.0277
 322/1000 [========>.....................] - ETA: 28:03 - loss: 0.3421 - regression_loss: 0.3143 - classification_loss: 0.0277
 323/1000 [========>.....................] - ETA: 28:03 - loss: 0.3425 - regression_loss: 0.3147 - classification_loss: 0.0278
 324/1000 [========>.....................] - ETA: 28:02 - loss: 0.3424 - regression_loss: 0.3146 - classification_loss: 0.0278
 325/1000 [========>.....................] - ETA: 27:59 - loss: 0.3416 - regression_loss: 0.3139 - classification_loss: 0.0277
 326/1000 [========>.....................] - ETA: 27:57 - loss: 0.3417 - regression_loss: 0.3140 - classification_loss: 0.0277
 327/1000 [========>.....................] - ETA: 27:56 - loss: 0.3422 - regression_loss: 0.3144 - classification_loss: 0.0278
 328/1000 [========>.....................] - ETA: 27:53 - loss: 0.3418 - regression_loss: 0.3140 - classification_loss: 0.0278
 329/1000 [========>.....................] - ETA: 27:51 - loss: 0.3420 - regression_loss: 0.3142 - classification_loss: 0.0278
 330/1000 [========>.....................] - ETA: 27:50 - loss: 0.3422 - regression_loss: 0.3144 - classification_loss: 0.0278
 331/1000 [========>.....................] - ETA: 27:47 - loss: 0.3417 - regression_loss: 0.3140 - classification_loss: 0.0278
 332/1000 [========>.....................] - ETA: 27:46 - loss: 0.3421 - regression_loss: 0.3143 - classification_loss: 0.0278
 333/1000 [========>.....................] - ETA: 27:44 - loss: 0.3422 - regression_loss: 0.3144 - classification_loss: 0.0278
 334/1000 [=========>....................] - ETA: 27:43 - loss: 0.3421 - regression_loss: 0.3142 - classification_loss: 0.0278
 335/1000 [=========>....................] - ETA: 27:41 - loss: 0.3424 - regression_loss: 0.3145 - classification_loss: 0.0278
 336/1000 [=========>....................] - ETA: 27:38 - loss: 0.3418 - regression_loss: 0.3140 - classification_loss: 0.0278
 337/1000 [=========>....................] - ETA: 27:37 - loss: 0.3421 - regression_loss: 0.3143 - classification_loss: 0.0278
 338/1000 [=========>....................] - ETA: 27:33 - loss: 0.3417 - regression_loss: 0.3139 - classification_loss: 0.0278
 339/1000 [=========>....................] - ETA: 27:32 - loss: 0.3421 - regression_loss: 0.3143 - classification_loss: 0.0278
 340/1000 [=========>....................] - ETA: 27:30 - loss: 0.3422 - regression_loss: 0.3144 - classification_loss: 0.0278
 341/1000 [=========>....................] - ETA: 27:29 - loss: 0.3420 - regression_loss: 0.3142 - classification_loss: 0.0278
 342/1000 [=========>....................] - ETA: 27:26 - loss: 0.3420 - regression_loss: 0.3142 - classification_loss: 0.0278
 343/1000 [=========>....................] - ETA: 27:23 - loss: 0.3411 - regression_loss: 0.3134 - classification_loss: 0.0277
 344/1000 [=========>....................] - ETA: 27:19 - loss: 0.3402 - regression_loss: 0.3125 - classification_loss: 0.0277
 345/1000 [=========>....................] - ETA: 27:16 - loss: 0.3400 - regression_loss: 0.3123 - classification_loss: 0.0277
 346/1000 [=========>....................] - ETA: 27:15 - loss: 0.3404 - regression_loss: 0.3127 - classification_loss: 0.0277
 347/1000 [=========>....................] - ETA: 27:13 - loss: 0.3407 - regression_loss: 0.3130 - classification_loss: 0.0277
 348/1000 [=========>....................] - ETA: 27:11 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
 349/1000 [=========>....................] - ETA: 27:09 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
 350/1000 [=========>....................] - ETA: 27:07 - loss: 0.3409 - regression_loss: 0.3131 - classification_loss: 0.0277
 351/1000 [=========>....................] - ETA: 27:04 - loss: 0.3401 - regression_loss: 0.3124 - classification_loss: 0.0277
 352/1000 [=========>....................] - ETA: 27:01 - loss: 0.3396 - regression_loss: 0.3120 - classification_loss: 0.0276
 353/1000 [=========>....................] - ETA: 26:59 - loss: 0.3398 - regression_loss: 0.3122 - classification_loss: 0.0276
 354/1000 [=========>....................] - ETA: 26:57 - loss: 0.3403 - regression_loss: 0.3126 - classification_loss: 0.0277
 355/1000 [=========>....................] - ETA: 26:56 - loss: 0.3402 - regression_loss: 0.3125 - classification_loss: 0.0277
 356/1000 [=========>....................] - ETA: 26:54 - loss: 0.3405 - regression_loss: 0.3128 - classification_loss: 0.0277
 357/1000 [=========>....................] - ETA: 26:53 - loss: 0.3407 - regression_loss: 0.3130 - classification_loss: 0.0277
 358/1000 [=========>....................] - ETA: 26:52 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
 359/1000 [=========>....................] - ETA: 26:51 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 360/1000 [=========>....................] - ETA: 26:49 - loss: 0.3412 - regression_loss: 0.3134 - classification_loss: 0.0277
 361/1000 [=========>....................] - ETA: 26:46 - loss: 0.3409 - regression_loss: 0.3131 - classification_loss: 0.0277
 362/1000 [=========>....................] - ETA: 26:44 - loss: 0.3411 - regression_loss: 0.3133 - classification_loss: 0.0277
 363/1000 [=========>....................] - ETA: 26:42 - loss: 0.3411 - regression_loss: 0.3134 - classification_loss: 0.0277
 364/1000 [=========>....................] - ETA: 26:39 - loss: 0.3404 - regression_loss: 0.3127 - classification_loss: 0.0277
 365/1000 [=========>....................] - ETA: 26:35 - loss: 0.3396 - regression_loss: 0.3120 - classification_loss: 0.0276
 366/1000 [=========>....................] - ETA: 26:33 - loss: 0.3399 - regression_loss: 0.3123 - classification_loss: 0.0276
 367/1000 [==========>...................] - ETA: 26:30 - loss: 0.3396 - regression_loss: 0.3120 - classification_loss: 0.0276
 368/1000 [==========>...................] - ETA: 26:28 - loss: 0.3400 - regression_loss: 0.3124 - classification_loss: 0.0276
 369/1000 [==========>...................] - ETA: 26:26 - loss: 0.3399 - regression_loss: 0.3123 - classification_loss: 0.0276
 370/1000 [==========>...................] - ETA: 26:24 - loss: 0.3403 - regression_loss: 0.3126 - classification_loss: 0.0276
 371/1000 [==========>...................] - ETA: 26:23 - loss: 0.3407 - regression_loss: 0.3130 - classification_loss: 0.0277
 372/1000 [==========>...................] - ETA: 26:20 - loss: 0.3399 - regression_loss: 0.3123 - classification_loss: 0.0276
 373/1000 [==========>...................] - ETA: 26:18 - loss: 0.3404 - regression_loss: 0.3127 - classification_loss: 0.0276
 374/1000 [==========>...................] - ETA: 26:16 - loss: 0.3406 - regression_loss: 0.3129 - classification_loss: 0.0276
 375/1000 [==========>...................] - ETA: 26:13 - loss: 0.3402 - regression_loss: 0.3126 - classification_loss: 0.0276
 376/1000 [==========>...................] - ETA: 26:11 - loss: 0.3405 - regression_loss: 0.3129 - classification_loss: 0.0276
 377/1000 [==========>...................] - ETA: 26:09 - loss: 0.3406 - regression_loss: 0.3130 - classification_loss: 0.0276
 378/1000 [==========>...................] - ETA: 26:08 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
 379/1000 [==========>...................] - ETA: 26:07 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 380/1000 [==========>...................] - ETA: 26:05 - loss: 0.3412 - regression_loss: 0.3135 - classification_loss: 0.0277
 381/1000 [==========>...................] - ETA: 26:03 - loss: 0.3415 - regression_loss: 0.3138 - classification_loss: 0.0277
 382/1000 [==========>...................] - ETA: 25:59 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 383/1000 [==========>...................] - ETA: 25:58 - loss: 0.3412 - regression_loss: 0.3136 - classification_loss: 0.0277
 384/1000 [==========>...................] - ETA: 25:55 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 385/1000 [==========>...................] - ETA: 25:52 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 386/1000 [==========>...................] - ETA: 25:50 - loss: 0.3409 - regression_loss: 0.3132 - classification_loss: 0.0276
 387/1000 [==========>...................] - ETA: 25:48 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 388/1000 [==========>...................] - ETA: 25:47 - loss: 0.3416 - regression_loss: 0.3139 - classification_loss: 0.0277
 389/1000 [==========>...................] - ETA: 25:44 - loss: 0.3412 - regression_loss: 0.3136 - classification_loss: 0.0277
 390/1000 [==========>...................] - ETA: 25:42 - loss: 0.3414 - regression_loss: 0.3138 - classification_loss: 0.0277
 391/1000 [==========>...................] - ETA: 25:38 - loss: 0.3412 - regression_loss: 0.3135 - classification_loss: 0.0276
 392/1000 [==========>...................] - ETA: 25:36 - loss: 0.3412 - regression_loss: 0.3136 - classification_loss: 0.0276
 393/1000 [==========>...................] - ETA: 25:34 - loss: 0.3410 - regression_loss: 0.3134 - classification_loss: 0.0276
 394/1000 [==========>...................] - ETA: 25:31 - loss: 0.3406 - regression_loss: 0.3130 - classification_loss: 0.0276
 395/1000 [==========>...................] - ETA: 25:27 - loss: 0.3401 - regression_loss: 0.3125 - classification_loss: 0.0276
 396/1000 [==========>...................] - ETA: 25:26 - loss: 0.3404 - regression_loss: 0.3128 - classification_loss: 0.0276
 397/1000 [==========>...................] - ETA: 25:25 - loss: 0.3406 - regression_loss: 0.3130 - classification_loss: 0.0276
 398/1000 [==========>...................] - ETA: 25:23 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 399/1000 [==========>...................] - ETA: 25:21 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 400/1000 [===========>..................] - ETA: 25:19 - loss: 0.3412 - regression_loss: 0.3136 - classification_loss: 0.0276
 401/1000 [===========>..................] - ETA: 25:17 - loss: 0.3413 - regression_loss: 0.3137 - classification_loss: 0.0276
 402/1000 [===========>..................] - ETA: 25:16 - loss: 0.3416 - regression_loss: 0.3140 - classification_loss: 0.0277
 403/1000 [===========>..................] - ETA: 25:14 - loss: 0.3415 - regression_loss: 0.3138 - classification_loss: 0.0277
 404/1000 [===========>..................] - ETA: 25:12 - loss: 0.3415 - regression_loss: 0.3139 - classification_loss: 0.0277
 405/1000 [===========>..................] - ETA: 25:08 - loss: 0.3408 - regression_loss: 0.3132 - classification_loss: 0.0276
 406/1000 [===========>..................] - ETA: 25:05 - loss: 0.3404 - regression_loss: 0.3129 - classification_loss: 0.0276
 407/1000 [===========>..................] - ETA: 25:02 - loss: 0.3397 - regression_loss: 0.3121 - classification_loss: 0.0275
 408/1000 [===========>..................] - ETA: 24:59 - loss: 0.3393 - regression_loss: 0.3118 - classification_loss: 0.0275
 409/1000 [===========>..................] - ETA: 24:57 - loss: 0.3396 - regression_loss: 0.3121 - classification_loss: 0.0275
 410/1000 [===========>..................] - ETA: 24:55 - loss: 0.3396 - regression_loss: 0.3120 - classification_loss: 0.0276
 411/1000 [===========>..................] - ETA: 24:53 - loss: 0.3398 - regression_loss: 0.3122 - classification_loss: 0.0276
 412/1000 [===========>..................] - ETA: 24:51 - loss: 0.3401 - regression_loss: 0.3125 - classification_loss: 0.0276
 413/1000 [===========>..................] - ETA: 24:49 - loss: 0.3402 - regression_loss: 0.3126 - classification_loss: 0.0276
 414/1000 [===========>..................] - ETA: 24:45 - loss: 0.3396 - regression_loss: 0.3121 - classification_loss: 0.0276
 415/1000 [===========>..................] - ETA: 24:44 - loss: 0.3401 - regression_loss: 0.3125 - classification_loss: 0.0276
 416/1000 [===========>..................] - ETA: 24:41 - loss: 0.3404 - regression_loss: 0.3128 - classification_loss: 0.0276
 417/1000 [===========>..................] - ETA: 24:39 - loss: 0.3406 - regression_loss: 0.3129 - classification_loss: 0.0276
 418/1000 [===========>..................] - ETA: 24:37 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 419/1000 [===========>..................] - ETA: 24:34 - loss: 0.3411 - regression_loss: 0.3135 - classification_loss: 0.0276
 420/1000 [===========>..................] - ETA: 24:32 - loss: 0.3410 - regression_loss: 0.3134 - classification_loss: 0.0276
 421/1000 [===========>..................] - ETA: 24:30 - loss: 0.3410 - regression_loss: 0.3133 - classification_loss: 0.0277
 422/1000 [===========>..................] - ETA: 24:28 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 423/1000 [===========>..................] - ETA: 24:25 - loss: 0.3413 - regression_loss: 0.3136 - classification_loss: 0.0277
 424/1000 [===========>..................] - ETA: 24:22 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 425/1000 [===========>..................] - ETA: 24:21 - loss: 0.3415 - regression_loss: 0.3138 - classification_loss: 0.0277
 426/1000 [===========>..................] - ETA: 24:17 - loss: 0.3409 - regression_loss: 0.3133 - classification_loss: 0.0276
 427/1000 [===========>..................] - ETA: 24:16 - loss: 0.3415 - regression_loss: 0.3139 - classification_loss: 0.0276
 428/1000 [===========>..................] - ETA: 24:14 - loss: 0.3419 - regression_loss: 0.3142 - classification_loss: 0.0276
 429/1000 [===========>..................] - ETA: 24:12 - loss: 0.3423 - regression_loss: 0.3146 - classification_loss: 0.0277
 430/1000 [===========>..................] - ETA: 24:09 - loss: 0.3416 - regression_loss: 0.3140 - classification_loss: 0.0276
 431/1000 [===========>..................] - ETA: 24:06 - loss: 0.3415 - regression_loss: 0.3139 - classification_loss: 0.0276
 432/1000 [===========>..................] - ETA: 24:04 - loss: 0.3417 - regression_loss: 0.3141 - classification_loss: 0.0276
 433/1000 [===========>..................] - ETA: 24:03 - loss: 0.3420 - regression_loss: 0.3144 - classification_loss: 0.0276
 434/1000 [============>.................] - ETA: 24:01 - loss: 0.3425 - regression_loss: 0.3149 - classification_loss: 0.0276
 435/1000 [============>.................] - ETA: 23:59 - loss: 0.3424 - regression_loss: 0.3147 - classification_loss: 0.0276
 436/1000 [============>.................] - ETA: 23:57 - loss: 0.3425 - regression_loss: 0.3148 - classification_loss: 0.0277
 437/1000 [============>.................] - ETA: 23:53 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 438/1000 [============>.................] - ETA: 23:52 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 439/1000 [============>.................] - ETA: 23:50 - loss: 0.3425 - regression_loss: 0.3148 - classification_loss: 0.0276
 440/1000 [============>.................] - ETA: 23:47 - loss: 0.3422 - regression_loss: 0.3146 - classification_loss: 0.0276
 441/1000 [============>.................] - ETA: 23:45 - loss: 0.3424 - regression_loss: 0.3147 - classification_loss: 0.0276
 442/1000 [============>.................] - ETA: 23:43 - loss: 0.3427 - regression_loss: 0.3150 - classification_loss: 0.0276
 443/1000 [============>.................] - ETA: 23:41 - loss: 0.3427 - regression_loss: 0.3151 - classification_loss: 0.0276
 444/1000 [============>.................] - ETA: 23:38 - loss: 0.3424 - regression_loss: 0.3147 - classification_loss: 0.0276
 445/1000 [============>.................] - ETA: 23:34 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 446/1000 [============>.................] - ETA: 23:32 - loss: 0.3418 - regression_loss: 0.3143 - classification_loss: 0.0276
 447/1000 [============>.................] - ETA: 23:30 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 448/1000 [============>.................] - ETA: 23:28 - loss: 0.3425 - regression_loss: 0.3149 - classification_loss: 0.0276
 449/1000 [============>.................] - ETA: 23:25 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 450/1000 [============>.................] - ETA: 23:23 - loss: 0.3424 - regression_loss: 0.3148 - classification_loss: 0.0276
 451/1000 [============>.................] - ETA: 23:20 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 452/1000 [============>.................] - ETA: 23:18 - loss: 0.3420 - regression_loss: 0.3144 - classification_loss: 0.0276
 453/1000 [============>.................] - ETA: 23:16 - loss: 0.3423 - regression_loss: 0.3147 - classification_loss: 0.0276
 454/1000 [============>.................] - ETA: 23:14 - loss: 0.3423 - regression_loss: 0.3146 - classification_loss: 0.0276
 455/1000 [============>.................] - ETA: 23:11 - loss: 0.3423 - regression_loss: 0.3147 - classification_loss: 0.0276
 456/1000 [============>.................] - ETA: 23:08 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 457/1000 [============>.................] - ETA: 23:06 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 458/1000 [============>.................] - ETA: 23:03 - loss: 0.3422 - regression_loss: 0.3146 - classification_loss: 0.0276
 459/1000 [============>.................] - ETA: 23:00 - loss: 0.3419 - regression_loss: 0.3144 - classification_loss: 0.0276
 460/1000 [============>.................] - ETA: 22:59 - loss: 0.3422 - regression_loss: 0.3146 - classification_loss: 0.0276
 461/1000 [============>.................] - ETA: 22:57 - loss: 0.3422 - regression_loss: 0.3146 - classification_loss: 0.0276
 462/1000 [============>.................] - ETA: 22:55 - loss: 0.3425 - regression_loss: 0.3149 - classification_loss: 0.0276
 463/1000 [============>.................] - ETA: 22:51 - loss: 0.3419 - regression_loss: 0.3143 - classification_loss: 0.0276
 464/1000 [============>.................] - ETA: 22:50 - loss: 0.3422 - regression_loss: 0.3146 - classification_loss: 0.0276
 465/1000 [============>.................] - ETA: 22:48 - loss: 0.3425 - regression_loss: 0.3148 - classification_loss: 0.0276
 466/1000 [============>.................] - ETA: 22:46 - loss: 0.3425 - regression_loss: 0.3149 - classification_loss: 0.0276
 467/1000 [=============>................] - ETA: 22:44 - loss: 0.3426 - regression_loss: 0.3150 - classification_loss: 0.0277
 468/1000 [=============>................] - ETA: 22:42 - loss: 0.3425 - regression_loss: 0.3148 - classification_loss: 0.0277
 469/1000 [=============>................] - ETA: 22:39 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 470/1000 [=============>................] - ETA: 22:36 - loss: 0.3422 - regression_loss: 0.3145 - classification_loss: 0.0276
 471/1000 [=============>................] - ETA: 22:34 - loss: 0.3423 - regression_loss: 0.3147 - classification_loss: 0.0276
 472/1000 [=============>................] - ETA: 22:30 - loss: 0.3419 - regression_loss: 0.3143 - classification_loss: 0.0276
 473/1000 [=============>................] - ETA: 22:27 - loss: 0.3413 - regression_loss: 0.3137 - classification_loss: 0.0276
 474/1000 [=============>................] - ETA: 22:25 - loss: 0.3415 - regression_loss: 0.3139 - classification_loss: 0.0276
 475/1000 [=============>................] - ETA: 22:23 - loss: 0.3417 - regression_loss: 0.3141 - classification_loss: 0.0276
 476/1000 [=============>................] - ETA: 22:20 - loss: 0.3416 - regression_loss: 0.3140 - classification_loss: 0.0276
 477/1000 [=============>................] - ETA: 22:18 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 478/1000 [=============>................] - ETA: 22:15 - loss: 0.3419 - regression_loss: 0.3143 - classification_loss: 0.0276
 479/1000 [=============>................] - ETA: 22:12 - loss: 0.3419 - regression_loss: 0.3143 - classification_loss: 0.0276
 480/1000 [=============>................] - ETA: 22:10 - loss: 0.3421 - regression_loss: 0.3145 - classification_loss: 0.0276
 481/1000 [=============>................] - ETA: 22:07 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 482/1000 [=============>................] - ETA: 22:04 - loss: 0.3418 - regression_loss: 0.3142 - classification_loss: 0.0276
 483/1000 [=============>................] - ETA: 22:00 - loss: 0.3411 - regression_loss: 0.3136 - classification_loss: 0.0276
 484/1000 [=============>................] - ETA: 21:58 - loss: 0.3414 - regression_loss: 0.3138 - classification_loss: 0.0276
 485/1000 [=============>................] - ETA: 21:55 - loss: 0.3411 - regression_loss: 0.3136 - classification_loss: 0.0275
 486/1000 [=============>................] - ETA: 21:52 - loss: 0.3411 - regression_loss: 0.3135 - classification_loss: 0.0276
 487/1000 [=============>................] - ETA: 21:48 - loss: 0.3405 - regression_loss: 0.3130 - classification_loss: 0.0275
 488/1000 [=============>................] - ETA: 21:46 - loss: 0.3405 - regression_loss: 0.3130 - classification_loss: 0.0275
 489/1000 [=============>................] - ETA: 21:43 - loss: 0.3406 - regression_loss: 0.3131 - classification_loss: 0.0275
 490/1000 [=============>................] - ETA: 21:41 - loss: 0.3408 - regression_loss: 0.3133 - classification_loss: 0.0275
 491/1000 [=============>................] - ETA: 21:37 - loss: 0.3406 - regression_loss: 0.3131 - classification_loss: 0.0275
 492/1000 [=============>................] - ETA: 21:35 - loss: 0.3408 - regression_loss: 0.3133 - classification_loss: 0.0275
 493/1000 [=============>................] - ETA: 21:32 - loss: 0.3410 - regression_loss: 0.3134 - classification_loss: 0.0275
 494/1000 [=============>................] - ETA: 21:30 - loss: 0.3410 - regression_loss: 0.3134 - classification_loss: 0.0275
 495/1000 [=============>................] - ETA: 21:27 - loss: 0.3408 - regression_loss: 0.3133 - classification_loss: 0.0275
 496/1000 [=============>................] - ETA: 21:24 - loss: 0.3409 - regression_loss: 0.3134 - classification_loss: 0.0275
 497/1000 [=============>................] - ETA: 21:21 - loss: 0.3403 - regression_loss: 0.3128 - classification_loss: 0.0275
 498/1000 [=============>................] - ETA: 21:18 - loss: 0.3405 - regression_loss: 0.3130 - classification_loss: 0.0275
 499/1000 [=============>................] - ETA: 21:15 - loss: 0.3399 - regression_loss: 0.3124 - classification_loss: 0.0275
 500/1000 [==============>...............] - ETA: 21:12 - loss: 0.3399 - regression_loss: 0.3124 - classification_loss: 0.0275
 501/1000 [==============>...............] - ETA: 21:09 - loss: 0.3396 - regression_loss: 0.3122 - classification_loss: 0.0274
 502/1000 [==============>...............] - ETA: 21:06 - loss: 0.3398 - regression_loss: 0.3123 - classification_loss: 0.0274
 503/1000 [==============>...............] - ETA: 21:04 - loss: 0.3400 - regression_loss: 0.3126 - classification_loss: 0.0274
 504/1000 [==============>...............] - ETA: 21:01 - loss: 0.3399 - regression_loss: 0.3124 - classification_loss: 0.0275
 505/1000 [==============>...............] - ETA: 20:59 - loss: 0.3401 - regression_loss: 0.3126 - classification_loss: 0.0275
 506/1000 [==============>...............] - ETA: 20:56 - loss: 0.3404 - regression_loss: 0.3129 - classification_loss: 0.0275
 507/1000 [==============>...............] - ETA: 20:54 - loss: 0.3405 - regression_loss: 0.3130 - classification_loss: 0.0275
 508/1000 [==============>...............] - ETA: 20:51 - loss: 0.3404 - regression_loss: 0.3129 - classification_loss: 0.0275
 509/1000 [==============>...............] - ETA: 20:47 - loss: 0.3400 - regression_loss: 0.3126 - classification_loss: 0.0275
 510/1000 [==============>...............] - ETA: 20:45 - loss: 0.3400 - regression_loss: 0.3126 - classification_loss: 0.0275
 511/1000 [==============>...............] - ETA: 20:41 - loss: 0.3395 - regression_loss: 0.3121 - classification_loss: 0.0274
 512/1000 [==============>...............] - ETA: 20:39 - loss: 0.3398 - regression_loss: 0.3124 - classification_loss: 0.0274
 513/1000 [==============>...............] - ETA: 20:36 - loss: 0.3400 - regression_loss: 0.3125 - classification_loss: 0.0274
 514/1000 [==============>...............] - ETA: 20:33 - loss: 0.3394 - regression_loss: 0.3120 - classification_loss: 0.0274
 515/1000 [==============>...............] - ETA: 20:30 - loss: 0.3392 - regression_loss: 0.3118 - classification_loss: 0.0274
 516/1000 [==============>...............] - ETA: 20:27 - loss: 0.3391 - regression_loss: 0.3118 - classification_loss: 0.0274
 517/1000 [==============>...............] - ETA: 20:25 - loss: 0.3394 - regression_loss: 0.3120 - classification_loss: 0.0274
 518/1000 [==============>...............] - ETA: 20:22 - loss: 0.3394 - regression_loss: 0.3120 - classification_loss: 0.0274
 519/1000 [==============>...............] - ETA: 20:19 - loss: 0.3395 - regression_loss: 0.3121 - classification_loss: 0.0274
 520/1000 [==============>...............] - ETA: 20:16 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0274
 521/1000 [==============>...............] - ETA: 20:13 - loss: 0.3392 - regression_loss: 0.3119 - classification_loss: 0.0274
 522/1000 [==============>...............] - ETA: 20:10 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0274
 523/1000 [==============>...............] - ETA: 20:07 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0274
 524/1000 [==============>...............] - ETA: 20:05 - loss: 0.3392 - regression_loss: 0.3118 - classification_loss: 0.0274
 525/1000 [==============>...............] - ETA: 20:02 - loss: 0.3391 - regression_loss: 0.3117 - classification_loss: 0.0274
 526/1000 [==============>...............] - ETA: 20:00 - loss: 0.3393 - regression_loss: 0.3119 - classification_loss: 0.0274
 527/1000 [==============>...............] - ETA: 19:56 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0274
 528/1000 [==============>...............] - ETA: 19:54 - loss: 0.3392 - regression_loss: 0.3118 - classification_loss: 0.0274
 529/1000 [==============>...............] - ETA: 19:51 - loss: 0.3393 - regression_loss: 0.3120 - classification_loss: 0.0274
 530/1000 [==============>...............] - ETA: 19:49 - loss: 0.3394 - regression_loss: 0.3120 - classification_loss: 0.0274
 531/1000 [==============>...............] - ETA: 19:46 - loss: 0.3392 - regression_loss: 0.3119 - classification_loss: 0.0274
 532/1000 [==============>...............] - ETA: 19:43 - loss: 0.3388 - regression_loss: 0.3114 - classification_loss: 0.0273
 533/1000 [==============>...............] - ETA: 19:40 - loss: 0.3387 - regression_loss: 0.3114 - classification_loss: 0.0274
 534/1000 [===============>..............] - ETA: 19:38 - loss: 0.3390 - regression_loss: 0.3116 - classification_loss: 0.0274
 535/1000 [===============>..............] - ETA: 19:34 - loss: 0.3387 - regression_loss: 0.3114 - classification_loss: 0.0274
 536/1000 [===============>..............] - ETA: 19:32 - loss: 0.3389 - regression_loss: 0.3115 - classification_loss: 0.0274
 537/1000 [===============>..............] - ETA: 19:29 - loss: 0.3391 - regression_loss: 0.3117 - classification_loss: 0.0274
 538/1000 [===============>..............] - ETA: 19:26 - loss: 0.3385 - regression_loss: 0.3112 - classification_loss: 0.0273
 539/1000 [===============>..............] - ETA: 19:23 - loss: 0.3385 - regression_loss: 0.3112 - classification_loss: 0.0273
 540/1000 [===============>..............] - ETA: 19:20 - loss: 0.3380 - regression_loss: 0.3107 - classification_loss: 0.0273
 541/1000 [===============>..............] - ETA: 19:17 - loss: 0.3379 - regression_loss: 0.3106 - classification_loss: 0.0273
 542/1000 [===============>..............] - ETA: 19:14 - loss: 0.3376 - regression_loss: 0.3103 - classification_loss: 0.0273
 543/1000 [===============>..............] - ETA: 19:12 - loss: 0.3377 - regression_loss: 0.3105 - classification_loss: 0.0273
 544/1000 [===============>..............] - ETA: 19:09 - loss: 0.3380 - regression_loss: 0.3107 - classification_loss: 0.0273
 545/1000 [===============>..............] - ETA: 19:07 - loss: 0.3382 - regression_loss: 0.3109 - classification_loss: 0.0273
 546/1000 [===============>..............] - ETA: 19:04 - loss: 0.3382 - regression_loss: 0.3109 - classification_loss: 0.0273
 547/1000 [===============>..............] - ETA: 19:01 - loss: 0.3382 - regression_loss: 0.3109 - classification_loss: 0.0273
 548/1000 [===============>..............] - ETA: 18:59 - loss: 0.3384 - regression_loss: 0.3111 - classification_loss: 0.0273
 549/1000 [===============>..............] - ETA: 18:56 - loss: 0.3386 - regression_loss: 0.3113 - classification_loss: 0.0273
 550/1000 [===============>..............] - ETA: 18:53 - loss: 0.3384 - regression_loss: 0.3111 - classification_loss: 0.0273
 551/1000 [===============>..............] - ETA: 18:51 - loss: 0.3387 - regression_loss: 0.3114 - classification_loss: 0.0273
 552/1000 [===============>..............] - ETA: 18:48 - loss: 0.3386 - regression_loss: 0.3113 - classification_loss: 0.0273
 553/1000 [===============>..............] - ETA: 18:45 - loss: 0.3382 - regression_loss: 0.3109 - classification_loss: 0.0273
 554/1000 [===============>..............] - ETA: 18:42 - loss: 0.3379 - regression_loss: 0.3106 - classification_loss: 0.0273
 555/1000 [===============>..............] - ETA: 18:39 - loss: 0.3380 - regression_loss: 0.3107 - classification_loss: 0.0273
 556/1000 [===============>..............] - ETA: 18:37 - loss: 0.3380 - regression_loss: 0.3107 - classification_loss: 0.0273
 557/1000 [===============>..............] - ETA: 18:34 - loss: 0.3383 - regression_loss: 0.3110 - classification_loss: 0.0273
 558/1000 [===============>..............] - ETA: 18:32 - loss: 0.3384 - regression_loss: 0.3111 - classification_loss: 0.0273
 559/1000 [===============>..............] - ETA: 18:29 - loss: 0.3387 - regression_loss: 0.3114 - classification_loss: 0.0273
 560/1000 [===============>..............] - ETA: 18:26 - loss: 0.3383 - regression_loss: 0.3111 - classification_loss: 0.0272
 561/1000 [===============>..............] - ETA: 18:24 - loss: 0.3386 - regression_loss: 0.3113 - classification_loss: 0.0273
 562/1000 [===============>..............] - ETA: 18:21 - loss: 0.3389 - regression_loss: 0.3116 - classification_loss: 0.0273
 563/1000 [===============>..............] - ETA: 18:18 - loss: 0.3383 - regression_loss: 0.3111 - classification_loss: 0.0272
 564/1000 [===============>..............] - ETA: 18:15 - loss: 0.3382 - regression_loss: 0.3109 - classification_loss: 0.0272
 565/1000 [===============>..............] - ETA: 18:12 - loss: 0.3383 - regression_loss: 0.3110 - classification_loss: 0.0272
 566/1000 [===============>..............] - ETA: 18:10 - loss: 0.3384 - regression_loss: 0.3111 - classification_loss: 0.0272
 567/1000 [================>.............] - ETA: 18:07 - loss: 0.3383 - regression_loss: 0.3111 - classification_loss: 0.0272
 568/1000 [================>.............] - ETA: 18:05 - loss: 0.3385 - regression_loss: 0.3113 - classification_loss: 0.0273
 569/1000 [================>.............] - ETA: 18:02 - loss: 0.3388 - regression_loss: 0.3115 - classification_loss: 0.0273
 570/1000 [================>.............] - ETA: 18:00 - loss: 0.3387 - regression_loss: 0.3114 - classification_loss: 0.0273
 571/1000 [================>.............] - ETA: 17:57 - loss: 0.3387 - regression_loss: 0.3115 - classification_loss: 0.0273
 572/1000 [================>.............] - ETA: 17:54 - loss: 0.3384 - regression_loss: 0.3112 - classification_loss: 0.0273
 573/1000 [================>.............] - ETA: 17:51 - loss: 0.3385 - regression_loss: 0.3113 - classification_loss: 0.0273
 574/1000 [================>.............] - ETA: 17:48 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 575/1000 [================>.............] - ETA: 17:45 - loss: 0.3375 - regression_loss: 0.3104 - classification_loss: 0.0272
 576/1000 [================>.............] - ETA: 17:43 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 577/1000 [================>.............] - ETA: 17:40 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 578/1000 [================>.............] - ETA: 17:37 - loss: 0.3377 - regression_loss: 0.3105 - classification_loss: 0.0272
 579/1000 [================>.............] - ETA: 17:34 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 580/1000 [================>.............] - ETA: 17:32 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 581/1000 [================>.............] - ETA: 17:29 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 582/1000 [================>.............] - ETA: 17:27 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 583/1000 [================>.............] - ETA: 17:24 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 584/1000 [================>.............] - ETA: 17:22 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 585/1000 [================>.............] - ETA: 17:19 - loss: 0.3378 - regression_loss: 0.3106 - classification_loss: 0.0272
 586/1000 [================>.............] - ETA: 17:16 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 587/1000 [================>.............] - ETA: 17:14 - loss: 0.3381 - regression_loss: 0.3109 - classification_loss: 0.0272
 588/1000 [================>.............] - ETA: 17:11 - loss: 0.3377 - regression_loss: 0.3105 - classification_loss: 0.0272
 589/1000 [================>.............] - ETA: 17:08 - loss: 0.3378 - regression_loss: 0.3106 - classification_loss: 0.0272
 590/1000 [================>.............] - ETA: 17:06 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 591/1000 [================>.............] - ETA: 17:03 - loss: 0.3380 - regression_loss: 0.3109 - classification_loss: 0.0272
 592/1000 [================>.............] - ETA: 17:01 - loss: 0.3382 - regression_loss: 0.3110 - classification_loss: 0.0272
 593/1000 [================>.............] - ETA: 16:57 - loss: 0.3378 - regression_loss: 0.3107 - classification_loss: 0.0271
 594/1000 [================>.............] - ETA: 16:54 - loss: 0.3376 - regression_loss: 0.3105 - classification_loss: 0.0271
 595/1000 [================>.............] - ETA: 16:52 - loss: 0.3375 - regression_loss: 0.3104 - classification_loss: 0.0271
 596/1000 [================>.............] - ETA: 16:49 - loss: 0.3377 - regression_loss: 0.3105 - classification_loss: 0.0271
 597/1000 [================>.............] - ETA: 16:47 - loss: 0.3379 - regression_loss: 0.3107 - classification_loss: 0.0272
 598/1000 [================>.............] - ETA: 16:44 - loss: 0.3378 - regression_loss: 0.3106 - classification_loss: 0.0272
 599/1000 [================>.............] - ETA: 16:42 - loss: 0.3380 - regression_loss: 0.3108 - classification_loss: 0.0272
 600/1000 [=================>............] - ETA: 16:39 - loss: 0.3375 - regression_loss: 0.3104 - classification_loss: 0.0271
 601/1000 [=================>............] - ETA: 16:36 - loss: 0.3373 - regression_loss: 0.3102 - classification_loss: 0.0271
 602/1000 [=================>............] - ETA: 16:33 - loss: 0.3375 - regression_loss: 0.3104 - classification_loss: 0.0271
 603/1000 [=================>............] - ETA: 16:30 - loss: 0.3370 - regression_loss: 0.3100 - classification_loss: 0.0271
 604/1000 [=================>............] - ETA: 16:27 - loss: 0.3371 - regression_loss: 0.3101 - classification_loss: 0.0271
 605/1000 [=================>............] - ETA: 16:24 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 606/1000 [=================>............] - ETA: 16:22 - loss: 0.3370 - regression_loss: 0.3099 - classification_loss: 0.0271
 607/1000 [=================>............] - ETA: 16:19 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 608/1000 [=================>............] - ETA: 16:17 - loss: 0.3371 - regression_loss: 0.3100 - classification_loss: 0.0271
 609/1000 [=================>............] - ETA: 16:15 - loss: 0.3372 - regression_loss: 0.3102 - classification_loss: 0.0271
 610/1000 [=================>............] - ETA: 16:12 - loss: 0.3373 - regression_loss: 0.3102 - classification_loss: 0.0271
 611/1000 [=================>............] - ETA: 16:09 - loss: 0.3374 - regression_loss: 0.3103 - classification_loss: 0.0271
 612/1000 [=================>............] - ETA: 16:06 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 613/1000 [=================>............] - ETA: 16:04 - loss: 0.3371 - regression_loss: 0.3101 - classification_loss: 0.0271
 614/1000 [=================>............] - ETA: 16:01 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 615/1000 [=================>............] - ETA: 15:59 - loss: 0.3371 - regression_loss: 0.3100 - classification_loss: 0.0271
 616/1000 [=================>............] - ETA: 15:56 - loss: 0.3370 - regression_loss: 0.3099 - classification_loss: 0.0271
 617/1000 [=================>............] - ETA: 15:54 - loss: 0.3371 - regression_loss: 0.3100 - classification_loss: 0.0271
 618/1000 [=================>............] - ETA: 15:51 - loss: 0.3373 - regression_loss: 0.3102 - classification_loss: 0.0271
 619/1000 [=================>............] - ETA: 15:49 - loss: 0.3373 - regression_loss: 0.3102 - classification_loss: 0.0271
 620/1000 [=================>............] - ETA: 15:46 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 621/1000 [=================>............] - ETA: 15:43 - loss: 0.3371 - regression_loss: 0.3100 - classification_loss: 0.0271
 622/1000 [=================>............] - ETA: 15:41 - loss: 0.3371 - regression_loss: 0.3101 - classification_loss: 0.0271
 623/1000 [=================>............] - ETA: 15:38 - loss: 0.3369 - regression_loss: 0.3099 - classification_loss: 0.0271
 624/1000 [=================>............] - ETA: 15:35 - loss: 0.3372 - regression_loss: 0.3101 - classification_loss: 0.0271
 625/1000 [=================>............] - ETA: 15:32 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 626/1000 [=================>............] - ETA: 15:29 - loss: 0.3365 - regression_loss: 0.3095 - classification_loss: 0.0271
 627/1000 [=================>............] - ETA: 15:27 - loss: 0.3366 - regression_loss: 0.3096 - classification_loss: 0.0271
 628/1000 [=================>............] - ETA: 15:24 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 629/1000 [=================>............] - ETA: 15:22 - loss: 0.3370 - regression_loss: 0.3099 - classification_loss: 0.0271
 630/1000 [=================>............] - ETA: 15:19 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 631/1000 [=================>............] - ETA: 15:16 - loss: 0.3364 - regression_loss: 0.3094 - classification_loss: 0.0271
 632/1000 [=================>............] - ETA: 15:13 - loss: 0.3363 - regression_loss: 0.3092 - classification_loss: 0.0271
 633/1000 [=================>............] - ETA: 15:11 - loss: 0.3365 - regression_loss: 0.3094 - classification_loss: 0.0271
 634/1000 [==================>...........] - ETA: 15:08 - loss: 0.3366 - regression_loss: 0.3096 - classification_loss: 0.0271
 635/1000 [==================>...........] - ETA: 15:06 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 636/1000 [==================>...........] - ETA: 15:04 - loss: 0.3370 - regression_loss: 0.3099 - classification_loss: 0.0271
 637/1000 [==================>...........] - ETA: 15:01 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 638/1000 [==================>...........] - ETA: 14:59 - loss: 0.3369 - regression_loss: 0.3098 - classification_loss: 0.0271
 639/1000 [==================>...........] - ETA: 14:56 - loss: 0.3364 - regression_loss: 0.3094 - classification_loss: 0.0271
 640/1000 [==================>...........] - ETA: 14:53 - loss: 0.3366 - regression_loss: 0.3095 - classification_loss: 0.0271
 641/1000 [==================>...........] - ETA: 14:50 - loss: 0.3366 - regression_loss: 0.3095 - classification_loss: 0.0271
 642/1000 [==================>...........] - ETA: 14:48 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 643/1000 [==================>...........] - ETA: 14:45 - loss: 0.3365 - regression_loss: 0.3095 - classification_loss: 0.0271
 644/1000 [==================>...........] - ETA: 14:43 - loss: 0.3367 - regression_loss: 0.3097 - classification_loss: 0.0271
 645/1000 [==================>...........] - ETA: 14:40 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 646/1000 [==================>...........] - ETA: 14:38 - loss: 0.3370 - regression_loss: 0.3099 - classification_loss: 0.0271
 647/1000 [==================>...........] - ETA: 14:35 - loss: 0.3369 - regression_loss: 0.3099 - classification_loss: 0.0271
 648/1000 [==================>...........] - ETA: 14:33 - loss: 0.3372 - regression_loss: 0.3101 - classification_loss: 0.0271
 649/1000 [==================>...........] - ETA: 14:30 - loss: 0.3367 - regression_loss: 0.3096 - classification_loss: 0.0271
 650/1000 [==================>...........] - ETA: 14:27 - loss: 0.3365 - regression_loss: 0.3095 - classification_loss: 0.0270
 651/1000 [==================>...........] - ETA: 14:25 - loss: 0.3367 - regression_loss: 0.3096 - classification_loss: 0.0271
 652/1000 [==================>...........] - ETA: 14:22 - loss: 0.3368 - regression_loss: 0.3097 - classification_loss: 0.0271
 653/1000 [==================>...........] - ETA: 14:19 - loss: 0.3363 - regression_loss: 0.3093 - classification_loss: 0.0270
 654/1000 [==================>...........] - ETA: 14:17 - loss: 0.3364 - regression_loss: 0.3094 - classification_loss: 0.0270
 655/1000 [==================>...........] - ETA: 14:14 - loss: 0.3361 - regression_loss: 0.3091 - classification_loss: 0.0270
 656/1000 [==================>...........] - ETA: 14:11 - loss: 0.3360 - regression_loss: 0.3090 - classification_loss: 0.0270
 657/1000 [==================>...........] - ETA: 14:09 - loss: 0.3362 - regression_loss: 0.3092 - classification_loss: 0.0270
 658/1000 [==================>...........] - ETA: 14:06 - loss: 0.3363 - regression_loss: 0.3093 - classification_loss: 0.0270
 659/1000 [==================>...........] - ETA: 14:04 - loss: 0.3365 - regression_loss: 0.3094 - classification_loss: 0.0271
 660/1000 [==================>...........] - ETA: 14:01 - loss: 0.3365 - regression_loss: 0.3094 - classification_loss: 0.0270
 661/1000 [==================>...........] - ETA: 13:59 - loss: 0.3364 - regression_loss: 0.3093 - classification_loss: 0.0271
 662/1000 [==================>...........] - ETA: 13:56 - loss: 0.3359 - regression_loss: 0.3089 - classification_loss: 0.0270
 663/1000 [==================>...........] - ETA: 13:54 - loss: 0.3361 - regression_loss: 0.3091 - classification_loss: 0.0270
 664/1000 [==================>...........] - ETA: 13:51 - loss: 0.3362 - regression_loss: 0.3092 - classification_loss: 0.0270
 665/1000 [==================>...........] - ETA: 13:48 - loss: 0.3360 - regression_loss: 0.3090 - classification_loss: 0.0270
 666/1000 [==================>...........] - ETA: 13:46 - loss: 0.3360 - regression_loss: 0.3089 - classification_loss: 0.0270
 667/1000 [===================>..........] - ETA: 13:43 - loss: 0.3360 - regression_loss: 0.3089 - classification_loss: 0.0270
 668/1000 [===================>..........] - ETA: 13:41 - loss: 0.3361 - regression_loss: 0.3091 - classification_loss: 0.0270
 669/1000 [===================>..........] - ETA: 13:38 - loss: 0.3358 - regression_loss: 0.3088 - classification_loss: 0.0270
 670/1000 [===================>..........] - ETA: 13:36 - loss: 0.3360 - regression_loss: 0.3090 - classification_loss: 0.0270
 671/1000 [===================>..........] - ETA: 13:33 - loss: 0.3356 - regression_loss: 0.3086 - classification_loss: 0.0270
 672/1000 [===================>..........] - ETA: 13:30 - loss: 0.3356 - regression_loss: 0.3086 - classification_loss: 0.0270
 673/1000 [===================>..........] - ETA: 13:28 - loss: 0.3357 - regression_loss: 0.3087 - classification_loss: 0.0270
 674/1000 [===================>..........] - ETA: 13:25 - loss: 0.3355 - regression_loss: 0.3085 - classification_loss: 0.0270
 675/1000 [===================>..........] - ETA: 13:22 - loss: 0.3355 - regression_loss: 0.3085 - classification_loss: 0.0270
 676/1000 [===================>..........] - ETA: 13:20 - loss: 0.3354 - regression_loss: 0.3084 - classification_loss: 0.0270
 677/1000 [===================>..........] - ETA: 13:17 - loss: 0.3355 - regression_loss: 0.3086 - classification_loss: 0.0270
 678/1000 [===================>..........] - ETA: 13:15 - loss: 0.3357 - regression_loss: 0.3087 - classification_loss: 0.0270
 679/1000 [===================>..........] - ETA: 13:12 - loss: 0.3352 - regression_loss: 0.3083 - classification_loss: 0.0270
 680/1000 [===================>..........] - ETA: 13:10 - loss: 0.3354 - regression_loss: 0.3084 - classification_loss: 0.0270
 681/1000 [===================>..........] - ETA: 13:07 - loss: 0.3350 - regression_loss: 0.3081 - classification_loss: 0.0269
 682/1000 [===================>..........] - ETA: 13:04 - loss: 0.3349 - regression_loss: 0.3080 - classification_loss: 0.0269
 683/1000 [===================>..........] - ETA: 13:02 - loss: 0.3347 - regression_loss: 0.3077 - classification_loss: 0.0269
 684/1000 [===================>..........] - ETA: 12:59 - loss: 0.3349 - regression_loss: 0.3079 - classification_loss: 0.0269
 685/1000 [===================>..........] - ETA: 12:57 - loss: 0.3349 - regression_loss: 0.3079 - classification_loss: 0.0269
 686/1000 [===================>..........] - ETA: 12:54 - loss: 0.3350 - regression_loss: 0.3081 - classification_loss: 0.0269
 687/1000 [===================>..........] - ETA: 12:51 - loss: 0.3348 - regression_loss: 0.3078 - classification_loss: 0.0269
 688/1000 [===================>..........] - ETA: 12:49 - loss: 0.3350 - regression_loss: 0.3080 - classification_loss: 0.0269
 689/1000 [===================>..........] - ETA: 12:46 - loss: 0.3351 - regression_loss: 0.3082 - classification_loss: 0.0269
 690/1000 [===================>..........] - ETA: 12:44 - loss: 0.3347 - regression_loss: 0.3078 - classification_loss: 0.0269
 691/1000 [===================>..........] - ETA: 12:41 - loss: 0.3348 - regression_loss: 0.3079 - classification_loss: 0.0269
 692/1000 [===================>..........] - ETA: 12:39 - loss: 0.3347 - regression_loss: 0.3078 - classification_loss: 0.0269
 693/1000 [===================>..........] - ETA: 12:36 - loss: 0.3348 - regression_loss: 0.3079 - classification_loss: 0.0269
 694/1000 [===================>..........] - ETA: 12:34 - loss: 0.3347 - regression_loss: 0.3078 - classification_loss: 0.0269
 695/1000 [===================>..........] - ETA: 12:31 - loss: 0.3347 - regression_loss: 0.3078 - classification_loss: 0.0269
 696/1000 [===================>..........] - ETA: 12:29 - loss: 0.3349 - regression_loss: 0.3080 - classification_loss: 0.0269
 697/1000 [===================>..........] - ETA: 12:27 - loss: 0.3350 - regression_loss: 0.3081 - classification_loss: 0.0269
 698/1000 [===================>..........] - ETA: 12:24 - loss: 0.3346 - regression_loss: 0.3077 - classification_loss: 0.0269
 699/1000 [===================>..........] - ETA: 12:21 - loss: 0.3347 - regression_loss: 0.3078 - classification_loss: 0.0269
 700/1000 [====================>.........] - ETA: 12:18 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0269
 701/1000 [====================>.........] - ETA: 12:16 - loss: 0.3345 - regression_loss: 0.3076 - classification_loss: 0.0269
 702/1000 [====================>.........] - ETA: 12:14 - loss: 0.3346 - regression_loss: 0.3077 - classification_loss: 0.0269
 703/1000 [====================>.........] - ETA: 12:11 - loss: 0.3343 - regression_loss: 0.3075 - classification_loss: 0.0269
 704/1000 [====================>.........] - ETA: 12:08 - loss: 0.3343 - regression_loss: 0.3075 - classification_loss: 0.0269
 705/1000 [====================>.........] - ETA: 12:06 - loss: 0.3345 - regression_loss: 0.3076 - classification_loss: 0.0269
 706/1000 [====================>.........] - ETA: 12:03 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0269
 707/1000 [====================>.........] - ETA: 12:01 - loss: 0.3341 - regression_loss: 0.3073 - classification_loss: 0.0268
 708/1000 [====================>.........] - ETA: 11:58 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 709/1000 [====================>.........] - ETA: 11:56 - loss: 0.3344 - regression_loss: 0.3075 - classification_loss: 0.0269
 710/1000 [====================>.........] - ETA: 11:53 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0269
 711/1000 [====================>.........] - ETA: 11:51 - loss: 0.3343 - regression_loss: 0.3075 - classification_loss: 0.0268
 712/1000 [====================>.........] - ETA: 11:48 - loss: 0.3339 - regression_loss: 0.3071 - classification_loss: 0.0268
 713/1000 [====================>.........] - ETA: 11:45 - loss: 0.3340 - regression_loss: 0.3072 - classification_loss: 0.0268
 714/1000 [====================>.........] - ETA: 11:43 - loss: 0.3340 - regression_loss: 0.3072 - classification_loss: 0.0268
 715/1000 [====================>.........] - ETA: 11:40 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 716/1000 [====================>.........] - ETA: 11:38 - loss: 0.3341 - regression_loss: 0.3073 - classification_loss: 0.0268
 717/1000 [====================>.........] - ETA: 11:36 - loss: 0.3343 - regression_loss: 0.3074 - classification_loss: 0.0268
 718/1000 [====================>.........] - ETA: 11:33 - loss: 0.3344 - regression_loss: 0.3076 - classification_loss: 0.0268
 719/1000 [====================>.........] - ETA: 11:30 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 720/1000 [====================>.........] - ETA: 11:28 - loss: 0.3344 - regression_loss: 0.3076 - classification_loss: 0.0268
 721/1000 [====================>.........] - ETA: 11:25 - loss: 0.3341 - regression_loss: 0.3073 - classification_loss: 0.0268
 722/1000 [====================>.........] - ETA: 11:23 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 723/1000 [====================>.........] - ETA: 11:20 - loss: 0.3340 - regression_loss: 0.3072 - classification_loss: 0.0268
 724/1000 [====================>.........] - ETA: 11:18 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 725/1000 [====================>.........] - ETA: 11:15 - loss: 0.3341 - regression_loss: 0.3073 - classification_loss: 0.0268
 726/1000 [====================>.........] - ETA: 11:13 - loss: 0.3342 - regression_loss: 0.3074 - classification_loss: 0.0268
 727/1000 [====================>.........] - ETA: 11:10 - loss: 0.3338 - regression_loss: 0.3071 - classification_loss: 0.0268
 728/1000 [====================>.........] - ETA: 11:07 - loss: 0.3340 - regression_loss: 0.3072 - classification_loss: 0.0268
 729/1000 [====================>.........] - ETA: 11:05 - loss: 0.3341 - regression_loss: 0.3073 - classification_loss: 0.0268
 730/1000 [====================>.........] - ETA: 11:02 - loss: 0.3337 - regression_loss: 0.3069 - classification_loss: 0.0267
 731/1000 [====================>.........] - ETA: 11:00 - loss: 0.3338 - regression_loss: 0.3071 - classification_loss: 0.0267
 732/1000 [====================>.........] - ETA: 10:57 - loss: 0.3338 - regression_loss: 0.3071 - classification_loss: 0.0267
 733/1000 [====================>.........] - ETA: 10:54 - loss: 0.3336 - regression_loss: 0.3069 - classification_loss: 0.0267
 734/1000 [=====================>........] - ETA: 10:52 - loss: 0.3339 - regression_loss: 0.3071 - classification_loss: 0.0267
 735/1000 [=====================>........] - ETA: 10:50 - loss: 0.3338 - regression_loss: 0.3071 - classification_loss: 0.0267
 736/1000 [=====================>........] - ETA: 10:47 - loss: 0.3336 - regression_loss: 0.3069 - classification_loss: 0.0267
 737/1000 [=====================>........] - ETA: 10:45 - loss: 0.3337 - regression_loss: 0.3069 - classification_loss: 0.0267
 738/1000 [=====================>........] - ETA: 10:42 - loss: 0.3333 - regression_loss: 0.3066 - classification_loss: 0.0267
 739/1000 [=====================>........] - ETA: 10:39 - loss: 0.3336 - regression_loss: 0.3069 - classification_loss: 0.0267
 740/1000 [=====================>........] - ETA: 10:37 - loss: 0.3338 - regression_loss: 0.3070 - classification_loss: 0.0267
 741/1000 [=====================>........] - ETA: 10:34 - loss: 0.3339 - regression_loss: 0.3072 - classification_loss: 0.0267
 742/1000 [=====================>........] - ETA: 10:32 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0267
 743/1000 [=====================>........] - ETA: 10:29 - loss: 0.3340 - regression_loss: 0.3073 - classification_loss: 0.0267
 744/1000 [=====================>........] - ETA: 10:27 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 745/1000 [=====================>........] - ETA: 10:25 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 746/1000 [=====================>........] - ETA: 10:22 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0268
 747/1000 [=====================>........] - ETA: 10:20 - loss: 0.3346 - regression_loss: 0.3078 - classification_loss: 0.0268
 748/1000 [=====================>........] - ETA: 10:17 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 749/1000 [=====================>........] - ETA: 10:14 - loss: 0.3343 - regression_loss: 0.3075 - classification_loss: 0.0267
 750/1000 [=====================>........] - ETA: 10:12 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0267
 751/1000 [=====================>........] - ETA: 10:09 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 752/1000 [=====================>........] - ETA: 10:07 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 753/1000 [=====================>........] - ETA: 10:05 - loss: 0.3345 - regression_loss: 0.3077 - classification_loss: 0.0267
 754/1000 [=====================>........] - ETA: 10:02 - loss: 0.3345 - regression_loss: 0.3078 - classification_loss: 0.0267
 755/1000 [=====================>........] - ETA: 10:00 - loss: 0.3346 - regression_loss: 0.3079 - classification_loss: 0.0267
 756/1000 [=====================>........] - ETA: 9:57 - loss: 0.3343 - regression_loss: 0.3075 - classification_loss: 0.0267 
 757/1000 [=====================>........] - ETA: 9:54 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 758/1000 [=====================>........] - ETA: 9:52 - loss: 0.3339 - regression_loss: 0.3072 - classification_loss: 0.0267
 759/1000 [=====================>........] - ETA: 9:49 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0267
 760/1000 [=====================>........] - ETA: 9:47 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 761/1000 [=====================>........] - ETA: 9:44 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 762/1000 [=====================>........] - ETA: 9:42 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 763/1000 [=====================>........] - ETA: 9:39 - loss: 0.3344 - regression_loss: 0.3077 - classification_loss: 0.0267
 764/1000 [=====================>........] - ETA: 9:37 - loss: 0.3343 - regression_loss: 0.3077 - classification_loss: 0.0267
 765/1000 [=====================>........] - ETA: 9:34 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0267
 766/1000 [=====================>........] - ETA: 9:32 - loss: 0.3341 - regression_loss: 0.3075 - classification_loss: 0.0267
 767/1000 [======================>.......] - ETA: 9:29 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 768/1000 [======================>.......] - ETA: 9:27 - loss: 0.3344 - regression_loss: 0.3078 - classification_loss: 0.0267
 769/1000 [======================>.......] - ETA: 9:24 - loss: 0.3343 - regression_loss: 0.3077 - classification_loss: 0.0267
 770/1000 [======================>.......] - ETA: 9:22 - loss: 0.3344 - regression_loss: 0.3077 - classification_loss: 0.0267
 771/1000 [======================>.......] - ETA: 9:19 - loss: 0.3344 - regression_loss: 0.3077 - classification_loss: 0.0267
 772/1000 [======================>.......] - ETA: 9:17 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 773/1000 [======================>.......] - ETA: 9:14 - loss: 0.3340 - regression_loss: 0.3073 - classification_loss: 0.0266
 774/1000 [======================>.......] - ETA: 9:12 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0267
 775/1000 [======================>.......] - ETA: 9:09 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 776/1000 [======================>.......] - ETA: 9:07 - loss: 0.3344 - regression_loss: 0.3078 - classification_loss: 0.0267
 777/1000 [======================>.......] - ETA: 9:04 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0267
 778/1000 [======================>.......] - ETA: 9:02 - loss: 0.3341 - regression_loss: 0.3075 - classification_loss: 0.0266
 779/1000 [======================>.......] - ETA: 8:59 - loss: 0.3342 - regression_loss: 0.3075 - classification_loss: 0.0266
 780/1000 [======================>.......] - ETA: 8:57 - loss: 0.3343 - regression_loss: 0.3077 - classification_loss: 0.0267
 781/1000 [======================>.......] - ETA: 8:54 - loss: 0.3345 - regression_loss: 0.3078 - classification_loss: 0.0267
 782/1000 [======================>.......] - ETA: 8:52 - loss: 0.3342 - regression_loss: 0.3076 - classification_loss: 0.0266
 783/1000 [======================>.......] - ETA: 8:49 - loss: 0.3344 - regression_loss: 0.3077 - classification_loss: 0.0266
 784/1000 [======================>.......] - ETA: 8:47 - loss: 0.3343 - regression_loss: 0.3077 - classification_loss: 0.0267
 785/1000 [======================>.......] - ETA: 8:44 - loss: 0.3345 - regression_loss: 0.3078 - classification_loss: 0.0267
 786/1000 [======================>.......] - ETA: 8:42 - loss: 0.3346 - regression_loss: 0.3079 - classification_loss: 0.0267
 787/1000 [======================>.......] - ETA: 8:39 - loss: 0.3346 - regression_loss: 0.3079 - classification_loss: 0.0267
 788/1000 [======================>.......] - ETA: 8:37 - loss: 0.3347 - regression_loss: 0.3080 - classification_loss: 0.0267
 789/1000 [======================>.......] - ETA: 8:34 - loss: 0.3347 - regression_loss: 0.3080 - classification_loss: 0.0267
 790/1000 [======================>.......] - ETA: 8:32 - loss: 0.3346 - regression_loss: 0.3080 - classification_loss: 0.0267
 791/1000 [======================>.......] - ETA: 8:29 - loss: 0.3342 - regression_loss: 0.3076 - classification_loss: 0.0266
 792/1000 [======================>.......] - ETA: 8:27 - loss: 0.3345 - regression_loss: 0.3078 - classification_loss: 0.0266
 793/1000 [======================>.......] - ETA: 8:24 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0266
 794/1000 [======================>.......] - ETA: 8:22 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 795/1000 [======================>.......] - ETA: 8:19 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0266
 796/1000 [======================>.......] - ETA: 8:17 - loss: 0.3342 - regression_loss: 0.3076 - classification_loss: 0.0266
 797/1000 [======================>.......] - ETA: 8:14 - loss: 0.3343 - regression_loss: 0.3076 - classification_loss: 0.0266
 798/1000 [======================>.......] - ETA: 8:12 - loss: 0.3341 - regression_loss: 0.3074 - classification_loss: 0.0266
 799/1000 [======================>.......] - ETA: 8:09 - loss: 0.3342 - regression_loss: 0.3076 - classification_loss: 0.0266
 800/1000 [=======================>......] - ETA: 8:07 - loss: 0.3343 - regression_loss: 0.3077 - classification_loss: 0.0266
 801/1000 [=======================>......] - ETA: 8:04 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 802/1000 [=======================>......] - ETA: 8:02 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 803/1000 [=======================>......] - ETA: 7:59 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 804/1000 [=======================>......] - ETA: 7:57 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 805/1000 [=======================>......] - ETA: 7:55 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 806/1000 [=======================>......] - ETA: 7:52 - loss: 0.3341 - regression_loss: 0.3075 - classification_loss: 0.0266
 807/1000 [=======================>......] - ETA: 7:50 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 808/1000 [=======================>......] - ETA: 7:47 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 809/1000 [=======================>......] - ETA: 7:45 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 810/1000 [=======================>......] - ETA: 7:42 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 811/1000 [=======================>......] - ETA: 7:40 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 812/1000 [=======================>......] - ETA: 7:37 - loss: 0.3336 - regression_loss: 0.3070 - classification_loss: 0.0266
 813/1000 [=======================>......] - ETA: 7:35 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0266
 814/1000 [=======================>......] - ETA: 7:32 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 815/1000 [=======================>......] - ETA: 7:30 - loss: 0.3332 - regression_loss: 0.3066 - classification_loss: 0.0266
 816/1000 [=======================>......] - ETA: 7:27 - loss: 0.3333 - regression_loss: 0.3068 - classification_loss: 0.0266
 817/1000 [=======================>......] - ETA: 7:25 - loss: 0.3333 - regression_loss: 0.3067 - classification_loss: 0.0266
 818/1000 [=======================>......] - ETA: 7:22 - loss: 0.3334 - regression_loss: 0.3068 - classification_loss: 0.0266
 819/1000 [=======================>......] - ETA: 7:20 - loss: 0.3335 - regression_loss: 0.3069 - classification_loss: 0.0266
 820/1000 [=======================>......] - ETA: 7:17 - loss: 0.3335 - regression_loss: 0.3069 - classification_loss: 0.0266
 821/1000 [=======================>......] - ETA: 7:15 - loss: 0.3332 - regression_loss: 0.3066 - classification_loss: 0.0265
 822/1000 [=======================>......] - ETA: 7:12 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0266
 823/1000 [=======================>......] - ETA: 7:10 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0266
 824/1000 [=======================>......] - ETA: 7:08 - loss: 0.3338 - regression_loss: 0.3072 - classification_loss: 0.0266
 825/1000 [=======================>......] - ETA: 7:05 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 826/1000 [=======================>......] - ETA: 7:03 - loss: 0.3337 - regression_loss: 0.3071 - classification_loss: 0.0266
 827/1000 [=======================>......] - ETA: 7:00 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 828/1000 [=======================>......] - ETA: 6:58 - loss: 0.3340 - regression_loss: 0.3074 - classification_loss: 0.0266
 829/1000 [=======================>......] - ETA: 6:55 - loss: 0.3338 - regression_loss: 0.3072 - classification_loss: 0.0266
 830/1000 [=======================>......] - ETA: 6:53 - loss: 0.3338 - regression_loss: 0.3072 - classification_loss: 0.0266
 831/1000 [=======================>......] - ETA: 6:50 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 832/1000 [=======================>......] - ETA: 6:48 - loss: 0.3339 - regression_loss: 0.3073 - classification_loss: 0.0266
 833/1000 [=======================>......] - ETA: 6:45 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0265
 834/1000 [========================>.....] - ETA: 6:43 - loss: 0.3337 - regression_loss: 0.3071 - classification_loss: 0.0266
 835/1000 [========================>.....] - ETA: 6:40 - loss: 0.3333 - regression_loss: 0.3068 - classification_loss: 0.0265
 836/1000 [========================>.....] - ETA: 6:38 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 837/1000 [========================>.....] - ETA: 6:35 - loss: 0.3333 - regression_loss: 0.3067 - classification_loss: 0.0265
 838/1000 [========================>.....] - ETA: 6:33 - loss: 0.3336 - regression_loss: 0.3070 - classification_loss: 0.0265
 839/1000 [========================>.....] - ETA: 6:31 - loss: 0.3337 - regression_loss: 0.3072 - classification_loss: 0.0266
 840/1000 [========================>.....] - ETA: 6:28 - loss: 0.3337 - regression_loss: 0.3072 - classification_loss: 0.0266
 841/1000 [========================>.....] - ETA: 6:26 - loss: 0.3336 - regression_loss: 0.3070 - classification_loss: 0.0265
 842/1000 [========================>.....] - ETA: 6:23 - loss: 0.3337 - regression_loss: 0.3071 - classification_loss: 0.0266
 843/1000 [========================>.....] - ETA: 6:21 - loss: 0.3338 - regression_loss: 0.3072 - classification_loss: 0.0266
 844/1000 [========================>.....] - ETA: 6:18 - loss: 0.3335 - regression_loss: 0.3069 - classification_loss: 0.0265
 845/1000 [========================>.....] - ETA: 6:16 - loss: 0.3336 - regression_loss: 0.3071 - classification_loss: 0.0265
 846/1000 [========================>.....] - ETA: 6:13 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0265
 847/1000 [========================>.....] - ETA: 6:11 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0265
 848/1000 [========================>.....] - ETA: 6:08 - loss: 0.3336 - regression_loss: 0.3071 - classification_loss: 0.0265
 849/1000 [========================>.....] - ETA: 6:06 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0265
 850/1000 [========================>.....] - ETA: 6:04 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0265
 851/1000 [========================>.....] - ETA: 6:01 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 852/1000 [========================>.....] - ETA: 5:59 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 853/1000 [========================>.....] - ETA: 5:56 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0265
 854/1000 [========================>.....] - ETA: 5:54 - loss: 0.3335 - regression_loss: 0.3070 - classification_loss: 0.0265
 855/1000 [========================>.....] - ETA: 5:51 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0265
 856/1000 [========================>.....] - ETA: 5:49 - loss: 0.3336 - regression_loss: 0.3070 - classification_loss: 0.0265
 857/1000 [========================>.....] - ETA: 5:46 - loss: 0.3334 - regression_loss: 0.3069 - classification_loss: 0.0265
 858/1000 [========================>.....] - ETA: 5:44 - loss: 0.3336 - regression_loss: 0.3071 - classification_loss: 0.0265
 859/1000 [========================>.....] - ETA: 5:41 - loss: 0.3333 - regression_loss: 0.3068 - classification_loss: 0.0265
 860/1000 [========================>.....] - ETA: 5:39 - loss: 0.3333 - regression_loss: 0.3067 - classification_loss: 0.0265
 861/1000 [========================>.....] - ETA: 5:37 - loss: 0.3333 - regression_loss: 0.3068 - classification_loss: 0.0265
 862/1000 [========================>.....] - ETA: 5:34 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 863/1000 [========================>.....] - ETA: 5:32 - loss: 0.3333 - regression_loss: 0.3068 - classification_loss: 0.0265
 864/1000 [========================>.....] - ETA: 5:29 - loss: 0.3329 - regression_loss: 0.3065 - classification_loss: 0.0265
 865/1000 [========================>.....] - ETA: 5:27 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 866/1000 [========================>.....] - ETA: 5:24 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 867/1000 [=========================>....] - ETA: 5:22 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 868/1000 [=========================>....] - ETA: 5:19 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 869/1000 [=========================>....] - ETA: 5:17 - loss: 0.3330 - regression_loss: 0.3066 - classification_loss: 0.0265
 870/1000 [=========================>....] - ETA: 5:14 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 871/1000 [=========================>....] - ETA: 5:12 - loss: 0.3327 - regression_loss: 0.3062 - classification_loss: 0.0265
 872/1000 [=========================>....] - ETA: 5:09 - loss: 0.3328 - regression_loss: 0.3063 - classification_loss: 0.0265
 873/1000 [=========================>....] - ETA: 5:07 - loss: 0.3329 - regression_loss: 0.3065 - classification_loss: 0.0265
 874/1000 [=========================>....] - ETA: 5:05 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 875/1000 [=========================>....] - ETA: 5:02 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 876/1000 [=========================>....] - ETA: 5:00 - loss: 0.3331 - regression_loss: 0.3067 - classification_loss: 0.0265
 877/1000 [=========================>....] - ETA: 4:57 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 878/1000 [=========================>....] - ETA: 4:55 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 879/1000 [=========================>....] - ETA: 4:52 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 880/1000 [=========================>....] - ETA: 4:50 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 881/1000 [=========================>....] - ETA: 4:48 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 882/1000 [=========================>....] - ETA: 4:45 - loss: 0.3329 - regression_loss: 0.3065 - classification_loss: 0.0265
 883/1000 [=========================>....] - ETA: 4:43 - loss: 0.3331 - regression_loss: 0.3066 - classification_loss: 0.0265
 884/1000 [=========================>....] - ETA: 4:40 - loss: 0.3331 - regression_loss: 0.3067 - classification_loss: 0.0265
 885/1000 [=========================>....] - ETA: 4:38 - loss: 0.3332 - regression_loss: 0.3067 - classification_loss: 0.0265
 886/1000 [=========================>....] - ETA: 4:35 - loss: 0.3330 - regression_loss: 0.3065 - classification_loss: 0.0265
 887/1000 [=========================>....] - ETA: 4:33 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 888/1000 [=========================>....] - ETA: 4:30 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 889/1000 [=========================>....] - ETA: 4:28 - loss: 0.3328 - regression_loss: 0.3063 - classification_loss: 0.0264
 890/1000 [=========================>....] - ETA: 4:26 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 891/1000 [=========================>....] - ETA: 4:23 - loss: 0.3326 - regression_loss: 0.3061 - classification_loss: 0.0264
 892/1000 [=========================>....] - ETA: 4:21 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 893/1000 [=========================>....] - ETA: 4:18 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 894/1000 [=========================>....] - ETA: 4:16 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 895/1000 [=========================>....] - ETA: 4:13 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 896/1000 [=========================>....] - ETA: 4:11 - loss: 0.3327 - regression_loss: 0.3062 - classification_loss: 0.0264
 897/1000 [=========================>....] - ETA: 4:08 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 898/1000 [=========================>....] - ETA: 4:06 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0264
 899/1000 [=========================>....] - ETA: 4:04 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 900/1000 [==========================>...] - ETA: 4:01 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 901/1000 [==========================>...] - ETA: 3:59 - loss: 0.3327 - regression_loss: 0.3063 - classification_loss: 0.0264
 902/1000 [==========================>...] - ETA: 3:56 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 903/1000 [==========================>...] - ETA: 3:54 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 904/1000 [==========================>...] - ETA: 3:52 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 905/1000 [==========================>...] - ETA: 3:49 - loss: 0.3329 - regression_loss: 0.3065 - classification_loss: 0.0264
 906/1000 [==========================>...] - ETA: 3:47 - loss: 0.3329 - regression_loss: 0.3065 - classification_loss: 0.0264
 907/1000 [==========================>...] - ETA: 3:44 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 908/1000 [==========================>...] - ETA: 3:42 - loss: 0.3327 - regression_loss: 0.3063 - classification_loss: 0.0264
 909/1000 [==========================>...] - ETA: 3:39 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 910/1000 [==========================>...] - ETA: 3:37 - loss: 0.3327 - regression_loss: 0.3063 - classification_loss: 0.0264
 911/1000 [==========================>...] - ETA: 3:35 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 912/1000 [==========================>...] - ETA: 3:32 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 913/1000 [==========================>...] - ETA: 3:30 - loss: 0.3328 - regression_loss: 0.3064 - classification_loss: 0.0264
 914/1000 [==========================>...] - ETA: 3:27 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 915/1000 [==========================>...] - ETA: 3:25 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 916/1000 [==========================>...] - ETA: 3:22 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 917/1000 [==========================>...] - ETA: 3:20 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 918/1000 [==========================>...] - ETA: 3:17 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 919/1000 [==========================>...] - ETA: 3:15 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 920/1000 [==========================>...] - ETA: 3:13 - loss: 0.3323 - regression_loss: 0.3059 - classification_loss: 0.0264
 921/1000 [==========================>...] - ETA: 3:10 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0264
 922/1000 [==========================>...] - ETA: 3:08 - loss: 0.3322 - regression_loss: 0.3059 - classification_loss: 0.0264
 923/1000 [==========================>...] - ETA: 3:05 - loss: 0.3323 - regression_loss: 0.3059 - classification_loss: 0.0264
 924/1000 [==========================>...] - ETA: 3:03 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 925/1000 [==========================>...] - ETA: 3:00 - loss: 0.3321 - regression_loss: 0.3058 - classification_loss: 0.0263
 926/1000 [==========================>...] - ETA: 2:58 - loss: 0.3320 - regression_loss: 0.3056 - classification_loss: 0.0263
 927/1000 [==========================>...] - ETA: 2:56 - loss: 0.3319 - regression_loss: 0.3056 - classification_loss: 0.0263
 928/1000 [==========================>...] - ETA: 2:53 - loss: 0.3320 - regression_loss: 0.3057 - classification_loss: 0.0263
 929/1000 [==========================>...] - ETA: 2:51 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0263
 930/1000 [==========================>...] - ETA: 2:48 - loss: 0.3323 - regression_loss: 0.3059 - classification_loss: 0.0264
 931/1000 [==========================>...] - ETA: 2:46 - loss: 0.3323 - regression_loss: 0.3060 - classification_loss: 0.0264
 932/1000 [==========================>...] - ETA: 2:44 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 933/1000 [==========================>...] - ETA: 2:41 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 934/1000 [===========================>..] - ETA: 2:39 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 935/1000 [===========================>..] - ETA: 2:36 - loss: 0.3325 - regression_loss: 0.3061 - classification_loss: 0.0264
 936/1000 [===========================>..] - ETA: 2:34 - loss: 0.3326 - regression_loss: 0.3062 - classification_loss: 0.0264
 937/1000 [===========================>..] - ETA: 2:31 - loss: 0.3323 - regression_loss: 0.3059 - classification_loss: 0.0264
 938/1000 [===========================>..] - ETA: 2:29 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0264
 939/1000 [===========================>..] - ETA: 2:27 - loss: 0.3323 - regression_loss: 0.3060 - classification_loss: 0.0264
 940/1000 [===========================>..] - ETA: 2:24 - loss: 0.3324 - regression_loss: 0.3060 - classification_loss: 0.0264
 941/1000 [===========================>..] - ETA: 2:22 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0264
 942/1000 [===========================>..] - ETA: 2:19 - loss: 0.3322 - regression_loss: 0.3058 - classification_loss: 0.0264
 943/1000 [===========================>..] - ETA: 2:17 - loss: 0.3319 - regression_loss: 0.3056 - classification_loss: 0.0263
 944/1000 [===========================>..] - ETA: 2:14 - loss: 0.3318 - regression_loss: 0.3055 - classification_loss: 0.0263
 945/1000 [===========================>..] - ETA: 2:12 - loss: 0.3319 - regression_loss: 0.3056 - classification_loss: 0.0264
 946/1000 [===========================>..] - ETA: 2:10 - loss: 0.3316 - regression_loss: 0.3053 - classification_loss: 0.0263
 947/1000 [===========================>..] - ETA: 2:07 - loss: 0.3316 - regression_loss: 0.3052 - classification_loss: 0.0263
 948/1000 [===========================>..] - ETA: 2:05 - loss: 0.3317 - regression_loss: 0.3053 - classification_loss: 0.0263
 949/1000 [===========================>..] - ETA: 2:02 - loss: 0.3317 - regression_loss: 0.3054 - classification_loss: 0.0263
 950/1000 [===========================>..] - ETA: 2:00 - loss: 0.3318 - regression_loss: 0.3054 - classification_loss: 0.0263
 951/1000 [===========================>..] - ETA: 1:58 - loss: 0.3317 - regression_loss: 0.3054 - classification_loss: 0.0263
 952/1000 [===========================>..] - ETA: 1:55 - loss: 0.3317 - regression_loss: 0.3054 - classification_loss: 0.0263
 953/1000 [===========================>..] - ETA: 1:53 - loss: 0.3318 - regression_loss: 0.3054 - classification_loss: 0.0263
 954/1000 [===========================>..] - ETA: 1:50 - loss: 0.3315 - regression_loss: 0.3051 - classification_loss: 0.0263
 955/1000 [===========================>..] - ETA: 1:48 - loss: 0.3316 - regression_loss: 0.3052 - classification_loss: 0.0263
 956/1000 [===========================>..] - ETA: 1:45 - loss: 0.3315 - regression_loss: 0.3052 - classification_loss: 0.0263
 957/1000 [===========================>..] - ETA: 1:43 - loss: 0.3316 - regression_loss: 0.3052 - classification_loss: 0.0263
 958/1000 [===========================>..] - ETA: 1:41 - loss: 0.3314 - regression_loss: 0.3051 - classification_loss: 0.0263
 959/1000 [===========================>..] - ETA: 1:38 - loss: 0.3314 - regression_loss: 0.3051 - classification_loss: 0.0263
 960/1000 [===========================>..] - ETA: 1:36 - loss: 0.3313 - regression_loss: 0.3050 - classification_loss: 0.0263
 961/1000 [===========================>..] - ETA: 1:33 - loss: 0.3314 - regression_loss: 0.3051 - classification_loss: 0.0263
 962/1000 [===========================>..] - ETA: 1:31 - loss: 0.3315 - regression_loss: 0.3052 - classification_loss: 0.0263
 963/1000 [===========================>..] - ETA: 1:29 - loss: 0.3313 - regression_loss: 0.3050 - classification_loss: 0.0263
 964/1000 [===========================>..] - ETA: 1:26 - loss: 0.3313 - regression_loss: 0.3050 - classification_loss: 0.0263
 965/1000 [===========================>..] - ETA: 1:24 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 966/1000 [===========================>..] - ETA: 1:21 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 967/1000 [============================>.] - ETA: 1:19 - loss: 0.3312 - regression_loss: 0.3049 - classification_loss: 0.0263
 968/1000 [============================>.] - ETA: 1:16 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 969/1000 [============================>.] - ETA: 1:14 - loss: 0.3310 - regression_loss: 0.3047 - classification_loss: 0.0263
 970/1000 [============================>.] - ETA: 1:12 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 971/1000 [============================>.] - ETA: 1:09 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 972/1000 [============================>.] - ETA: 1:07 - loss: 0.3312 - regression_loss: 0.3049 - classification_loss: 0.0263
 973/1000 [============================>.] - ETA: 1:04 - loss: 0.3309 - regression_loss: 0.3046 - classification_loss: 0.0263
 974/1000 [============================>.] - ETA: 1:02 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 975/1000 [============================>.] - ETA: 1:00 - loss: 0.3311 - regression_loss: 0.3048 - classification_loss: 0.0263
 976/1000 [============================>.] - ETA: 57s - loss: 0.3308 - regression_loss: 0.3046 - classification_loss: 0.0263 
 977/1000 [============================>.] - ETA: 55s - loss: 0.3308 - regression_loss: 0.3045 - classification_loss: 0.0263
 978/1000 [============================>.] - ETA: 52s - loss: 0.3309 - regression_loss: 0.3046 - classification_loss: 0.0263
 979/1000 [============================>.] - ETA: 50s - loss: 0.3310 - regression_loss: 0.3047 - classification_loss: 0.0263
 980/1000 [============================>.] - ETA: 48s - loss: 0.3308 - regression_loss: 0.3046 - classification_loss: 0.0262
 981/1000 [============================>.] - ETA: 45s - loss: 0.3309 - regression_loss: 0.3046 - classification_loss: 0.0262
 982/1000 [============================>.] - ETA: 43s - loss: 0.3310 - regression_loss: 0.3047 - classification_loss: 0.0263
 983/1000 [============================>.] - ETA: 40s - loss: 0.3310 - regression_loss: 0.3047 - classification_loss: 0.0263
 984/1000 [============================>.] - ETA: 38s - loss: 0.3308 - regression_loss: 0.3045 - classification_loss: 0.0262
 985/1000 [============================>.] - ETA: 36s - loss: 0.3307 - regression_loss: 0.3045 - classification_loss: 0.0262
 986/1000 [============================>.] - ETA: 33s - loss: 0.3305 - regression_loss: 0.3043 - classification_loss: 0.0262
 987/1000 [============================>.] - ETA: 31s - loss: 0.3306 - regression_loss: 0.3044 - classification_loss: 0.0262
 988/1000 [============================>.] - ETA: 28s - loss: 0.3304 - regression_loss: 0.3042 - classification_loss: 0.0262
 989/1000 [============================>.] - ETA: 26s - loss: 0.3304 - regression_loss: 0.3042 - classification_loss: 0.0262
 990/1000 [============================>.] - ETA: 24s - loss: 0.3305 - regression_loss: 0.3043 - classification_loss: 0.0262
 991/1000 [============================>.] - ETA: 21s - loss: 0.3302 - regression_loss: 0.3040 - classification_loss: 0.0262
 992/1000 [============================>.] - ETA: 19s - loss: 0.3303 - regression_loss: 0.3041 - classification_loss: 0.0262
 993/1000 [============================>.] - ETA: 16s - loss: 0.3304 - regression_loss: 0.3042 - classification_loss: 0.0262
 994/1000 [============================>.] - ETA: 14s - loss: 0.3303 - regression_loss: 0.3041 - classification_loss: 0.0262
 995/1000 [============================>.] - ETA: 12s - loss: 0.3303 - regression_loss: 0.3041 - classification_loss: 0.0262
 996/1000 [============================>.] - ETA: 9s - loss: 0.3304 - regression_loss: 0.3042 - classification_loss: 0.0262 
 997/1000 [============================>.] - ETA: 7s - loss: 0.3301 - regression_loss: 0.3039 - classification_loss: 0.0262
 998/1000 [============================>.] - ETA: 4s - loss: 0.3299 - regression_loss: 0.3038 - classification_loss: 0.0262
 999/1000 [============================>.] - ETA: 2s - loss: 0.3300 - regression_loss: 0.3038 - classification_loss: 0.0262
1000/1000 [==============================] - 2401s 2s/step - loss: 0.3301 - regression_loss: 0.3039 - classification_loss: 0.0262

Epoch 00008: saving model to ./snapshots/resnet50_csv_08.h5
Epoch 9/10

   1/1000 [..............................] - ETA: 27:46 - loss: 0.2535 - regression_loss: 0.2274 - classification_loss: 0.0261
   2/1000 [..............................] - ETA: 34:15 - loss: 0.2546 - regression_loss: 0.2289 - classification_loss: 0.0258
   3/1000 [..............................] - ETA: 36:53 - loss: 0.2913 - regression_loss: 0.2655 - classification_loss: 0.0258
   4/1000 [..............................] - ETA: 36:35 - loss: 0.2966 - regression_loss: 0.2712 - classification_loss: 0.0253
   5/1000 [..............................] - ETA: 34:27 - loss: 0.2451 - regression_loss: 0.2237 - classification_loss: 0.0214
   6/1000 [..............................] - ETA: 33:27 - loss: 0.2303 - regression_loss: 0.2100 - classification_loss: 0.0203
   7/1000 [..............................] - ETA: 34:47 - loss: 0.2787 - regression_loss: 0.2561 - classification_loss: 0.0226
   8/1000 [..............................] - ETA: 36:00 - loss: 0.3191 - regression_loss: 0.2945 - classification_loss: 0.0246
   9/1000 [..............................] - ETA: 36:36 - loss: 0.3372 - regression_loss: 0.3119 - classification_loss: 0.0253
  10/1000 [..............................] - ETA: 36:55 - loss: 0.3371 - regression_loss: 0.3113 - classification_loss: 0.0258
  11/1000 [..............................] - ETA: 35:52 - loss: 0.3102 - regression_loss: 0.2861 - classification_loss: 0.0241
  12/1000 [..............................] - ETA: 36:25 - loss: 0.3294 - regression_loss: 0.3042 - classification_loss: 0.0253
  13/1000 [..............................] - ETA: 37:02 - loss: 0.3475 - regression_loss: 0.3212 - classification_loss: 0.0263
  14/1000 [..............................] - ETA: 36:22 - loss: 0.3397 - regression_loss: 0.3140 - classification_loss: 0.0258
  15/1000 [..............................] - ETA: 36:16 - loss: 0.3471 - regression_loss: 0.3211 - classification_loss: 0.0260
  16/1000 [..............................] - ETA: 35:44 - loss: 0.3371 - regression_loss: 0.3117 - classification_loss: 0.0253
  17/1000 [..............................] - ETA: 35:57 - loss: 0.3346 - regression_loss: 0.3090 - classification_loss: 0.0256
  18/1000 [..............................] - ETA: 36:12 - loss: 0.3377 - regression_loss: 0.3117 - classification_loss: 0.0260
  19/1000 [..............................] - ETA: 35:37 - loss: 0.3228 - regression_loss: 0.2979 - classification_loss: 0.0249
  20/1000 [..............................] - ETA: 35:33 - loss: 0.3257 - regression_loss: 0.3006 - classification_loss: 0.0251
  21/1000 [..............................] - ETA: 35:51 - loss: 0.3325 - regression_loss: 0.3070 - classification_loss: 0.0256
  22/1000 [..............................] - ETA: 36:11 - loss: 0.3371 - regression_loss: 0.3112 - classification_loss: 0.0259
  23/1000 [..............................] - ETA: 36:06 - loss: 0.3368 - regression_loss: 0.3110 - classification_loss: 0.0258
  24/1000 [..............................] - ETA: 36:13 - loss: 0.3337 - regression_loss: 0.3077 - classification_loss: 0.0260
  25/1000 [..............................] - ETA: 35:45 - loss: 0.3229 - regression_loss: 0.2976 - classification_loss: 0.0252
  26/1000 [..............................] - ETA: 35:55 - loss: 0.3252 - regression_loss: 0.2997 - classification_loss: 0.0255
  27/1000 [..............................] - ETA: 36:07 - loss: 0.3291 - regression_loss: 0.3033 - classification_loss: 0.0258
  28/1000 [..............................] - ETA: 35:47 - loss: 0.3278 - regression_loss: 0.3023 - classification_loss: 0.0256
  29/1000 [..............................] - ETA: 36:02 - loss: 0.3325 - regression_loss: 0.3067 - classification_loss: 0.0258
  30/1000 [..............................] - ETA: 35:57 - loss: 0.3338 - regression_loss: 0.3081 - classification_loss: 0.0257
  31/1000 [..............................] - ETA: 36:11 - loss: 0.3367 - regression_loss: 0.3108 - classification_loss: 0.0260
  32/1000 [..............................] - ETA: 36:19 - loss: 0.3395 - regression_loss: 0.3133 - classification_loss: 0.0262
  33/1000 [..............................] - ETA: 36:25 - loss: 0.3406 - regression_loss: 0.3143 - classification_loss: 0.0262
  34/1000 [>.............................] - ETA: 36:28 - loss: 0.3379 - regression_loss: 0.3117 - classification_loss: 0.0262
  35/1000 [>.............................] - ETA: 36:06 - loss: 0.3295 - regression_loss: 0.3038 - classification_loss: 0.0257
  36/1000 [>.............................] - ETA: 35:50 - loss: 0.3245 - regression_loss: 0.2992 - classification_loss: 0.0253
  37/1000 [>.............................] - ETA: 35:46 - loss: 0.3252 - regression_loss: 0.2997 - classification_loss: 0.0255
  38/1000 [>.............................] - ETA: 35:51 - loss: 0.3267 - regression_loss: 0.3011 - classification_loss: 0.0256
  39/1000 [>.............................] - ETA: 36:01 - loss: 0.3289 - regression_loss: 0.3031 - classification_loss: 0.0257
  40/1000 [>.............................] - ETA: 36:06 - loss: 0.3312 - regression_loss: 0.3054 - classification_loss: 0.0258
  41/1000 [>.............................] - ETA: 36:08 - loss: 0.3303 - regression_loss: 0.3043 - classification_loss: 0.0259
  42/1000 [>.............................] - ETA: 35:50 - loss: 0.3248 - regression_loss: 0.2994 - classification_loss: 0.0255
  43/1000 [>.............................] - ETA: 35:37 - loss: 0.3227 - regression_loss: 0.2973 - classification_loss: 0.0254
  44/1000 [>.............................] - ETA: 35:45 - loss: 0.3259 - regression_loss: 0.3004 - classification_loss: 0.0255
  45/1000 [>.............................] - ETA: 35:50 - loss: 0.3294 - regression_loss: 0.3037 - classification_loss: 0.0257
  46/1000 [>.............................] - ETA: 35:46 - loss: 0.3294 - regression_loss: 0.3038 - classification_loss: 0.0256
  47/1000 [>.............................] - ETA: 35:33 - loss: 0.3260 - regression_loss: 0.3006 - classification_loss: 0.0254
  48/1000 [>.............................] - ETA: 35:34 - loss: 0.3265 - regression_loss: 0.3011 - classification_loss: 0.0254
  49/1000 [>.............................] - ETA: 35:38 - loss: 0.3286 - regression_loss: 0.3031 - classification_loss: 0.0255
  50/1000 [>.............................] - ETA: 35:23 - loss: 0.3246 - regression_loss: 0.2996 - classification_loss: 0.0251
  51/1000 [>.............................] - ETA: 35:08 - loss: 0.3189 - regression_loss: 0.2941 - classification_loss: 0.0247
  52/1000 [>.............................] - ETA: 35:15 - loss: 0.3215 - regression_loss: 0.2966 - classification_loss: 0.0249
  53/1000 [>.............................] - ETA: 35:12 - loss: 0.3228 - regression_loss: 0.2979 - classification_loss: 0.0249
  54/1000 [>.............................] - ETA: 35:16 - loss: 0.3256 - regression_loss: 0.3006 - classification_loss: 0.0250
  55/1000 [>.............................] - ETA: 35:06 - loss: 0.3240 - regression_loss: 0.2992 - classification_loss: 0.0249
  56/1000 [>.............................] - ETA: 35:08 - loss: 0.3249 - regression_loss: 0.3000 - classification_loss: 0.0249
  57/1000 [>.............................] - ETA: 35:10 - loss: 0.3238 - regression_loss: 0.2988 - classification_loss: 0.0250
  58/1000 [>.............................] - ETA: 34:57 - loss: 0.3186 - regression_loss: 0.2940 - classification_loss: 0.0246
  59/1000 [>.............................] - ETA: 34:54 - loss: 0.3192 - regression_loss: 0.2946 - classification_loss: 0.0246
  60/1000 [>.............................] - ETA: 34:57 - loss: 0.3206 - regression_loss: 0.2959 - classification_loss: 0.0247
  61/1000 [>.............................] - ETA: 34:58 - loss: 0.3198 - regression_loss: 0.2950 - classification_loss: 0.0247
  62/1000 [>.............................] - ETA: 34:48 - loss: 0.3175 - regression_loss: 0.2929 - classification_loss: 0.0246
  63/1000 [>.............................] - ETA: 34:52 - loss: 0.3196 - regression_loss: 0.2948 - classification_loss: 0.0247
  64/1000 [>.............................] - ETA: 34:57 - loss: 0.3214 - regression_loss: 0.2965 - classification_loss: 0.0249
  65/1000 [>.............................] - ETA: 35:00 - loss: 0.3231 - regression_loss: 0.2982 - classification_loss: 0.0250
  66/1000 [>.............................] - ETA: 34:48 - loss: 0.3190 - regression_loss: 0.2943 - classification_loss: 0.0247
  67/1000 [=>............................] - ETA: 34:49 - loss: 0.3182 - regression_loss: 0.2935 - classification_loss: 0.0247
  68/1000 [=>............................] - ETA: 34:46 - loss: 0.3183 - regression_loss: 0.2936 - classification_loss: 0.0247
  69/1000 [=>............................] - ETA: 34:47 - loss: 0.3193 - regression_loss: 0.2945 - classification_loss: 0.0248
  70/1000 [=>............................] - ETA: 34:38 - loss: 0.3173 - regression_loss: 0.2926 - classification_loss: 0.0247
  71/1000 [=>............................] - ETA: 34:42 - loss: 0.3198 - regression_loss: 0.2950 - classification_loss: 0.0249
  72/1000 [=>............................] - ETA: 34:43 - loss: 0.3212 - regression_loss: 0.2964 - classification_loss: 0.0249
  73/1000 [=>............................] - ETA: 34:46 - loss: 0.3232 - regression_loss: 0.2982 - classification_loss: 0.0250
  74/1000 [=>............................] - ETA: 34:46 - loss: 0.3224 - regression_loss: 0.2975 - classification_loss: 0.0250
  75/1000 [=>............................] - ETA: 34:35 - loss: 0.3189 - regression_loss: 0.2942 - classification_loss: 0.0247
  76/1000 [=>............................] - ETA: 34:32 - loss: 0.3197 - regression_loss: 0.2950 - classification_loss: 0.0248
  77/1000 [=>............................] - ETA: 34:23 - loss: 0.3182 - regression_loss: 0.2935 - classification_loss: 0.0247
  78/1000 [=>............................] - ETA: 34:27 - loss: 0.3204 - regression_loss: 0.2956 - classification_loss: 0.0248
  79/1000 [=>............................] - ETA: 34:17 - loss: 0.3168 - regression_loss: 0.2922 - classification_loss: 0.0245
  80/1000 [=>............................] - ETA: 34:18 - loss: 0.3179 - regression_loss: 0.2933 - classification_loss: 0.0246
  81/1000 [=>............................] - ETA: 34:14 - loss: 0.3184 - regression_loss: 0.2938 - classification_loss: 0.0246
  82/1000 [=>............................] - ETA: 34:18 - loss: 0.3196 - regression_loss: 0.2949 - classification_loss: 0.0247
  83/1000 [=>............................] - ETA: 34:19 - loss: 0.3209 - regression_loss: 0.2961 - classification_loss: 0.0248
  84/1000 [=>............................] - ETA: 34:11 - loss: 0.3201 - regression_loss: 0.2955 - classification_loss: 0.0247
  85/1000 [=>............................] - ETA: 34:12 - loss: 0.3194 - regression_loss: 0.2948 - classification_loss: 0.0247
  86/1000 [=>............................] - ETA: 34:08 - loss: 0.3195 - regression_loss: 0.2948 - classification_loss: 0.0247
  87/1000 [=>............................] - ETA: 34:11 - loss: 0.3206 - regression_loss: 0.2959 - classification_loss: 0.0247
  88/1000 [=>............................] - ETA: 34:12 - loss: 0.3211 - regression_loss: 0.2964 - classification_loss: 0.0247
  89/1000 [=>............................] - ETA: 34:03 - loss: 0.3178 - regression_loss: 0.2933 - classification_loss: 0.0245
  90/1000 [=>............................] - ETA: 33:55 - loss: 0.3161 - regression_loss: 0.2916 - classification_loss: 0.0244
  91/1000 [=>............................] - ETA: 33:55 - loss: 0.3163 - regression_loss: 0.2918 - classification_loss: 0.0245
  92/1000 [=>............................] - ETA: 33:57 - loss: 0.3178 - regression_loss: 0.2932 - classification_loss: 0.0246
  93/1000 [=>............................] - ETA: 33:48 - loss: 0.3158 - regression_loss: 0.2913 - classification_loss: 0.0244
  94/1000 [=>............................] - ETA: 33:46 - loss: 0.3160 - regression_loss: 0.2915 - classification_loss: 0.0244
  95/1000 [=>............................] - ETA: 33:47 - loss: 0.3167 - regression_loss: 0.2922 - classification_loss: 0.0245
  96/1000 [=>............................] - ETA: 33:49 - loss: 0.3177 - regression_loss: 0.2932 - classification_loss: 0.0245
  97/1000 [=>............................] - ETA: 33:49 - loss: 0.3171 - regression_loss: 0.2925 - classification_loss: 0.0246
  98/1000 [=>............................] - ETA: 33:51 - loss: 0.3182 - regression_loss: 0.2936 - classification_loss: 0.0246
  99/1000 [=>............................] - ETA: 33:44 - loss: 0.3171 - regression_loss: 0.2925 - classification_loss: 0.0246
 100/1000 [==>...........................] - ETA: 33:44 - loss: 0.3165 - regression_loss: 0.2920 - classification_loss: 0.0246
 101/1000 [==>...........................] - ETA: 33:44 - loss: 0.3171 - regression_loss: 0.2925 - classification_loss: 0.0246
 102/1000 [==>...........................] - ETA: 33:37 - loss: 0.3154 - regression_loss: 0.2910 - classification_loss: 0.0245
 103/1000 [==>...........................] - ETA: 33:29 - loss: 0.3128 - regression_loss: 0.2885 - classification_loss: 0.0243
 104/1000 [==>...........................] - ETA: 33:31 - loss: 0.3139 - regression_loss: 0.2896 - classification_loss: 0.0244
 105/1000 [==>...........................] - ETA: 33:32 - loss: 0.3150 - regression_loss: 0.2906 - classification_loss: 0.0244
 106/1000 [==>...........................] - ETA: 33:29 - loss: 0.3152 - regression_loss: 0.2907 - classification_loss: 0.0244
 107/1000 [==>...........................] - ETA: 33:21 - loss: 0.3126 - regression_loss: 0.2884 - classification_loss: 0.0242
 108/1000 [==>...........................] - ETA: 33:15 - loss: 0.3116 - regression_loss: 0.2874 - classification_loss: 0.0242
 109/1000 [==>...........................] - ETA: 33:12 - loss: 0.3118 - regression_loss: 0.2877 - classification_loss: 0.0241
 110/1000 [==>...........................] - ETA: 33:13 - loss: 0.3133 - regression_loss: 0.2891 - classification_loss: 0.0242
 111/1000 [==>...........................] - ETA: 33:13 - loss: 0.3140 - regression_loss: 0.2897 - classification_loss: 0.0242
 112/1000 [==>...........................] - ETA: 33:12 - loss: 0.3135 - regression_loss: 0.2892 - classification_loss: 0.0242
 113/1000 [==>...........................] - ETA: 33:14 - loss: 0.3144 - regression_loss: 0.2901 - classification_loss: 0.0243
 114/1000 [==>...........................] - ETA: 33:15 - loss: 0.3153 - regression_loss: 0.2910 - classification_loss: 0.0244
 115/1000 [==>...........................] - ETA: 33:16 - loss: 0.3163 - regression_loss: 0.2919 - classification_loss: 0.0244
 116/1000 [==>...........................] - ETA: 33:10 - loss: 0.3149 - regression_loss: 0.2906 - classification_loss: 0.0243
 117/1000 [==>...........................] - ETA: 33:02 - loss: 0.3126 - regression_loss: 0.2884 - classification_loss: 0.0242
 118/1000 [==>...........................] - ETA: 33:01 - loss: 0.3129 - regression_loss: 0.2887 - classification_loss: 0.0242
 119/1000 [==>...........................] - ETA: 33:01 - loss: 0.3140 - regression_loss: 0.2898 - classification_loss: 0.0242
 120/1000 [==>...........................] - ETA: 32:58 - loss: 0.3142 - regression_loss: 0.2899 - classification_loss: 0.0242
 121/1000 [==>...........................] - ETA: 32:59 - loss: 0.3151 - regression_loss: 0.2908 - classification_loss: 0.0243
 122/1000 [==>...........................] - ETA: 33:00 - loss: 0.3160 - regression_loss: 0.2917 - classification_loss: 0.0243
 123/1000 [==>...........................] - ETA: 32:59 - loss: 0.3169 - regression_loss: 0.2926 - classification_loss: 0.0243
 124/1000 [==>...........................] - ETA: 32:53 - loss: 0.3160 - regression_loss: 0.2917 - classification_loss: 0.0243
 125/1000 [==>...........................] - ETA: 32:46 - loss: 0.3143 - regression_loss: 0.2902 - classification_loss: 0.0241
 126/1000 [==>...........................] - ETA: 32:43 - loss: 0.3147 - regression_loss: 0.2906 - classification_loss: 0.0241
 127/1000 [==>...........................] - ETA: 32:43 - loss: 0.3145 - regression_loss: 0.2904 - classification_loss: 0.0241
 128/1000 [==>...........................] - ETA: 32:42 - loss: 0.3154 - regression_loss: 0.2912 - classification_loss: 0.0241
 129/1000 [==>...........................] - ETA: 32:43 - loss: 0.3167 - regression_loss: 0.2925 - classification_loss: 0.0242
 130/1000 [==>...........................] - ETA: 32:36 - loss: 0.3145 - regression_loss: 0.2904 - classification_loss: 0.0241
 131/1000 [==>...........................] - ETA: 32:34 - loss: 0.3145 - regression_loss: 0.2905 - classification_loss: 0.0240
 132/1000 [==>...........................] - ETA: 32:28 - loss: 0.3137 - regression_loss: 0.2897 - classification_loss: 0.0240
 133/1000 [==>...........................] - ETA: 32:28 - loss: 0.3148 - regression_loss: 0.2907 - classification_loss: 0.0240
 134/1000 [===>..........................] - ETA: 32:27 - loss: 0.3147 - regression_loss: 0.2906 - classification_loss: 0.0241
 135/1000 [===>..........................] - ETA: 32:24 - loss: 0.3147 - regression_loss: 0.2907 - classification_loss: 0.0241
 136/1000 [===>..........................] - ETA: 32:23 - loss: 0.3142 - regression_loss: 0.2902 - classification_loss: 0.0241
 137/1000 [===>..........................] - ETA: 32:17 - loss: 0.3126 - regression_loss: 0.2887 - classification_loss: 0.0239
 138/1000 [===>..........................] - ETA: 32:16 - loss: 0.3133 - regression_loss: 0.2893 - classification_loss: 0.0240
 139/1000 [===>..........................] - ETA: 32:17 - loss: 0.3144 - regression_loss: 0.2904 - classification_loss: 0.0240
 140/1000 [===>..........................] - ETA: 32:17 - loss: 0.3152 - regression_loss: 0.2912 - classification_loss: 0.0241
 141/1000 [===>..........................] - ETA: 32:11 - loss: 0.3142 - regression_loss: 0.2902 - classification_loss: 0.0240
 142/1000 [===>..........................] - ETA: 32:11 - loss: 0.3148 - regression_loss: 0.2908 - classification_loss: 0.0240
 143/1000 [===>..........................] - ETA: 32:04 - loss: 0.3128 - regression_loss: 0.2889 - classification_loss: 0.0239
 144/1000 [===>..........................] - ETA: 32:02 - loss: 0.3128 - regression_loss: 0.2889 - classification_loss: 0.0239
 145/1000 [===>..........................] - ETA: 31:57 - loss: 0.3118 - regression_loss: 0.2879 - classification_loss: 0.0238
 146/1000 [===>..........................] - ETA: 31:56 - loss: 0.3125 - regression_loss: 0.2886 - classification_loss: 0.0239
 147/1000 [===>..........................] - ETA: 31:55 - loss: 0.3120 - regression_loss: 0.2882 - classification_loss: 0.0239
 148/1000 [===>..........................] - ETA: 31:56 - loss: 0.3127 - regression_loss: 0.2887 - classification_loss: 0.0239
 149/1000 [===>..........................] - ETA: 31:53 - loss: 0.3127 - regression_loss: 0.2887 - classification_loss: 0.0239
 150/1000 [===>..........................] - ETA: 31:52 - loss: 0.3130 - regression_loss: 0.2891 - classification_loss: 0.0239
 151/1000 [===>..........................] - ETA: 31:47 - loss: 0.3119 - regression_loss: 0.2880 - classification_loss: 0.0239
 152/1000 [===>..........................] - ETA: 31:46 - loss: 0.3117 - regression_loss: 0.2878 - classification_loss: 0.0239
 153/1000 [===>..........................] - ETA: 31:46 - loss: 0.3125 - regression_loss: 0.2885 - classification_loss: 0.0239
 154/1000 [===>..........................] - ETA: 31:40 - loss: 0.3108 - regression_loss: 0.2870 - classification_loss: 0.0238
 155/1000 [===>..........................] - ETA: 31:40 - loss: 0.3116 - regression_loss: 0.2878 - classification_loss: 0.0239
 156/1000 [===>..........................] - ETA: 31:40 - loss: 0.3124 - regression_loss: 0.2885 - classification_loss: 0.0239
 157/1000 [===>..........................] - ETA: 31:38 - loss: 0.3124 - regression_loss: 0.2884 - classification_loss: 0.0239
 158/1000 [===>..........................] - ETA: 31:37 - loss: 0.3130 - regression_loss: 0.2891 - classification_loss: 0.0239
 159/1000 [===>..........................] - ETA: 31:32 - loss: 0.3113 - regression_loss: 0.2875 - classification_loss: 0.0238
 160/1000 [===>..........................] - ETA: 31:31 - loss: 0.3118 - regression_loss: 0.2879 - classification_loss: 0.0239
 161/1000 [===>..........................] - ETA: 31:30 - loss: 0.3116 - regression_loss: 0.2877 - classification_loss: 0.0239
 162/1000 [===>..........................] - ETA: 31:25 - loss: 0.3116 - regression_loss: 0.2877 - classification_loss: 0.0238
 163/1000 [===>..........................] - ETA: 31:24 - loss: 0.3121 - regression_loss: 0.2883 - classification_loss: 0.0239
 164/1000 [===>..........................] - ETA: 31:18 - loss: 0.3105 - regression_loss: 0.2868 - classification_loss: 0.0237
 165/1000 [===>..........................] - ETA: 31:19 - loss: 0.3116 - regression_loss: 0.2878 - classification_loss: 0.0238
 166/1000 [===>..........................] - ETA: 31:14 - loss: 0.3106 - regression_loss: 0.2869 - classification_loss: 0.0237
 167/1000 [====>.........................] - ETA: 31:11 - loss: 0.3107 - regression_loss: 0.2870 - classification_loss: 0.0237
 168/1000 [====>.........................] - ETA: 31:11 - loss: 0.3113 - regression_loss: 0.2876 - classification_loss: 0.0238
 169/1000 [====>.........................] - ETA: 31:09 - loss: 0.3110 - regression_loss: 0.2873 - classification_loss: 0.0238
 170/1000 [====>.........................] - ETA: 31:07 - loss: 0.3112 - regression_loss: 0.2874 - classification_loss: 0.0238
 171/1000 [====>.........................] - ETA: 31:06 - loss: 0.3115 - regression_loss: 0.2877 - classification_loss: 0.0238
 172/1000 [====>.........................] - ETA: 31:06 - loss: 0.3121 - regression_loss: 0.2882 - classification_loss: 0.0238
 173/1000 [====>.........................] - ETA: 31:01 - loss: 0.3117 - regression_loss: 0.2879 - classification_loss: 0.0238
 174/1000 [====>.........................] - ETA: 31:01 - loss: 0.3124 - regression_loss: 0.2885 - classification_loss: 0.0238
 175/1000 [====>.........................] - ETA: 30:55 - loss: 0.3108 - regression_loss: 0.2870 - classification_loss: 0.0237
 176/1000 [====>.........................] - ETA: 30:54 - loss: 0.3104 - regression_loss: 0.2867 - classification_loss: 0.0237
 177/1000 [====>.........................] - ETA: 30:53 - loss: 0.3101 - regression_loss: 0.2863 - classification_loss: 0.0237
 178/1000 [====>.........................] - ETA: 30:52 - loss: 0.3108 - regression_loss: 0.2870 - classification_loss: 0.0238
 179/1000 [====>.........................] - ETA: 30:47 - loss: 0.3098 - regression_loss: 0.2861 - classification_loss: 0.0237
 180/1000 [====>.........................] - ETA: 30:46 - loss: 0.3102 - regression_loss: 0.2865 - classification_loss: 0.0237
 181/1000 [====>.........................] - ETA: 30:43 - loss: 0.3102 - regression_loss: 0.2865 - classification_loss: 0.0237
 182/1000 [====>.........................] - ETA: 30:38 - loss: 0.3095 - regression_loss: 0.2858 - classification_loss: 0.0236
 183/1000 [====>.........................] - ETA: 30:38 - loss: 0.3101 - regression_loss: 0.2864 - classification_loss: 0.0237
 184/1000 [====>.........................] - ETA: 30:37 - loss: 0.3103 - regression_loss: 0.2866 - classification_loss: 0.0237
 185/1000 [====>.........................] - ETA: 30:37 - loss: 0.3109 - regression_loss: 0.2872 - classification_loss: 0.0237
 186/1000 [====>.........................] - ETA: 30:36 - loss: 0.3115 - regression_loss: 0.2877 - classification_loss: 0.0238
 187/1000 [====>.........................] - ETA: 30:34 - loss: 0.3114 - regression_loss: 0.2876 - classification_loss: 0.0238
 188/1000 [====>.........................] - ETA: 30:29 - loss: 0.3100 - regression_loss: 0.2863 - classification_loss: 0.0237
 189/1000 [====>.........................] - ETA: 30:27 - loss: 0.3098 - regression_loss: 0.2861 - classification_loss: 0.0237
 190/1000 [====>.........................] - ETA: 30:23 - loss: 0.3093 - regression_loss: 0.2856 - classification_loss: 0.0236
 191/1000 [====>.........................] - ETA: 30:22 - loss: 0.3097 - regression_loss: 0.2860 - classification_loss: 0.0237
 192/1000 [====>.........................] - ETA: 30:21 - loss: 0.3103 - regression_loss: 0.2866 - classification_loss: 0.0237
 193/1000 [====>.........................] - ETA: 30:17 - loss: 0.3095 - regression_loss: 0.2858 - classification_loss: 0.0236
 194/1000 [====>.........................] - ETA: 30:16 - loss: 0.3100 - regression_loss: 0.2863 - classification_loss: 0.0237
 195/1000 [====>.........................] - ETA: 30:15 - loss: 0.3097 - regression_loss: 0.2860 - classification_loss: 0.0237
 196/1000 [====>.........................] - ETA: 30:10 - loss: 0.3085 - regression_loss: 0.2849 - classification_loss: 0.0236
 197/1000 [====>.........................] - ETA: 30:07 - loss: 0.3088 - regression_loss: 0.2852 - classification_loss: 0.0236
 198/1000 [====>.........................] - ETA: 30:02 - loss: 0.3073 - regression_loss: 0.2839 - classification_loss: 0.0235
 199/1000 [====>.........................] - ETA: 30:01 - loss: 0.3081 - regression_loss: 0.2846 - classification_loss: 0.0235
 200/1000 [=====>........................] - ETA: 30:00 - loss: 0.3078 - regression_loss: 0.2843 - classification_loss: 0.0235
 201/1000 [=====>........................] - ETA: 29:59 - loss: 0.3084 - regression_loss: 0.2848 - classification_loss: 0.0236
 202/1000 [=====>........................] - ETA: 29:58 - loss: 0.3088 - regression_loss: 0.2852 - classification_loss: 0.0236
 203/1000 [=====>........................] - ETA: 29:54 - loss: 0.3087 - regression_loss: 0.2851 - classification_loss: 0.0236
 204/1000 [=====>........................] - ETA: 29:51 - loss: 0.3088 - regression_loss: 0.2852 - classification_loss: 0.0236
 205/1000 [=====>........................] - ETA: 29:46 - loss: 0.3077 - regression_loss: 0.2843 - classification_loss: 0.0235
 206/1000 [=====>........................] - ETA: 29:45 - loss: 0.3084 - regression_loss: 0.2849 - classification_loss: 0.0235
 207/1000 [=====>........................] - ETA: 29:45 - loss: 0.3094 - regression_loss: 0.2859 - classification_loss: 0.0236
 208/1000 [=====>........................] - ETA: 29:44 - loss: 0.3103 - regression_loss: 0.2867 - classification_loss: 0.0236
 209/1000 [=====>........................] - ETA: 29:40 - loss: 0.3097 - regression_loss: 0.2861 - classification_loss: 0.0236
 210/1000 [=====>........................] - ETA: 29:38 - loss: 0.3096 - regression_loss: 0.2860 - classification_loss: 0.0236
 211/1000 [=====>........................] - ETA: 29:35 - loss: 0.3098 - regression_loss: 0.2862 - classification_loss: 0.0236
 212/1000 [=====>........................] - ETA: 29:35 - loss: 0.3104 - regression_loss: 0.2868 - classification_loss: 0.0236
 213/1000 [=====>........................] - ETA: 29:31 - loss: 0.3098 - regression_loss: 0.2862 - classification_loss: 0.0236
 214/1000 [=====>........................] - ETA: 29:30 - loss: 0.3100 - regression_loss: 0.2864 - classification_loss: 0.0236
 215/1000 [=====>........................] - ETA: 29:27 - loss: 0.3101 - regression_loss: 0.2865 - classification_loss: 0.0236
 216/1000 [=====>........................] - ETA: 29:26 - loss: 0.3106 - regression_loss: 0.2869 - classification_loss: 0.0236
 217/1000 [=====>........................] - ETA: 29:25 - loss: 0.3103 - regression_loss: 0.2866 - classification_loss: 0.0237
 218/1000 [=====>........................] - ETA: 29:20 - loss: 0.3090 - regression_loss: 0.2855 - classification_loss: 0.0236
 219/1000 [=====>........................] - ETA: 29:15 - loss: 0.3079 - regression_loss: 0.2844 - classification_loss: 0.0235
 220/1000 [=====>........................] - ETA: 29:14 - loss: 0.3082 - regression_loss: 0.2847 - classification_loss: 0.0235
 221/1000 [=====>........................] - ETA: 29:14 - loss: 0.3087 - regression_loss: 0.2851 - classification_loss: 0.0236
 222/1000 [=====>........................] - ETA: 29:12 - loss: 0.3084 - regression_loss: 0.2849 - classification_loss: 0.0236
 223/1000 [=====>........................] - ETA: 29:08 - loss: 0.3079 - regression_loss: 0.2843 - classification_loss: 0.0235
 224/1000 [=====>........................] - ETA: 29:07 - loss: 0.3084 - regression_loss: 0.2848 - classification_loss: 0.0235
 225/1000 [=====>........................] - ETA: 29:04 - loss: 0.3084 - regression_loss: 0.2849 - classification_loss: 0.0235
 226/1000 [=====>........................] - ETA: 29:00 - loss: 0.3077 - regression_loss: 0.2842 - classification_loss: 0.0235
 227/1000 [=====>........................] - ETA: 28:59 - loss: 0.3075 - regression_loss: 0.2839 - classification_loss: 0.0235
 228/1000 [=====>........................] - ETA: 28:54 - loss: 0.3063 - regression_loss: 0.2829 - classification_loss: 0.0234
 229/1000 [=====>........................] - ETA: 28:54 - loss: 0.3068 - regression_loss: 0.2833 - classification_loss: 0.0235
 230/1000 [=====>........................] - ETA: 28:52 - loss: 0.3070 - regression_loss: 0.2835 - classification_loss: 0.0235
 231/1000 [=====>........................] - ETA: 28:50 - loss: 0.3070 - regression_loss: 0.2835 - classification_loss: 0.0235
 232/1000 [=====>........................] - ETA: 28:49 - loss: 0.3074 - regression_loss: 0.2839 - classification_loss: 0.0235
 233/1000 [=====>........................] - ETA: 28:48 - loss: 0.3079 - regression_loss: 0.2843 - classification_loss: 0.0235
 234/1000 [======>.......................] - ETA: 28:46 - loss: 0.3080 - regression_loss: 0.2845 - classification_loss: 0.0235
 235/1000 [======>.......................] - ETA: 28:45 - loss: 0.3077 - regression_loss: 0.2842 - classification_loss: 0.0235
 236/1000 [======>.......................] - ETA: 28:44 - loss: 0.3081 - regression_loss: 0.2846 - classification_loss: 0.0236
 237/1000 [======>.......................] - ETA: 28:42 - loss: 0.3081 - regression_loss: 0.2845 - classification_loss: 0.0236
 238/1000 [======>.......................] - ETA: 28:38 - loss: 0.3078 - regression_loss: 0.2843 - classification_loss: 0.0235
 239/1000 [======>.......................] - ETA: 28:33 - loss: 0.3069 - regression_loss: 0.2835 - classification_loss: 0.0234
 240/1000 [======>.......................] - ETA: 28:32 - loss: 0.3079 - regression_loss: 0.2844 - classification_loss: 0.0235
 241/1000 [======>.......................] - ETA: 28:31 - loss: 0.3089 - regression_loss: 0.2853 - classification_loss: 0.0235
 242/1000 [======>.......................] - ETA: 28:27 - loss: 0.3078 - regression_loss: 0.2843 - classification_loss: 0.0235
 243/1000 [======>.......................] - ETA: 28:26 - loss: 0.3081 - regression_loss: 0.2846 - classification_loss: 0.0235
 244/1000 [======>.......................] - ETA: 28:22 - loss: 0.3074 - regression_loss: 0.2840 - classification_loss: 0.0234
 245/1000 [======>.......................] - ETA: 28:20 - loss: 0.3074 - regression_loss: 0.2839 - classification_loss: 0.0235
 246/1000 [======>.......................] - ETA: 28:18 - loss: 0.3076 - regression_loss: 0.2841 - classification_loss: 0.0235
 247/1000 [======>.......................] - ETA: 28:17 - loss: 0.3082 - regression_loss: 0.2847 - classification_loss: 0.0235
 248/1000 [======>.......................] - ETA: 28:16 - loss: 0.3086 - regression_loss: 0.2851 - classification_loss: 0.0235
 249/1000 [======>.......................] - ETA: 28:13 - loss: 0.3088 - regression_loss: 0.2853 - classification_loss: 0.0235
 250/1000 [======>.......................] - ETA: 28:09 - loss: 0.3086 - regression_loss: 0.2851 - classification_loss: 0.0235
 251/1000 [======>.......................] - ETA: 28:08 - loss: 0.3086 - regression_loss: 0.2851 - classification_loss: 0.0235
 252/1000 [======>.......................] - ETA: 28:03 - loss: 0.3075 - regression_loss: 0.2841 - classification_loss: 0.0234
 253/1000 [======>.......................] - ETA: 28:02 - loss: 0.3077 - regression_loss: 0.2843 - classification_loss: 0.0234
 254/1000 [======>.......................] - ETA: 28:00 - loss: 0.3075 - regression_loss: 0.2840 - classification_loss: 0.0234
 255/1000 [======>.......................] - ETA: 27:59 - loss: 0.3077 - regression_loss: 0.2842 - classification_loss: 0.0234
 256/1000 [======>.......................] - ETA: 27:55 - loss: 0.3066 - regression_loss: 0.2833 - classification_loss: 0.0234
 257/1000 [======>.......................] - ETA: 27:53 - loss: 0.3071 - regression_loss: 0.2837 - classification_loss: 0.0234
 258/1000 [======>.......................] - ETA: 27:53 - loss: 0.3074 - regression_loss: 0.2840 - classification_loss: 0.0234
 259/1000 [======>.......................] - ETA: 27:49 - loss: 0.3068 - regression_loss: 0.2835 - classification_loss: 0.0234
 260/1000 [======>.......................] - ETA: 27:46 - loss: 0.3068 - regression_loss: 0.2834 - classification_loss: 0.0234
 261/1000 [======>.......................] - ETA: 27:45 - loss: 0.3065 - regression_loss: 0.2832 - classification_loss: 0.0234
 262/1000 [======>.......................] - ETA: 27:43 - loss: 0.3069 - regression_loss: 0.2835 - classification_loss: 0.0234
 263/1000 [======>.......................] - ETA: 27:42 - loss: 0.3071 - regression_loss: 0.2837 - classification_loss: 0.0234
 264/1000 [======>.......................] - ETA: 27:41 - loss: 0.3075 - regression_loss: 0.2841 - classification_loss: 0.0234
 265/1000 [======>.......................] - ETA: 27:38 - loss: 0.3076 - regression_loss: 0.2841 - classification_loss: 0.0234
 266/1000 [======>.......................] - ETA: 27:34 - loss: 0.3066 - regression_loss: 0.2833 - classification_loss: 0.0234
 267/1000 [=======>......................] - ETA: 27:30 - loss: 0.3062 - regression_loss: 0.2828 - classification_loss: 0.0233
 268/1000 [=======>......................] - ETA: 27:26 - loss: 0.3051 - regression_loss: 0.2818 - classification_loss: 0.0233
 269/1000 [=======>......................] - ETA: 27:24 - loss: 0.3054 - regression_loss: 0.2822 - classification_loss: 0.0233
 270/1000 [=======>......................] - ETA: 27:20 - loss: 0.3048 - regression_loss: 0.2816 - classification_loss: 0.0232
 271/1000 [=======>......................] - ETA: 27:19 - loss: 0.3053 - regression_loss: 0.2820 - classification_loss: 0.0232
 272/1000 [=======>......................] - ETA: 27:17 - loss: 0.3058 - regression_loss: 0.2825 - classification_loss: 0.0233
 273/1000 [=======>......................] - ETA: 27:16 - loss: 0.3062 - regression_loss: 0.2829 - classification_loss: 0.0233
 274/1000 [=======>......................] - ETA: 27:15 - loss: 0.3062 - regression_loss: 0.2830 - classification_loss: 0.0233
 275/1000 [=======>......................] - ETA: 27:11 - loss: 0.3065 - regression_loss: 0.2833 - classification_loss: 0.0233
 276/1000 [=======>......................] - ETA: 27:09 - loss: 0.3065 - regression_loss: 0.2833 - classification_loss: 0.0233
 277/1000 [=======>......................] - ETA: 27:05 - loss: 0.3060 - regression_loss: 0.2828 - classification_loss: 0.0232
 278/1000 [=======>......................] - ETA: 27:04 - loss: 0.3066 - regression_loss: 0.2833 - classification_loss: 0.0232
 279/1000 [=======>......................] - ETA: 27:03 - loss: 0.3070 - regression_loss: 0.2837 - classification_loss: 0.0233
 280/1000 [=======>......................] - ETA: 27:01 - loss: 0.3076 - regression_loss: 0.2843 - classification_loss: 0.0233
 281/1000 [=======>......................] - ETA: 26:59 - loss: 0.3076 - regression_loss: 0.2843 - classification_loss: 0.0233
 282/1000 [=======>......................] - ETA: 26:57 - loss: 0.3074 - regression_loss: 0.2842 - classification_loss: 0.0233
 283/1000 [=======>......................] - ETA: 26:53 - loss: 0.3065 - regression_loss: 0.2833 - classification_loss: 0.0232
 284/1000 [=======>......................] - ETA: 26:51 - loss: 0.3068 - regression_loss: 0.2836 - classification_loss: 0.0232
 285/1000 [=======>......................] - ETA: 26:48 - loss: 0.3065 - regression_loss: 0.2833 - classification_loss: 0.0232
 286/1000 [=======>......................] - ETA: 26:47 - loss: 0.3069 - regression_loss: 0.2836 - classification_loss: 0.0232
 287/1000 [=======>......................] - ETA: 26:44 - loss: 0.3070 - regression_loss: 0.2837 - classification_loss: 0.0232
 288/1000 [=======>......................] - ETA: 26:43 - loss: 0.3074 - regression_loss: 0.2841 - classification_loss: 0.0233
 289/1000 [=======>......................] - ETA: 26:42 - loss: 0.3078 - regression_loss: 0.2846 - classification_loss: 0.0233
 290/1000 [=======>......................] - ETA: 26:39 - loss: 0.3079 - regression_loss: 0.2846 - classification_loss: 0.0233
 291/1000 [=======>......................] - ETA: 26:36 - loss: 0.3074 - regression_loss: 0.2842 - classification_loss: 0.0232
 292/1000 [=======>......................] - ETA: 26:32 - loss: 0.3067 - regression_loss: 0.2835 - classification_loss: 0.0232
 293/1000 [=======>......................] - ETA: 26:30 - loss: 0.3072 - regression_loss: 0.2840 - classification_loss: 0.0232
 294/1000 [=======>......................] - ETA: 26:28 - loss: 0.3072 - regression_loss: 0.2840 - classification_loss: 0.0232
 295/1000 [=======>......................] - ETA: 26:27 - loss: 0.3078 - regression_loss: 0.2846 - classification_loss: 0.0232
 296/1000 [=======>......................] - ETA: 26:23 - loss: 0.3068 - regression_loss: 0.2836 - classification_loss: 0.0232
 297/1000 [=======>......................] - ETA: 26:22 - loss: 0.3070 - regression_loss: 0.2838 - classification_loss: 0.0232
 298/1000 [=======>......................] - ETA: 26:20 - loss: 0.3069 - regression_loss: 0.2837 - classification_loss: 0.0232
 299/1000 [=======>......................] - ETA: 26:18 - loss: 0.3072 - regression_loss: 0.2840 - classification_loss: 0.0232
 300/1000 [========>.....................] - ETA: 26:17 - loss: 0.3075 - regression_loss: 0.2843 - classification_loss: 0.0232
 301/1000 [========>.....................] - ETA: 26:14 - loss: 0.3071 - regression_loss: 0.2839 - classification_loss: 0.0232
 302/1000 [========>.....................] - ETA: 26:11 - loss: 0.3072 - regression_loss: 0.2840 - classification_loss: 0.0232
 303/1000 [========>.....................] - ETA: 26:10 - loss: 0.3070 - regression_loss: 0.2838 - classification_loss: 0.0232
 304/1000 [========>.....................] - ETA: 26:08 - loss: 0.3075 - regression_loss: 0.2842 - classification_loss: 0.0232
 305/1000 [========>.....................] - ETA: 26:05 - loss: 0.3069 - regression_loss: 0.2837 - classification_loss: 0.0232
 306/1000 [========>.....................] - ETA: 26:01 - loss: 0.3063 - regression_loss: 0.2831 - classification_loss: 0.0232
 307/1000 [========>.....................] - ETA: 25:59 - loss: 0.3065 - regression_loss: 0.2833 - classification_loss: 0.0232
 308/1000 [========>.....................] - ETA: 25:57 - loss: 0.3066 - regression_loss: 0.2834 - classification_loss: 0.0232
 309/1000 [========>.....................] - ETA: 25:56 - loss: 0.3069 - regression_loss: 0.2837 - classification_loss: 0.0232
 310/1000 [========>.....................] - ETA: 25:52 - loss: 0.3064 - regression_loss: 0.2833 - classification_loss: 0.0232
 311/1000 [========>.....................] - ETA: 25:51 - loss: 0.3067 - regression_loss: 0.2836 - classification_loss: 0.0232
 312/1000 [========>.....................] - ETA: 25:50 - loss: 0.3071 - regression_loss: 0.2839 - classification_loss: 0.0232
 313/1000 [========>.....................] - ETA: 25:46 - loss: 0.3063 - regression_loss: 0.2832 - classification_loss: 0.0232
 314/1000 [========>.....................] - ETA: 25:44 - loss: 0.3062 - regression_loss: 0.2830 - classification_loss: 0.0232
 315/1000 [========>.....................] - ETA: 25:42 - loss: 0.3062 - regression_loss: 0.2830 - classification_loss: 0.0232
 316/1000 [========>.....................] - ETA: 25:40 - loss: 0.3063 - regression_loss: 0.2831 - classification_loss: 0.0232
 317/1000 [========>.....................] - ETA: 25:38 - loss: 0.3066 - regression_loss: 0.2834 - classification_loss: 0.0232
 318/1000 [========>.....................] - ETA: 25:37 - loss: 0.3069 - regression_loss: 0.2837 - classification_loss: 0.0232
 319/1000 [========>.....................] - ETA: 25:34 - loss: 0.3064 - regression_loss: 0.2832 - classification_loss: 0.0232
 320/1000 [========>.....................] - ETA: 25:32 - loss: 0.3062 - regression_loss: 0.2830 - classification_loss: 0.0232
 321/1000 [========>.....................] - ETA: 25:28 - loss: 0.3053 - regression_loss: 0.2822 - classification_loss: 0.0231
 322/1000 [========>.....................] - ETA: 25:26 - loss: 0.3053 - regression_loss: 0.2822 - classification_loss: 0.0231
 323/1000 [========>.....................] - ETA: 25:24 - loss: 0.3054 - regression_loss: 0.2823 - classification_loss: 0.0231
 324/1000 [========>.....................] - ETA: 25:23 - loss: 0.3058 - regression_loss: 0.2826 - classification_loss: 0.0231
 325/1000 [========>.....................] - ETA: 25:19 - loss: 0.3049 - regression_loss: 0.2818 - classification_loss: 0.0231
 326/1000 [========>.....................] - ETA: 25:18 - loss: 0.3048 - regression_loss: 0.2817 - classification_loss: 0.0231
 327/1000 [========>.....................] - ETA: 25:16 - loss: 0.3049 - regression_loss: 0.2818 - classification_loss: 0.0231
 328/1000 [========>.....................] - ETA: 25:13 - loss: 0.3050 - regression_loss: 0.2819 - classification_loss: 0.0231
 329/1000 [========>.....................] - ETA: 25:10 - loss: 0.3045 - regression_loss: 0.2814 - classification_loss: 0.0231
 330/1000 [========>.....................] - ETA: 25:09 - loss: 0.3050 - regression_loss: 0.2819 - classification_loss: 0.0231
 331/1000 [========>.....................] - ETA: 25:07 - loss: 0.3053 - regression_loss: 0.2822 - classification_loss: 0.0231
 332/1000 [========>.....................] - ETA: 25:04 - loss: 0.3048 - regression_loss: 0.2818 - classification_loss: 0.0231
 333/1000 [========>.....................] - ETA: 25:01 - loss: 0.3048 - regression_loss: 0.2818 - classification_loss: 0.0231
 334/1000 [=========>....................] - ETA: 25:00 - loss: 0.3052 - regression_loss: 0.2821 - classification_loss: 0.0231
 335/1000 [=========>....................] - ETA: 24:58 - loss: 0.3056 - regression_loss: 0.2824 - classification_loss: 0.0231
 336/1000 [=========>....................] - ETA: 24:56 - loss: 0.3054 - regression_loss: 0.2822 - classification_loss: 0.0231
 337/1000 [=========>....................] - ETA: 24:53 - loss: 0.3045 - regression_loss: 0.2814 - classification_loss: 0.0231
 338/1000 [=========>....................] - ETA: 24:50 - loss: 0.3046 - regression_loss: 0.2815 - classification_loss: 0.0231
 339/1000 [=========>....................] - ETA: 24:49 - loss: 0.3050 - regression_loss: 0.2819 - classification_loss: 0.0231
 340/1000 [=========>....................] - ETA: 24:45 - loss: 0.3046 - regression_loss: 0.2816 - classification_loss: 0.0231
 341/1000 [=========>....................] - ETA: 24:44 - loss: 0.3045 - regression_loss: 0.2815 - classification_loss: 0.0231
 342/1000 [=========>....................] - ETA: 24:42 - loss: 0.3047 - regression_loss: 0.2816 - classification_loss: 0.0231
 343/1000 [=========>....................] - ETA: 24:38 - loss: 0.3039 - regression_loss: 0.2809 - classification_loss: 0.0230
 344/1000 [=========>....................] - ETA: 24:37 - loss: 0.3043 - regression_loss: 0.2812 - classification_loss: 0.0230
 345/1000 [=========>....................] - ETA: 24:35 - loss: 0.3045 - regression_loss: 0.2814 - classification_loss: 0.0231
 346/1000 [=========>....................] - ETA: 24:33 - loss: 0.3045 - regression_loss: 0.2815 - classification_loss: 0.0231
 347/1000 [=========>....................] - ETA: 24:29 - loss: 0.3038 - regression_loss: 0.2808 - classification_loss: 0.0230
 348/1000 [=========>....................] - ETA: 24:28 - loss: 0.3043 - regression_loss: 0.2813 - classification_loss: 0.0230
 349/1000 [=========>....................] - ETA: 24:24 - loss: 0.3040 - regression_loss: 0.2810 - classification_loss: 0.0230
 350/1000 [=========>....................] - ETA: 24:23 - loss: 0.3039 - regression_loss: 0.2809 - classification_loss: 0.0230
 351/1000 [=========>....................] - ETA: 24:21 - loss: 0.3043 - regression_loss: 0.2812 - classification_loss: 0.0230
 352/1000 [=========>....................] - ETA: 24:18 - loss: 0.3040 - regression_loss: 0.2810 - classification_loss: 0.0230
 353/1000 [=========>....................] - ETA: 24:16 - loss: 0.3040 - regression_loss: 0.2809 - classification_loss: 0.0230
 354/1000 [=========>....................] - ETA: 24:14 - loss: 0.3042 - regression_loss: 0.2811 - classification_loss: 0.0230
 355/1000 [=========>....................] - ETA: 24:13 - loss: 0.3045 - regression_loss: 0.2814 - classification_loss: 0.0230
 356/1000 [=========>....................] - ETA: 24:11 - loss: 0.3047 - regression_loss: 0.2817 - classification_loss: 0.0231
 357/1000 [=========>....................] - ETA: 24:09 - loss: 0.3048 - regression_loss: 0.2817 - classification_loss: 0.0231
 358/1000 [=========>....................] - ETA: 24:06 - loss: 0.3040 - regression_loss: 0.2810 - classification_loss: 0.0230
 359/1000 [=========>....................] - ETA: 24:04 - loss: 0.3044 - regression_loss: 0.2814 - classification_loss: 0.0230
 360/1000 [=========>....................] - ETA: 24:02 - loss: 0.3044 - regression_loss: 0.2814 - classification_loss: 0.0230
 361/1000 [=========>....................] - ETA: 24:00 - loss: 0.3045 - regression_loss: 0.2814 - classification_loss: 0.0230
 362/1000 [=========>....................] - ETA: 23:58 - loss: 0.3043 - regression_loss: 0.2813 - classification_loss: 0.0230
 363/1000 [=========>....................] - ETA: 23:55 - loss: 0.3037 - regression_loss: 0.2807 - classification_loss: 0.0230
 364/1000 [=========>....................] - ETA: 23:53 - loss: 0.3040 - regression_loss: 0.2810 - classification_loss: 0.0230
 365/1000 [=========>....................] - ETA: 23:50 - loss: 0.3036 - regression_loss: 0.2806 - classification_loss: 0.0230
 366/1000 [=========>....................] - ETA: 23:48 - loss: 0.3039 - regression_loss: 0.2809 - classification_loss: 0.0230
 367/1000 [==========>...................] - ETA: 23:47 - loss: 0.3041 - regression_loss: 0.2811 - classification_loss: 0.0230
 368/1000 [==========>...................] - ETA: 23:43 - loss: 0.3037 - regression_loss: 0.2807 - classification_loss: 0.0230
 369/1000 [==========>...................] - ETA: 23:42 - loss: 0.3039 - regression_loss: 0.2809 - classification_loss: 0.0230
 370/1000 [==========>...................] - ETA: 23:39 - loss: 0.3040 - regression_loss: 0.2810 - classification_loss: 0.0230
 371/1000 [==========>...................] - ETA: 23:36 - loss: 0.3033 - regression_loss: 0.2803 - classification_loss: 0.0229
 372/1000 [==========>...................] - ETA: 23:34 - loss: 0.3031 - regression_loss: 0.2802 - classification_loss: 0.0229
 373/1000 [==========>...................] - ETA: 23:33 - loss: 0.3035 - regression_loss: 0.2805 - classification_loss: 0.0230
 374/1000 [==========>...................] - ETA: 23:31 - loss: 0.3033 - regression_loss: 0.2803 - classification_loss: 0.0230
 375/1000 [==========>...................] - ETA: 23:28 - loss: 0.3033 - regression_loss: 0.2803 - classification_loss: 0.0230
 376/1000 [==========>...................] - ETA: 23:27 - loss: 0.3036 - regression_loss: 0.2806 - classification_loss: 0.0230
 377/1000 [==========>...................] - ETA: 23:24 - loss: 0.3032 - regression_loss: 0.2802 - classification_loss: 0.0230
 378/1000 [==========>...................] - ETA: 23:20 - loss: 0.3025 - regression_loss: 0.2796 - classification_loss: 0.0229
 379/1000 [==========>...................] - ETA: 23:18 - loss: 0.3027 - regression_loss: 0.2798 - classification_loss: 0.0229
 380/1000 [==========>...................] - ETA: 23:15 - loss: 0.3020 - regression_loss: 0.2791 - classification_loss: 0.0229
 381/1000 [==========>...................] - ETA: 23:14 - loss: 0.3023 - regression_loss: 0.2794 - classification_loss: 0.0229
 382/1000 [==========>...................] - ETA: 23:12 - loss: 0.3022 - regression_loss: 0.2793 - classification_loss: 0.0229
 383/1000 [==========>...................] - ETA: 23:08 - loss: 0.3018 - regression_loss: 0.2790 - classification_loss: 0.0229
 384/1000 [==========>...................] - ETA: 23:06 - loss: 0.3019 - regression_loss: 0.2790 - classification_loss: 0.0229
 385/1000 [==========>...................] - ETA: 23:04 - loss: 0.3022 - regression_loss: 0.2793 - classification_loss: 0.0229
 386/1000 [==========>...................] - ETA: 23:03 - loss: 0.3024 - regression_loss: 0.2795 - classification_loss: 0.0229
 387/1000 [==========>...................] - ETA: 23:01 - loss: 0.3027 - regression_loss: 0.2798 - classification_loss: 0.0229
 388/1000 [==========>...................] - ETA: 22:59 - loss: 0.3028 - regression_loss: 0.2798 - classification_loss: 0.0229
 389/1000 [==========>...................] - ETA: 22:55 - loss: 0.3021 - regression_loss: 0.2792 - classification_loss: 0.0229
 390/1000 [==========>...................] - ETA: 22:54 - loss: 0.3023 - regression_loss: 0.2794 - classification_loss: 0.0229
 391/1000 [==========>...................] - ETA: 22:52 - loss: 0.3025 - regression_loss: 0.2796 - classification_loss: 0.0229
 392/1000 [==========>...................] - ETA: 22:50 - loss: 0.3023 - regression_loss: 0.2794 - classification_loss: 0.0229
 393/1000 [==========>...................] - ETA: 22:47 - loss: 0.3020 - regression_loss: 0.2791 - classification_loss: 0.0229
 394/1000 [==========>...................] - ETA: 22:45 - loss: 0.3021 - regression_loss: 0.2792 - classification_loss: 0.0229
 395/1000 [==========>...................] - ETA: 22:43 - loss: 0.3020 - regression_loss: 0.2791 - classification_loss: 0.0229
 396/1000 [==========>...................] - ETA: 22:40 - loss: 0.3014 - regression_loss: 0.2785 - classification_loss: 0.0229
 397/1000 [==========>...................] - ETA: 22:37 - loss: 0.3014 - regression_loss: 0.2786 - classification_loss: 0.0229
 398/1000 [==========>...................] - ETA: 22:36 - loss: 0.3017 - regression_loss: 0.2788 - classification_loss: 0.0229
 399/1000 [==========>...................] - ETA: 22:33 - loss: 0.3012 - regression_loss: 0.2784 - classification_loss: 0.0228
 400/1000 [===========>..................] - ETA: 22:31 - loss: 0.3015 - regression_loss: 0.2786 - classification_loss: 0.0229
 401/1000 [===========>..................] - ETA: 22:30 - loss: 0.3018 - regression_loss: 0.2789 - classification_loss: 0.0229
 402/1000 [===========>..................] - ETA: 22:26 - loss: 0.3012 - regression_loss: 0.2783 - classification_loss: 0.0228
 403/1000 [===========>..................] - ETA: 22:23 - loss: 0.3009 - regression_loss: 0.2781 - classification_loss: 0.0228
 404/1000 [===========>..................] - ETA: 22:21 - loss: 0.3010 - regression_loss: 0.2782 - classification_loss: 0.0228
 405/1000 [===========>..................] - ETA: 22:19 - loss: 0.3009 - regression_loss: 0.2781 - classification_loss: 0.0228
 406/1000 [===========>..................] - ETA: 22:17 - loss: 0.3011 - regression_loss: 0.2782 - classification_loss: 0.0228
 407/1000 [===========>..................] - ETA: 22:15 - loss: 0.3013 - regression_loss: 0.2785 - classification_loss: 0.0228
 408/1000 [===========>..................] - ETA: 22:14 - loss: 0.3014 - regression_loss: 0.2786 - classification_loss: 0.0228
 409/1000 [===========>..................] - ETA: 22:12 - loss: 0.3017 - regression_loss: 0.2789 - classification_loss: 0.0229
 410/1000 [===========>..................] - ETA: 22:10 - loss: 0.3017 - regression_loss: 0.2788 - classification_loss: 0.0229
 411/1000 [===========>..................] - ETA: 22:07 - loss: 0.3012 - regression_loss: 0.2784 - classification_loss: 0.0228
 412/1000 [===========>..................] - ETA: 22:04 - loss: 0.3013 - regression_loss: 0.2784 - classification_loss: 0.0228
 413/1000 [===========>..................] - ETA: 22:01 - loss: 0.3006 - regression_loss: 0.2778 - classification_loss: 0.0228
 414/1000 [===========>..................] - ETA: 22:00 - loss: 0.3009 - regression_loss: 0.2781 - classification_loss: 0.0228
 415/1000 [===========>..................] - ETA: 21:57 - loss: 0.3010 - regression_loss: 0.2781 - classification_loss: 0.0228
 416/1000 [===========>..................] - ETA: 21:54 - loss: 0.3003 - regression_loss: 0.2776 - classification_loss: 0.0228
 417/1000 [===========>..................] - ETA: 21:52 - loss: 0.3002 - regression_loss: 0.2774 - classification_loss: 0.0228
 418/1000 [===========>..................] - ETA: 21:51 - loss: 0.3004 - regression_loss: 0.2776 - classification_loss: 0.0228
 419/1000 [===========>..................] - ETA: 21:48 - loss: 0.3001 - regression_loss: 0.2773 - classification_loss: 0.0228
 420/1000 [===========>..................] - ETA: 21:46 - loss: 0.3004 - regression_loss: 0.2776 - classification_loss: 0.0228
 421/1000 [===========>..................] - ETA: 21:44 - loss: 0.3005 - regression_loss: 0.2778 - classification_loss: 0.0228
 422/1000 [===========>..................] - ETA: 21:41 - loss: 0.3002 - regression_loss: 0.2774 - classification_loss: 0.0228
 423/1000 [===========>..................] - ETA: 21:39 - loss: 0.3000 - regression_loss: 0.2773 - classification_loss: 0.0228
 424/1000 [===========>..................] - ETA: 21:37 - loss: 0.3003 - regression_loss: 0.2775 - classification_loss: 0.0228
 425/1000 [===========>..................] - ETA: 21:34 - loss: 0.2996 - regression_loss: 0.2769 - classification_loss: 0.0227
 426/1000 [===========>..................] - ETA: 21:33 - loss: 0.2999 - regression_loss: 0.2771 - classification_loss: 0.0228
 427/1000 [===========>..................] - ETA: 21:30 - loss: 0.2999 - regression_loss: 0.2772 - classification_loss: 0.0228
 428/1000 [===========>..................] - ETA: 21:28 - loss: 0.3001 - regression_loss: 0.2773 - classification_loss: 0.0228
 429/1000 [===========>..................] - ETA: 21:25 - loss: 0.2995 - regression_loss: 0.2767 - classification_loss: 0.0227
 430/1000 [===========>..................] - ETA: 21:23 - loss: 0.2997 - regression_loss: 0.2770 - classification_loss: 0.0227
 431/1000 [===========>..................] - ETA: 21:21 - loss: 0.2998 - regression_loss: 0.2770 - classification_loss: 0.0227
 432/1000 [===========>..................] - ETA: 21:19 - loss: 0.3000 - regression_loss: 0.2773 - classification_loss: 0.0228
 433/1000 [===========>..................] - ETA: 21:17 - loss: 0.2999 - regression_loss: 0.2771 - classification_loss: 0.0228
 434/1000 [============>.................] - ETA: 21:15 - loss: 0.3000 - regression_loss: 0.2772 - classification_loss: 0.0228
 435/1000 [============>.................] - ETA: 21:12 - loss: 0.2999 - regression_loss: 0.2771 - classification_loss: 0.0227
 436/1000 [============>.................] - ETA: 21:11 - loss: 0.3000 - regression_loss: 0.2772 - classification_loss: 0.0227
 437/1000 [============>.................] - ETA: 21:09 - loss: 0.3003 - regression_loss: 0.2775 - classification_loss: 0.0228
 438/1000 [============>.................] - ETA: 21:07 - loss: 0.3002 - regression_loss: 0.2774 - classification_loss: 0.0228
 439/1000 [============>.................] - ETA: 21:04 - loss: 0.2999 - regression_loss: 0.2771 - classification_loss: 0.0228
 440/1000 [============>.................] - ETA: 21:02 - loss: 0.2999 - regression_loss: 0.2772 - classification_loss: 0.0227
 441/1000 [============>.................] - ETA: 21:00 - loss: 0.3002 - regression_loss: 0.2774 - classification_loss: 0.0228
 442/1000 [============>.................] - ETA: 20:57 - loss: 0.2996 - regression_loss: 0.2768 - classification_loss: 0.0227
 443/1000 [============>.................] - ETA: 20:54 - loss: 0.2993 - regression_loss: 0.2766 - classification_loss: 0.0227
 444/1000 [============>.................] - ETA: 20:52 - loss: 0.2995 - regression_loss: 0.2768 - classification_loss: 0.0227
 445/1000 [============>.................] - ETA: 20:50 - loss: 0.2994 - regression_loss: 0.2767 - classification_loss: 0.0227
 446/1000 [============>.................] - ETA: 20:47 - loss: 0.2989 - regression_loss: 0.2762 - classification_loss: 0.0227
 447/1000 [============>.................] - ETA: 20:45 - loss: 0.2989 - regression_loss: 0.2762 - classification_loss: 0.0227
 448/1000 [============>.................] - ETA: 20:43 - loss: 0.2990 - regression_loss: 0.2763 - classification_loss: 0.0227
 449/1000 [============>.................] - ETA: 20:41 - loss: 0.2993 - regression_loss: 0.2766 - classification_loss: 0.0227
 450/1000 [============>.................] - ETA: 20:39 - loss: 0.2994 - regression_loss: 0.2766 - classification_loss: 0.0227
 451/1000 [============>.................] - ETA: 20:36 - loss: 0.2988 - regression_loss: 0.2761 - classification_loss: 0.0227
 452/1000 [============>.................] - ETA: 20:34 - loss: 0.2990 - regression_loss: 0.2763 - classification_loss: 0.0227
 453/1000 [============>.................] - ETA: 20:33 - loss: 0.2992 - regression_loss: 0.2765 - classification_loss: 0.0227
 454/1000 [============>.................] - ETA: 20:30 - loss: 0.2992 - regression_loss: 0.2765 - classification_loss: 0.0227
 455/1000 [============>.................] - ETA: 20:28 - loss: 0.2991 - regression_loss: 0.2764 - classification_loss: 0.0227
 456/1000 [============>.................] - ETA: 20:25 - loss: 0.2987 - regression_loss: 0.2760 - classification_loss: 0.0227
 457/1000 [============>.................] - ETA: 20:23 - loss: 0.2983 - regression_loss: 0.2757 - classification_loss: 0.0227
 458/1000 [============>.................] - ETA: 20:21 - loss: 0.2986 - regression_loss: 0.2759 - classification_loss: 0.0227
 459/1000 [============>.................] - ETA: 20:19 - loss: 0.2986 - regression_loss: 0.2759 - classification_loss: 0.0227
 460/1000 [============>.................] - ETA: 20:17 - loss: 0.2988 - regression_loss: 0.2761 - classification_loss: 0.0227
 461/1000 [============>.................] - ETA: 20:15 - loss: 0.2987 - regression_loss: 0.2760 - classification_loss: 0.0227
 462/1000 [============>.................] - ETA: 20:12 - loss: 0.2982 - regression_loss: 0.2755 - classification_loss: 0.0227
 463/1000 [============>.................] - ETA: 20:10 - loss: 0.2983 - regression_loss: 0.2756 - classification_loss: 0.0227
 464/1000 [============>.................] - ETA: 20:08 - loss: 0.2986 - regression_loss: 0.2759 - classification_loss: 0.0227
 465/1000 [============>.................] - ETA: 20:06 - loss: 0.2987 - regression_loss: 0.2760 - classification_loss: 0.0227
 466/1000 [============>.................] - ETA: 20:03 - loss: 0.2988 - regression_loss: 0.2761 - classification_loss: 0.0227
 467/1000 [=============>................] - ETA: 20:02 - loss: 0.2991 - regression_loss: 0.2764 - classification_loss: 0.0227
 468/1000 [=============>................] - ETA: 19:58 - loss: 0.2985 - regression_loss: 0.2759 - classification_loss: 0.0227
 469/1000 [=============>................] - ETA: 19:56 - loss: 0.2986 - regression_loss: 0.2759 - classification_loss: 0.0227
 470/1000 [=============>................] - ETA: 19:54 - loss: 0.2984 - regression_loss: 0.2758 - classification_loss: 0.0227
 471/1000 [=============>................] - ETA: 19:51 - loss: 0.2979 - regression_loss: 0.2753 - classification_loss: 0.0226
 472/1000 [=============>................] - ETA: 19:48 - loss: 0.2978 - regression_loss: 0.2752 - classification_loss: 0.0226
 473/1000 [=============>................] - ETA: 19:46 - loss: 0.2977 - regression_loss: 0.2751 - classification_loss: 0.0226
 474/1000 [=============>................] - ETA: 19:44 - loss: 0.2979 - regression_loss: 0.2752 - classification_loss: 0.0226
 475/1000 [=============>................] - ETA: 19:43 - loss: 0.2981 - regression_loss: 0.2754 - classification_loss: 0.0226
 476/1000 [=============>................] - ETA: 19:41 - loss: 0.2981 - regression_loss: 0.2755 - classification_loss: 0.0226
 477/1000 [=============>................] - ETA: 19:38 - loss: 0.2981 - regression_loss: 0.2755 - classification_loss: 0.0226
 478/1000 [=============>................] - ETA: 19:35 - loss: 0.2978 - regression_loss: 0.2752 - classification_loss: 0.0226
 479/1000 [=============>................] - ETA: 19:33 - loss: 0.2980 - regression_loss: 0.2753 - classification_loss: 0.0226
 480/1000 [=============>................] - ETA: 19:32 - loss: 0.2982 - regression_loss: 0.2755 - classification_loss: 0.0226
 481/1000 [=============>................] - ETA: 19:30 - loss: 0.2980 - regression_loss: 0.2754 - classification_loss: 0.0226
 482/1000 [=============>................] - ETA: 19:27 - loss: 0.2980 - regression_loss: 0.2754 - classification_loss: 0.0226
 483/1000 [=============>................] - ETA: 19:24 - loss: 0.2975 - regression_loss: 0.2749 - classification_loss: 0.0226
 484/1000 [=============>................] - ETA: 19:22 - loss: 0.2977 - regression_loss: 0.2751 - classification_loss: 0.0226
 485/1000 [=============>................] - ETA: 19:20 - loss: 0.2977 - regression_loss: 0.2751 - classification_loss: 0.0226
 486/1000 [=============>................] - ETA: 19:18 - loss: 0.2979 - regression_loss: 0.2753 - classification_loss: 0.0226
 487/1000 [=============>................] - ETA: 19:16 - loss: 0.2978 - regression_loss: 0.2752 - classification_loss: 0.0226
 488/1000 [=============>................] - ETA: 19:13 - loss: 0.2972 - regression_loss: 0.2746 - classification_loss: 0.0226
 489/1000 [=============>................] - ETA: 19:11 - loss: 0.2974 - regression_loss: 0.2748 - classification_loss: 0.0226
 490/1000 [=============>................] - ETA: 19:09 - loss: 0.2975 - regression_loss: 0.2749 - classification_loss: 0.0226
 491/1000 [=============>................] - ETA: 19:07 - loss: 0.2972 - regression_loss: 0.2746 - classification_loss: 0.0226
 492/1000 [=============>................] - ETA: 19:04 - loss: 0.2967 - regression_loss: 0.2741 - classification_loss: 0.0225
 493/1000 [=============>................] - ETA: 19:02 - loss: 0.2966 - regression_loss: 0.2740 - classification_loss: 0.0225
 494/1000 [=============>................] - ETA: 19:00 - loss: 0.2968 - regression_loss: 0.2742 - classification_loss: 0.0226
 495/1000 [=============>................] - ETA: 18:57 - loss: 0.2968 - regression_loss: 0.2742 - classification_loss: 0.0226
 496/1000 [=============>................] - ETA: 18:55 - loss: 0.2970 - regression_loss: 0.2744 - classification_loss: 0.0226
 497/1000 [=============>................] - ETA: 18:53 - loss: 0.2966 - regression_loss: 0.2741 - classification_loss: 0.0225
 498/1000 [=============>................] - ETA: 18:51 - loss: 0.2967 - regression_loss: 0.2742 - classification_loss: 0.0226
 499/1000 [=============>................] - ETA: 18:49 - loss: 0.2970 - regression_loss: 0.2744 - classification_loss: 0.0226
 500/1000 [==============>...............] - ETA: 18:47 - loss: 0.2971 - regression_loss: 0.2746 - classification_loss: 0.0226
 501/1000 [==============>...............] - ETA: 18:45 - loss: 0.2972 - regression_loss: 0.2747 - classification_loss: 0.0226
 502/1000 [==============>...............] - ETA: 18:42 - loss: 0.2967 - regression_loss: 0.2742 - classification_loss: 0.0225
 503/1000 [==============>...............] - ETA: 18:40 - loss: 0.2966 - regression_loss: 0.2741 - classification_loss: 0.0225
 504/1000 [==============>...............] - ETA: 18:38 - loss: 0.2967 - regression_loss: 0.2741 - classification_loss: 0.0225
 505/1000 [==============>...............] - ETA: 18:35 - loss: 0.2965 - regression_loss: 0.2740 - classification_loss: 0.0225
 506/1000 [==============>...............] - ETA: 18:33 - loss: 0.2966 - regression_loss: 0.2740 - classification_loss: 0.0225
 507/1000 [==============>...............] - ETA: 18:31 - loss: 0.2967 - regression_loss: 0.2741 - classification_loss: 0.0225
 508/1000 [==============>...............] - ETA: 18:28 - loss: 0.2965 - regression_loss: 0.2740 - classification_loss: 0.0225
 509/1000 [==============>...............] - ETA: 18:27 - loss: 0.2968 - regression_loss: 0.2742 - classification_loss: 0.0225
 510/1000 [==============>...............] - ETA: 18:24 - loss: 0.2965 - regression_loss: 0.2739 - classification_loss: 0.0225
 511/1000 [==============>...............] - ETA: 18:21 - loss: 0.2960 - regression_loss: 0.2735 - classification_loss: 0.0225
 512/1000 [==============>...............] - ETA: 18:19 - loss: 0.2962 - regression_loss: 0.2737 - classification_loss: 0.0225
 513/1000 [==============>...............] - ETA: 18:16 - loss: 0.2959 - regression_loss: 0.2734 - classification_loss: 0.0225
 514/1000 [==============>...............] - ETA: 18:14 - loss: 0.2961 - regression_loss: 0.2736 - classification_loss: 0.0225
 515/1000 [==============>...............] - ETA: 18:12 - loss: 0.2961 - regression_loss: 0.2736 - classification_loss: 0.0225
 516/1000 [==============>...............] - ETA: 18:10 - loss: 0.2960 - regression_loss: 0.2735 - classification_loss: 0.0225
 517/1000 [==============>...............] - ETA: 18:07 - loss: 0.2955 - regression_loss: 0.2730 - classification_loss: 0.0225
 518/1000 [==============>...............] - ETA: 18:05 - loss: 0.2956 - regression_loss: 0.2731 - classification_loss: 0.0225
 519/1000 [==============>...............] - ETA: 18:03 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0225
 520/1000 [==============>...............] - ETA: 18:01 - loss: 0.2958 - regression_loss: 0.2733 - classification_loss: 0.0225
 521/1000 [==============>...............] - ETA: 17:59 - loss: 0.2958 - regression_loss: 0.2733 - classification_loss: 0.0225
 522/1000 [==============>...............] - ETA: 17:57 - loss: 0.2960 - regression_loss: 0.2735 - classification_loss: 0.0225
 523/1000 [==============>...............] - ETA: 17:55 - loss: 0.2958 - regression_loss: 0.2733 - classification_loss: 0.0225
 524/1000 [==============>...............] - ETA: 17:52 - loss: 0.2953 - regression_loss: 0.2728 - classification_loss: 0.0225
 525/1000 [==============>...............] - ETA: 17:49 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 526/1000 [==============>...............] - ETA: 17:48 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 527/1000 [==============>...............] - ETA: 17:45 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 528/1000 [==============>...............] - ETA: 17:43 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 529/1000 [==============>...............] - ETA: 17:41 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 530/1000 [==============>...............] - ETA: 17:38 - loss: 0.2953 - regression_loss: 0.2728 - classification_loss: 0.0224
 531/1000 [==============>...............] - ETA: 17:37 - loss: 0.2954 - regression_loss: 0.2729 - classification_loss: 0.0224
 532/1000 [==============>...............] - ETA: 17:35 - loss: 0.2956 - regression_loss: 0.2731 - classification_loss: 0.0225
 533/1000 [==============>...............] - ETA: 17:32 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 534/1000 [===============>..............] - ETA: 17:29 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 535/1000 [===============>..............] - ETA: 17:28 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 536/1000 [===============>..............] - ETA: 17:25 - loss: 0.2949 - regression_loss: 0.2725 - classification_loss: 0.0224
 537/1000 [===============>..............] - ETA: 17:22 - loss: 0.2947 - regression_loss: 0.2723 - classification_loss: 0.0224
 538/1000 [===============>..............] - ETA: 17:20 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 539/1000 [===============>..............] - ETA: 17:18 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 540/1000 [===============>..............] - ETA: 17:16 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 541/1000 [===============>..............] - ETA: 17:14 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 542/1000 [===============>..............] - ETA: 17:12 - loss: 0.2955 - regression_loss: 0.2730 - classification_loss: 0.0224
 543/1000 [===============>..............] - ETA: 17:10 - loss: 0.2956 - regression_loss: 0.2732 - classification_loss: 0.0224
 544/1000 [===============>..............] - ETA: 17:08 - loss: 0.2956 - regression_loss: 0.2732 - classification_loss: 0.0224
 545/1000 [===============>..............] - ETA: 17:06 - loss: 0.2955 - regression_loss: 0.2730 - classification_loss: 0.0224
 546/1000 [===============>..............] - ETA: 17:03 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 547/1000 [===============>..............] - ETA: 17:00 - loss: 0.2948 - regression_loss: 0.2724 - classification_loss: 0.0224
 548/1000 [===============>..............] - ETA: 16:58 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 549/1000 [===============>..............] - ETA: 16:56 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 550/1000 [===============>..............] - ETA: 16:54 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 551/1000 [===============>..............] - ETA: 16:52 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 552/1000 [===============>..............] - ETA: 16:49 - loss: 0.2947 - regression_loss: 0.2723 - classification_loss: 0.0224
 553/1000 [===============>..............] - ETA: 16:47 - loss: 0.2949 - regression_loss: 0.2725 - classification_loss: 0.0224
 554/1000 [===============>..............] - ETA: 16:44 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 555/1000 [===============>..............] - ETA: 16:42 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 556/1000 [===============>..............] - ETA: 16:40 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 557/1000 [===============>..............] - ETA: 16:38 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 558/1000 [===============>..............] - ETA: 16:35 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0224
 559/1000 [===============>..............] - ETA: 16:33 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 560/1000 [===============>..............] - ETA: 16:30 - loss: 0.2949 - regression_loss: 0.2725 - classification_loss: 0.0224
 561/1000 [===============>..............] - ETA: 16:29 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 562/1000 [===============>..............] - ETA: 16:26 - loss: 0.2947 - regression_loss: 0.2724 - classification_loss: 0.0223
 563/1000 [===============>..............] - ETA: 16:24 - loss: 0.2948 - regression_loss: 0.2725 - classification_loss: 0.0223
 564/1000 [===============>..............] - ETA: 16:22 - loss: 0.2950 - regression_loss: 0.2727 - classification_loss: 0.0224
 565/1000 [===============>..............] - ETA: 16:20 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0224
 566/1000 [===============>..............] - ETA: 16:18 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 567/1000 [================>.............] - ETA: 16:15 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 568/1000 [================>.............] - ETA: 16:13 - loss: 0.2949 - regression_loss: 0.2726 - classification_loss: 0.0224
 569/1000 [================>.............] - ETA: 16:10 - loss: 0.2950 - regression_loss: 0.2726 - classification_loss: 0.0224
 570/1000 [================>.............] - ETA: 16:08 - loss: 0.2947 - regression_loss: 0.2723 - classification_loss: 0.0223
 571/1000 [================>.............] - ETA: 16:06 - loss: 0.2949 - regression_loss: 0.2725 - classification_loss: 0.0224
 572/1000 [================>.............] - ETA: 16:04 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 573/1000 [================>.............] - ETA: 16:02 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 574/1000 [================>.............] - ETA: 16:00 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 575/1000 [================>.............] - ETA: 15:57 - loss: 0.2948 - regression_loss: 0.2725 - classification_loss: 0.0224
 576/1000 [================>.............] - ETA: 15:55 - loss: 0.2950 - regression_loss: 0.2727 - classification_loss: 0.0224
 577/1000 [================>.............] - ETA: 15:53 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 578/1000 [================>.............] - ETA: 15:51 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 579/1000 [================>.............] - ETA: 15:49 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 580/1000 [================>.............] - ETA: 15:47 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 581/1000 [================>.............] - ETA: 15:44 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0224
 582/1000 [================>.............] - ETA: 15:41 - loss: 0.2947 - regression_loss: 0.2724 - classification_loss: 0.0224
 583/1000 [================>.............] - ETA: 15:38 - loss: 0.2943 - regression_loss: 0.2720 - classification_loss: 0.0223
 584/1000 [================>.............] - ETA: 15:36 - loss: 0.2941 - regression_loss: 0.2718 - classification_loss: 0.0223
 585/1000 [================>.............] - ETA: 15:34 - loss: 0.2944 - regression_loss: 0.2721 - classification_loss: 0.0223
 586/1000 [================>.............] - ETA: 15:32 - loss: 0.2945 - regression_loss: 0.2722 - classification_loss: 0.0223
 587/1000 [================>.............] - ETA: 15:29 - loss: 0.2945 - regression_loss: 0.2722 - classification_loss: 0.0223
 588/1000 [================>.............] - ETA: 15:27 - loss: 0.2947 - regression_loss: 0.2723 - classification_loss: 0.0224
 589/1000 [================>.............] - ETA: 15:25 - loss: 0.2949 - regression_loss: 0.2725 - classification_loss: 0.0224
 590/1000 [================>.............] - ETA: 15:24 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 591/1000 [================>.............] - ETA: 15:21 - loss: 0.2950 - regression_loss: 0.2727 - classification_loss: 0.0224
 592/1000 [================>.............] - ETA: 15:19 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 593/1000 [================>.............] - ETA: 15:16 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 594/1000 [================>.............] - ETA: 15:14 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 595/1000 [================>.............] - ETA: 15:12 - loss: 0.2958 - regression_loss: 0.2734 - classification_loss: 0.0224
 596/1000 [================>.............] - ETA: 15:10 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 597/1000 [================>.............] - ETA: 15:08 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0224
 598/1000 [================>.............] - ETA: 15:06 - loss: 0.2959 - regression_loss: 0.2735 - classification_loss: 0.0224
 599/1000 [================>.............] - ETA: 15:03 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 600/1000 [=================>............] - ETA: 15:01 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 601/1000 [=================>............] - ETA: 14:59 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 602/1000 [=================>............] - ETA: 14:56 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0224
 603/1000 [=================>............] - ETA: 14:54 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 604/1000 [=================>............] - ETA: 14:52 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0224
 605/1000 [=================>............] - ETA: 14:49 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 606/1000 [=================>............] - ETA: 14:47 - loss: 0.2951 - regression_loss: 0.2727 - classification_loss: 0.0224
 607/1000 [=================>............] - ETA: 14:45 - loss: 0.2953 - regression_loss: 0.2729 - classification_loss: 0.0224
 608/1000 [=================>............] - ETA: 14:43 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0224
 609/1000 [=================>............] - ETA: 14:40 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 610/1000 [=================>............] - ETA: 14:38 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 611/1000 [=================>............] - ETA: 14:36 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0224
 612/1000 [=================>............] - ETA: 14:34 - loss: 0.2959 - regression_loss: 0.2735 - classification_loss: 0.0224
 613/1000 [=================>............] - ETA: 14:31 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 614/1000 [=================>............] - ETA: 14:29 - loss: 0.2955 - regression_loss: 0.2731 - classification_loss: 0.0224
 615/1000 [=================>............] - ETA: 14:27 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0224
 616/1000 [=================>............] - ETA: 14:25 - loss: 0.2956 - regression_loss: 0.2732 - classification_loss: 0.0224
 617/1000 [=================>............] - ETA: 14:22 - loss: 0.2957 - regression_loss: 0.2733 - classification_loss: 0.0224
 618/1000 [=================>............] - ETA: 14:20 - loss: 0.2958 - regression_loss: 0.2735 - classification_loss: 0.0224
 619/1000 [=================>............] - ETA: 14:18 - loss: 0.2958 - regression_loss: 0.2735 - classification_loss: 0.0224
 620/1000 [=================>............] - ETA: 14:15 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0223
 621/1000 [=================>............] - ETA: 14:13 - loss: 0.2955 - regression_loss: 0.2732 - classification_loss: 0.0224
 622/1000 [=================>............] - ETA: 14:11 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 623/1000 [=================>............] - ETA: 14:09 - loss: 0.2955 - regression_loss: 0.2732 - classification_loss: 0.0224
 624/1000 [=================>............] - ETA: 14:07 - loss: 0.2954 - regression_loss: 0.2731 - classification_loss: 0.0224
 625/1000 [=================>............] - ETA: 14:04 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 626/1000 [=================>............] - ETA: 14:02 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0223
 627/1000 [=================>............] - ETA: 14:00 - loss: 0.2956 - regression_loss: 0.2732 - classification_loss: 0.0224
 628/1000 [=================>............] - ETA: 13:58 - loss: 0.2957 - regression_loss: 0.2734 - classification_loss: 0.0224
 629/1000 [=================>............] - ETA: 13:55 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 630/1000 [=================>............] - ETA: 13:53 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 631/1000 [=================>............] - ETA: 13:51 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 632/1000 [=================>............] - ETA: 13:49 - loss: 0.2955 - regression_loss: 0.2732 - classification_loss: 0.0223
 633/1000 [=================>............] - ETA: 13:46 - loss: 0.2951 - regression_loss: 0.2728 - classification_loss: 0.0223
 634/1000 [==================>...........] - ETA: 13:44 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 635/1000 [==================>...........] - ETA: 13:42 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 636/1000 [==================>...........] - ETA: 13:39 - loss: 0.2952 - regression_loss: 0.2728 - classification_loss: 0.0223
 637/1000 [==================>...........] - ETA: 13:37 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 638/1000 [==================>...........] - ETA: 13:35 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 639/1000 [==================>...........] - ETA: 13:33 - loss: 0.2955 - regression_loss: 0.2732 - classification_loss: 0.0223
 640/1000 [==================>...........] - ETA: 13:30 - loss: 0.2951 - regression_loss: 0.2728 - classification_loss: 0.0223
 641/1000 [==================>...........] - ETA: 13:28 - loss: 0.2949 - regression_loss: 0.2726 - classification_loss: 0.0223
 642/1000 [==================>...........] - ETA: 13:26 - loss: 0.2951 - regression_loss: 0.2728 - classification_loss: 0.0223
 643/1000 [==================>...........] - ETA: 13:24 - loss: 0.2951 - regression_loss: 0.2728 - classification_loss: 0.0223
 644/1000 [==================>...........] - ETA: 13:21 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 645/1000 [==================>...........] - ETA: 13:19 - loss: 0.2951 - regression_loss: 0.2728 - classification_loss: 0.0223
 646/1000 [==================>...........] - ETA: 13:17 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 647/1000 [==================>...........] - ETA: 13:15 - loss: 0.2953 - regression_loss: 0.2730 - classification_loss: 0.0223
 648/1000 [==================>...........] - ETA: 13:13 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 649/1000 [==================>...........] - ETA: 13:11 - loss: 0.2952 - regression_loss: 0.2729 - classification_loss: 0.0223
 650/1000 [==================>...........] - ETA: 13:09 - loss: 0.2954 - regression_loss: 0.2730 - classification_loss: 0.0223
 651/1000 [==================>...........] - ETA: 13:06 - loss: 0.2950 - regression_loss: 0.2727 - classification_loss: 0.0223
 652/1000 [==================>...........] - ETA: 13:04 - loss: 0.2948 - regression_loss: 0.2726 - classification_loss: 0.0223
 653/1000 [==================>...........] - ETA: 13:01 - loss: 0.2948 - regression_loss: 0.2725 - classification_loss: 0.0223
 654/1000 [==================>...........] - ETA: 12:59 - loss: 0.2949 - regression_loss: 0.2726 - classification_loss: 0.0223
 655/1000 [==================>...........] - ETA: 12:57 - loss: 0.2950 - regression_loss: 0.2727 - classification_loss: 0.0223
 656/1000 [==================>...........] - ETA: 12:55 - loss: 0.2947 - regression_loss: 0.2725 - classification_loss: 0.0223
 657/1000 [==================>...........] - ETA: 12:53 - loss: 0.2949 - regression_loss: 0.2726 - classification_loss: 0.0223
 658/1000 [==================>...........] - ETA: 12:50 - loss: 0.2949 - regression_loss: 0.2726 - classification_loss: 0.0223
 659/1000 [==================>...........] - ETA: 12:48 - loss: 0.2945 - regression_loss: 0.2722 - classification_loss: 0.0223
 660/1000 [==================>...........] - ETA: 12:46 - loss: 0.2944 - regression_loss: 0.2722 - classification_loss: 0.0223
 661/1000 [==================>...........] - ETA: 12:44 - loss: 0.2946 - regression_loss: 0.2723 - classification_loss: 0.0223
 662/1000 [==================>...........] - ETA: 12:41 - loss: 0.2946 - regression_loss: 0.2723 - classification_loss: 0.0223
 663/1000 [==================>...........] - ETA: 12:39 - loss: 0.2942 - regression_loss: 0.2719 - classification_loss: 0.0222
 664/1000 [==================>...........] - ETA: 12:37 - loss: 0.2943 - regression_loss: 0.2721 - classification_loss: 0.0222
 665/1000 [==================>...........] - ETA: 12:35 - loss: 0.2945 - regression_loss: 0.2722 - classification_loss: 0.0222
 666/1000 [==================>...........] - ETA: 12:32 - loss: 0.2944 - regression_loss: 0.2721 - classification_loss: 0.0222
 667/1000 [===================>..........] - ETA: 12:30 - loss: 0.2944 - regression_loss: 0.2722 - classification_loss: 0.0222
 668/1000 [===================>..........] - ETA: 12:27 - loss: 0.2941 - regression_loss: 0.2718 - classification_loss: 0.0222
 669/1000 [===================>..........] - ETA: 12:25 - loss: 0.2939 - regression_loss: 0.2717 - classification_loss: 0.0222
 670/1000 [===================>..........] - ETA: 12:23 - loss: 0.2940 - regression_loss: 0.2718 - classification_loss: 0.0222
 671/1000 [===================>..........] - ETA: 12:21 - loss: 0.2942 - regression_loss: 0.2719 - classification_loss: 0.0222
 672/1000 [===================>..........] - ETA: 12:19 - loss: 0.2941 - regression_loss: 0.2718 - classification_loss: 0.0222
 673/1000 [===================>..........] - ETA: 12:16 - loss: 0.2941 - regression_loss: 0.2719 - classification_loss: 0.0222
 674/1000 [===================>..........] - ETA: 12:14 - loss: 0.2941 - regression_loss: 0.2719 - classification_loss: 0.0222
 675/1000 [===================>..........] - ETA: 12:12 - loss: 0.2940 - regression_loss: 0.2718 - classification_loss: 0.0222
 676/1000 [===================>..........] - ETA: 12:10 - loss: 0.2941 - regression_loss: 0.2719 - classification_loss: 0.0222
 677/1000 [===================>..........] - ETA: 12:08 - loss: 0.2942 - regression_loss: 0.2720 - classification_loss: 0.0222
 678/1000 [===================>..........] - ETA: 12:05 - loss: 0.2939 - regression_loss: 0.2717 - classification_loss: 0.0222
 679/1000 [===================>..........] - ETA: 12:03 - loss: 0.2939 - regression_loss: 0.2717 - classification_loss: 0.0222
 680/1000 [===================>..........] - ETA: 12:01 - loss: 0.2938 - regression_loss: 0.2716 - classification_loss: 0.0222
 681/1000 [===================>..........] - ETA: 11:58 - loss: 0.2939 - regression_loss: 0.2717 - classification_loss: 0.0222
 682/1000 [===================>..........] - ETA: 11:56 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0222
 683/1000 [===================>..........] - ETA: 11:54 - loss: 0.2937 - regression_loss: 0.2715 - classification_loss: 0.0222
 684/1000 [===================>..........] - ETA: 11:52 - loss: 0.2936 - regression_loss: 0.2714 - classification_loss: 0.0222
 685/1000 [===================>..........] - ETA: 11:49 - loss: 0.2934 - regression_loss: 0.2712 - classification_loss: 0.0222
 686/1000 [===================>..........] - ETA: 11:47 - loss: 0.2934 - regression_loss: 0.2712 - classification_loss: 0.0222
 687/1000 [===================>..........] - ETA: 11:45 - loss: 0.2936 - regression_loss: 0.2714 - classification_loss: 0.0222
 688/1000 [===================>..........] - ETA: 11:43 - loss: 0.2937 - regression_loss: 0.2715 - classification_loss: 0.0222
 689/1000 [===================>..........] - ETA: 11:41 - loss: 0.2936 - regression_loss: 0.2714 - classification_loss: 0.0222
 690/1000 [===================>..........] - ETA: 11:39 - loss: 0.2938 - regression_loss: 0.2716 - classification_loss: 0.0222
 691/1000 [===================>..........] - ETA: 11:36 - loss: 0.2936 - regression_loss: 0.2714 - classification_loss: 0.0222
 692/1000 [===================>..........] - ETA: 11:34 - loss: 0.2937 - regression_loss: 0.2715 - classification_loss: 0.0222
 693/1000 [===================>..........] - ETA: 11:31 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 694/1000 [===================>..........] - ETA: 11:29 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 695/1000 [===================>..........] - ETA: 11:27 - loss: 0.2935 - regression_loss: 0.2713 - classification_loss: 0.0221
 696/1000 [===================>..........] - ETA: 11:25 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0221
 697/1000 [===================>..........] - ETA: 11:22 - loss: 0.2931 - regression_loss: 0.2710 - classification_loss: 0.0221
 698/1000 [===================>..........] - ETA: 11:20 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 699/1000 [===================>..........] - ETA: 11:18 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 700/1000 [====================>.........] - ETA: 11:15 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0221
 701/1000 [====================>.........] - ETA: 11:13 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 702/1000 [====================>.........] - ETA: 11:11 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 703/1000 [====================>.........] - ETA: 11:08 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 704/1000 [====================>.........] - ETA: 11:06 - loss: 0.2926 - regression_loss: 0.2705 - classification_loss: 0.0221
 705/1000 [====================>.........] - ETA: 11:04 - loss: 0.2925 - regression_loss: 0.2704 - classification_loss: 0.0221
 706/1000 [====================>.........] - ETA: 11:02 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 707/1000 [====================>.........] - ETA: 11:00 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 708/1000 [====================>.........] - ETA: 10:57 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0221
 709/1000 [====================>.........] - ETA: 10:55 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 710/1000 [====================>.........] - ETA: 10:53 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 711/1000 [====================>.........] - ETA: 10:51 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 712/1000 [====================>.........] - ETA: 10:49 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0221
 713/1000 [====================>.........] - ETA: 10:46 - loss: 0.2926 - regression_loss: 0.2705 - classification_loss: 0.0221
 714/1000 [====================>.........] - ETA: 10:44 - loss: 0.2926 - regression_loss: 0.2705 - classification_loss: 0.0221
 715/1000 [====================>.........] - ETA: 10:42 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 716/1000 [====================>.........] - ETA: 10:39 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 717/1000 [====================>.........] - ETA: 10:37 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 718/1000 [====================>.........] - ETA: 10:35 - loss: 0.2932 - regression_loss: 0.2711 - classification_loss: 0.0221
 719/1000 [====================>.........] - ETA: 10:33 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 720/1000 [====================>.........] - ETA: 10:31 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 721/1000 [====================>.........] - ETA: 10:28 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 722/1000 [====================>.........] - ETA: 10:26 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 723/1000 [====================>.........] - ETA: 10:24 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0221
 724/1000 [====================>.........] - ETA: 10:21 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0221
 725/1000 [====================>.........] - ETA: 10:19 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0221
 726/1000 [====================>.........] - ETA: 10:17 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 727/1000 [====================>.........] - ETA: 10:15 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 728/1000 [====================>.........] - ETA: 10:12 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0221
 729/1000 [====================>.........] - ETA: 10:10 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 730/1000 [====================>.........] - ETA: 10:08 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0221
 731/1000 [====================>.........] - ETA: 10:06 - loss: 0.2924 - regression_loss: 0.2704 - classification_loss: 0.0220
 732/1000 [====================>.........] - ETA: 10:03 - loss: 0.2924 - regression_loss: 0.2704 - classification_loss: 0.0220
 733/1000 [====================>.........] - ETA: 10:01 - loss: 0.2925 - regression_loss: 0.2705 - classification_loss: 0.0220
 734/1000 [=====================>........] - ETA: 9:59 - loss: 0.2926 - regression_loss: 0.2705 - classification_loss: 0.0220 
 735/1000 [=====================>........] - ETA: 9:57 - loss: 0.2927 - regression_loss: 0.2707 - classification_loss: 0.0220
 736/1000 [=====================>........] - ETA: 9:54 - loss: 0.2928 - regression_loss: 0.2708 - classification_loss: 0.0221
 737/1000 [=====================>........] - ETA: 9:52 - loss: 0.2925 - regression_loss: 0.2704 - classification_loss: 0.0220
 738/1000 [=====================>........] - ETA: 9:50 - loss: 0.2927 - regression_loss: 0.2706 - classification_loss: 0.0220
 739/1000 [=====================>........] - ETA: 9:48 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0220
 740/1000 [=====================>........] - ETA: 9:45 - loss: 0.2927 - regression_loss: 0.2707 - classification_loss: 0.0220
 741/1000 [=====================>........] - ETA: 9:43 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0220
 742/1000 [=====================>........] - ETA: 9:41 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 743/1000 [=====================>........] - ETA: 9:39 - loss: 0.2928 - regression_loss: 0.2708 - classification_loss: 0.0220
 744/1000 [=====================>........] - ETA: 9:36 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0220
 745/1000 [=====================>........] - ETA: 9:34 - loss: 0.2929 - regression_loss: 0.2708 - classification_loss: 0.0220
 746/1000 [=====================>........] - ETA: 9:32 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0220
 747/1000 [=====================>........] - ETA: 9:30 - loss: 0.2931 - regression_loss: 0.2710 - classification_loss: 0.0220
 748/1000 [=====================>........] - ETA: 9:28 - loss: 0.2930 - regression_loss: 0.2710 - classification_loss: 0.0220
 749/1000 [=====================>........] - ETA: 9:25 - loss: 0.2927 - regression_loss: 0.2707 - classification_loss: 0.0220
 750/1000 [=====================>........] - ETA: 9:23 - loss: 0.2929 - regression_loss: 0.2709 - classification_loss: 0.0220
 751/1000 [=====================>........] - ETA: 9:21 - loss: 0.2931 - regression_loss: 0.2710 - classification_loss: 0.0221
 752/1000 [=====================>........] - ETA: 9:19 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 753/1000 [=====================>........] - ETA: 9:16 - loss: 0.2934 - regression_loss: 0.2713 - classification_loss: 0.0221
 754/1000 [=====================>........] - ETA: 9:14 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0221
 755/1000 [=====================>........] - ETA: 9:12 - loss: 0.2932 - regression_loss: 0.2711 - classification_loss: 0.0220
 756/1000 [=====================>........] - ETA: 9:09 - loss: 0.2931 - regression_loss: 0.2711 - classification_loss: 0.0221
 757/1000 [=====================>........] - ETA: 9:07 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 758/1000 [=====================>........] - ETA: 9:05 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0221
 759/1000 [=====================>........] - ETA: 9:02 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0221
 760/1000 [=====================>........] - ETA: 9:00 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0221
 761/1000 [=====================>........] - ETA: 8:58 - loss: 0.2931 - regression_loss: 0.2710 - classification_loss: 0.0220
 762/1000 [=====================>........] - ETA: 8:56 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0221
 763/1000 [=====================>........] - ETA: 8:54 - loss: 0.2932 - regression_loss: 0.2711 - classification_loss: 0.0221
 764/1000 [=====================>........] - ETA: 8:51 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 765/1000 [=====================>........] - ETA: 8:49 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 766/1000 [=====================>........] - ETA: 8:47 - loss: 0.2927 - regression_loss: 0.2707 - classification_loss: 0.0220
 767/1000 [======================>.......] - ETA: 8:44 - loss: 0.2928 - regression_loss: 0.2707 - classification_loss: 0.0220
 768/1000 [======================>.......] - ETA: 8:42 - loss: 0.2929 - regression_loss: 0.2709 - classification_loss: 0.0221
 769/1000 [======================>.......] - ETA: 8:40 - loss: 0.2930 - regression_loss: 0.2710 - classification_loss: 0.0221
 770/1000 [======================>.......] - ETA: 8:38 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 771/1000 [======================>.......] - ETA: 8:36 - loss: 0.2931 - regression_loss: 0.2711 - classification_loss: 0.0221
 772/1000 [======================>.......] - ETA: 8:34 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0221
 773/1000 [======================>.......] - ETA: 8:31 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0220
 774/1000 [======================>.......] - ETA: 8:29 - loss: 0.2930 - regression_loss: 0.2709 - classification_loss: 0.0221
 775/1000 [======================>.......] - ETA: 8:26 - loss: 0.2928 - regression_loss: 0.2708 - classification_loss: 0.0220
 776/1000 [======================>.......] - ETA: 8:24 - loss: 0.2929 - regression_loss: 0.2709 - classification_loss: 0.0221
 777/1000 [======================>.......] - ETA: 8:22 - loss: 0.2931 - regression_loss: 0.2710 - classification_loss: 0.0221
 778/1000 [======================>.......] - ETA: 8:20 - loss: 0.2933 - regression_loss: 0.2712 - classification_loss: 0.0221
 779/1000 [======================>.......] - ETA: 8:18 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0221
 780/1000 [======================>.......] - ETA: 8:16 - loss: 0.2935 - regression_loss: 0.2715 - classification_loss: 0.0221
 781/1000 [======================>.......] - ETA: 8:13 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0221
 782/1000 [======================>.......] - ETA: 8:11 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0221
 783/1000 [======================>.......] - ETA: 8:09 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0221
 784/1000 [======================>.......] - ETA: 8:06 - loss: 0.2932 - regression_loss: 0.2711 - classification_loss: 0.0220
 785/1000 [======================>.......] - ETA: 8:04 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0221
 786/1000 [======================>.......] - ETA: 8:02 - loss: 0.2935 - regression_loss: 0.2715 - classification_loss: 0.0221
 787/1000 [======================>.......] - ETA: 8:00 - loss: 0.2937 - regression_loss: 0.2717 - classification_loss: 0.0221
 788/1000 [======================>.......] - ETA: 7:58 - loss: 0.2939 - regression_loss: 0.2719 - classification_loss: 0.0221
 789/1000 [======================>.......] - ETA: 7:55 - loss: 0.2936 - regression_loss: 0.2716 - classification_loss: 0.0220
 790/1000 [======================>.......] - ETA: 7:53 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0220
 791/1000 [======================>.......] - ETA: 7:50 - loss: 0.2935 - regression_loss: 0.2714 - classification_loss: 0.0220
 792/1000 [======================>.......] - ETA: 7:48 - loss: 0.2937 - regression_loss: 0.2717 - classification_loss: 0.0220
 793/1000 [======================>.......] - ETA: 7:46 - loss: 0.2935 - regression_loss: 0.2715 - classification_loss: 0.0220
 794/1000 [======================>.......] - ETA: 7:44 - loss: 0.2937 - regression_loss: 0.2716 - classification_loss: 0.0220
 795/1000 [======================>.......] - ETA: 7:42 - loss: 0.2936 - regression_loss: 0.2716 - classification_loss: 0.0220
 796/1000 [======================>.......] - ETA: 7:39 - loss: 0.2937 - regression_loss: 0.2717 - classification_loss: 0.0220
 797/1000 [======================>.......] - ETA: 7:37 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0220
 798/1000 [======================>.......] - ETA: 7:35 - loss: 0.2935 - regression_loss: 0.2715 - classification_loss: 0.0220
 799/1000 [======================>.......] - ETA: 7:33 - loss: 0.2935 - regression_loss: 0.2715 - classification_loss: 0.0220
 800/1000 [=======================>......] - ETA: 7:30 - loss: 0.2937 - regression_loss: 0.2716 - classification_loss: 0.0220
 801/1000 [=======================>......] - ETA: 7:28 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0220
 802/1000 [=======================>......] - ETA: 7:26 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0220
 803/1000 [=======================>......] - ETA: 7:24 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0220
 804/1000 [=======================>......] - ETA: 7:21 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0220
 805/1000 [=======================>......] - ETA: 7:19 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0220
 806/1000 [=======================>......] - ETA: 7:17 - loss: 0.2933 - regression_loss: 0.2713 - classification_loss: 0.0220
 807/1000 [=======================>......] - ETA: 7:15 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0220
 808/1000 [=======================>......] - ETA: 7:12 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0220
 809/1000 [=======================>......] - ETA: 7:10 - loss: 0.2934 - regression_loss: 0.2714 - classification_loss: 0.0220
 810/1000 [=======================>......] - ETA: 7:08 - loss: 0.2936 - regression_loss: 0.2715 - classification_loss: 0.0220
 811/1000 [=======================>......] - ETA: 7:06 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0220
 812/1000 [=======================>......] - ETA: 7:03 - loss: 0.2930 - regression_loss: 0.2711 - classification_loss: 0.0220
 813/1000 [=======================>......] - ETA: 7:01 - loss: 0.2930 - regression_loss: 0.2710 - classification_loss: 0.0220
 814/1000 [=======================>......] - ETA: 6:59 - loss: 0.2930 - regression_loss: 0.2710 - classification_loss: 0.0220
 815/1000 [=======================>......] - ETA: 6:57 - loss: 0.2930 - regression_loss: 0.2710 - classification_loss: 0.0220
 816/1000 [=======================>......] - ETA: 6:54 - loss: 0.2929 - regression_loss: 0.2710 - classification_loss: 0.0220
 817/1000 [=======================>......] - ETA: 6:52 - loss: 0.2930 - regression_loss: 0.2711 - classification_loss: 0.0220
 818/1000 [=======================>......] - ETA: 6:50 - loss: 0.2932 - regression_loss: 0.2712 - classification_loss: 0.0220
 819/1000 [=======================>......] - ETA: 6:48 - loss: 0.2928 - regression_loss: 0.2709 - classification_loss: 0.0220
 820/1000 [=======================>......] - ETA: 6:45 - loss: 0.2926 - regression_loss: 0.2707 - classification_loss: 0.0220
 821/1000 [=======================>......] - ETA: 6:43 - loss: 0.2928 - regression_loss: 0.2708 - classification_loss: 0.0220
 822/1000 [=======================>......] - ETA: 6:41 - loss: 0.2929 - regression_loss: 0.2709 - classification_loss: 0.0220
 823/1000 [=======================>......] - ETA: 6:38 - loss: 0.2927 - regression_loss: 0.2707 - classification_loss: 0.0220
 824/1000 [=======================>......] - ETA: 6:36 - loss: 0.2925 - regression_loss: 0.2705 - classification_loss: 0.0219
 825/1000 [=======================>......] - ETA: 6:34 - loss: 0.2924 - regression_loss: 0.2705 - classification_loss: 0.0219
 826/1000 [=======================>......] - ETA: 6:32 - loss: 0.2925 - regression_loss: 0.2706 - classification_loss: 0.0220
 827/1000 [=======================>......] - ETA: 6:29 - loss: 0.2925 - regression_loss: 0.2706 - classification_loss: 0.0219
 828/1000 [=======================>......] - ETA: 6:27 - loss: 0.2924 - regression_loss: 0.2704 - classification_loss: 0.0219
 829/1000 [=======================>......] - ETA: 6:25 - loss: 0.2924 - regression_loss: 0.2705 - classification_loss: 0.0219
 830/1000 [=======================>......] - ETA: 6:23 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0219
 831/1000 [=======================>......] - ETA: 6:20 - loss: 0.2922 - regression_loss: 0.2703 - classification_loss: 0.0219
 832/1000 [=======================>......] - ETA: 6:18 - loss: 0.2923 - regression_loss: 0.2704 - classification_loss: 0.0219
 833/1000 [=======================>......] - ETA: 6:16 - loss: 0.2924 - regression_loss: 0.2705 - classification_loss: 0.0219
 834/1000 [========================>.....] - ETA: 6:14 - loss: 0.2923 - regression_loss: 0.2704 - classification_loss: 0.0219
 835/1000 [========================>.....] - ETA: 6:11 - loss: 0.2924 - regression_loss: 0.2705 - classification_loss: 0.0219
 836/1000 [========================>.....] - ETA: 6:09 - loss: 0.2926 - regression_loss: 0.2706 - classification_loss: 0.0219
 837/1000 [========================>.....] - ETA: 6:07 - loss: 0.2926 - regression_loss: 0.2707 - classification_loss: 0.0219
 838/1000 [========================>.....] - ETA: 6:05 - loss: 0.2925 - regression_loss: 0.2705 - classification_loss: 0.0219
 839/1000 [========================>.....] - ETA: 6:02 - loss: 0.2925 - regression_loss: 0.2705 - classification_loss: 0.0219
 840/1000 [========================>.....] - ETA: 6:00 - loss: 0.2921 - regression_loss: 0.2702 - classification_loss: 0.0219
 841/1000 [========================>.....] - ETA: 5:58 - loss: 0.2921 - regression_loss: 0.2702 - classification_loss: 0.0219
 842/1000 [========================>.....] - ETA: 5:56 - loss: 0.2920 - regression_loss: 0.2701 - classification_loss: 0.0219
 843/1000 [========================>.....] - ETA: 5:53 - loss: 0.2921 - regression_loss: 0.2702 - classification_loss: 0.0219
 844/1000 [========================>.....] - ETA: 5:51 - loss: 0.2920 - regression_loss: 0.2701 - classification_loss: 0.0219
 845/1000 [========================>.....] - ETA: 5:49 - loss: 0.2921 - regression_loss: 0.2702 - classification_loss: 0.0219
 846/1000 [========================>.....] - ETA: 5:47 - loss: 0.2922 - regression_loss: 0.2703 - classification_loss: 0.0219
 847/1000 [========================>.....] - ETA: 5:44 - loss: 0.2922 - regression_loss: 0.2703 - classification_loss: 0.0219
 848/1000 [========================>.....] - ETA: 5:42 - loss: 0.2920 - regression_loss: 0.2701 - classification_loss: 0.0219
 849/1000 [========================>.....] - ETA: 5:40 - loss: 0.2919 - regression_loss: 0.2700 - classification_loss: 0.0219
 850/1000 [========================>.....] - ETA: 5:38 - loss: 0.2919 - regression_loss: 0.2701 - classification_loss: 0.0219
 851/1000 [========================>.....] - ETA: 5:35 - loss: 0.2921 - regression_loss: 0.2702 - classification_loss: 0.0219
 852/1000 [========================>.....] - ETA: 5:33 - loss: 0.2918 - regression_loss: 0.2699 - classification_loss: 0.0219
 853/1000 [========================>.....] - ETA: 5:31 - loss: 0.2917 - regression_loss: 0.2698 - classification_loss: 0.0219
 854/1000 [========================>.....] - ETA: 5:29 - loss: 0.2918 - regression_loss: 0.2699 - classification_loss: 0.0219
 855/1000 [========================>.....] - ETA: 5:26 - loss: 0.2919 - regression_loss: 0.2700 - classification_loss: 0.0219
 856/1000 [========================>.....] - ETA: 5:24 - loss: 0.2916 - regression_loss: 0.2697 - classification_loss: 0.0219
 857/1000 [========================>.....] - ETA: 5:22 - loss: 0.2917 - regression_loss: 0.2698 - classification_loss: 0.0219
 858/1000 [========================>.....] - ETA: 5:20 - loss: 0.2916 - regression_loss: 0.2697 - classification_loss: 0.0219
 859/1000 [========================>.....] - ETA: 5:17 - loss: 0.2916 - regression_loss: 0.2697 - classification_loss: 0.0219
 860/1000 [========================>.....] - ETA: 5:15 - loss: 0.2917 - regression_loss: 0.2698 - classification_loss: 0.0219
 861/1000 [========================>.....] - ETA: 5:13 - loss: 0.2915 - regression_loss: 0.2696 - classification_loss: 0.0219
 862/1000 [========================>.....] - ETA: 5:11 - loss: 0.2916 - regression_loss: 0.2697 - classification_loss: 0.0219
 863/1000 [========================>.....] - ETA: 5:08 - loss: 0.2916 - regression_loss: 0.2698 - classification_loss: 0.0219
 864/1000 [========================>.....] - ETA: 5:06 - loss: 0.2917 - regression_loss: 0.2698 - classification_loss: 0.0219
 865/1000 [========================>.....] - ETA: 5:04 - loss: 0.2915 - regression_loss: 0.2697 - classification_loss: 0.0219
 866/1000 [========================>.....] - ETA: 5:02 - loss: 0.2917 - regression_loss: 0.2698 - classification_loss: 0.0219
 867/1000 [=========================>....] - ETA: 4:59 - loss: 0.2914 - regression_loss: 0.2695 - classification_loss: 0.0218
 868/1000 [=========================>....] - ETA: 4:57 - loss: 0.2914 - regression_loss: 0.2696 - classification_loss: 0.0218
 869/1000 [=========================>....] - ETA: 4:55 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 870/1000 [=========================>....] - ETA: 4:53 - loss: 0.2913 - regression_loss: 0.2694 - classification_loss: 0.0218
 871/1000 [=========================>....] - ETA: 4:50 - loss: 0.2914 - regression_loss: 0.2695 - classification_loss: 0.0219
 872/1000 [=========================>....] - ETA: 4:48 - loss: 0.2914 - regression_loss: 0.2695 - classification_loss: 0.0219
 873/1000 [=========================>....] - ETA: 4:46 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 874/1000 [=========================>....] - ETA: 4:43 - loss: 0.2909 - regression_loss: 0.2690 - classification_loss: 0.0218
 875/1000 [=========================>....] - ETA: 4:41 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 876/1000 [=========================>....] - ETA: 4:39 - loss: 0.2912 - regression_loss: 0.2693 - classification_loss: 0.0218
 877/1000 [=========================>....] - ETA: 4:37 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 878/1000 [=========================>....] - ETA: 4:35 - loss: 0.2914 - regression_loss: 0.2696 - classification_loss: 0.0218
 879/1000 [=========================>....] - ETA: 4:32 - loss: 0.2912 - regression_loss: 0.2693 - classification_loss: 0.0218
 880/1000 [=========================>....] - ETA: 4:30 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 881/1000 [=========================>....] - ETA: 4:28 - loss: 0.2911 - regression_loss: 0.2692 - classification_loss: 0.0218
 882/1000 [=========================>....] - ETA: 4:25 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 883/1000 [=========================>....] - ETA: 4:23 - loss: 0.2912 - regression_loss: 0.2694 - classification_loss: 0.0218
 884/1000 [=========================>....] - ETA: 4:21 - loss: 0.2912 - regression_loss: 0.2694 - classification_loss: 0.0218
 885/1000 [=========================>....] - ETA: 4:19 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 886/1000 [=========================>....] - ETA: 4:16 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 887/1000 [=========================>....] - ETA: 4:14 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 888/1000 [=========================>....] - ETA: 4:12 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 889/1000 [=========================>....] - ETA: 4:10 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 890/1000 [=========================>....] - ETA: 4:07 - loss: 0.2912 - regression_loss: 0.2694 - classification_loss: 0.0218
 891/1000 [=========================>....] - ETA: 4:05 - loss: 0.2914 - regression_loss: 0.2695 - classification_loss: 0.0218
 892/1000 [=========================>....] - ETA: 4:03 - loss: 0.2915 - regression_loss: 0.2696 - classification_loss: 0.0218
 893/1000 [=========================>....] - ETA: 4:01 - loss: 0.2915 - regression_loss: 0.2697 - classification_loss: 0.0218
 894/1000 [=========================>....] - ETA: 3:59 - loss: 0.2915 - regression_loss: 0.2696 - classification_loss: 0.0218
 895/1000 [=========================>....] - ETA: 3:56 - loss: 0.2915 - regression_loss: 0.2697 - classification_loss: 0.0218
 896/1000 [=========================>....] - ETA: 3:54 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 897/1000 [=========================>....] - ETA: 3:52 - loss: 0.2912 - regression_loss: 0.2694 - classification_loss: 0.0218
 898/1000 [=========================>....] - ETA: 3:49 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 899/1000 [=========================>....] - ETA: 3:47 - loss: 0.2914 - regression_loss: 0.2696 - classification_loss: 0.0218
 900/1000 [==========================>...] - ETA: 3:45 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 901/1000 [==========================>...] - ETA: 3:43 - loss: 0.2913 - regression_loss: 0.2695 - classification_loss: 0.0218
 902/1000 [==========================>...] - ETA: 3:40 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 903/1000 [==========================>...] - ETA: 3:38 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 904/1000 [==========================>...] - ETA: 3:36 - loss: 0.2909 - regression_loss: 0.2692 - classification_loss: 0.0218
 905/1000 [==========================>...] - ETA: 3:34 - loss: 0.2910 - regression_loss: 0.2693 - classification_loss: 0.0218
 906/1000 [==========================>...] - ETA: 3:31 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 907/1000 [==========================>...] - ETA: 3:29 - loss: 0.2910 - regression_loss: 0.2692 - classification_loss: 0.0218
 908/1000 [==========================>...] - ETA: 3:27 - loss: 0.2911 - regression_loss: 0.2693 - classification_loss: 0.0218
 909/1000 [==========================>...] - ETA: 3:25 - loss: 0.2909 - regression_loss: 0.2691 - classification_loss: 0.0218
 910/1000 [==========================>...] - ETA: 3:22 - loss: 0.2907 - regression_loss: 0.2689 - classification_loss: 0.0218
 911/1000 [==========================>...] - ETA: 3:20 - loss: 0.2907 - regression_loss: 0.2689 - classification_loss: 0.0218
 912/1000 [==========================>...] - ETA: 3:18 - loss: 0.2908 - regression_loss: 0.2690 - classification_loss: 0.0218
 913/1000 [==========================>...] - ETA: 3:16 - loss: 0.2908 - regression_loss: 0.2690 - classification_loss: 0.0218
 914/1000 [==========================>...] - ETA: 3:13 - loss: 0.2907 - regression_loss: 0.2689 - classification_loss: 0.0218
 915/1000 [==========================>...] - ETA: 3:11 - loss: 0.2905 - regression_loss: 0.2688 - classification_loss: 0.0218
 916/1000 [==========================>...] - ETA: 3:09 - loss: 0.2907 - regression_loss: 0.2689 - classification_loss: 0.0218
 917/1000 [==========================>...] - ETA: 3:07 - loss: 0.2907 - regression_loss: 0.2689 - classification_loss: 0.0218
 918/1000 [==========================>...] - ETA: 3:04 - loss: 0.2904 - regression_loss: 0.2687 - classification_loss: 0.0218
 919/1000 [==========================>...] - ETA: 3:02 - loss: 0.2905 - regression_loss: 0.2687 - classification_loss: 0.0218
 920/1000 [==========================>...] - ETA: 3:00 - loss: 0.2904 - regression_loss: 0.2686 - classification_loss: 0.0217
 921/1000 [==========================>...] - ETA: 2:58 - loss: 0.2903 - regression_loss: 0.2686 - classification_loss: 0.0217
 922/1000 [==========================>...] - ETA: 2:55 - loss: 0.2900 - regression_loss: 0.2683 - classification_loss: 0.0217
 923/1000 [==========================>...] - ETA: 2:53 - loss: 0.2901 - regression_loss: 0.2684 - classification_loss: 0.0217
 924/1000 [==========================>...] - ETA: 2:51 - loss: 0.2902 - regression_loss: 0.2685 - classification_loss: 0.0217
 925/1000 [==========================>...] - ETA: 2:49 - loss: 0.2902 - regression_loss: 0.2685 - classification_loss: 0.0217
 926/1000 [==========================>...] - ETA: 2:46 - loss: 0.2899 - regression_loss: 0.2682 - classification_loss: 0.0217
 927/1000 [==========================>...] - ETA: 2:44 - loss: 0.2899 - regression_loss: 0.2682 - classification_loss: 0.0217
 928/1000 [==========================>...] - ETA: 2:42 - loss: 0.2900 - regression_loss: 0.2683 - classification_loss: 0.0217
 929/1000 [==========================>...] - ETA: 2:40 - loss: 0.2898 - regression_loss: 0.2681 - classification_loss: 0.0217
 930/1000 [==========================>...] - ETA: 2:37 - loss: 0.2900 - regression_loss: 0.2682 - classification_loss: 0.0217
 931/1000 [==========================>...] - ETA: 2:35 - loss: 0.2900 - regression_loss: 0.2683 - classification_loss: 0.0217
 932/1000 [==========================>...] - ETA: 2:33 - loss: 0.2901 - regression_loss: 0.2683 - classification_loss: 0.0217
 933/1000 [==========================>...] - ETA: 2:31 - loss: 0.2901 - regression_loss: 0.2684 - classification_loss: 0.0217
 934/1000 [===========================>..] - ETA: 2:28 - loss: 0.2899 - regression_loss: 0.2682 - classification_loss: 0.0217
 935/1000 [===========================>..] - ETA: 2:26 - loss: 0.2901 - regression_loss: 0.2683 - classification_loss: 0.0217
 936/1000 [===========================>..] - ETA: 2:24 - loss: 0.2900 - regression_loss: 0.2683 - classification_loss: 0.0217
 937/1000 [===========================>..] - ETA: 2:22 - loss: 0.2901 - regression_loss: 0.2684 - classification_loss: 0.0217
 938/1000 [===========================>..] - ETA: 2:19 - loss: 0.2901 - regression_loss: 0.2684 - classification_loss: 0.0217
 939/1000 [===========================>..] - ETA: 2:17 - loss: 0.2899 - regression_loss: 0.2682 - classification_loss: 0.0217
 940/1000 [===========================>..] - ETA: 2:15 - loss: 0.2899 - regression_loss: 0.2682 - classification_loss: 0.0217
 941/1000 [===========================>..] - ETA: 2:12 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 942/1000 [===========================>..] - ETA: 2:10 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 943/1000 [===========================>..] - ETA: 2:08 - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0217
 944/1000 [===========================>..] - ETA: 2:06 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 945/1000 [===========================>..] - ETA: 2:03 - loss: 0.2895 - regression_loss: 0.2678 - classification_loss: 0.0217
 946/1000 [===========================>..] - ETA: 2:01 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 947/1000 [===========================>..] - ETA: 1:59 - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0217
 948/1000 [===========================>..] - ETA: 1:57 - loss: 0.2898 - regression_loss: 0.2681 - classification_loss: 0.0217
 949/1000 [===========================>..] - ETA: 1:54 - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0217
 950/1000 [===========================>..] - ETA: 1:52 - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0217
 951/1000 [===========================>..] - ETA: 1:50 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 952/1000 [===========================>..] - ETA: 1:48 - loss: 0.2896 - regression_loss: 0.2679 - classification_loss: 0.0217
 953/1000 [===========================>..] - ETA: 1:45 - loss: 0.2893 - regression_loss: 0.2677 - classification_loss: 0.0217
 954/1000 [===========================>..] - ETA: 1:43 - loss: 0.2893 - regression_loss: 0.2677 - classification_loss: 0.0217
 955/1000 [===========================>..] - ETA: 1:41 - loss: 0.2894 - regression_loss: 0.2678 - classification_loss: 0.0217
 956/1000 [===========================>..] - ETA: 1:39 - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0217
 957/1000 [===========================>..] - ETA: 1:36 - loss: 0.2893 - regression_loss: 0.2676 - classification_loss: 0.0217
 958/1000 [===========================>..] - ETA: 1:34 - loss: 0.2892 - regression_loss: 0.2676 - classification_loss: 0.0217
 959/1000 [===========================>..] - ETA: 1:32 - loss: 0.2891 - regression_loss: 0.2674 - classification_loss: 0.0216
 960/1000 [===========================>..] - ETA: 1:30 - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 961/1000 [===========================>..] - ETA: 1:27 - loss: 0.2890 - regression_loss: 0.2674 - classification_loss: 0.0216
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.2890 - regression_loss: 0.2674 - classification_loss: 0.0216
 963/1000 [===========================>..] - ETA: 1:23 - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 964/1000 [===========================>..] - ETA: 1:21 - loss: 0.2892 - regression_loss: 0.2676 - classification_loss: 0.0216
 965/1000 [===========================>..] - ETA: 1:18 - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.2892 - regression_loss: 0.2675 - classification_loss: 0.0217
 967/1000 [============================>.] - ETA: 1:14 - loss: 0.2889 - regression_loss: 0.2673 - classification_loss: 0.0216
 968/1000 [============================>.] - ETA: 1:12 - loss: 0.2886 - regression_loss: 0.2670 - classification_loss: 0.0216
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.2886 - regression_loss: 0.2670 - classification_loss: 0.0216
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.2888 - regression_loss: 0.2672 - classification_loss: 0.0216
 971/1000 [============================>.] - ETA: 1:05 - loss: 0.2889 - regression_loss: 0.2673 - classification_loss: 0.0216
 972/1000 [============================>.] - ETA: 1:03 - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 974/1000 [============================>.] - ETA: 58s - loss: 0.2892 - regression_loss: 0.2676 - classification_loss: 0.0216 
 975/1000 [============================>.] - ETA: 56s - loss: 0.2890 - regression_loss: 0.2674 - classification_loss: 0.0216
 976/1000 [============================>.] - ETA: 54s - loss: 0.2891 - regression_loss: 0.2675 - classification_loss: 0.0216
 977/1000 [============================>.] - ETA: 51s - loss: 0.2893 - regression_loss: 0.2677 - classification_loss: 0.0216
 978/1000 [============================>.] - ETA: 49s - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0216
 979/1000 [============================>.] - ETA: 47s - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0217
 980/1000 [============================>.] - ETA: 45s - loss: 0.2894 - regression_loss: 0.2678 - classification_loss: 0.0216
 981/1000 [============================>.] - ETA: 42s - loss: 0.2895 - regression_loss: 0.2678 - classification_loss: 0.0216
 982/1000 [============================>.] - ETA: 40s - loss: 0.2896 - regression_loss: 0.2680 - classification_loss: 0.0216
 983/1000 [============================>.] - ETA: 38s - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0216
 984/1000 [============================>.] - ETA: 36s - loss: 0.2894 - regression_loss: 0.2678 - classification_loss: 0.0216
 985/1000 [============================>.] - ETA: 33s - loss: 0.2895 - regression_loss: 0.2678 - classification_loss: 0.0216
 986/1000 [============================>.] - ETA: 31s - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0217
 987/1000 [============================>.] - ETA: 29s - loss: 0.2898 - regression_loss: 0.2681 - classification_loss: 0.0217
 988/1000 [============================>.] - ETA: 27s - loss: 0.2897 - regression_loss: 0.2681 - classification_loss: 0.0216
 989/1000 [============================>.] - ETA: 24s - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0216
 990/1000 [============================>.] - ETA: 22s - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0216
 991/1000 [============================>.] - ETA: 20s - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0216
 992/1000 [============================>.] - ETA: 18s - loss: 0.2896 - regression_loss: 0.2680 - classification_loss: 0.0216
 993/1000 [============================>.] - ETA: 15s - loss: 0.2897 - regression_loss: 0.2681 - classification_loss: 0.0216
 994/1000 [============================>.] - ETA: 13s - loss: 0.2898 - regression_loss: 0.2681 - classification_loss: 0.0216
 995/1000 [============================>.] - ETA: 11s - loss: 0.2897 - regression_loss: 0.2681 - classification_loss: 0.0216
 996/1000 [============================>.] - ETA: 9s - loss: 0.2894 - regression_loss: 0.2678 - classification_loss: 0.0216 
 997/1000 [============================>.] - ETA: 6s - loss: 0.2894 - regression_loss: 0.2678 - classification_loss: 0.0216
 998/1000 [============================>.] - ETA: 4s - loss: 0.2895 - regression_loss: 0.2679 - classification_loss: 0.0216
 999/1000 [============================>.] - ETA: 2s - loss: 0.2897 - regression_loss: 0.2680 - classification_loss: 0.0216
1000/1000 [==============================] - 2254s 2s/step - loss: 0.2896 - regression_loss: 0.2680 - classification_loss: 0.0216

Epoch 00009: saving model to ./snapshots/resnet50_csv_09.h5
Epoch 10/10

   1/1000 [..............................] - ETA: 27:24 - loss: 0.2621 - regression_loss: 0.2409 - classification_loss: 0.0212
   2/1000 [..............................] - ETA: 36:19 - loss: 0.3218 - regression_loss: 0.2983 - classification_loss: 0.0235
   3/1000 [..............................] - ETA: 36:10 - loss: 0.3128 - regression_loss: 0.2904 - classification_loss: 0.0225
   4/1000 [..............................] - ETA: 34:15 - loss: 0.2808 - regression_loss: 0.2610 - classification_loss: 0.0199
   5/1000 [..............................] - ETA: 35:28 - loss: 0.2789 - regression_loss: 0.2586 - classification_loss: 0.0203
   6/1000 [..............................] - ETA: 37:01 - loss: 0.3087 - regression_loss: 0.2871 - classification_loss: 0.0216
   7/1000 [..............................] - ETA: 35:25 - loss: 0.2780 - regression_loss: 0.2588 - classification_loss: 0.0192
   8/1000 [..............................] - ETA: 36:10 - loss: 0.2863 - regression_loss: 0.2665 - classification_loss: 0.0198
   9/1000 [..............................] - ETA: 36:54 - loss: 0.2968 - regression_loss: 0.2764 - classification_loss: 0.0204
  10/1000 [..............................] - ETA: 37:20 - loss: 0.3017 - regression_loss: 0.2811 - classification_loss: 0.0206
  11/1000 [..............................] - ETA: 37:09 - loss: 0.3014 - regression_loss: 0.2809 - classification_loss: 0.0205
  12/1000 [..............................] - ETA: 36:24 - loss: 0.2933 - regression_loss: 0.2734 - classification_loss: 0.0198
  13/1000 [..............................] - ETA: 36:51 - loss: 0.3026 - regression_loss: 0.2823 - classification_loss: 0.0204
  14/1000 [..............................] - ETA: 37:03 - loss: 0.2979 - regression_loss: 0.2774 - classification_loss: 0.0205
  15/1000 [..............................] - ETA: 37:31 - loss: 0.3042 - regression_loss: 0.2832 - classification_loss: 0.0210
  16/1000 [..............................] - ETA: 36:44 - loss: 0.2868 - regression_loss: 0.2669 - classification_loss: 0.0200
  17/1000 [..............................] - ETA: 36:04 - loss: 0.2710 - regression_loss: 0.2519 - classification_loss: 0.0191
  18/1000 [..............................] - ETA: 36:14 - loss: 0.2706 - regression_loss: 0.2514 - classification_loss: 0.0192
  19/1000 [..............................] - ETA: 36:38 - loss: 0.2772 - regression_loss: 0.2575 - classification_loss: 0.0197
  20/1000 [..............................] - ETA: 36:49 - loss: 0.2802 - regression_loss: 0.2603 - classification_loss: 0.0199
  21/1000 [..............................] - ETA: 36:23 - loss: 0.2776 - regression_loss: 0.2581 - classification_loss: 0.0195
  22/1000 [..............................] - ETA: 36:36 - loss: 0.2829 - regression_loss: 0.2631 - classification_loss: 0.0197
  23/1000 [..............................] - ETA: 36:29 - loss: 0.2832 - regression_loss: 0.2635 - classification_loss: 0.0198
  24/1000 [..............................] - ETA: 36:00 - loss: 0.2762 - regression_loss: 0.2571 - classification_loss: 0.0191
  25/1000 [..............................] - ETA: 36:09 - loss: 0.2824 - regression_loss: 0.2630 - classification_loss: 0.0194
  26/1000 [..............................] - ETA: 35:48 - loss: 0.2788 - regression_loss: 0.2596 - classification_loss: 0.0192
  27/1000 [..............................] - ETA: 35:54 - loss: 0.2798 - regression_loss: 0.2605 - classification_loss: 0.0193
  28/1000 [..............................] - ETA: 35:49 - loss: 0.2812 - regression_loss: 0.2619 - classification_loss: 0.0193
  29/1000 [..............................] - ETA: 36:03 - loss: 0.2847 - regression_loss: 0.2651 - classification_loss: 0.0196
  30/1000 [..............................] - ETA: 36:14 - loss: 0.2886 - regression_loss: 0.2688 - classification_loss: 0.0198
  31/1000 [..............................] - ETA: 36:22 - loss: 0.2930 - regression_loss: 0.2730 - classification_loss: 0.0200
  32/1000 [..............................] - ETA: 36:33 - loss: 0.2967 - regression_loss: 0.2765 - classification_loss: 0.0202
  33/1000 [..............................] - ETA: 36:10 - loss: 0.2935 - regression_loss: 0.2738 - classification_loss: 0.0197
  34/1000 [>.............................] - ETA: 35:53 - loss: 0.2900 - regression_loss: 0.2705 - classification_loss: 0.0195
  35/1000 [>.............................] - ETA: 35:49 - loss: 0.2907 - regression_loss: 0.2712 - classification_loss: 0.0195
  36/1000 [>.............................] - ETA: 35:55 - loss: 0.2941 - regression_loss: 0.2745 - classification_loss: 0.0196
  37/1000 [>.............................] - ETA: 35:58 - loss: 0.2942 - regression_loss: 0.2745 - classification_loss: 0.0197
  38/1000 [>.............................] - ETA: 36:05 - loss: 0.2974 - regression_loss: 0.2775 - classification_loss: 0.0199
  39/1000 [>.............................] - ETA: 35:46 - loss: 0.2908 - regression_loss: 0.2713 - classification_loss: 0.0195
  40/1000 [>.............................] - ETA: 35:48 - loss: 0.2906 - regression_loss: 0.2710 - classification_loss: 0.0196
  41/1000 [>.............................] - ETA: 35:44 - loss: 0.2931 - regression_loss: 0.2734 - classification_loss: 0.0197
  42/1000 [>.............................] - ETA: 35:49 - loss: 0.2953 - regression_loss: 0.2755 - classification_loss: 0.0199
  43/1000 [>.............................] - ETA: 35:35 - loss: 0.2919 - regression_loss: 0.2722 - classification_loss: 0.0197
  44/1000 [>.............................] - ETA: 35:43 - loss: 0.2941 - regression_loss: 0.2743 - classification_loss: 0.0199
  45/1000 [>.............................] - ETA: 35:51 - loss: 0.2962 - regression_loss: 0.2762 - classification_loss: 0.0200
  46/1000 [>.............................] - ETA: 35:38 - loss: 0.2930 - regression_loss: 0.2731 - classification_loss: 0.0198
  47/1000 [>.............................] - ETA: 35:22 - loss: 0.2881 - regression_loss: 0.2686 - classification_loss: 0.0195
  48/1000 [>.............................] - ETA: 35:19 - loss: 0.2891 - regression_loss: 0.2696 - classification_loss: 0.0195
  49/1000 [>.............................] - ETA: 35:22 - loss: 0.2906 - regression_loss: 0.2709 - classification_loss: 0.0196
  50/1000 [>.............................] - ETA: 35:27 - loss: 0.2926 - regression_loss: 0.2728 - classification_loss: 0.0198
  51/1000 [>.............................] - ETA: 35:29 - loss: 0.2913 - regression_loss: 0.2715 - classification_loss: 0.0198
  52/1000 [>.............................] - ETA: 35:14 - loss: 0.2867 - regression_loss: 0.2672 - classification_loss: 0.0195
  53/1000 [>.............................] - ETA: 35:18 - loss: 0.2890 - regression_loss: 0.2693 - classification_loss: 0.0197
  54/1000 [>.............................] - ETA: 35:21 - loss: 0.2900 - regression_loss: 0.2702 - classification_loss: 0.0197
  55/1000 [>.............................] - ETA: 35:17 - loss: 0.2899 - regression_loss: 0.2701 - classification_loss: 0.0198
  56/1000 [>.............................] - ETA: 35:23 - loss: 0.2917 - regression_loss: 0.2718 - classification_loss: 0.0199
  57/1000 [>.............................] - ETA: 35:13 - loss: 0.2906 - regression_loss: 0.2709 - classification_loss: 0.0197
  58/1000 [>.............................] - ETA: 35:13 - loss: 0.2895 - regression_loss: 0.2698 - classification_loss: 0.0198
  59/1000 [>.............................] - ETA: 35:16 - loss: 0.2903 - regression_loss: 0.2705 - classification_loss: 0.0198
  60/1000 [>.............................] - ETA: 35:19 - loss: 0.2919 - regression_loss: 0.2720 - classification_loss: 0.0199
  61/1000 [>.............................] - ETA: 35:06 - loss: 0.2875 - regression_loss: 0.2679 - classification_loss: 0.0197
  62/1000 [>.............................] - ETA: 35:07 - loss: 0.2865 - regression_loss: 0.2668 - classification_loss: 0.0197
  63/1000 [>.............................] - ETA: 34:57 - loss: 0.2839 - regression_loss: 0.2644 - classification_loss: 0.0196
  64/1000 [>.............................] - ETA: 34:53 - loss: 0.2839 - regression_loss: 0.2644 - classification_loss: 0.0196
  65/1000 [>.............................] - ETA: 34:58 - loss: 0.2852 - regression_loss: 0.2656 - classification_loss: 0.0197
  66/1000 [>.............................] - ETA: 34:46 - loss: 0.2814 - regression_loss: 0.2620 - classification_loss: 0.0194
  67/1000 [=>............................] - ETA: 34:48 - loss: 0.2822 - regression_loss: 0.2627 - classification_loss: 0.0195
  68/1000 [=>............................] - ETA: 34:45 - loss: 0.2829 - regression_loss: 0.2634 - classification_loss: 0.0195
  69/1000 [=>............................] - ETA: 34:46 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0195
  70/1000 [=>............................] - ETA: 34:50 - loss: 0.2837 - regression_loss: 0.2641 - classification_loss: 0.0196
  71/1000 [=>............................] - ETA: 34:41 - loss: 0.2827 - regression_loss: 0.2632 - classification_loss: 0.0195
  72/1000 [=>............................] - ETA: 34:43 - loss: 0.2842 - regression_loss: 0.2646 - classification_loss: 0.0196
  73/1000 [=>............................] - ETA: 34:32 - loss: 0.2807 - regression_loss: 0.2613 - classification_loss: 0.0194
  74/1000 [=>............................] - ETA: 34:36 - loss: 0.2822 - regression_loss: 0.2627 - classification_loss: 0.0195
  75/1000 [=>............................] - ETA: 34:34 - loss: 0.2822 - regression_loss: 0.2627 - classification_loss: 0.0195
  76/1000 [=>............................] - ETA: 34:25 - loss: 0.2804 - regression_loss: 0.2609 - classification_loss: 0.0194
  77/1000 [=>............................] - ETA: 34:26 - loss: 0.2795 - regression_loss: 0.2600 - classification_loss: 0.0195
  78/1000 [=>............................] - ETA: 34:28 - loss: 0.2807 - regression_loss: 0.2612 - classification_loss: 0.0195
  79/1000 [=>............................] - ETA: 34:29 - loss: 0.2812 - regression_loss: 0.2617 - classification_loss: 0.0196
  80/1000 [=>............................] - ETA: 34:30 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0196
  81/1000 [=>............................] - ETA: 34:21 - loss: 0.2797 - regression_loss: 0.2603 - classification_loss: 0.0194
  82/1000 [=>............................] - ETA: 34:20 - loss: 0.2790 - regression_loss: 0.2596 - classification_loss: 0.0195
  83/1000 [=>............................] - ETA: 34:17 - loss: 0.2791 - regression_loss: 0.2596 - classification_loss: 0.0195
  84/1000 [=>............................] - ETA: 34:18 - loss: 0.2797 - regression_loss: 0.2602 - classification_loss: 0.0195
  85/1000 [=>............................] - ETA: 34:10 - loss: 0.2782 - regression_loss: 0.2588 - classification_loss: 0.0194
  86/1000 [=>............................] - ETA: 34:12 - loss: 0.2793 - regression_loss: 0.2598 - classification_loss: 0.0195
  87/1000 [=>............................] - ETA: 34:03 - loss: 0.2762 - regression_loss: 0.2569 - classification_loss: 0.0193
  88/1000 [=>............................] - ETA: 33:55 - loss: 0.2745 - regression_loss: 0.2553 - classification_loss: 0.0192
  89/1000 [=>............................] - ETA: 33:56 - loss: 0.2751 - regression_loss: 0.2559 - classification_loss: 0.0193
  90/1000 [=>............................] - ETA: 33:56 - loss: 0.2745 - regression_loss: 0.2552 - classification_loss: 0.0193
  91/1000 [=>............................] - ETA: 33:58 - loss: 0.2755 - regression_loss: 0.2561 - classification_loss: 0.0194
  92/1000 [=>............................] - ETA: 34:00 - loss: 0.2767 - regression_loss: 0.2573 - classification_loss: 0.0194
  93/1000 [=>............................] - ETA: 33:57 - loss: 0.2767 - regression_loss: 0.2573 - classification_loss: 0.0194
  94/1000 [=>............................] - ETA: 33:57 - loss: 0.2772 - regression_loss: 0.2578 - classification_loss: 0.0195
  95/1000 [=>............................] - ETA: 33:48 - loss: 0.2746 - regression_loss: 0.2553 - classification_loss: 0.0193
  96/1000 [=>............................] - ETA: 33:51 - loss: 0.2758 - regression_loss: 0.2564 - classification_loss: 0.0194
  97/1000 [=>............................] - ETA: 33:50 - loss: 0.2753 - regression_loss: 0.2559 - classification_loss: 0.0194
  98/1000 [=>............................] - ETA: 33:51 - loss: 0.2763 - regression_loss: 0.2569 - classification_loss: 0.0195
  99/1000 [=>............................] - ETA: 33:48 - loss: 0.2764 - regression_loss: 0.2570 - classification_loss: 0.0195
 100/1000 [==>...........................] - ETA: 33:41 - loss: 0.2760 - regression_loss: 0.2566 - classification_loss: 0.0194
 101/1000 [==>...........................] - ETA: 33:34 - loss: 0.2751 - regression_loss: 0.2558 - classification_loss: 0.0193
 102/1000 [==>...........................] - ETA: 33:25 - loss: 0.2731 - regression_loss: 0.2540 - classification_loss: 0.0192
 103/1000 [==>...........................] - ETA: 33:25 - loss: 0.2734 - regression_loss: 0.2542 - classification_loss: 0.0192
 104/1000 [==>...........................] - ETA: 33:27 - loss: 0.2749 - regression_loss: 0.2556 - classification_loss: 0.0193
 105/1000 [==>...........................] - ETA: 33:27 - loss: 0.2757 - regression_loss: 0.2564 - classification_loss: 0.0193
 106/1000 [==>...........................] - ETA: 33:27 - loss: 0.2769 - regression_loss: 0.2575 - classification_loss: 0.0194
 107/1000 [==>...........................] - ETA: 33:24 - loss: 0.2772 - regression_loss: 0.2578 - classification_loss: 0.0194
 108/1000 [==>...........................] - ETA: 33:16 - loss: 0.2757 - regression_loss: 0.2564 - classification_loss: 0.0192
 109/1000 [==>...........................] - ETA: 33:17 - loss: 0.2766 - regression_loss: 0.2573 - classification_loss: 0.0193
 110/1000 [==>...........................] - ETA: 33:16 - loss: 0.2764 - regression_loss: 0.2571 - classification_loss: 0.0193
 111/1000 [==>...........................] - ETA: 33:17 - loss: 0.2775 - regression_loss: 0.2580 - classification_loss: 0.0194
 112/1000 [==>...........................] - ETA: 33:14 - loss: 0.2782 - regression_loss: 0.2588 - classification_loss: 0.0194
 113/1000 [==>...........................] - ETA: 33:08 - loss: 0.2784 - regression_loss: 0.2591 - classification_loss: 0.0194
 114/1000 [==>...........................] - ETA: 33:09 - loss: 0.2797 - regression_loss: 0.2602 - classification_loss: 0.0194
 115/1000 [==>...........................] - ETA: 33:08 - loss: 0.2793 - regression_loss: 0.2598 - classification_loss: 0.0195
 116/1000 [==>...........................] - ETA: 33:02 - loss: 0.2785 - regression_loss: 0.2591 - classification_loss: 0.0194
 117/1000 [==>...........................] - ETA: 33:03 - loss: 0.2797 - regression_loss: 0.2602 - classification_loss: 0.0195
 118/1000 [==>...........................] - ETA: 32:56 - loss: 0.2776 - regression_loss: 0.2583 - classification_loss: 0.0194
 119/1000 [==>...........................] - ETA: 32:53 - loss: 0.2785 - regression_loss: 0.2591 - classification_loss: 0.0194
 120/1000 [==>...........................] - ETA: 32:52 - loss: 0.2795 - regression_loss: 0.2601 - classification_loss: 0.0194
 121/1000 [==>...........................] - ETA: 32:53 - loss: 0.2805 - regression_loss: 0.2610 - classification_loss: 0.0195
 122/1000 [==>...........................] - ETA: 32:47 - loss: 0.2794 - regression_loss: 0.2600 - classification_loss: 0.0194
 123/1000 [==>...........................] - ETA: 32:46 - loss: 0.2806 - regression_loss: 0.2611 - classification_loss: 0.0195
 124/1000 [==>...........................] - ETA: 32:45 - loss: 0.2812 - regression_loss: 0.2617 - classification_loss: 0.0195
 125/1000 [==>...........................] - ETA: 32:46 - loss: 0.2830 - regression_loss: 0.2634 - classification_loss: 0.0196
 126/1000 [==>...........................] - ETA: 32:47 - loss: 0.2844 - regression_loss: 0.2647 - classification_loss: 0.0197
 127/1000 [==>...........................] - ETA: 32:40 - loss: 0.2828 - regression_loss: 0.2633 - classification_loss: 0.0196
 128/1000 [==>...........................] - ETA: 32:37 - loss: 0.2831 - regression_loss: 0.2635 - classification_loss: 0.0196
 129/1000 [==>...........................] - ETA: 32:36 - loss: 0.2836 - regression_loss: 0.2640 - classification_loss: 0.0196
 130/1000 [==>...........................] - ETA: 32:31 - loss: 0.2833 - regression_loss: 0.2637 - classification_loss: 0.0196
 131/1000 [==>...........................] - ETA: 32:28 - loss: 0.2838 - regression_loss: 0.2642 - classification_loss: 0.0196
 132/1000 [==>...........................] - ETA: 32:21 - loss: 0.2818 - regression_loss: 0.2623 - classification_loss: 0.0195
 133/1000 [==>...........................] - ETA: 32:21 - loss: 0.2824 - regression_loss: 0.2629 - classification_loss: 0.0195
 134/1000 [===>..........................] - ETA: 32:22 - loss: 0.2831 - regression_loss: 0.2636 - classification_loss: 0.0196
 135/1000 [===>..........................] - ETA: 32:22 - loss: 0.2839 - regression_loss: 0.2643 - classification_loss: 0.0196
 136/1000 [===>..........................] - ETA: 32:23 - loss: 0.2848 - regression_loss: 0.2652 - classification_loss: 0.0197
 137/1000 [===>..........................] - ETA: 32:21 - loss: 0.2846 - regression_loss: 0.2650 - classification_loss: 0.0197
 138/1000 [===>..........................] - ETA: 32:16 - loss: 0.2836 - regression_loss: 0.2640 - classification_loss: 0.0196
 139/1000 [===>..........................] - ETA: 32:16 - loss: 0.2840 - regression_loss: 0.2644 - classification_loss: 0.0196
 140/1000 [===>..........................] - ETA: 32:13 - loss: 0.2841 - regression_loss: 0.2644 - classification_loss: 0.0197
 141/1000 [===>..........................] - ETA: 32:13 - loss: 0.2852 - regression_loss: 0.2655 - classification_loss: 0.0197
 142/1000 [===>..........................] - ETA: 32:06 - loss: 0.2835 - regression_loss: 0.2639 - classification_loss: 0.0196
 143/1000 [===>..........................] - ETA: 32:04 - loss: 0.2838 - regression_loss: 0.2642 - classification_loss: 0.0196
 144/1000 [===>..........................] - ETA: 32:03 - loss: 0.2850 - regression_loss: 0.2653 - classification_loss: 0.0197
 145/1000 [===>..........................] - ETA: 32:04 - loss: 0.2858 - regression_loss: 0.2661 - classification_loss: 0.0197
 146/1000 [===>..........................] - ETA: 31:58 - loss: 0.2841 - regression_loss: 0.2645 - classification_loss: 0.0196
 147/1000 [===>..........................] - ETA: 31:53 - loss: 0.2831 - regression_loss: 0.2635 - classification_loss: 0.0196
 148/1000 [===>..........................] - ETA: 31:52 - loss: 0.2839 - regression_loss: 0.2643 - classification_loss: 0.0196
 149/1000 [===>..........................] - ETA: 31:51 - loss: 0.2838 - regression_loss: 0.2643 - classification_loss: 0.0196
 150/1000 [===>..........................] - ETA: 31:46 - loss: 0.2829 - regression_loss: 0.2634 - classification_loss: 0.0195
 151/1000 [===>..........................] - ETA: 31:44 - loss: 0.2826 - regression_loss: 0.2631 - classification_loss: 0.0196
 152/1000 [===>..........................] - ETA: 31:44 - loss: 0.2834 - regression_loss: 0.2638 - classification_loss: 0.0196
 153/1000 [===>..........................] - ETA: 31:45 - loss: 0.2839 - regression_loss: 0.2642 - classification_loss: 0.0196
 154/1000 [===>..........................] - ETA: 31:42 - loss: 0.2839 - regression_loss: 0.2643 - classification_loss: 0.0197
 155/1000 [===>..........................] - ETA: 31:36 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0196
 156/1000 [===>..........................] - ETA: 31:35 - loss: 0.2830 - regression_loss: 0.2634 - classification_loss: 0.0196
 157/1000 [===>..........................] - ETA: 31:34 - loss: 0.2835 - regression_loss: 0.2639 - classification_loss: 0.0196
 158/1000 [===>..........................] - ETA: 31:35 - loss: 0.2842 - regression_loss: 0.2645 - classification_loss: 0.0197
 159/1000 [===>..........................] - ETA: 31:29 - loss: 0.2826 - regression_loss: 0.2630 - classification_loss: 0.0196
 160/1000 [===>..........................] - ETA: 31:26 - loss: 0.2828 - regression_loss: 0.2633 - classification_loss: 0.0196
 161/1000 [===>..........................] - ETA: 31:25 - loss: 0.2825 - regression_loss: 0.2629 - classification_loss: 0.0196
 162/1000 [===>..........................] - ETA: 31:24 - loss: 0.2831 - regression_loss: 0.2635 - classification_loss: 0.0196
 163/1000 [===>..........................] - ETA: 31:20 - loss: 0.2825 - regression_loss: 0.2629 - classification_loss: 0.0196
 164/1000 [===>..........................] - ETA: 31:19 - loss: 0.2833 - regression_loss: 0.2637 - classification_loss: 0.0196
 165/1000 [===>..........................] - ETA: 31:14 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0195
 166/1000 [===>..........................] - ETA: 31:09 - loss: 0.2813 - regression_loss: 0.2618 - classification_loss: 0.0195
 167/1000 [====>.........................] - ETA: 31:08 - loss: 0.2818 - regression_loss: 0.2624 - classification_loss: 0.0195
 168/1000 [====>.........................] - ETA: 31:05 - loss: 0.2821 - regression_loss: 0.2627 - classification_loss: 0.0195
 169/1000 [====>.........................] - ETA: 31:04 - loss: 0.2819 - regression_loss: 0.2624 - classification_loss: 0.0195
 170/1000 [====>.........................] - ETA: 31:04 - loss: 0.2827 - regression_loss: 0.2632 - classification_loss: 0.0195
 171/1000 [====>.........................] - ETA: 31:04 - loss: 0.2836 - regression_loss: 0.2640 - classification_loss: 0.0196
 172/1000 [====>.........................] - ETA: 31:03 - loss: 0.2840 - regression_loss: 0.2644 - classification_loss: 0.0196
 173/1000 [====>.........................] - ETA: 30:57 - loss: 0.2826 - regression_loss: 0.2630 - classification_loss: 0.0195
 174/1000 [====>.........................] - ETA: 30:56 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0196
 175/1000 [====>.........................] - ETA: 30:55 - loss: 0.2831 - regression_loss: 0.2635 - classification_loss: 0.0196
 176/1000 [====>.........................] - ETA: 30:53 - loss: 0.2832 - regression_loss: 0.2636 - classification_loss: 0.0196
 177/1000 [====>.........................] - ETA: 30:48 - loss: 0.2828 - regression_loss: 0.2632 - classification_loss: 0.0196
 178/1000 [====>.........................] - ETA: 30:47 - loss: 0.2831 - regression_loss: 0.2635 - classification_loss: 0.0196
 179/1000 [====>.........................] - ETA: 30:44 - loss: 0.2832 - regression_loss: 0.2636 - classification_loss: 0.0196
 180/1000 [====>.........................] - ETA: 30:39 - loss: 0.2824 - regression_loss: 0.2628 - classification_loss: 0.0196
 181/1000 [====>.........................] - ETA: 30:39 - loss: 0.2829 - regression_loss: 0.2633 - classification_loss: 0.0196
 182/1000 [====>.........................] - ETA: 30:34 - loss: 0.2816 - regression_loss: 0.2621 - classification_loss: 0.0195
 183/1000 [====>.........................] - ETA: 30:33 - loss: 0.2813 - regression_loss: 0.2617 - classification_loss: 0.0195
 184/1000 [====>.........................] - ETA: 30:32 - loss: 0.2818 - regression_loss: 0.2623 - classification_loss: 0.0196
 185/1000 [====>.........................] - ETA: 30:31 - loss: 0.2821 - regression_loss: 0.2625 - classification_loss: 0.0196
 186/1000 [====>.........................] - ETA: 30:28 - loss: 0.2821 - regression_loss: 0.2625 - classification_loss: 0.0196
 187/1000 [====>.........................] - ETA: 30:28 - loss: 0.2826 - regression_loss: 0.2630 - classification_loss: 0.0196
 188/1000 [====>.........................] - ETA: 30:24 - loss: 0.2820 - regression_loss: 0.2624 - classification_loss: 0.0196
 189/1000 [====>.........................] - ETA: 30:22 - loss: 0.2820 - regression_loss: 0.2624 - classification_loss: 0.0196
 190/1000 [====>.........................] - ETA: 30:21 - loss: 0.2827 - regression_loss: 0.2631 - classification_loss: 0.0196
 191/1000 [====>.........................] - ETA: 30:16 - loss: 0.2815 - regression_loss: 0.2620 - classification_loss: 0.0195
 192/1000 [====>.........................] - ETA: 30:15 - loss: 0.2817 - regression_loss: 0.2622 - classification_loss: 0.0195
 193/1000 [====>.........................] - ETA: 30:14 - loss: 0.2823 - regression_loss: 0.2627 - classification_loss: 0.0196
 194/1000 [====>.........................] - ETA: 30:12 - loss: 0.2823 - regression_loss: 0.2628 - classification_loss: 0.0196
 195/1000 [====>.........................] - ETA: 30:12 - loss: 0.2829 - regression_loss: 0.2633 - classification_loss: 0.0196
 196/1000 [====>.........................] - ETA: 30:07 - loss: 0.2815 - regression_loss: 0.2620 - classification_loss: 0.0195
 197/1000 [====>.........................] - ETA: 30:03 - loss: 0.2808 - regression_loss: 0.2613 - classification_loss: 0.0195
 198/1000 [====>.........................] - ETA: 30:01 - loss: 0.2807 - regression_loss: 0.2612 - classification_loss: 0.0195
 199/1000 [====>.........................] - ETA: 29:56 - loss: 0.2793 - regression_loss: 0.2599 - classification_loss: 0.0194
 200/1000 [=====>........................] - ETA: 29:55 - loss: 0.2797 - regression_loss: 0.2602 - classification_loss: 0.0194
 201/1000 [=====>........................] - ETA: 29:52 - loss: 0.2797 - regression_loss: 0.2603 - classification_loss: 0.0194
 202/1000 [=====>........................] - ETA: 29:51 - loss: 0.2794 - regression_loss: 0.2599 - classification_loss: 0.0194
 203/1000 [=====>........................] - ETA: 29:47 - loss: 0.2787 - regression_loss: 0.2593 - classification_loss: 0.0194
 204/1000 [=====>........................] - ETA: 29:46 - loss: 0.2793 - regression_loss: 0.2599 - classification_loss: 0.0194
 205/1000 [=====>........................] - ETA: 29:45 - loss: 0.2800 - regression_loss: 0.2605 - classification_loss: 0.0195
 206/1000 [=====>........................] - ETA: 29:41 - loss: 0.2789 - regression_loss: 0.2595 - classification_loss: 0.0194
 207/1000 [=====>........................] - ETA: 29:39 - loss: 0.2791 - regression_loss: 0.2597 - classification_loss: 0.0194
 208/1000 [=====>........................] - ETA: 29:35 - loss: 0.2785 - regression_loss: 0.2592 - classification_loss: 0.0194
 209/1000 [=====>........................] - ETA: 29:33 - loss: 0.2785 - regression_loss: 0.2592 - classification_loss: 0.0194
 210/1000 [=====>........................] - ETA: 29:31 - loss: 0.2783 - regression_loss: 0.2589 - classification_loss: 0.0194
 211/1000 [=====>........................] - ETA: 29:31 - loss: 0.2787 - regression_loss: 0.2593 - classification_loss: 0.0194
 212/1000 [=====>........................] - ETA: 29:30 - loss: 0.2791 - regression_loss: 0.2597 - classification_loss: 0.0194
 213/1000 [=====>........................] - ETA: 29:28 - loss: 0.2794 - regression_loss: 0.2599 - classification_loss: 0.0195
 214/1000 [=====>........................] - ETA: 29:27 - loss: 0.2798 - regression_loss: 0.2603 - classification_loss: 0.0195
 215/1000 [=====>........................] - ETA: 29:27 - loss: 0.2802 - regression_loss: 0.2607 - classification_loss: 0.0195
 216/1000 [=====>........................] - ETA: 29:23 - loss: 0.2795 - regression_loss: 0.2601 - classification_loss: 0.0195
 217/1000 [=====>........................] - ETA: 29:18 - loss: 0.2784 - regression_loss: 0.2590 - classification_loss: 0.0194
 218/1000 [=====>........................] - ETA: 29:15 - loss: 0.2784 - regression_loss: 0.2590 - classification_loss: 0.0194
 219/1000 [=====>........................] - ETA: 29:14 - loss: 0.2781 - regression_loss: 0.2587 - classification_loss: 0.0194
 220/1000 [=====>........................] - ETA: 29:11 - loss: 0.2782 - regression_loss: 0.2588 - classification_loss: 0.0194
 221/1000 [=====>........................] - ETA: 29:07 - loss: 0.2770 - regression_loss: 0.2576 - classification_loss: 0.0194
 222/1000 [=====>........................] - ETA: 29:03 - loss: 0.2763 - regression_loss: 0.2570 - classification_loss: 0.0193
 223/1000 [=====>........................] - ETA: 29:02 - loss: 0.2768 - regression_loss: 0.2575 - classification_loss: 0.0194
 224/1000 [=====>........................] - ETA: 29:01 - loss: 0.2774 - regression_loss: 0.2580 - classification_loss: 0.0194
 225/1000 [=====>........................] - ETA: 29:00 - loss: 0.2772 - regression_loss: 0.2578 - classification_loss: 0.0194
 226/1000 [=====>........................] - ETA: 28:58 - loss: 0.2774 - regression_loss: 0.2579 - classification_loss: 0.0194
 227/1000 [=====>........................] - ETA: 28:58 - loss: 0.2777 - regression_loss: 0.2583 - classification_loss: 0.0195
 228/1000 [=====>........................] - ETA: 28:54 - loss: 0.2771 - regression_loss: 0.2577 - classification_loss: 0.0194
 229/1000 [=====>........................] - ETA: 28:52 - loss: 0.2769 - regression_loss: 0.2575 - classification_loss: 0.0194
 230/1000 [=====>........................] - ETA: 28:51 - loss: 0.2771 - regression_loss: 0.2577 - classification_loss: 0.0195
 231/1000 [=====>........................] - ETA: 28:46 - loss: 0.2761 - regression_loss: 0.2567 - classification_loss: 0.0194
 232/1000 [=====>........................] - ETA: 28:44 - loss: 0.2762 - regression_loss: 0.2568 - classification_loss: 0.0194
 233/1000 [=====>........................] - ETA: 28:43 - loss: 0.2767 - regression_loss: 0.2573 - classification_loss: 0.0194
 234/1000 [======>.......................] - ETA: 28:40 - loss: 0.2769 - regression_loss: 0.2574 - classification_loss: 0.0194
 235/1000 [======>.......................] - ETA: 28:38 - loss: 0.2766 - regression_loss: 0.2572 - classification_loss: 0.0194
 236/1000 [======>.......................] - ETA: 28:38 - loss: 0.2770 - regression_loss: 0.2575 - classification_loss: 0.0195
 237/1000 [======>.......................] - ETA: 28:36 - loss: 0.2772 - regression_loss: 0.2577 - classification_loss: 0.0195
 238/1000 [======>.......................] - ETA: 28:32 - loss: 0.2761 - regression_loss: 0.2567 - classification_loss: 0.0194
 239/1000 [======>.......................] - ETA: 28:31 - loss: 0.2766 - regression_loss: 0.2571 - classification_loss: 0.0194
 240/1000 [======>.......................] - ETA: 28:27 - loss: 0.2760 - regression_loss: 0.2566 - classification_loss: 0.0194
 241/1000 [======>.......................] - ETA: 28:25 - loss: 0.2760 - regression_loss: 0.2566 - classification_loss: 0.0194
 242/1000 [======>.......................] - ETA: 28:23 - loss: 0.2764 - regression_loss: 0.2570 - classification_loss: 0.0194
 243/1000 [======>.......................] - ETA: 28:22 - loss: 0.2766 - regression_loss: 0.2572 - classification_loss: 0.0194
 244/1000 [======>.......................] - ETA: 28:21 - loss: 0.2770 - regression_loss: 0.2575 - classification_loss: 0.0195
 245/1000 [======>.......................] - ETA: 28:17 - loss: 0.2760 - regression_loss: 0.2566 - classification_loss: 0.0194
 246/1000 [======>.......................] - ETA: 28:13 - loss: 0.2756 - regression_loss: 0.2562 - classification_loss: 0.0194
 247/1000 [======>.......................] - ETA: 28:11 - loss: 0.2756 - regression_loss: 0.2561 - classification_loss: 0.0194
 248/1000 [======>.......................] - ETA: 28:10 - loss: 0.2762 - regression_loss: 0.2567 - classification_loss: 0.0195
 249/1000 [======>.......................] - ETA: 28:08 - loss: 0.2763 - regression_loss: 0.2568 - classification_loss: 0.0195
 250/1000 [======>.......................] - ETA: 28:06 - loss: 0.2761 - regression_loss: 0.2566 - classification_loss: 0.0195
 251/1000 [======>.......................] - ETA: 28:05 - loss: 0.2764 - regression_loss: 0.2568 - classification_loss: 0.0195
 252/1000 [======>.......................] - ETA: 28:01 - loss: 0.2759 - regression_loss: 0.2564 - classification_loss: 0.0195
 253/1000 [======>.......................] - ETA: 28:00 - loss: 0.2763 - regression_loss: 0.2568 - classification_loss: 0.0196
 254/1000 [======>.......................] - ETA: 27:56 - loss: 0.2753 - regression_loss: 0.2558 - classification_loss: 0.0195
 255/1000 [======>.......................] - ETA: 27:55 - loss: 0.2758 - regression_loss: 0.2562 - classification_loss: 0.0195
 256/1000 [======>.......................] - ETA: 27:51 - loss: 0.2748 - regression_loss: 0.2553 - classification_loss: 0.0195
 257/1000 [======>.......................] - ETA: 27:49 - loss: 0.2746 - regression_loss: 0.2551 - classification_loss: 0.0195
 258/1000 [======>.......................] - ETA: 27:45 - loss: 0.2741 - regression_loss: 0.2546 - classification_loss: 0.0194
 259/1000 [======>.......................] - ETA: 27:44 - loss: 0.2743 - regression_loss: 0.2549 - classification_loss: 0.0195
 260/1000 [======>.......................] - ETA: 27:43 - loss: 0.2748 - regression_loss: 0.2553 - classification_loss: 0.0195
 261/1000 [======>.......................] - ETA: 27:40 - loss: 0.2748 - regression_loss: 0.2553 - classification_loss: 0.0195
 262/1000 [======>.......................] - ETA: 27:39 - loss: 0.2749 - regression_loss: 0.2554 - classification_loss: 0.0195
 263/1000 [======>.......................] - ETA: 27:36 - loss: 0.2749 - regression_loss: 0.2554 - classification_loss: 0.0195
 264/1000 [======>.......................] - ETA: 27:35 - loss: 0.2746 - regression_loss: 0.2551 - classification_loss: 0.0195
 265/1000 [======>.......................] - ETA: 27:30 - loss: 0.2737 - regression_loss: 0.2542 - classification_loss: 0.0194
 266/1000 [======>.......................] - ETA: 27:29 - loss: 0.2741 - regression_loss: 0.2547 - classification_loss: 0.0195
 267/1000 [=======>......................] - ETA: 27:25 - loss: 0.2736 - regression_loss: 0.2542 - classification_loss: 0.0194
 268/1000 [=======>......................] - ETA: 27:25 - loss: 0.2741 - regression_loss: 0.2546 - classification_loss: 0.0194
 269/1000 [=======>......................] - ETA: 27:21 - loss: 0.2735 - regression_loss: 0.2541 - classification_loss: 0.0194
 270/1000 [=======>......................] - ETA: 27:20 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0194
 271/1000 [=======>......................] - ETA: 27:18 - loss: 0.2742 - regression_loss: 0.2548 - classification_loss: 0.0195
 272/1000 [=======>......................] - ETA: 27:16 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0195
 273/1000 [=======>......................] - ETA: 27:12 - loss: 0.2731 - regression_loss: 0.2537 - classification_loss: 0.0194
 274/1000 [=======>......................] - ETA: 27:11 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 275/1000 [=======>......................] - ETA: 27:09 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 276/1000 [=======>......................] - ETA: 27:06 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 277/1000 [=======>......................] - ETA: 27:05 - loss: 0.2737 - regression_loss: 0.2543 - classification_loss: 0.0194
 278/1000 [=======>......................] - ETA: 27:04 - loss: 0.2741 - regression_loss: 0.2547 - classification_loss: 0.0194
 279/1000 [=======>......................] - ETA: 27:00 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 280/1000 [=======>......................] - ETA: 26:56 - loss: 0.2730 - regression_loss: 0.2536 - classification_loss: 0.0194
 281/1000 [=======>......................] - ETA: 26:54 - loss: 0.2729 - regression_loss: 0.2535 - classification_loss: 0.0194
 282/1000 [=======>......................] - ETA: 26:53 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 283/1000 [=======>......................] - ETA: 26:49 - loss: 0.2729 - regression_loss: 0.2536 - classification_loss: 0.0194
 284/1000 [=======>......................] - ETA: 26:48 - loss: 0.2731 - regression_loss: 0.2537 - classification_loss: 0.0194
 285/1000 [=======>......................] - ETA: 26:45 - loss: 0.2732 - regression_loss: 0.2538 - classification_loss: 0.0194
 286/1000 [=======>......................] - ETA: 26:41 - loss: 0.2724 - regression_loss: 0.2530 - classification_loss: 0.0193
 287/1000 [=======>......................] - ETA: 26:40 - loss: 0.2723 - regression_loss: 0.2529 - classification_loss: 0.0193
 288/1000 [=======>......................] - ETA: 26:38 - loss: 0.2726 - regression_loss: 0.2532 - classification_loss: 0.0194
 289/1000 [=======>......................] - ETA: 26:37 - loss: 0.2730 - regression_loss: 0.2536 - classification_loss: 0.0194
 290/1000 [=======>......................] - ETA: 26:36 - loss: 0.2732 - regression_loss: 0.2538 - classification_loss: 0.0194
 291/1000 [=======>......................] - ETA: 26:34 - loss: 0.2736 - regression_loss: 0.2542 - classification_loss: 0.0194
 292/1000 [=======>......................] - ETA: 26:32 - loss: 0.2736 - regression_loss: 0.2542 - classification_loss: 0.0194
 293/1000 [=======>......................] - ETA: 26:28 - loss: 0.2733 - regression_loss: 0.2539 - classification_loss: 0.0194
 294/1000 [=======>......................] - ETA: 26:27 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 295/1000 [=======>......................] - ETA: 26:25 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 296/1000 [=======>......................] - ETA: 26:21 - loss: 0.2731 - regression_loss: 0.2537 - classification_loss: 0.0194
 297/1000 [=======>......................] - ETA: 26:20 - loss: 0.2735 - regression_loss: 0.2541 - classification_loss: 0.0194
 298/1000 [=======>......................] - ETA: 26:17 - loss: 0.2731 - regression_loss: 0.2537 - classification_loss: 0.0194
 299/1000 [=======>......................] - ETA: 26:15 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 300/1000 [========>.....................] - ETA: 26:13 - loss: 0.2736 - regression_loss: 0.2541 - classification_loss: 0.0194
 301/1000 [========>.....................] - ETA: 26:09 - loss: 0.2728 - regression_loss: 0.2534 - classification_loss: 0.0194
 302/1000 [========>.....................] - ETA: 26:07 - loss: 0.2726 - regression_loss: 0.2532 - classification_loss: 0.0194
 303/1000 [========>.....................] - ETA: 26:06 - loss: 0.2728 - regression_loss: 0.2534 - classification_loss: 0.0194
 304/1000 [========>.....................] - ETA: 26:04 - loss: 0.2732 - regression_loss: 0.2538 - classification_loss: 0.0194
 305/1000 [========>.....................] - ETA: 26:02 - loss: 0.2732 - regression_loss: 0.2538 - classification_loss: 0.0194
 306/1000 [========>.....................] - ETA: 26:01 - loss: 0.2737 - regression_loss: 0.2543 - classification_loss: 0.0194
 307/1000 [========>.....................] - ETA: 25:57 - loss: 0.2729 - regression_loss: 0.2535 - classification_loss: 0.0194
 308/1000 [========>.....................] - ETA: 25:54 - loss: 0.2725 - regression_loss: 0.2531 - classification_loss: 0.0194
 309/1000 [========>.....................] - ETA: 25:52 - loss: 0.2729 - regression_loss: 0.2536 - classification_loss: 0.0194
 310/1000 [========>.....................] - ETA: 25:50 - loss: 0.2732 - regression_loss: 0.2538 - classification_loss: 0.0194
 311/1000 [========>.....................] - ETA: 25:48 - loss: 0.2734 - regression_loss: 0.2540 - classification_loss: 0.0194
 312/1000 [========>.....................] - ETA: 25:46 - loss: 0.2733 - regression_loss: 0.2539 - classification_loss: 0.0194
 313/1000 [========>.....................] - ETA: 25:44 - loss: 0.2735 - regression_loss: 0.2541 - classification_loss: 0.0194
 314/1000 [========>.....................] - ETA: 25:43 - loss: 0.2740 - regression_loss: 0.2545 - classification_loss: 0.0195
 315/1000 [========>.....................] - ETA: 25:40 - loss: 0.2737 - regression_loss: 0.2543 - classification_loss: 0.0194
 316/1000 [========>.....................] - ETA: 25:38 - loss: 0.2741 - regression_loss: 0.2546 - classification_loss: 0.0194
 317/1000 [========>.....................] - ETA: 25:34 - loss: 0.2733 - regression_loss: 0.2539 - classification_loss: 0.0194
 318/1000 [========>.....................] - ETA: 25:33 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 319/1000 [========>.....................] - ETA: 25:30 - loss: 0.2737 - regression_loss: 0.2543 - classification_loss: 0.0194
 320/1000 [========>.....................] - ETA: 25:27 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0194
 321/1000 [========>.....................] - ETA: 25:26 - loss: 0.2746 - regression_loss: 0.2551 - classification_loss: 0.0194
 322/1000 [========>.....................] - ETA: 25:22 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 323/1000 [========>.....................] - ETA: 25:21 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0194
 324/1000 [========>.....................] - ETA: 25:19 - loss: 0.2739 - regression_loss: 0.2545 - classification_loss: 0.0194
 325/1000 [========>.....................] - ETA: 25:18 - loss: 0.2742 - regression_loss: 0.2548 - classification_loss: 0.0194
 326/1000 [========>.....................] - ETA: 25:14 - loss: 0.2735 - regression_loss: 0.2541 - classification_loss: 0.0194
 327/1000 [========>.....................] - ETA: 25:12 - loss: 0.2736 - regression_loss: 0.2543 - classification_loss: 0.0194
 328/1000 [========>.....................] - ETA: 25:11 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0194
 329/1000 [========>.....................] - ETA: 25:08 - loss: 0.2740 - regression_loss: 0.2546 - classification_loss: 0.0194
 330/1000 [========>.....................] - ETA: 25:06 - loss: 0.2739 - regression_loss: 0.2545 - classification_loss: 0.0194
 331/1000 [========>.....................] - ETA: 25:03 - loss: 0.2734 - regression_loss: 0.2541 - classification_loss: 0.0194
 332/1000 [========>.....................] - ETA: 25:00 - loss: 0.2729 - regression_loss: 0.2536 - classification_loss: 0.0193
 333/1000 [========>.....................] - ETA: 24:59 - loss: 0.2733 - regression_loss: 0.2539 - classification_loss: 0.0194
 334/1000 [=========>....................] - ETA: 24:57 - loss: 0.2735 - regression_loss: 0.2541 - classification_loss: 0.0194
 335/1000 [=========>....................] - ETA: 24:55 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 336/1000 [=========>....................] - ETA: 24:53 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0194
 337/1000 [=========>....................] - ETA: 24:49 - loss: 0.2731 - regression_loss: 0.2538 - classification_loss: 0.0193
 338/1000 [=========>....................] - ETA: 24:48 - loss: 0.2731 - regression_loss: 0.2538 - classification_loss: 0.0193
 339/1000 [=========>....................] - ETA: 24:46 - loss: 0.2736 - regression_loss: 0.2543 - classification_loss: 0.0194
 340/1000 [=========>....................] - ETA: 24:43 - loss: 0.2736 - regression_loss: 0.2542 - classification_loss: 0.0193
 341/1000 [=========>....................] - ETA: 24:41 - loss: 0.2739 - regression_loss: 0.2546 - classification_loss: 0.0193
 342/1000 [=========>....................] - ETA: 24:39 - loss: 0.2739 - regression_loss: 0.2545 - classification_loss: 0.0194
 343/1000 [=========>....................] - ETA: 24:38 - loss: 0.2742 - regression_loss: 0.2548 - classification_loss: 0.0194
 344/1000 [=========>....................] - ETA: 24:35 - loss: 0.2742 - regression_loss: 0.2549 - classification_loss: 0.0194
 345/1000 [=========>....................] - ETA: 24:32 - loss: 0.2735 - regression_loss: 0.2542 - classification_loss: 0.0193
 346/1000 [=========>....................] - ETA: 24:30 - loss: 0.2738 - regression_loss: 0.2544 - classification_loss: 0.0193
 347/1000 [=========>....................] - ETA: 24:28 - loss: 0.2742 - regression_loss: 0.2548 - classification_loss: 0.0194
 348/1000 [=========>....................] - ETA: 24:26 - loss: 0.2743 - regression_loss: 0.2549 - classification_loss: 0.0194
 349/1000 [=========>....................] - ETA: 24:24 - loss: 0.2746 - regression_loss: 0.2552 - classification_loss: 0.0194
 350/1000 [=========>....................] - ETA: 24:21 - loss: 0.2743 - regression_loss: 0.2550 - classification_loss: 0.0193
 351/1000 [=========>....................] - ETA: 24:19 - loss: 0.2743 - regression_loss: 0.2549 - classification_loss: 0.0194
 352/1000 [=========>....................] - ETA: 24:16 - loss: 0.2738 - regression_loss: 0.2545 - classification_loss: 0.0193
 353/1000 [=========>....................] - ETA: 24:14 - loss: 0.2742 - regression_loss: 0.2549 - classification_loss: 0.0193
 354/1000 [=========>....................] - ETA: 24:12 - loss: 0.2742 - regression_loss: 0.2548 - classification_loss: 0.0193
 355/1000 [=========>....................] - ETA: 24:09 - loss: 0.2736 - regression_loss: 0.2543 - classification_loss: 0.0193
 356/1000 [=========>....................] - ETA: 24:06 - loss: 0.2737 - regression_loss: 0.2544 - classification_loss: 0.0193
 357/1000 [=========>....................] - ETA: 24:03 - loss: 0.2734 - regression_loss: 0.2541 - classification_loss: 0.0193
 358/1000 [=========>....................] - ETA: 24:02 - loss: 0.2737 - regression_loss: 0.2544 - classification_loss: 0.0193
 359/1000 [=========>....................] - ETA: 24:00 - loss: 0.2738 - regression_loss: 0.2545 - classification_loss: 0.0193
 360/1000 [=========>....................] - ETA: 23:57 - loss: 0.2734 - regression_loss: 0.2542 - classification_loss: 0.0193
 361/1000 [=========>....................] - ETA: 23:54 - loss: 0.2736 - regression_loss: 0.2543 - classification_loss: 0.0193
 362/1000 [=========>....................] - ETA: 23:53 - loss: 0.2739 - regression_loss: 0.2546 - classification_loss: 0.0193
 363/1000 [=========>....................] - ETA: 23:51 - loss: 0.2740 - regression_loss: 0.2547 - classification_loss: 0.0193
 364/1000 [=========>....................] - ETA: 23:48 - loss: 0.2734 - regression_loss: 0.2541 - classification_loss: 0.0193
 365/1000 [=========>....................] - ETA: 23:46 - loss: 0.2733 - regression_loss: 0.2540 - classification_loss: 0.0193
 366/1000 [=========>....................] - ETA: 23:44 - loss: 0.2737 - regression_loss: 0.2544 - classification_loss: 0.0193
 367/1000 [==========>...................] - ETA: 23:42 - loss: 0.2740 - regression_loss: 0.2547 - classification_loss: 0.0193
 368/1000 [==========>...................] - ETA: 23:40 - loss: 0.2741 - regression_loss: 0.2548 - classification_loss: 0.0193
 369/1000 [==========>...................] - ETA: 23:39 - loss: 0.2744 - regression_loss: 0.2551 - classification_loss: 0.0193
 370/1000 [==========>...................] - ETA: 23:37 - loss: 0.2743 - regression_loss: 0.2550 - classification_loss: 0.0193
 371/1000 [==========>...................] - ETA: 23:33 - loss: 0.2736 - regression_loss: 0.2543 - classification_loss: 0.0193
 372/1000 [==========>...................] - ETA: 23:30 - loss: 0.2732 - regression_loss: 0.2539 - classification_loss: 0.0192
 373/1000 [==========>...................] - ETA: 23:28 - loss: 0.2735 - regression_loss: 0.2542 - classification_loss: 0.0193
 374/1000 [==========>...................] - ETA: 23:27 - loss: 0.2738 - regression_loss: 0.2545 - classification_loss: 0.0193
 375/1000 [==========>...................] - ETA: 23:24 - loss: 0.2738 - regression_loss: 0.2546 - classification_loss: 0.0193
 376/1000 [==========>...................] - ETA: 23:21 - loss: 0.2732 - regression_loss: 0.2540 - classification_loss: 0.0192
 377/1000 [==========>...................] - ETA: 23:20 - loss: 0.2735 - regression_loss: 0.2543 - classification_loss: 0.0192
 378/1000 [==========>...................] - ETA: 23:18 - loss: 0.2737 - regression_loss: 0.2545 - classification_loss: 0.0193
 379/1000 [==========>...................] - ETA: 23:16 - loss: 0.2735 - regression_loss: 0.2543 - classification_loss: 0.0193
 380/1000 [==========>...................] - ETA: 23:13 - loss: 0.2734 - regression_loss: 0.2542 - classification_loss: 0.0192
 381/1000 [==========>...................] - ETA: 23:11 - loss: 0.2733 - regression_loss: 0.2540 - classification_loss: 0.0192
 382/1000 [==========>...................] - ETA: 23:08 - loss: 0.2729 - regression_loss: 0.2537 - classification_loss: 0.0192
 383/1000 [==========>...................] - ETA: 23:05 - loss: 0.2729 - regression_loss: 0.2537 - classification_loss: 0.0192
 384/1000 [==========>...................] - ETA: 23:04 - loss: 0.2732 - regression_loss: 0.2540 - classification_loss: 0.0192
 385/1000 [==========>...................] - ETA: 23:02 - loss: 0.2733 - regression_loss: 0.2541 - classification_loss: 0.0192
 386/1000 [==========>...................] - ETA: 22:59 - loss: 0.2728 - regression_loss: 0.2536 - classification_loss: 0.0192
 387/1000 [==========>...................] - ETA: 22:57 - loss: 0.2731 - regression_loss: 0.2539 - classification_loss: 0.0192
 388/1000 [==========>...................] - ETA: 22:55 - loss: 0.2734 - regression_loss: 0.2541 - classification_loss: 0.0192
 389/1000 [==========>...................] - ETA: 22:52 - loss: 0.2727 - regression_loss: 0.2535 - classification_loss: 0.0192
 390/1000 [==========>...................] - ETA: 22:50 - loss: 0.2728 - regression_loss: 0.2536 - classification_loss: 0.0192
 391/1000 [==========>...................] - ETA: 22:47 - loss: 0.2727 - regression_loss: 0.2535 - classification_loss: 0.0192
 392/1000 [==========>...................] - ETA: 22:45 - loss: 0.2730 - regression_loss: 0.2538 - classification_loss: 0.0192
 393/1000 [==========>...................] - ETA: 22:43 - loss: 0.2731 - regression_loss: 0.2539 - classification_loss: 0.0192
 394/1000 [==========>...................] - ETA: 22:41 - loss: 0.2729 - regression_loss: 0.2537 - classification_loss: 0.0192
 395/1000 [==========>...................] - ETA: 22:38 - loss: 0.2723 - regression_loss: 0.2531 - classification_loss: 0.0192
 396/1000 [==========>...................] - ETA: 22:36 - loss: 0.2724 - regression_loss: 0.2532 - classification_loss: 0.0192
 397/1000 [==========>...................] - ETA: 22:34 - loss: 0.2726 - regression_loss: 0.2535 - classification_loss: 0.0192
 398/1000 [==========>...................] - ETA: 22:32 - loss: 0.2729 - regression_loss: 0.2537 - classification_loss: 0.0192
 399/1000 [==========>...................] - ETA: 22:30 - loss: 0.2731 - regression_loss: 0.2539 - classification_loss: 0.0192
 400/1000 [===========>..................] - ETA: 22:28 - loss: 0.2729 - regression_loss: 0.2537 - classification_loss: 0.0192
 401/1000 [===========>..................] - ETA: 22:25 - loss: 0.2727 - regression_loss: 0.2535 - classification_loss: 0.0192
 402/1000 [===========>..................] - ETA: 22:23 - loss: 0.2728 - regression_loss: 0.2536 - classification_loss: 0.0192
 403/1000 [===========>..................] - ETA: 22:21 - loss: 0.2727 - regression_loss: 0.2536 - classification_loss: 0.0192
 404/1000 [===========>..................] - ETA: 22:18 - loss: 0.2722 - regression_loss: 0.2530 - classification_loss: 0.0191
 405/1000 [===========>..................] - ETA: 22:16 - loss: 0.2725 - regression_loss: 0.2533 - classification_loss: 0.0192
 406/1000 [===========>..................] - ETA: 22:14 - loss: 0.2726 - regression_loss: 0.2534 - classification_loss: 0.0192
 407/1000 [===========>..................] - ETA: 22:11 - loss: 0.2723 - regression_loss: 0.2531 - classification_loss: 0.0192
 408/1000 [===========>..................] - ETA: 22:10 - loss: 0.2726 - regression_loss: 0.2534 - classification_loss: 0.0192
 409/1000 [===========>..................] - ETA: 22:08 - loss: 0.2728 - regression_loss: 0.2537 - classification_loss: 0.0192
 410/1000 [===========>..................] - ETA: 22:05 - loss: 0.2722 - regression_loss: 0.2531 - classification_loss: 0.0191
 411/1000 [===========>..................] - ETA: 22:02 - loss: 0.2719 - regression_loss: 0.2528 - classification_loss: 0.0191
 412/1000 [===========>..................] - ETA: 22:00 - loss: 0.2718 - regression_loss: 0.2527 - classification_loss: 0.0191
 413/1000 [===========>..................] - ETA: 21:57 - loss: 0.2719 - regression_loss: 0.2528 - classification_loss: 0.0191
 414/1000 [===========>..................] - ETA: 21:56 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 415/1000 [===========>..................] - ETA: 21:54 - loss: 0.2723 - regression_loss: 0.2531 - classification_loss: 0.0191
 416/1000 [===========>..................] - ETA: 21:51 - loss: 0.2724 - regression_loss: 0.2533 - classification_loss: 0.0191
 417/1000 [===========>..................] - ETA: 21:50 - loss: 0.2726 - regression_loss: 0.2534 - classification_loss: 0.0191
 418/1000 [===========>..................] - ETA: 21:48 - loss: 0.2728 - regression_loss: 0.2537 - classification_loss: 0.0192
 419/1000 [===========>..................] - ETA: 21:45 - loss: 0.2723 - regression_loss: 0.2532 - classification_loss: 0.0191
 420/1000 [===========>..................] - ETA: 21:43 - loss: 0.2722 - regression_loss: 0.2530 - classification_loss: 0.0191
 421/1000 [===========>..................] - ETA: 21:40 - loss: 0.2718 - regression_loss: 0.2528 - classification_loss: 0.0191
 422/1000 [===========>..................] - ETA: 21:38 - loss: 0.2722 - regression_loss: 0.2530 - classification_loss: 0.0191
 423/1000 [===========>..................] - ETA: 21:36 - loss: 0.2722 - regression_loss: 0.2531 - classification_loss: 0.0191
 424/1000 [===========>..................] - ETA: 21:34 - loss: 0.2724 - regression_loss: 0.2533 - classification_loss: 0.0191
 425/1000 [===========>..................] - ETA: 21:32 - loss: 0.2726 - regression_loss: 0.2535 - classification_loss: 0.0191
 426/1000 [===========>..................] - ETA: 21:30 - loss: 0.2727 - regression_loss: 0.2536 - classification_loss: 0.0191
 427/1000 [===========>..................] - ETA: 21:27 - loss: 0.2721 - regression_loss: 0.2530 - classification_loss: 0.0191
 428/1000 [===========>..................] - ETA: 21:25 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 429/1000 [===========>..................] - ETA: 21:22 - loss: 0.2717 - regression_loss: 0.2526 - classification_loss: 0.0191
 430/1000 [===========>..................] - ETA: 21:20 - loss: 0.2721 - regression_loss: 0.2530 - classification_loss: 0.0191
 431/1000 [===========>..................] - ETA: 21:18 - loss: 0.2724 - regression_loss: 0.2533 - classification_loss: 0.0191
 432/1000 [===========>..................] - ETA: 21:17 - loss: 0.2728 - regression_loss: 0.2537 - classification_loss: 0.0191
 433/1000 [===========>..................] - ETA: 21:14 - loss: 0.2723 - regression_loss: 0.2532 - classification_loss: 0.0191
 434/1000 [============>.................] - ETA: 21:11 - loss: 0.2724 - regression_loss: 0.2533 - classification_loss: 0.0191
 435/1000 [============>.................] - ETA: 21:08 - loss: 0.2721 - regression_loss: 0.2530 - classification_loss: 0.0191
 436/1000 [============>.................] - ETA: 21:06 - loss: 0.2721 - regression_loss: 0.2530 - classification_loss: 0.0191
 437/1000 [============>.................] - ETA: 21:04 - loss: 0.2717 - regression_loss: 0.2527 - classification_loss: 0.0191
 438/1000 [============>.................] - ETA: 21:01 - loss: 0.2718 - regression_loss: 0.2528 - classification_loss: 0.0191
 439/1000 [============>.................] - ETA: 20:58 - loss: 0.2712 - regression_loss: 0.2522 - classification_loss: 0.0190
 440/1000 [============>.................] - ETA: 20:56 - loss: 0.2714 - regression_loss: 0.2524 - classification_loss: 0.0190
 441/1000 [============>.................] - ETA: 20:55 - loss: 0.2716 - regression_loss: 0.2526 - classification_loss: 0.0191
 442/1000 [============>.................] - ETA: 20:53 - loss: 0.2719 - regression_loss: 0.2528 - classification_loss: 0.0191
 443/1000 [============>.................] - ETA: 20:51 - loss: 0.2717 - regression_loss: 0.2527 - classification_loss: 0.0191
 444/1000 [============>.................] - ETA: 20:49 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 445/1000 [============>.................] - ETA: 20:47 - loss: 0.2718 - regression_loss: 0.2527 - classification_loss: 0.0191
 446/1000 [============>.................] - ETA: 20:45 - loss: 0.2718 - regression_loss: 0.2527 - classification_loss: 0.0191
 447/1000 [============>.................] - ETA: 20:43 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 448/1000 [============>.................] - ETA: 20:40 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 449/1000 [============>.................] - ETA: 20:37 - loss: 0.2715 - regression_loss: 0.2525 - classification_loss: 0.0190
 450/1000 [============>.................] - ETA: 20:35 - loss: 0.2717 - regression_loss: 0.2527 - classification_loss: 0.0191
 451/1000 [============>.................] - ETA: 20:33 - loss: 0.2717 - regression_loss: 0.2526 - classification_loss: 0.0191
 452/1000 [============>.................] - ETA: 20:30 - loss: 0.2712 - regression_loss: 0.2522 - classification_loss: 0.0190
 453/1000 [============>.................] - ETA: 20:28 - loss: 0.2714 - regression_loss: 0.2523 - classification_loss: 0.0190
 454/1000 [============>.................] - ETA: 20:26 - loss: 0.2716 - regression_loss: 0.2526 - classification_loss: 0.0191
 455/1000 [============>.................] - ETA: 20:25 - loss: 0.2719 - regression_loss: 0.2528 - classification_loss: 0.0191
 456/1000 [============>.................] - ETA: 20:22 - loss: 0.2717 - regression_loss: 0.2526 - classification_loss: 0.0191
 457/1000 [============>.................] - ETA: 20:19 - loss: 0.2717 - regression_loss: 0.2527 - classification_loss: 0.0191
 458/1000 [============>.................] - ETA: 20:17 - loss: 0.2714 - regression_loss: 0.2523 - classification_loss: 0.0190
 459/1000 [============>.................] - ETA: 20:15 - loss: 0.2717 - regression_loss: 0.2526 - classification_loss: 0.0191
 460/1000 [============>.................] - ETA: 20:13 - loss: 0.2720 - regression_loss: 0.2529 - classification_loss: 0.0191
 461/1000 [============>.................] - ETA: 20:10 - loss: 0.2715 - regression_loss: 0.2525 - classification_loss: 0.0190
 462/1000 [============>.................] - ETA: 20:08 - loss: 0.2716 - regression_loss: 0.2526 - classification_loss: 0.0190
 463/1000 [============>.................] - ETA: 20:06 - loss: 0.2715 - regression_loss: 0.2525 - classification_loss: 0.0190
 464/1000 [============>.................] - ETA: 20:04 - loss: 0.2715 - regression_loss: 0.2525 - classification_loss: 0.0190
 465/1000 [============>.................] - ETA: 20:02 - loss: 0.2716 - regression_loss: 0.2526 - classification_loss: 0.0190
 466/1000 [============>.................] - ETA: 20:00 - loss: 0.2718 - regression_loss: 0.2528 - classification_loss: 0.0191
 467/1000 [=============>................] - ETA: 19:58 - loss: 0.2719 - regression_loss: 0.2528 - classification_loss: 0.0191
 468/1000 [=============>................] - ETA: 19:55 - loss: 0.2713 - regression_loss: 0.2523 - classification_loss: 0.0190
 469/1000 [=============>................] - ETA: 19:53 - loss: 0.2715 - regression_loss: 0.2525 - classification_loss: 0.0190
 470/1000 [=============>................] - ETA: 19:50 - loss: 0.2713 - regression_loss: 0.2523 - classification_loss: 0.0190
 471/1000 [=============>................] - ETA: 19:48 - loss: 0.2712 - regression_loss: 0.2522 - classification_loss: 0.0190
 472/1000 [=============>................] - ETA: 19:46 - loss: 0.2714 - regression_loss: 0.2524 - classification_loss: 0.0190
 473/1000 [=============>................] - ETA: 19:44 - loss: 0.2715 - regression_loss: 0.2524 - classification_loss: 0.0190
 474/1000 [=============>................] - ETA: 19:41 - loss: 0.2710 - regression_loss: 0.2520 - classification_loss: 0.0190
 475/1000 [=============>................] - ETA: 19:39 - loss: 0.2710 - regression_loss: 0.2520 - classification_loss: 0.0190
 476/1000 [=============>................] - ETA: 19:37 - loss: 0.2713 - regression_loss: 0.2523 - classification_loss: 0.0190
 477/1000 [=============>................] - ETA: 19:34 - loss: 0.2711 - regression_loss: 0.2521 - classification_loss: 0.0190
 478/1000 [=============>................] - ETA: 19:32 - loss: 0.2710 - regression_loss: 0.2520 - classification_loss: 0.0190
 479/1000 [=============>................] - ETA: 19:31 - loss: 0.2712 - regression_loss: 0.2522 - classification_loss: 0.0190
 480/1000 [=============>................] - ETA: 19:28 - loss: 0.2713 - regression_loss: 0.2522 - classification_loss: 0.0190
 481/1000 [=============>................] - ETA: 19:25 - loss: 0.2708 - regression_loss: 0.2518 - classification_loss: 0.0190
 482/1000 [=============>................] - ETA: 19:23 - loss: 0.2710 - regression_loss: 0.2520 - classification_loss: 0.0190
 483/1000 [=============>................] - ETA: 19:21 - loss: 0.2712 - regression_loss: 0.2522 - classification_loss: 0.0190
 484/1000 [=============>................] - ETA: 19:19 - loss: 0.2710 - regression_loss: 0.2520 - classification_loss: 0.0190
 485/1000 [=============>................] - ETA: 19:17 - loss: 0.2708 - regression_loss: 0.2518 - classification_loss: 0.0190
 486/1000 [=============>................] - ETA: 19:15 - loss: 0.2706 - regression_loss: 0.2516 - classification_loss: 0.0190
 487/1000 [=============>................] - ETA: 19:12 - loss: 0.2703 - regression_loss: 0.2514 - classification_loss: 0.0190
 488/1000 [=============>................] - ETA: 19:10 - loss: 0.2704 - regression_loss: 0.2514 - classification_loss: 0.0190
 489/1000 [=============>................] - ETA: 19:08 - loss: 0.2706 - regression_loss: 0.2516 - classification_loss: 0.0190
 490/1000 [=============>................] - ETA: 19:05 - loss: 0.2701 - regression_loss: 0.2512 - classification_loss: 0.0190
 491/1000 [=============>................] - ETA: 19:03 - loss: 0.2701 - regression_loss: 0.2512 - classification_loss: 0.0190
 492/1000 [=============>................] - ETA: 19:01 - loss: 0.2703 - regression_loss: 0.2514 - classification_loss: 0.0190
 493/1000 [=============>................] - ETA: 18:59 - loss: 0.2704 - regression_loss: 0.2514 - classification_loss: 0.0190
 494/1000 [=============>................] - ETA: 18:56 - loss: 0.2702 - regression_loss: 0.2513 - classification_loss: 0.0189
 495/1000 [=============>................] - ETA: 18:53 - loss: 0.2697 - regression_loss: 0.2508 - classification_loss: 0.0189
 496/1000 [=============>................] - ETA: 18:51 - loss: 0.2699 - regression_loss: 0.2510 - classification_loss: 0.0189
 497/1000 [=============>................] - ETA: 18:49 - loss: 0.2698 - regression_loss: 0.2509 - classification_loss: 0.0189
 498/1000 [=============>................] - ETA: 18:47 - loss: 0.2698 - regression_loss: 0.2509 - classification_loss: 0.0189
 499/1000 [=============>................] - ETA: 18:45 - loss: 0.2700 - regression_loss: 0.2511 - classification_loss: 0.0189
 500/1000 [==============>...............] - ETA: 18:43 - loss: 0.2701 - regression_loss: 0.2511 - classification_loss: 0.0189
 501/1000 [==============>...............] - ETA: 18:41 - loss: 0.2698 - regression_loss: 0.2509 - classification_loss: 0.0189
 502/1000 [==============>...............] - ETA: 18:38 - loss: 0.2697 - regression_loss: 0.2507 - classification_loss: 0.0189
 503/1000 [==============>...............] - ETA: 18:37 - loss: 0.2699 - regression_loss: 0.2509 - classification_loss: 0.0189
 504/1000 [==============>...............] - ETA: 18:34 - loss: 0.2694 - regression_loss: 0.2505 - classification_loss: 0.0189
 505/1000 [==============>...............] - ETA: 18:32 - loss: 0.2696 - regression_loss: 0.2507 - classification_loss: 0.0189
 506/1000 [==============>...............] - ETA: 18:30 - loss: 0.2696 - regression_loss: 0.2507 - classification_loss: 0.0189
 507/1000 [==============>...............] - ETA: 18:27 - loss: 0.2696 - regression_loss: 0.2507 - classification_loss: 0.0189
 508/1000 [==============>...............] - ETA: 18:25 - loss: 0.2698 - regression_loss: 0.2508 - classification_loss: 0.0189
 509/1000 [==============>...............] - ETA: 18:23 - loss: 0.2695 - regression_loss: 0.2506 - classification_loss: 0.0189
 510/1000 [==============>...............] - ETA: 18:20 - loss: 0.2694 - regression_loss: 0.2505 - classification_loss: 0.0189
 511/1000 [==============>...............] - ETA: 18:18 - loss: 0.2689 - regression_loss: 0.2500 - classification_loss: 0.0189
 512/1000 [==============>...............] - ETA: 18:16 - loss: 0.2690 - regression_loss: 0.2501 - classification_loss: 0.0189
 513/1000 [==============>...............] - ETA: 18:14 - loss: 0.2692 - regression_loss: 0.2503 - classification_loss: 0.0189
 514/1000 [==============>...............] - ETA: 18:11 - loss: 0.2687 - regression_loss: 0.2498 - classification_loss: 0.0189
 515/1000 [==============>...............] - ETA: 18:09 - loss: 0.2688 - regression_loss: 0.2499 - classification_loss: 0.0189
 516/1000 [==============>...............] - ETA: 18:06 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0189
 517/1000 [==============>...............] - ETA: 18:04 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0188
 518/1000 [==============>...............] - ETA: 18:02 - loss: 0.2687 - regression_loss: 0.2499 - classification_loss: 0.0189
 519/1000 [==============>...............] - ETA: 18:00 - loss: 0.2689 - regression_loss: 0.2500 - classification_loss: 0.0189
 520/1000 [==============>...............] - ETA: 17:58 - loss: 0.2688 - regression_loss: 0.2499 - classification_loss: 0.0189
 521/1000 [==============>...............] - ETA: 17:56 - loss: 0.2688 - regression_loss: 0.2499 - classification_loss: 0.0189
 522/1000 [==============>...............] - ETA: 17:54 - loss: 0.2689 - regression_loss: 0.2500 - classification_loss: 0.0189
 523/1000 [==============>...............] - ETA: 17:52 - loss: 0.2687 - regression_loss: 0.2499 - classification_loss: 0.0189
 524/1000 [==============>...............] - ETA: 17:49 - loss: 0.2684 - regression_loss: 0.2495 - classification_loss: 0.0188
 525/1000 [==============>...............] - ETA: 17:47 - loss: 0.2686 - regression_loss: 0.2497 - classification_loss: 0.0189
 526/1000 [==============>...............] - ETA: 17:44 - loss: 0.2684 - regression_loss: 0.2495 - classification_loss: 0.0188
 527/1000 [==============>...............] - ETA: 17:42 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0188
 528/1000 [==============>...............] - ETA: 17:40 - loss: 0.2684 - regression_loss: 0.2496 - classification_loss: 0.0188
 529/1000 [==============>...............] - ETA: 17:38 - loss: 0.2686 - regression_loss: 0.2497 - classification_loss: 0.0189
 530/1000 [==============>...............] - ETA: 17:36 - loss: 0.2687 - regression_loss: 0.2499 - classification_loss: 0.0189
 531/1000 [==============>...............] - ETA: 17:34 - loss: 0.2688 - regression_loss: 0.2499 - classification_loss: 0.0189
 532/1000 [==============>...............] - ETA: 17:32 - loss: 0.2688 - regression_loss: 0.2500 - classification_loss: 0.0189
 533/1000 [==============>...............] - ETA: 17:29 - loss: 0.2684 - regression_loss: 0.2495 - classification_loss: 0.0188
 534/1000 [===============>..............] - ETA: 17:27 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 535/1000 [===============>..............] - ETA: 17:25 - loss: 0.2683 - regression_loss: 0.2495 - classification_loss: 0.0188
 536/1000 [===============>..............] - ETA: 17:23 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0188
 537/1000 [===============>..............] - ETA: 17:20 - loss: 0.2680 - regression_loss: 0.2492 - classification_loss: 0.0188
 538/1000 [===============>..............] - ETA: 17:18 - loss: 0.2682 - regression_loss: 0.2494 - classification_loss: 0.0188
 539/1000 [===============>..............] - ETA: 17:16 - loss: 0.2683 - regression_loss: 0.2495 - classification_loss: 0.0188
 540/1000 [===============>..............] - ETA: 17:14 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 541/1000 [===============>..............] - ETA: 17:11 - loss: 0.2679 - regression_loss: 0.2491 - classification_loss: 0.0188
 542/1000 [===============>..............] - ETA: 17:09 - loss: 0.2682 - regression_loss: 0.2494 - classification_loss: 0.0188
 543/1000 [===============>..............] - ETA: 17:07 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0188
 544/1000 [===============>..............] - ETA: 17:05 - loss: 0.2685 - regression_loss: 0.2497 - classification_loss: 0.0188
 545/1000 [===============>..............] - ETA: 17:02 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 546/1000 [===============>..............] - ETA: 17:00 - loss: 0.2682 - regression_loss: 0.2494 - classification_loss: 0.0188
 547/1000 [===============>..............] - ETA: 16:57 - loss: 0.2679 - regression_loss: 0.2491 - classification_loss: 0.0188
 548/1000 [===============>..............] - ETA: 16:55 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 549/1000 [===============>..............] - ETA: 16:53 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 550/1000 [===============>..............] - ETA: 16:50 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0188
 551/1000 [===============>..............] - ETA: 16:48 - loss: 0.2679 - regression_loss: 0.2492 - classification_loss: 0.0188
 552/1000 [===============>..............] - ETA: 16:46 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0188
 553/1000 [===============>..............] - ETA: 16:44 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0188
 554/1000 [===============>..............] - ETA: 16:42 - loss: 0.2683 - regression_loss: 0.2495 - classification_loss: 0.0188
 555/1000 [===============>..............] - ETA: 16:39 - loss: 0.2682 - regression_loss: 0.2494 - classification_loss: 0.0188
 556/1000 [===============>..............] - ETA: 16:37 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 557/1000 [===============>..............] - ETA: 16:35 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 558/1000 [===============>..............] - ETA: 16:32 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 559/1000 [===============>..............] - ETA: 16:30 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 560/1000 [===============>..............] - ETA: 16:28 - loss: 0.2680 - regression_loss: 0.2492 - classification_loss: 0.0187
 561/1000 [===============>..............] - ETA: 16:26 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0187
 562/1000 [===============>..............] - ETA: 16:24 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0187
 563/1000 [===============>..............] - ETA: 16:22 - loss: 0.2682 - regression_loss: 0.2495 - classification_loss: 0.0187
 564/1000 [===============>..............] - ETA: 16:19 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0187
 565/1000 [===============>..............] - ETA: 16:17 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0187
 566/1000 [===============>..............] - ETA: 16:15 - loss: 0.2681 - regression_loss: 0.2493 - classification_loss: 0.0187
 567/1000 [================>.............] - ETA: 16:13 - loss: 0.2682 - regression_loss: 0.2495 - classification_loss: 0.0187
 568/1000 [================>.............] - ETA: 16:11 - loss: 0.2684 - regression_loss: 0.2497 - classification_loss: 0.0188
 569/1000 [================>.............] - ETA: 16:08 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0187
 570/1000 [================>.............] - ETA: 16:06 - loss: 0.2683 - regression_loss: 0.2496 - classification_loss: 0.0187
 571/1000 [================>.............] - ETA: 16:04 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0187
 572/1000 [================>.............] - ETA: 16:01 - loss: 0.2681 - regression_loss: 0.2494 - classification_loss: 0.0187
 573/1000 [================>.............] - ETA: 15:58 - loss: 0.2676 - regression_loss: 0.2489 - classification_loss: 0.0187
 574/1000 [================>.............] - ETA: 15:57 - loss: 0.2679 - regression_loss: 0.2492 - classification_loss: 0.0187
 575/1000 [================>.............] - ETA: 15:55 - loss: 0.2680 - regression_loss: 0.2493 - classification_loss: 0.0187
 576/1000 [================>.............] - ETA: 15:52 - loss: 0.2682 - regression_loss: 0.2495 - classification_loss: 0.0187
 577/1000 [================>.............] - ETA: 15:50 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 578/1000 [================>.............] - ETA: 15:48 - loss: 0.2676 - regression_loss: 0.2489 - classification_loss: 0.0187
 579/1000 [================>.............] - ETA: 15:46 - loss: 0.2678 - regression_loss: 0.2491 - classification_loss: 0.0187
 580/1000 [================>.............] - ETA: 15:43 - loss: 0.2675 - regression_loss: 0.2489 - classification_loss: 0.0187
 581/1000 [================>.............] - ETA: 15:41 - loss: 0.2676 - regression_loss: 0.2489 - classification_loss: 0.0187
 582/1000 [================>.............] - ETA: 15:39 - loss: 0.2677 - regression_loss: 0.2491 - classification_loss: 0.0187
 583/1000 [================>.............] - ETA: 15:37 - loss: 0.2678 - regression_loss: 0.2491 - classification_loss: 0.0187
 584/1000 [================>.............] - ETA: 15:35 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 585/1000 [================>.............] - ETA: 15:32 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 586/1000 [================>.............] - ETA: 15:30 - loss: 0.2675 - regression_loss: 0.2488 - classification_loss: 0.0187
 587/1000 [================>.............] - ETA: 15:28 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 588/1000 [================>.............] - ETA: 15:26 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 589/1000 [================>.............] - ETA: 15:23 - loss: 0.2675 - regression_loss: 0.2488 - classification_loss: 0.0187
 590/1000 [================>.............] - ETA: 15:21 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 591/1000 [================>.............] - ETA: 15:19 - loss: 0.2676 - regression_loss: 0.2489 - classification_loss: 0.0187
 592/1000 [================>.............] - ETA: 15:16 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 593/1000 [================>.............] - ETA: 15:14 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 594/1000 [================>.............] - ETA: 15:12 - loss: 0.2676 - regression_loss: 0.2489 - classification_loss: 0.0187
 595/1000 [================>.............] - ETA: 15:10 - loss: 0.2677 - regression_loss: 0.2490 - classification_loss: 0.0187
 596/1000 [================>.............] - ETA: 15:07 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 597/1000 [================>.............] - ETA: 15:05 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 598/1000 [================>.............] - ETA: 15:03 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0187
 599/1000 [================>.............] - ETA: 15:01 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 600/1000 [=================>............] - ETA: 14:58 - loss: 0.2672 - regression_loss: 0.2485 - classification_loss: 0.0187
 601/1000 [=================>............] - ETA: 14:56 - loss: 0.2672 - regression_loss: 0.2485 - classification_loss: 0.0187
 602/1000 [=================>............] - ETA: 14:54 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 603/1000 [=================>............] - ETA: 14:52 - loss: 0.2675 - regression_loss: 0.2488 - classification_loss: 0.0187
 604/1000 [=================>............] - ETA: 14:50 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 605/1000 [=================>............] - ETA: 14:47 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 606/1000 [=================>............] - ETA: 14:45 - loss: 0.2674 - regression_loss: 0.2487 - classification_loss: 0.0187
 607/1000 [=================>............] - ETA: 14:43 - loss: 0.2675 - regression_loss: 0.2488 - classification_loss: 0.0187
 608/1000 [=================>............] - ETA: 14:41 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0187
 609/1000 [=================>............] - ETA: 14:38 - loss: 0.2669 - regression_loss: 0.2482 - classification_loss: 0.0187
 610/1000 [=================>............] - ETA: 14:36 - loss: 0.2669 - regression_loss: 0.2482 - classification_loss: 0.0187
 611/1000 [=================>............] - ETA: 14:34 - loss: 0.2670 - regression_loss: 0.2483 - classification_loss: 0.0187
 612/1000 [=================>............] - ETA: 14:31 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 613/1000 [=================>............] - ETA: 14:29 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 614/1000 [=================>............] - ETA: 14:27 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 615/1000 [=================>............] - ETA: 14:24 - loss: 0.2665 - regression_loss: 0.2478 - classification_loss: 0.0187
 616/1000 [=================>............] - ETA: 14:22 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 617/1000 [=================>............] - ETA: 14:20 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0187
 618/1000 [=================>............] - ETA: 14:18 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0187
 619/1000 [=================>............] - ETA: 14:16 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 620/1000 [=================>............] - ETA: 14:13 - loss: 0.2664 - regression_loss: 0.2477 - classification_loss: 0.0186
 621/1000 [=================>............] - ETA: 14:11 - loss: 0.2660 - regression_loss: 0.2474 - classification_loss: 0.0186
 622/1000 [=================>............] - ETA: 14:09 - loss: 0.2662 - regression_loss: 0.2475 - classification_loss: 0.0186
 623/1000 [=================>............] - ETA: 14:07 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 624/1000 [=================>............] - ETA: 14:05 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 625/1000 [=================>............] - ETA: 14:02 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0187
 626/1000 [=================>............] - ETA: 14:00 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0187
 627/1000 [=================>............] - ETA: 13:58 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0187
 628/1000 [=================>............] - ETA: 13:56 - loss: 0.2668 - regression_loss: 0.2481 - classification_loss: 0.0187
 629/1000 [=================>............] - ETA: 13:54 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0187
 630/1000 [=================>............] - ETA: 13:52 - loss: 0.2670 - regression_loss: 0.2484 - classification_loss: 0.0187
 631/1000 [=================>............] - ETA: 13:49 - loss: 0.2670 - regression_loss: 0.2483 - classification_loss: 0.0187
 632/1000 [=================>............] - ETA: 13:47 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 633/1000 [=================>............] - ETA: 13:44 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 634/1000 [==================>...........] - ETA: 13:41 - loss: 0.2661 - regression_loss: 0.2475 - classification_loss: 0.0186
 635/1000 [==================>...........] - ETA: 13:39 - loss: 0.2661 - regression_loss: 0.2475 - classification_loss: 0.0186
 636/1000 [==================>...........] - ETA: 13:37 - loss: 0.2662 - regression_loss: 0.2476 - classification_loss: 0.0186
 637/1000 [==================>...........] - ETA: 13:35 - loss: 0.2665 - regression_loss: 0.2478 - classification_loss: 0.0186
 638/1000 [==================>...........] - ETA: 13:33 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 639/1000 [==================>...........] - ETA: 13:31 - loss: 0.2667 - regression_loss: 0.2481 - classification_loss: 0.0186
 640/1000 [==================>...........] - ETA: 13:28 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 641/1000 [==================>...........] - ETA: 13:26 - loss: 0.2666 - regression_loss: 0.2479 - classification_loss: 0.0186
 642/1000 [==================>...........] - ETA: 13:24 - loss: 0.2668 - regression_loss: 0.2482 - classification_loss: 0.0186
 643/1000 [==================>...........] - ETA: 13:22 - loss: 0.2670 - regression_loss: 0.2483 - classification_loss: 0.0186
 644/1000 [==================>...........] - ETA: 13:20 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 645/1000 [==================>...........] - ETA: 13:18 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 646/1000 [==================>...........] - ETA: 13:15 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0186
 647/1000 [==================>...........] - ETA: 13:13 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0186
 648/1000 [==================>...........] - ETA: 13:11 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 649/1000 [==================>...........] - ETA: 13:08 - loss: 0.2673 - regression_loss: 0.2487 - classification_loss: 0.0186
 650/1000 [==================>...........] - ETA: 13:06 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 651/1000 [==================>...........] - ETA: 13:04 - loss: 0.2671 - regression_loss: 0.2485 - classification_loss: 0.0186
 652/1000 [==================>...........] - ETA: 13:01 - loss: 0.2672 - regression_loss: 0.2485 - classification_loss: 0.0186
 653/1000 [==================>...........] - ETA: 12:59 - loss: 0.2672 - regression_loss: 0.2486 - classification_loss: 0.0186
 654/1000 [==================>...........] - ETA: 12:57 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 655/1000 [==================>...........] - ETA: 12:55 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0186
 656/1000 [==================>...........] - ETA: 12:53 - loss: 0.2672 - regression_loss: 0.2486 - classification_loss: 0.0186
 657/1000 [==================>...........] - ETA: 12:50 - loss: 0.2671 - regression_loss: 0.2485 - classification_loss: 0.0187
 658/1000 [==================>...........] - ETA: 12:48 - loss: 0.2670 - regression_loss: 0.2483 - classification_loss: 0.0186
 659/1000 [==================>...........] - ETA: 12:46 - loss: 0.2671 - regression_loss: 0.2485 - classification_loss: 0.0186
 660/1000 [==================>...........] - ETA: 12:44 - loss: 0.2671 - regression_loss: 0.2485 - classification_loss: 0.0186
 661/1000 [==================>...........] - ETA: 12:42 - loss: 0.2673 - regression_loss: 0.2486 - classification_loss: 0.0187
 662/1000 [==================>...........] - ETA: 12:39 - loss: 0.2673 - regression_loss: 0.2487 - classification_loss: 0.0186
 663/1000 [==================>...........] - ETA: 12:37 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 664/1000 [==================>...........] - ETA: 12:35 - loss: 0.2671 - regression_loss: 0.2485 - classification_loss: 0.0186
 665/1000 [==================>...........] - ETA: 12:32 - loss: 0.2670 - regression_loss: 0.2484 - classification_loss: 0.0186
 666/1000 [==================>...........] - ETA: 12:30 - loss: 0.2671 - regression_loss: 0.2484 - classification_loss: 0.0186
 667/1000 [===================>..........] - ETA: 12:28 - loss: 0.2668 - regression_loss: 0.2482 - classification_loss: 0.0186
 668/1000 [===================>..........] - ETA: 12:25 - loss: 0.2665 - regression_loss: 0.2479 - classification_loss: 0.0186
 669/1000 [===================>..........] - ETA: 12:23 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 670/1000 [===================>..........] - ETA: 12:21 - loss: 0.2667 - regression_loss: 0.2481 - classification_loss: 0.0186
 671/1000 [===================>..........] - ETA: 12:19 - loss: 0.2669 - regression_loss: 0.2483 - classification_loss: 0.0186
 672/1000 [===================>..........] - ETA: 12:17 - loss: 0.2668 - regression_loss: 0.2482 - classification_loss: 0.0186
 673/1000 [===================>..........] - ETA: 12:14 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 674/1000 [===================>..........] - ETA: 12:12 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0186
 675/1000 [===================>..........] - ETA: 12:10 - loss: 0.2668 - regression_loss: 0.2482 - classification_loss: 0.0186
 676/1000 [===================>..........] - ETA: 12:07 - loss: 0.2665 - regression_loss: 0.2479 - classification_loss: 0.0186
 677/1000 [===================>..........] - ETA: 12:05 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 678/1000 [===================>..........] - ETA: 12:03 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0186
 679/1000 [===================>..........] - ETA: 12:01 - loss: 0.2665 - regression_loss: 0.2479 - classification_loss: 0.0186
 680/1000 [===================>..........] - ETA: 11:58 - loss: 0.2667 - regression_loss: 0.2480 - classification_loss: 0.0186
 681/1000 [===================>..........] - ETA: 11:56 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 682/1000 [===================>..........] - ETA: 11:54 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 683/1000 [===================>..........] - ETA: 11:52 - loss: 0.2664 - regression_loss: 0.2478 - classification_loss: 0.0186
 684/1000 [===================>..........] - ETA: 11:49 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 685/1000 [===================>..........] - ETA: 11:47 - loss: 0.2665 - regression_loss: 0.2479 - classification_loss: 0.0186
 686/1000 [===================>..........] - ETA: 11:45 - loss: 0.2666 - regression_loss: 0.2480 - classification_loss: 0.0186
 687/1000 [===================>..........] - ETA: 11:43 - loss: 0.2663 - regression_loss: 0.2477 - classification_loss: 0.0186
 688/1000 [===================>..........] - ETA: 11:41 - loss: 0.2663 - regression_loss: 0.2477 - classification_loss: 0.0186
 689/1000 [===================>..........] - ETA: 11:38 - loss: 0.2663 - regression_loss: 0.2477 - classification_loss: 0.0186
 690/1000 [===================>..........] - ETA: 11:36 - loss: 0.2661 - regression_loss: 0.2475 - classification_loss: 0.0186
 691/1000 [===================>..........] - ETA: 11:34 - loss: 0.2663 - regression_loss: 0.2477 - classification_loss: 0.0186
 692/1000 [===================>..........] - ETA: 11:32 - loss: 0.2663 - regression_loss: 0.2477 - classification_loss: 0.0186
 693/1000 [===================>..........] - ETA: 11:29 - loss: 0.2660 - regression_loss: 0.2475 - classification_loss: 0.0186
 694/1000 [===================>..........] - ETA: 11:27 - loss: 0.2662 - regression_loss: 0.2476 - classification_loss: 0.0186
 695/1000 [===================>..........] - ETA: 11:25 - loss: 0.2661 - regression_loss: 0.2475 - classification_loss: 0.0186
 696/1000 [===================>..........] - ETA: 11:22 - loss: 0.2657 - regression_loss: 0.2472 - classification_loss: 0.0186
 697/1000 [===================>..........] - ETA: 11:20 - loss: 0.2657 - regression_loss: 0.2471 - classification_loss: 0.0186
 698/1000 [===================>..........] - ETA: 11:18 - loss: 0.2655 - regression_loss: 0.2469 - classification_loss: 0.0185
 699/1000 [===================>..........] - ETA: 11:16 - loss: 0.2656 - regression_loss: 0.2470 - classification_loss: 0.0186
 700/1000 [====================>.........] - ETA: 11:13 - loss: 0.2656 - regression_loss: 0.2471 - classification_loss: 0.0186
 701/1000 [====================>.........] - ETA: 11:11 - loss: 0.2658 - regression_loss: 0.2472 - classification_loss: 0.0186
 702/1000 [====================>.........] - ETA: 11:09 - loss: 0.2657 - regression_loss: 0.2472 - classification_loss: 0.0186
 703/1000 [====================>.........] - ETA: 11:07 - loss: 0.2658 - regression_loss: 0.2472 - classification_loss: 0.0186
 704/1000 [====================>.........] - ETA: 11:04 - loss: 0.2655 - regression_loss: 0.2470 - classification_loss: 0.0185
 705/1000 [====================>.........] - ETA: 11:02 - loss: 0.2657 - regression_loss: 0.2471 - classification_loss: 0.0185
 706/1000 [====================>.........] - ETA: 11:00 - loss: 0.2658 - regression_loss: 0.2472 - classification_loss: 0.0186
 707/1000 [====================>.........] - ETA: 10:58 - loss: 0.2657 - regression_loss: 0.2471 - classification_loss: 0.0186
 708/1000 [====================>.........] - ETA: 10:56 - loss: 0.2657 - regression_loss: 0.2471 - classification_loss: 0.0186
 709/1000 [====================>.........] - ETA: 10:53 - loss: 0.2655 - regression_loss: 0.2469 - classification_loss: 0.0185
 710/1000 [====================>.........] - ETA: 10:51 - loss: 0.2653 - regression_loss: 0.2467 - classification_loss: 0.0185
 711/1000 [====================>.........] - ETA: 10:49 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 712/1000 [====================>.........] - ETA: 10:47 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 713/1000 [====================>.........] - ETA: 10:44 - loss: 0.2653 - regression_loss: 0.2467 - classification_loss: 0.0185
 714/1000 [====================>.........] - ETA: 10:42 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 715/1000 [====================>.........] - ETA: 10:40 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 716/1000 [====================>.........] - ETA: 10:38 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 717/1000 [====================>.........] - ETA: 10:36 - loss: 0.2655 - regression_loss: 0.2470 - classification_loss: 0.0185
 718/1000 [====================>.........] - ETA: 10:33 - loss: 0.2655 - regression_loss: 0.2470 - classification_loss: 0.0185
 719/1000 [====================>.........] - ETA: 10:31 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 720/1000 [====================>.........] - ETA: 10:29 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 721/1000 [====================>.........] - ETA: 10:26 - loss: 0.2651 - regression_loss: 0.2465 - classification_loss: 0.0185
 722/1000 [====================>.........] - ETA: 10:24 - loss: 0.2652 - regression_loss: 0.2467 - classification_loss: 0.0185
 723/1000 [====================>.........] - ETA: 10:22 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 724/1000 [====================>.........] - ETA: 10:20 - loss: 0.2656 - regression_loss: 0.2471 - classification_loss: 0.0185
 725/1000 [====================>.........] - ETA: 10:18 - loss: 0.2657 - regression_loss: 0.2472 - classification_loss: 0.0185
 726/1000 [====================>.........] - ETA: 10:15 - loss: 0.2655 - regression_loss: 0.2470 - classification_loss: 0.0185
 727/1000 [====================>.........] - ETA: 10:13 - loss: 0.2655 - regression_loss: 0.2469 - classification_loss: 0.0185
 728/1000 [====================>.........] - ETA: 10:10 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 729/1000 [====================>.........] - ETA: 10:08 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 730/1000 [====================>.........] - ETA: 10:06 - loss: 0.2656 - regression_loss: 0.2471 - classification_loss: 0.0185
 731/1000 [====================>.........] - ETA: 10:04 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 732/1000 [====================>.........] - ETA: 10:02 - loss: 0.2655 - regression_loss: 0.2470 - classification_loss: 0.0185
 733/1000 [====================>.........] - ETA: 9:59 - loss: 0.2652 - regression_loss: 0.2467 - classification_loss: 0.0185 
 734/1000 [=====================>........] - ETA: 9:57 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 735/1000 [=====================>........] - ETA: 9:55 - loss: 0.2652 - regression_loss: 0.2467 - classification_loss: 0.0185
 736/1000 [=====================>........] - ETA: 9:53 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 737/1000 [=====================>........] - ETA: 9:50 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 738/1000 [=====================>........] - ETA: 9:48 - loss: 0.2651 - regression_loss: 0.2466 - classification_loss: 0.0185
 739/1000 [=====================>........] - ETA: 9:46 - loss: 0.2651 - regression_loss: 0.2466 - classification_loss: 0.0185
 740/1000 [=====================>........] - ETA: 9:44 - loss: 0.2651 - regression_loss: 0.2466 - classification_loss: 0.0185
 741/1000 [=====================>........] - ETA: 9:41 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 742/1000 [=====================>........] - ETA: 9:39 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 743/1000 [=====================>........] - ETA: 9:37 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 744/1000 [=====================>........] - ETA: 9:35 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 745/1000 [=====================>........] - ETA: 9:33 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 746/1000 [=====================>........] - ETA: 9:30 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0185
 747/1000 [=====================>........] - ETA: 9:28 - loss: 0.2652 - regression_loss: 0.2467 - classification_loss: 0.0185
 748/1000 [=====================>........] - ETA: 9:26 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0185
 749/1000 [=====================>........] - ETA: 9:23 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0185
 750/1000 [=====================>........] - ETA: 9:21 - loss: 0.2650 - regression_loss: 0.2465 - classification_loss: 0.0185
 751/1000 [=====================>........] - ETA: 9:19 - loss: 0.2650 - regression_loss: 0.2465 - classification_loss: 0.0185
 752/1000 [=====================>........] - ETA: 9:17 - loss: 0.2649 - regression_loss: 0.2464 - classification_loss: 0.0185
 753/1000 [=====================>........] - ETA: 9:14 - loss: 0.2649 - regression_loss: 0.2464 - classification_loss: 0.0185
 754/1000 [=====================>........] - ETA: 9:12 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0185
 755/1000 [=====================>........] - ETA: 9:10 - loss: 0.2647 - regression_loss: 0.2462 - classification_loss: 0.0185
 756/1000 [=====================>........] - ETA: 9:07 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 757/1000 [=====================>........] - ETA: 9:05 - loss: 0.2647 - regression_loss: 0.2463 - classification_loss: 0.0185
 758/1000 [=====================>........] - ETA: 9:03 - loss: 0.2649 - regression_loss: 0.2464 - classification_loss: 0.0185
 759/1000 [=====================>........] - ETA: 9:01 - loss: 0.2647 - regression_loss: 0.2463 - classification_loss: 0.0184
 760/1000 [=====================>........] - ETA: 8:59 - loss: 0.2647 - regression_loss: 0.2463 - classification_loss: 0.0184
 761/1000 [=====================>........] - ETA: 8:56 - loss: 0.2644 - regression_loss: 0.2460 - classification_loss: 0.0184
 762/1000 [=====================>........] - ETA: 8:54 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 763/1000 [=====================>........] - ETA: 8:52 - loss: 0.2646 - regression_loss: 0.2461 - classification_loss: 0.0184
 764/1000 [=====================>........] - ETA: 8:50 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 765/1000 [=====================>........] - ETA: 8:47 - loss: 0.2648 - regression_loss: 0.2463 - classification_loss: 0.0184
 766/1000 [=====================>........] - ETA: 8:45 - loss: 0.2647 - regression_loss: 0.2463 - classification_loss: 0.0184
 767/1000 [======================>.......] - ETA: 8:43 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 768/1000 [======================>.......] - ETA: 8:41 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0184
 769/1000 [======================>.......] - ETA: 8:38 - loss: 0.2644 - regression_loss: 0.2460 - classification_loss: 0.0184
 770/1000 [======================>.......] - ETA: 8:36 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 771/1000 [======================>.......] - ETA: 8:34 - loss: 0.2647 - regression_loss: 0.2462 - classification_loss: 0.0184
 772/1000 [======================>.......] - ETA: 8:32 - loss: 0.2649 - regression_loss: 0.2464 - classification_loss: 0.0184
 773/1000 [======================>.......] - ETA: 8:30 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 774/1000 [======================>.......] - ETA: 8:27 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 775/1000 [======================>.......] - ETA: 8:25 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 776/1000 [======================>.......] - ETA: 8:23 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 777/1000 [======================>.......] - ETA: 8:21 - loss: 0.2653 - regression_loss: 0.2468 - classification_loss: 0.0184
 778/1000 [======================>.......] - ETA: 8:18 - loss: 0.2650 - regression_loss: 0.2465 - classification_loss: 0.0184
 779/1000 [======================>.......] - ETA: 8:16 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 780/1000 [======================>.......] - ETA: 8:14 - loss: 0.2650 - regression_loss: 0.2465 - classification_loss: 0.0184
 781/1000 [======================>.......] - ETA: 8:11 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 782/1000 [======================>.......] - ETA: 8:09 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 783/1000 [======================>.......] - ETA: 8:07 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 784/1000 [======================>.......] - ETA: 8:05 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 785/1000 [======================>.......] - ETA: 8:02 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 786/1000 [======================>.......] - ETA: 8:00 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 787/1000 [======================>.......] - ETA: 7:58 - loss: 0.2652 - regression_loss: 0.2468 - classification_loss: 0.0184
 788/1000 [======================>.......] - ETA: 7:56 - loss: 0.2653 - regression_loss: 0.2469 - classification_loss: 0.0184
 789/1000 [======================>.......] - ETA: 7:54 - loss: 0.2652 - regression_loss: 0.2468 - classification_loss: 0.0184
 790/1000 [======================>.......] - ETA: 7:51 - loss: 0.2654 - regression_loss: 0.2469 - classification_loss: 0.0184
 791/1000 [======================>.......] - ETA: 7:49 - loss: 0.2654 - regression_loss: 0.2470 - classification_loss: 0.0184
 792/1000 [======================>.......] - ETA: 7:47 - loss: 0.2653 - regression_loss: 0.2469 - classification_loss: 0.0184
 793/1000 [======================>.......] - ETA: 7:45 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 794/1000 [======================>.......] - ETA: 7:42 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 795/1000 [======================>.......] - ETA: 7:40 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 796/1000 [======================>.......] - ETA: 7:38 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 797/1000 [======================>.......] - ETA: 7:36 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 798/1000 [======================>.......] - ETA: 7:33 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 799/1000 [======================>.......] - ETA: 7:31 - loss: 0.2652 - regression_loss: 0.2468 - classification_loss: 0.0184
 800/1000 [=======================>......] - ETA: 7:29 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 801/1000 [=======================>......] - ETA: 7:27 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 802/1000 [=======================>......] - ETA: 7:24 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 803/1000 [=======================>......] - ETA: 7:22 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 804/1000 [=======================>......] - ETA: 7:20 - loss: 0.2651 - regression_loss: 0.2467 - classification_loss: 0.0184
 805/1000 [=======================>......] - ETA: 7:18 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 806/1000 [=======================>......] - ETA: 7:15 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 807/1000 [=======================>......] - ETA: 7:13 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 808/1000 [=======================>......] - ETA: 7:11 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 809/1000 [=======================>......] - ETA: 7:09 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 810/1000 [=======================>......] - ETA: 7:06 - loss: 0.2649 - regression_loss: 0.2465 - classification_loss: 0.0184
 811/1000 [=======================>......] - ETA: 7:04 - loss: 0.2650 - regression_loss: 0.2466 - classification_loss: 0.0184
 812/1000 [=======================>......] - ETA: 7:02 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 813/1000 [=======================>......] - ETA: 7:00 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 814/1000 [=======================>......] - ETA: 6:57 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 815/1000 [=======================>......] - ETA: 6:55 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 816/1000 [=======================>......] - ETA: 6:53 - loss: 0.2646 - regression_loss: 0.2463 - classification_loss: 0.0184
 817/1000 [=======================>......] - ETA: 6:51 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 818/1000 [=======================>......] - ETA: 6:48 - loss: 0.2645 - regression_loss: 0.2462 - classification_loss: 0.0184
 819/1000 [=======================>......] - ETA: 6:46 - loss: 0.2646 - regression_loss: 0.2463 - classification_loss: 0.0184
 820/1000 [=======================>......] - ETA: 6:44 - loss: 0.2648 - regression_loss: 0.2464 - classification_loss: 0.0184
 821/1000 [=======================>......] - ETA: 6:42 - loss: 0.2647 - regression_loss: 0.2464 - classification_loss: 0.0184
 822/1000 [=======================>......] - ETA: 6:39 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 823/1000 [=======================>......] - ETA: 6:37 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0183
 824/1000 [=======================>......] - ETA: 6:35 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 825/1000 [=======================>......] - ETA: 6:33 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 826/1000 [=======================>......] - ETA: 6:30 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0183
 827/1000 [=======================>......] - ETA: 6:28 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 828/1000 [=======================>......] - ETA: 6:26 - loss: 0.2646 - regression_loss: 0.2462 - classification_loss: 0.0184
 829/1000 [=======================>......] - ETA: 6:24 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 830/1000 [=======================>......] - ETA: 6:21 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0183
 831/1000 [=======================>......] - ETA: 6:19 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 832/1000 [=======================>......] - ETA: 6:17 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 833/1000 [=======================>......] - ETA: 6:15 - loss: 0.2644 - regression_loss: 0.2460 - classification_loss: 0.0183
 834/1000 [========================>.....] - ETA: 6:12 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 835/1000 [========================>.....] - ETA: 6:10 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0183
 836/1000 [========================>.....] - ETA: 6:08 - loss: 0.2645 - regression_loss: 0.2462 - classification_loss: 0.0183
 837/1000 [========================>.....] - ETA: 6:06 - loss: 0.2644 - regression_loss: 0.2460 - classification_loss: 0.0183
 838/1000 [========================>.....] - ETA: 6:03 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 839/1000 [========================>.....] - ETA: 6:01 - loss: 0.2640 - regression_loss: 0.2457 - classification_loss: 0.0183
 840/1000 [========================>.....] - ETA: 5:59 - loss: 0.2641 - regression_loss: 0.2458 - classification_loss: 0.0183
 841/1000 [========================>.....] - ETA: 5:57 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 842/1000 [========================>.....] - ETA: 5:55 - loss: 0.2643 - regression_loss: 0.2459 - classification_loss: 0.0183
 843/1000 [========================>.....] - ETA: 5:52 - loss: 0.2644 - regression_loss: 0.2460 - classification_loss: 0.0183
 844/1000 [========================>.....] - ETA: 5:50 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 845/1000 [========================>.....] - ETA: 5:48 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 846/1000 [========================>.....] - ETA: 5:46 - loss: 0.2645 - regression_loss: 0.2461 - classification_loss: 0.0183
 847/1000 [========================>.....] - ETA: 5:43 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 848/1000 [========================>.....] - ETA: 5:41 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 849/1000 [========================>.....] - ETA: 5:39 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 850/1000 [========================>.....] - ETA: 5:37 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 851/1000 [========================>.....] - ETA: 5:34 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 852/1000 [========================>.....] - ETA: 5:32 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 853/1000 [========================>.....] - ETA: 5:30 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0183
 854/1000 [========================>.....] - ETA: 5:28 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0183
 855/1000 [========================>.....] - ETA: 5:25 - loss: 0.2640 - regression_loss: 0.2457 - classification_loss: 0.0183
 856/1000 [========================>.....] - ETA: 5:23 - loss: 0.2640 - regression_loss: 0.2457 - classification_loss: 0.0183
 857/1000 [========================>.....] - ETA: 5:21 - loss: 0.2638 - regression_loss: 0.2455 - classification_loss: 0.0183
 858/1000 [========================>.....] - ETA: 5:19 - loss: 0.2638 - regression_loss: 0.2455 - classification_loss: 0.0183
 859/1000 [========================>.....] - ETA: 5:16 - loss: 0.2639 - regression_loss: 0.2456 - classification_loss: 0.0183
 860/1000 [========================>.....] - ETA: 5:14 - loss: 0.2636 - regression_loss: 0.2454 - classification_loss: 0.0183
 861/1000 [========================>.....] - ETA: 5:12 - loss: 0.2637 - regression_loss: 0.2455 - classification_loss: 0.0183
 862/1000 [========================>.....] - ETA: 5:10 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 863/1000 [========================>.....] - ETA: 5:07 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 864/1000 [========================>.....] - ETA: 5:05 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0183
 865/1000 [========================>.....] - ETA: 5:03 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 866/1000 [========================>.....] - ETA: 5:01 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 867/1000 [=========================>....] - ETA: 4:58 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 868/1000 [=========================>....] - ETA: 4:56 - loss: 0.2633 - regression_loss: 0.2450 - classification_loss: 0.0182
 869/1000 [=========================>....] - ETA: 4:54 - loss: 0.2634 - regression_loss: 0.2451 - classification_loss: 0.0183
 870/1000 [=========================>....] - ETA: 4:52 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 871/1000 [=========================>....] - ETA: 4:49 - loss: 0.2634 - regression_loss: 0.2451 - classification_loss: 0.0183
 872/1000 [=========================>....] - ETA: 4:47 - loss: 0.2631 - regression_loss: 0.2448 - classification_loss: 0.0182
 873/1000 [=========================>....] - ETA: 4:45 - loss: 0.2631 - regression_loss: 0.2448 - classification_loss: 0.0182
 874/1000 [=========================>....] - ETA: 4:43 - loss: 0.2631 - regression_loss: 0.2449 - classification_loss: 0.0182
 875/1000 [=========================>....] - ETA: 4:40 - loss: 0.2632 - regression_loss: 0.2450 - classification_loss: 0.0183
 876/1000 [=========================>....] - ETA: 4:38 - loss: 0.2631 - regression_loss: 0.2448 - classification_loss: 0.0182
 877/1000 [=========================>....] - ETA: 4:36 - loss: 0.2632 - regression_loss: 0.2450 - classification_loss: 0.0183
 878/1000 [=========================>....] - ETA: 4:34 - loss: 0.2631 - regression_loss: 0.2448 - classification_loss: 0.0182
 879/1000 [=========================>....] - ETA: 4:31 - loss: 0.2631 - regression_loss: 0.2449 - classification_loss: 0.0182
 880/1000 [=========================>....] - ETA: 4:29 - loss: 0.2632 - regression_loss: 0.2450 - classification_loss: 0.0183
 881/1000 [=========================>....] - ETA: 4:27 - loss: 0.2632 - regression_loss: 0.2449 - classification_loss: 0.0183
 882/1000 [=========================>....] - ETA: 4:25 - loss: 0.2630 - regression_loss: 0.2448 - classification_loss: 0.0182
 883/1000 [=========================>....] - ETA: 4:22 - loss: 0.2631 - regression_loss: 0.2449 - classification_loss: 0.0182
 884/1000 [=========================>....] - ETA: 4:20 - loss: 0.2633 - regression_loss: 0.2451 - classification_loss: 0.0183
 885/1000 [=========================>....] - ETA: 4:18 - loss: 0.2633 - regression_loss: 0.2451 - classification_loss: 0.0183
 886/1000 [=========================>....] - ETA: 4:16 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 887/1000 [=========================>....] - ETA: 4:14 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0183
 888/1000 [=========================>....] - ETA: 4:11 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 889/1000 [=========================>....] - ETA: 4:09 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0183
 890/1000 [=========================>....] - ETA: 4:07 - loss: 0.2634 - regression_loss: 0.2452 - classification_loss: 0.0183
 891/1000 [=========================>....] - ETA: 4:04 - loss: 0.2634 - regression_loss: 0.2452 - classification_loss: 0.0183
 892/1000 [=========================>....] - ETA: 4:02 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0183
 893/1000 [=========================>....] - ETA: 4:00 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 894/1000 [=========================>....] - ETA: 3:58 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0183
 895/1000 [=========================>....] - ETA: 3:55 - loss: 0.2635 - regression_loss: 0.2453 - classification_loss: 0.0183
 896/1000 [=========================>....] - ETA: 3:53 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 897/1000 [=========================>....] - ETA: 3:51 - loss: 0.2635 - regression_loss: 0.2453 - classification_loss: 0.0183
 898/1000 [=========================>....] - ETA: 3:49 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0183
 899/1000 [=========================>....] - ETA: 3:47 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0183
 900/1000 [==========================>...] - ETA: 3:44 - loss: 0.2636 - regression_loss: 0.2454 - classification_loss: 0.0182
 901/1000 [==========================>...] - ETA: 3:42 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0182
 902/1000 [==========================>...] - ETA: 3:40 - loss: 0.2636 - regression_loss: 0.2454 - classification_loss: 0.0182
 903/1000 [==========================>...] - ETA: 3:37 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0182
 904/1000 [==========================>...] - ETA: 3:35 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0182
 905/1000 [==========================>...] - ETA: 3:33 - loss: 0.2637 - regression_loss: 0.2454 - classification_loss: 0.0182
 906/1000 [==========================>...] - ETA: 3:31 - loss: 0.2638 - regression_loss: 0.2455 - classification_loss: 0.0182
 907/1000 [==========================>...] - ETA: 3:28 - loss: 0.2636 - regression_loss: 0.2454 - classification_loss: 0.0182
 908/1000 [==========================>...] - ETA: 3:26 - loss: 0.2637 - regression_loss: 0.2455 - classification_loss: 0.0182
 909/1000 [==========================>...] - ETA: 3:24 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0182
 910/1000 [==========================>...] - ETA: 3:22 - loss: 0.2637 - regression_loss: 0.2455 - classification_loss: 0.0183
 911/1000 [==========================>...] - ETA: 3:20 - loss: 0.2635 - regression_loss: 0.2452 - classification_loss: 0.0182
 912/1000 [==========================>...] - ETA: 3:17 - loss: 0.2635 - regression_loss: 0.2453 - classification_loss: 0.0182
 913/1000 [==========================>...] - ETA: 3:15 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0182
 914/1000 [==========================>...] - ETA: 3:13 - loss: 0.2637 - regression_loss: 0.2455 - classification_loss: 0.0182
 915/1000 [==========================>...] - ETA: 3:11 - loss: 0.2636 - regression_loss: 0.2453 - classification_loss: 0.0182
 916/1000 [==========================>...] - ETA: 3:08 - loss: 0.2634 - regression_loss: 0.2452 - classification_loss: 0.0182
 917/1000 [==========================>...] - ETA: 3:06 - loss: 0.2635 - regression_loss: 0.2453 - classification_loss: 0.0182
 918/1000 [==========================>...] - ETA: 3:04 - loss: 0.2638 - regression_loss: 0.2455 - classification_loss: 0.0182
 919/1000 [==========================>...] - ETA: 3:02 - loss: 0.2640 - regression_loss: 0.2458 - classification_loss: 0.0182
 920/1000 [==========================>...] - ETA: 2:59 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0183
 921/1000 [==========================>...] - ETA: 2:57 - loss: 0.2642 - regression_loss: 0.2460 - classification_loss: 0.0183
 922/1000 [==========================>...] - ETA: 2:55 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 923/1000 [==========================>...] - ETA: 2:53 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 924/1000 [==========================>...] - ETA: 2:50 - loss: 0.2644 - regression_loss: 0.2461 - classification_loss: 0.0183
 925/1000 [==========================>...] - ETA: 2:48 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0182
 926/1000 [==========================>...] - ETA: 2:46 - loss: 0.2641 - regression_loss: 0.2458 - classification_loss: 0.0182
 927/1000 [==========================>...] - ETA: 2:44 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0182
 928/1000 [==========================>...] - ETA: 2:41 - loss: 0.2642 - regression_loss: 0.2460 - classification_loss: 0.0182
 929/1000 [==========================>...] - ETA: 2:39 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0182
 930/1000 [==========================>...] - ETA: 2:37 - loss: 0.2641 - regression_loss: 0.2458 - classification_loss: 0.0182
 931/1000 [==========================>...] - ETA: 2:35 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0182
 932/1000 [==========================>...] - ETA: 2:32 - loss: 0.2639 - regression_loss: 0.2457 - classification_loss: 0.0182
 933/1000 [==========================>...] - ETA: 2:30 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0182
 934/1000 [===========================>..] - ETA: 2:28 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0182
 935/1000 [===========================>..] - ETA: 2:26 - loss: 0.2643 - regression_loss: 0.2460 - classification_loss: 0.0182
 936/1000 [===========================>..] - ETA: 2:23 - loss: 0.2642 - regression_loss: 0.2459 - classification_loss: 0.0182
 937/1000 [===========================>..] - ETA: 2:21 - loss: 0.2640 - regression_loss: 0.2458 - classification_loss: 0.0182
 938/1000 [===========================>..] - ETA: 2:19 - loss: 0.2642 - regression_loss: 0.2460 - classification_loss: 0.0182
 939/1000 [===========================>..] - ETA: 2:17 - loss: 0.2639 - regression_loss: 0.2457 - classification_loss: 0.0182
 940/1000 [===========================>..] - ETA: 2:14 - loss: 0.2641 - regression_loss: 0.2458 - classification_loss: 0.0182
 941/1000 [===========================>..] - ETA: 2:12 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0182
 942/1000 [===========================>..] - ETA: 2:10 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0182
 943/1000 [===========================>..] - ETA: 2:08 - loss: 0.2639 - regression_loss: 0.2457 - classification_loss: 0.0182
 944/1000 [===========================>..] - ETA: 2:05 - loss: 0.2640 - regression_loss: 0.2458 - classification_loss: 0.0182
 945/1000 [===========================>..] - ETA: 2:03 - loss: 0.2641 - regression_loss: 0.2459 - classification_loss: 0.0182
 946/1000 [===========================>..] - ETA: 2:01 - loss: 0.2640 - regression_loss: 0.2458 - classification_loss: 0.0182
 947/1000 [===========================>..] - ETA: 1:59 - loss: 0.2639 - regression_loss: 0.2457 - classification_loss: 0.0182
 948/1000 [===========================>..] - ETA: 1:56 - loss: 0.2638 - regression_loss: 0.2456 - classification_loss: 0.0182
 949/1000 [===========================>..] - ETA: 1:54 - loss: 0.2640 - regression_loss: 0.2458 - classification_loss: 0.0182
 950/1000 [===========================>..] - ETA: 1:52 - loss: 0.2643 - regression_loss: 0.2461 - classification_loss: 0.0182
 951/1000 [===========================>..] - ETA: 1:50 - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 952/1000 [===========================>..] - ETA: 1:47 - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 953/1000 [===========================>..] - ETA: 1:45 - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 954/1000 [===========================>..] - ETA: 1:43 - loss: 0.2644 - regression_loss: 0.2462 - classification_loss: 0.0182
 955/1000 [===========================>..] - ETA: 1:41 - loss: 0.2642 - regression_loss: 0.2460 - classification_loss: 0.0182
 956/1000 [===========================>..] - ETA: 1:38 - loss: 0.2643 - regression_loss: 0.2461 - classification_loss: 0.0182
 957/1000 [===========================>..] - ETA: 1:36 - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 958/1000 [===========================>..] - ETA: 1:34 - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 959/1000 [===========================>..] - ETA: 1:32 - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 960/1000 [===========================>..] - ETA: 1:29 - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 961/1000 [===========================>..] - ETA: 1:27 - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 962/1000 [===========================>..] - ETA: 1:25 - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 963/1000 [===========================>..] - ETA: 1:23 - loss: 0.2649 - regression_loss: 0.2467 - classification_loss: 0.0182
 964/1000 [===========================>..] - ETA: 1:20 - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 965/1000 [===========================>..] - ETA: 1:18 - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 966/1000 [===========================>..] - ETA: 1:16 - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 967/1000 [============================>.] - ETA: 1:14 - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 968/1000 [============================>.] - ETA: 1:11 - loss: 0.2648 - regression_loss: 0.2467 - classification_loss: 0.0182
 969/1000 [============================>.] - ETA: 1:09 - loss: 0.2649 - regression_loss: 0.2467 - classification_loss: 0.0182
 970/1000 [============================>.] - ETA: 1:07 - loss: 0.2650 - regression_loss: 0.2468 - classification_loss: 0.0182
 971/1000 [============================>.] - ETA: 1:05 - loss: 0.2650 - regression_loss: 0.2468 - classification_loss: 0.0182
 972/1000 [============================>.] - ETA: 1:02 - loss: 0.2647 - regression_loss: 0.2466 - classification_loss: 0.0182
 973/1000 [============================>.] - ETA: 1:00 - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 974/1000 [============================>.] - ETA: 58s - loss: 0.2648 - regression_loss: 0.2467 - classification_loss: 0.0182 
 975/1000 [============================>.] - ETA: 56s - loss: 0.2649 - regression_loss: 0.2468 - classification_loss: 0.0182
 976/1000 [============================>.] - ETA: 53s - loss: 0.2650 - regression_loss: 0.2468 - classification_loss: 0.0182
 977/1000 [============================>.] - ETA: 51s - loss: 0.2649 - regression_loss: 0.2467 - classification_loss: 0.0182
 978/1000 [============================>.] - ETA: 49s - loss: 0.2649 - regression_loss: 0.2468 - classification_loss: 0.0182
 979/1000 [============================>.] - ETA: 47s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 980/1000 [============================>.] - ETA: 44s - loss: 0.2648 - regression_loss: 0.2467 - classification_loss: 0.0182
 981/1000 [============================>.] - ETA: 42s - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 982/1000 [============================>.] - ETA: 40s - loss: 0.2649 - regression_loss: 0.2467 - classification_loss: 0.0182
 983/1000 [============================>.] - ETA: 38s - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 984/1000 [============================>.] - ETA: 35s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 985/1000 [============================>.] - ETA: 33s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 986/1000 [============================>.] - ETA: 31s - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0182
 987/1000 [============================>.] - ETA: 29s - loss: 0.2646 - regression_loss: 0.2465 - classification_loss: 0.0182
 988/1000 [============================>.] - ETA: 26s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 989/1000 [============================>.] - ETA: 24s - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 990/1000 [============================>.] - ETA: 22s - loss: 0.2648 - regression_loss: 0.2466 - classification_loss: 0.0182
 991/1000 [============================>.] - ETA: 20s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182
 992/1000 [============================>.] - ETA: 17s - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0181
 993/1000 [============================>.] - ETA: 15s - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 994/1000 [============================>.] - ETA: 13s - loss: 0.2645 - regression_loss: 0.2464 - classification_loss: 0.0182
 995/1000 [============================>.] - ETA: 11s - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 996/1000 [============================>.] - ETA: 8s - loss: 0.2647 - regression_loss: 0.2465 - classification_loss: 0.0182 
 997/1000 [============================>.] - ETA: 6s - loss: 0.2646 - regression_loss: 0.2465 - classification_loss: 0.0182
 998/1000 [============================>.] - ETA: 4s - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
 999/1000 [============================>.] - ETA: 2s - loss: 0.2645 - regression_loss: 0.2463 - classification_loss: 0.0181
1000/1000 [==============================] - 2247s 2s/step - loss: 0.2646 - regression_loss: 0.2464 - classification_loss: 0.0182
Using TensorFlow backend.
/home/mpanoff/.conda/envs/tf-gpu-cuda8/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.
  warnings.warn('`epsilon` argument is deprecated and '

Epoch 00010: saving model to ./snapshots/resnet50_csv_10.h5
